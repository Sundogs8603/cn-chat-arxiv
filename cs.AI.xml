<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FMCW&#38647;&#36798;&#30340;&#22312;&#23478;&#20013;&#36827;&#34892;&#27493;&#38271;&#27979;&#37327;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#28857;&#20113;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#23545;&#36527;&#24178;&#30340;&#22810;&#26222;&#21202;&#36895;&#24230;&#20998;&#26512;&#26469;&#33719;&#21462;&#27493;&#38271;&#20449;&#24687;&#12290;&#36825;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20110;&#38647;&#36798;&#30340;&#27493;&#38271;&#27979;&#37327;&#26041;&#27861;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20855;&#26377;&#39044;&#27979;&#39118;&#38505;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.01868</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#20351;&#29992;FMCW&#38647;&#36798;&#36827;&#34892;&#27493;&#38271;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Step length measurement in the wild using FMCW radar. (arXiv:2401.01868v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FMCW&#38647;&#36798;&#30340;&#22312;&#23478;&#20013;&#36827;&#34892;&#27493;&#38271;&#27979;&#37327;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#28857;&#20113;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#23545;&#36527;&#24178;&#30340;&#22810;&#26222;&#21202;&#36895;&#24230;&#20998;&#26512;&#26469;&#33719;&#21462;&#27493;&#38271;&#20449;&#24687;&#12290;&#36825;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20110;&#38647;&#36798;&#30340;&#27493;&#38271;&#27979;&#37327;&#26041;&#27861;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20855;&#26377;&#39044;&#27979;&#39118;&#38505;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#21475;&#32769;&#40836;&#21270;&#65292;&#35768;&#22810;&#36741;&#21161;&#21644;&#30417;&#27979;&#25216;&#26415;&#27491;&#22312;&#24320;&#21457;&#20013;&#65292;&#20197;&#20351;&#32769;&#24180;&#20154;&#33021;&#22815;&#22312;&#23478;&#20013;&#20859;&#32769;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#23478;&#20013;&#20859;&#32769;&#65292;&#39044;&#27979;&#36300;&#20498;&#12289;&#20303;&#38498;&#31561;&#39118;&#38505;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#26089;&#26399;&#24178;&#39044;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#20851;&#20110;&#39118;&#38505;&#39044;&#27979;&#30340;&#29615;&#22659;&#30417;&#27979;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27493;&#24577;&#36895;&#24230;&#20998;&#26512;&#26041;&#38754;&#65292;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#20256;&#24863;&#22120;&#22914;&#38647;&#36798;&#26469;&#36827;&#34892;&#12290;&#23613;&#31649;&#24050;&#26377;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#38500;&#27493;&#24577;&#36895;&#24230;&#22806;&#65292;&#27493;&#38271;&#30340;&#30417;&#27979;&#23545;&#20110;&#39044;&#27979;&#39118;&#38505;&#21516;&#26679;&#37325;&#35201;&#65292;&#20294;&#22522;&#20110;&#38647;&#36798;&#30340;&#26041;&#27861;&#23578;&#26410;&#30740;&#31350;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27493;&#38271;&#27979;&#37327;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#38647;&#36798;&#30340;&#27493;&#38271;&#27979;&#37327;&#23454;&#39564;&#20165;&#38480;&#20110;&#20960;&#20301;&#20581;&#24247;&#21463;&#35797;&#32773;&#30340;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38647;&#36798;&#28857;&#20113;&#26816;&#27979;&#21644;&#36319;&#36394;&#12289;&#36890;&#36807;&#38647;&#36798;&#22810;&#26222;&#21202;&#36895;&#24230;&#23545;&#36527;&#24178;&#36827;&#34892;&#20998;&#26512;&#20197;&#33719;&#21462;&#27493;&#38271;&#30340;&#23478;&#24237;&#22522;&#20110;&#38647;&#36798;&#30340;&#27493;&#38271;&#27979;&#37327;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an aging population, numerous assistive and monitoring technologies are under development to enable older adults to age in place. To facilitate aging in place predicting risk factors such as falls, and hospitalization and providing early interventions are important. Much of the work on ambient monitoring for risk prediction has centered on gait speed analysis, utilizing privacy-preserving sensors like radar. Despite compelling evidence that monitoring step length, in addition to gait speed, is crucial for predicting risk, radar-based methods have not explored step length measurement in the home. Furthermore, laboratory experiments on step length measurement using radars are limited to proof of concept studies with few healthy subjects. To address this gap, a radar-based step length measurement system for the home is proposed based on detection and tracking using radar point cloud, followed by Doppler speed profiling of the torso to obtain step lengths in the home. The proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01841</link><description>&lt;p&gt;
&#25353;&#29031;&#20320;&#30340;&#23398;&#20064;&#34892;&#21160;&#65306;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#22788;&#29702;&#38750;&#31283;&#24577;&#29615;&#22659;&#26159;&#19968;&#20010;&#22522;&#26412;&#65288;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#35299;&#20915;&#30340;&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#22806;&#37096;&#29615;&#22659;&#26465;&#20214;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#31867;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NSMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NSMDP&#20915;&#31574;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#35774;&#24403;&#21069;&#26102;&#21051;&#26356;&#26032;&#30340;&#29615;&#22659;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65288;&#23613;&#31649;&#26410;&#26469;&#21160;&#24577;&#21487;&#33021;&#20250;&#25913;&#21464;&#65289;&#65307;&#20854;&#27425;&#65292;&#35268;&#21010;&#36807;&#31243;&#20027;&#35201;&#26159;&#24754;&#35266;&#30340;&#65292;&#21363;&#20195;&#29702;&#20154;&#20250;&#8220;&#23433;&#20840;&#34892;&#21160;&#8221;&#20197;&#32771;&#34385;&#29615;&#22659;&#30340;&#38750;&#31283;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#25928;&#30340;-&#26356;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#24456;&#23569;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24403;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#26102;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#26356;&#26032;&#30340;&#21160;&#24577;&#24182;&#36991;&#20813;&#24754;&#35266;&#65292;&#33267;&#23569;&#22312;&#20854;&#23545;&#21160;&#24577;&#26377;&#20449;&#24515;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
&lt;/p&gt;</description></item><item><title>NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01836</link><description>&lt;p&gt;
NODEC: &#29992;&#20110;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#26368;&#20248;&#25511;&#21046;&#30340;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01836
&lt;/p&gt;
&lt;p&gt;
NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#22312;&#21464;&#21270;&#35745;&#31639;&#26694;&#26550;&#19979;&#26368;&#23567;&#21270;&#20855;&#26377;&#24050;&#30693;&#21160;&#21147;&#23398;&#30340;&#26576;&#20123;&#25511;&#21046;&#30446;&#26631;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#39069;&#22806;&#36827;&#34892;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#20219;&#20309;&#19981;&#20934;&#30830;&#37117;&#20250;&#23548;&#33268;&#32467;&#26524;&#25511;&#21046;&#20989;&#25968;&#30340;&#27425;&#20248;&#24615;&#12290;&#21478;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861; - &#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#34701;&#20837;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#24191;&#27867;&#20132;&#20114;&#26469;&#36817;&#20284;&#20540;&#20989;&#25968;&#25110;&#31574;&#30053;&#26799;&#24230;&#65292;&#20294;&#23427;&#30340;&#25968;&#25454;&#25928;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NODEC&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20004;&#20010;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20114;&#20316;&#29992;&#65292;NODEC&#23398;&#20064;&#20102;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#25351;&#23548;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#24182;&#21457;&#30340;&#22836;&#33041;&#39118;&#26292;&#21644;&#20551;&#35774;&#28385;&#36275;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#39640;&#24230;&#30456;&#20851;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#24182;&#31616;&#21270;&#20102;&#26597;&#35810;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01835</link><description>&lt;p&gt;
&#24182;&#21457;&#22836;&#33041;&#39118;&#26292;&#21644;&#20551;&#35774;&#28385;&#36275;&#65306;&#19968;&#31181;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#36845;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Concurrent Brainstorming &amp; Hypothesis Satisfying: An Iterative Framework for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR). (arXiv:2401.01835v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#24182;&#21457;&#30340;&#22836;&#33041;&#39118;&#26292;&#21644;&#20551;&#35774;&#28385;&#36275;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#21152;&#24555;&#20102;&#39640;&#24230;&#30456;&#20851;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#24182;&#31616;&#21270;&#20102;&#26597;&#35810;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20840;&#38754;&#20449;&#24687;&#26816;&#32034;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23558;&#21521;&#37327;&#31354;&#38388;&#39537;&#21160;&#30340;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#19982;&#24182;&#21457;&#30340;&#22836;&#33041;&#39118;&#26292;&#30456;&#32467;&#21512;&#65292;&#21152;&#24555;&#39640;&#24230;&#30456;&#20851;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#20174;&#32780;&#31616;&#21270;&#28508;&#22312;&#26597;&#35810;&#30340;&#29983;&#25104;&#12290;&#36825;&#20026;&#25105;&#20204;&#30340;&#26032;&#39062;&#28151;&#21512;&#36807;&#31243;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#35813;&#36807;&#31243;&#20197;&#20551;&#35774;&#24418;&#25104;&#21644;&#28385;&#36275;&#20915;&#31574;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;&#25216;&#26415;&#30830;&#23450;&#20869;&#23481;&#30340;&#20805;&#20998;&#24615;&#12290;&#36825;&#20010;&#32479;&#19968;&#30340;&#20551;&#35774;&#28385;&#24847;&#38454;&#27573;&#26234;&#33021;&#22320;&#25552;&#28860;&#20449;&#24687;&#65292;&#30830;&#23450;&#29992;&#25143;&#30340;&#26597;&#35810;&#26159;&#21542;&#24471;&#21040;&#28385;&#24847;&#30340;&#35299;&#31572;&#12290;&#36798;&#21040;&#36825;&#20010;&#26631;&#20934;&#21518;&#65292;&#31995;&#32479;&#23558;&#20854;&#36755;&#20986;&#31934;&#28860;&#20026;&#31616;&#27905;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27010;&#24565;&#23494;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20887;&#38271;&#12290;&#36845;&#20195;&#30340;&#24037;&#20316;&#27969;&#22686;&#24378;&#20102;&#22788;&#29702;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the complexity of comprehensive information retrieval, this study introduces an innovative, iterative retrieval-augmented generation system. Our approach uniquely integrates a vector-space driven re-ranking mechanism with concurrent brainstorming to expedite the retrieval of highly relevant documents, thereby streamlining the generation of potential queries. This sets the stage for our novel hybrid process, which synergistically combines hypothesis formulation with satisfying decision-making strategy to determine content adequacy, leveraging a chain of thought-based prompting technique. This unified hypothesize-satisfied phase intelligently distills information to ascertain whether user queries have been satisfactorily addressed. Upon reaching this criterion, the system refines its output into a concise representation, maximizing conceptual density with minimal verbosity. The iterative nature of the workflow enhances process efficiency and accuracy. Crucially, the concurrenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#65292;&#36890;&#36807;&#36845;&#20195;&#25513;&#30721;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01830</link><description>&lt;p&gt;
&#36845;&#20195;&#25513;&#30721;&#22635;&#20805;&#65306;&#19968;&#31181;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30340;&#26377;&#25928;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#65292;&#36890;&#36807;&#36845;&#20195;&#25513;&#30721;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23427;&#30340;&#24212;&#29992;&#36828;&#19981;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#36845;&#20195;&#25513;&#30721;&#65292;&#24182;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#37117;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20027;&#39064;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#27010;&#24565;&#21040;&#19981;&#21516;&#23618;&#32423;&#21644;&#20462;&#21098;&#21069;&#30340;&#31070;&#32463;&#20803;&#23454;&#29616;&#23545;&#24050;&#21024;&#38500;&#27010;&#24565;&#30340;&#37325;&#26032;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#22810;&#20041;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#22312;&#25913;&#21892;&#27169;&#22411;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#37325;&#26032;&#23398;&#20064;&#24050;&#21024;&#38500;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Relearn Removed Concepts. (arXiv:2401.01814v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#27010;&#24565;&#21040;&#19981;&#21516;&#23618;&#32423;&#21644;&#20462;&#21098;&#21069;&#30340;&#31070;&#32463;&#20803;&#23454;&#29616;&#23545;&#24050;&#21024;&#38500;&#27010;&#24565;&#30340;&#37325;&#26032;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#22810;&#20041;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#22312;&#25913;&#21892;&#27169;&#22411;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#20803;&#20462;&#21098;&#26469;&#25913;&#36827;&#27169;&#22411;&#32534;&#36753;&#26377;&#26395;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#29702;&#24819;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#37325;&#26032;&#23398;&#20064;&#20462;&#21098;&#25481;&#30340;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#36319;&#36394;&#20462;&#21098;&#31070;&#32463;&#20803;&#20013;&#30340;&#27010;&#24565;&#26174;&#33879;&#24615;&#21644;&#30456;&#20284;&#24615;&#65292;&#22312;&#27169;&#22411;&#20013;&#35780;&#20272;&#20102;&#27010;&#24565;&#30340;&#37325;&#26032;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#39640;&#32423;&#27010;&#24565;&#37325;&#26032;&#20998;&#37197;&#32473;&#36739;&#26089;&#30340;&#23618;&#65292;&#24182;&#23558;&#20462;&#21098;&#25481;&#30340;&#27010;&#24565;&#37325;&#26032;&#20998;&#37197;&#32473;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#30340;&#31070;&#32463;&#20803;&#26469;&#36805;&#36895;&#24674;&#22797;&#20462;&#21098;&#21518;&#30340;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#22810;&#20041;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#31070;&#32463;&#20803;&#20013;&#34701;&#21512;&#26087;&#30340;&#21644;&#26032;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#31070;&#32463;&#20803;&#20462;&#21098;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#27704;&#20037;&#21024;&#38500;&#27010;&#24565;&#20197;&#25913;&#21892;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#25361;&#25112;&#12290;&#30417;&#25511;&#27010;&#24565;&#37325;&#26032;&#20986;&#29616;&#24182;&#24320;&#21457;&#20943;&#23569;&#37325;&#26032;&#23398;&#20064;&#19981;&#23433;&#20840;&#27010;&#24565;&#30340;&#25216;&#26415;&#23558;&#26159;&#26356;&#22909;&#30340;&#27169;&#22411;&#23433;&#20840;&#26041;&#21521;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \textit{safety}. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.01801</link><description>&lt;p&gt;
&#19968;&#20010;&#37327;&#23376;&#21551;&#21457;&#30340;&#29992;&#20110;&#20960;&#20309;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#29289;&#29702;&#31995;&#32479;&#26500;&#24819;&#20026;3D&#22810;&#20307;&#28857;&#20113;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#22914;SE(3)/E(3)&#31561;&#25928;GNN&#65292;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#39640;&#25928;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20351;&#23427;&#20204;&#33021;&#22815;&#29087;&#32451;&#22320;&#23545;&#20998;&#23376;&#21644;&#26230;&#20307;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20960;&#20309;GNN&#21482;&#25552;&#20379;&#20102;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#23553;&#35013;&#22312;&#20004;&#20307;&#28040;&#24687;&#20256;&#36882;&#20013;&#65292;&#22240;&#27492;&#22312;&#25429;&#25417;&#36825;&#20123;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#26377;&#25152;&#27424;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#35745;&#31639;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#39640;&#38454;&#24352;&#37327;&#26469;&#22788;&#29702;&#22810;&#20307;&#31995;&#32479;&#30340;&#24352;&#37327;&#32593;&#32476;&#34987;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#24352;&#37327;&#21270;&#32593;&#32476;&#25972;&#21512;&#21040;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#20013;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#21644;&#23545;&#31216;&#24615;&#20445;&#25345;&#65288;&#22914;&#32622;&#25442;&#21644;&#26059;&#36716;&#65289;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31561;&#21464;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#25193;&#25955;&#25945;&#24072;&#27169;&#22411;&#21644;&#25552;&#21462;&#33258;&#19968;&#33268;&#24615;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#22768;&#38899;&#29983;&#25104;&#21644;&#39640;&#36895;&#30340;&#37319;&#26679;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#22768;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01792</link><description>&lt;p&gt;
CoMoSVC: &#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
CoMoSVC: Consistency Model-based Singing Voice Conversion. (arXiv:2401.01792v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#25193;&#25955;&#25945;&#24072;&#27169;&#22411;&#21644;&#25552;&#21462;&#33258;&#19968;&#33268;&#24615;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#22768;&#38899;&#29983;&#25104;&#21644;&#39640;&#36895;&#30340;&#37319;&#26679;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#22768;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#21809;&#27468;&#22768;&#38899;&#36716;&#25442;&#65288;SVC&#65289;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20135;&#29983;&#20102;&#19982;&#30446;&#26631;&#38899;&#33394;&#30456;&#20284;&#24230;&#39640;&#30340;&#33258;&#28982;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#37319;&#26679;&#36807;&#31243;&#23548;&#33268;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#22240;&#27492;&#21152;&#36895;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;CoMoSVC&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#39640;&#36895;&#37319;&#26679;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#38024;&#23545;SVC&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#29305;&#24615;&#36827;&#19968;&#27493;&#25552;&#21462;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#19968;&#27493;&#37319;&#26679;&#12290;&#22312;&#21333;&#20010;NVIDIA GTX4090 GPU&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;CoMoSVC&#30340;&#25512;&#29702;&#36895;&#24230;&#27604;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#22522;&#20110;&#25193;&#25955;&#30340;SVC&#31995;&#32479;&#35201;&#24555;&#24471;&#22810;&#65292;&#20294;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#19978;&#20173;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#36716;&#25442;&#24615;&#33021;&#12290;&#38899;&#39057;&#26679;&#26412;&#21644;&#20195;&#30721;&#21487;&#22312;&#32593;&#22336;https://comosvc.github.io/&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diffusion-based Singing Voice Conversion (SVC) methods have achieved remarkable performances, producing natural audios with high similarity to the target timbre. However, the iterative sampling process results in slow inference speed, and acceleration thus becomes crucial. In this paper, we propose CoMoSVC, a consistency model-based SVC method, which aims to achieve both high-quality generation and high-speed sampling. A diffusion-based teacher model is first specially designed for SVC, and a student model is further distilled under self-consistency properties to achieve one-step sampling. Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a significantly faster inference speed than the state-of-the-art (SOTA) diffusion-based SVC system, it still achieves comparable or superior conversion performance based on both subjective and objective metrics. Audio samples and codes are available at https://comosvc.github.io/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;LSTM&#32593;&#32476;&#65289;&#35780;&#20272;&#20102;&#20998;&#25968;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;Hurst&#21442;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#21644;&#20998;&#25968;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20013;&#65292;LSTM&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#65292;&#20294;&#22312;&#32447;&#24615;&#20998;&#25968;&#31283;&#23450;&#36816;&#21160;&#36807;&#31243;&#20013;&#20934;&#30830;&#24615;&#21463;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01789</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32447;&#24615;&#20998;&#25968;&#36807;&#31243;&#30340;Hurst&#21442;&#25968;&#21450;&#20854;&#21487;&#38752;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep learning the Hurst parameter of linear fractional processes and assessing its reliability. (arXiv:2401.01789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;LSTM&#32593;&#32476;&#65289;&#35780;&#20272;&#20102;&#20998;&#25968;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;Hurst&#21442;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#21644;&#20998;&#25968;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20013;&#65292;LSTM&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#65292;&#20294;&#22312;&#32447;&#24615;&#20998;&#25968;&#31283;&#23450;&#36816;&#21160;&#36807;&#31243;&#20013;&#20934;&#30830;&#24615;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;&#29305;&#21035;&#26159;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65289;&#22312;&#20272;&#35745;&#20998;&#25968;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;Hurst&#21442;&#25968;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#38598;&#20013;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#36807;&#31243;&#19978;&#65306;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;fBm&#65289;&#65292;&#20998;&#25968;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#65288;fOU&#65289;&#36807;&#31243;&#21644;&#32447;&#24615;&#20998;&#25968;&#31283;&#23450;&#36816;&#21160;&#65288;lfsm&#65289;&#12290;&#30740;&#31350;&#21253;&#25324;&#23545;fBm&#21644;fOU&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#29983;&#25104;&#65292;&#20197;&#20415;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#35757;&#32451;LSTM&#32593;&#32476;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;LSTM&#32593;&#32476;&#22312;Hurst&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#65292;&#30456;&#23545;&#35823;&#24046;&#30340;&#20998;&#20301;&#25968;&#31561;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;fBm&#21644;fOU&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;LSTM&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;lfsm&#36807;&#31243;&#19978;&#30340;&#20934;&#30830;&#24615;&#26377;&#38480;&#12290;&#30740;&#31350;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#35757;&#32451;&#38271;&#24230;&#21644;&#35780;&#20272;&#24207;&#21015;&#38271;&#24230;&#23545;LSTM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This research explores the reliability of deep learning, specifically Long Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in fractional stochastic processes. The study focuses on three types of processes: fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process, and linear fractional stable motions (lfsm). The work involves a fast generation of extensive datasets for fBm and fOU to train the LSTM network on a large volume of data in a feasible time. The study analyses the accuracy of the LSTM network's Hurst parameter estimation regarding various performance measures like RMSE, MAE, MRE, and quantiles of the absolute and relative errors. It finds that LSTM outperforms the traditional statistical methods in the case of fBm and fOU processes; however, it has limited accuracy on lfsm processes. The research also delves into the implications of training length and valuation sequence length on the LSTM's performance. The methodology is applied by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#21644;&#36755;&#20837;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#39640;&#25104;&#26412;&#30417;&#27979;&#12289;&#20302;&#25104;&#26412;&#29289;&#32852;&#32593;&#21644;&#28151;&#21512;&#39044;&#27979;&#36825;&#19977;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review. (arXiv:2401.01788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#21644;&#36755;&#20837;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#39640;&#25104;&#26412;&#30417;&#27979;&#12289;&#20302;&#25104;&#26412;&#29289;&#32852;&#32593;&#21644;&#28151;&#21512;&#39044;&#27979;&#36825;&#19977;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#31354;&#27668;&#27745;&#26579;&#27599;&#24180;&#33268;&#20351;&#19971;&#30334;&#19975;&#20154;&#27515;&#20129;&#12290;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#26159;&#24433;&#21709;&#20302;&#25910;&#20837;&#12289;&#20013;&#25910;&#20837;&#21644;&#39640;&#25910;&#20837;&#22269;&#23478;&#30340;&#20027;&#35201;&#29615;&#22659;&#20581;&#24247;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#30740;&#31350;&#30028;&#24050;&#32463;&#25506;&#32034;&#20102;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#31995;&#32479;&#22320;&#32508;&#36848;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#22312;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#20351;&#29992;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#21644;&#36755;&#20837;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#26412;&#25991;&#20026;&#27492;&#32508;&#36848;&#21046;&#23450;&#20102;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#21021;&#22987;PRISMA&#38454;&#27573;&#20849;&#25910;&#38598;&#20102;1086&#31687;&#25991;&#29486;&#12290;&#36890;&#36807;&#31579;&#36873;&#21644;&#30830;&#23450;&#36164;&#26684;&#30340;&#38454;&#27573;&#65292;&#26368;&#32456;&#36873;&#25321;&#20102;37&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#22522;&#20110;&#25104;&#26412;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#39640;&#25104;&#26412;&#30417;&#27979;&#12289;&#20302;&#25104;&#26412;&#29289;&#32852;&#32593;&#21644;&#28151;&#21512;&#39044;&#27979;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#19977;&#31181;&#39044;&#27979;&#26041;&#27861;&#65306;&#26102;&#38388;&#24207;&#21015;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the World Health Organization (WHO), air pollution kills seven million people every year. Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries. In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction. The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used. Two research questions were formulated for this review. 1086 publications were collected in the initial PRISMA stage. After the screening and eligibility phases, 37 papers were selected for inclusion. A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction. Three methods of prediction were identified: time series, feature-based and spatio-t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Net&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#20132;&#26367;&#21453;&#21521;&#20256;&#25773;&#26426;&#21046;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23548;&#25968;&#20449;&#24687;&#21160;&#24577;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01772</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#35745;&#31639;&#33539;&#24335;&#65306;&#21487;&#23398;&#20064;&#31070;&#32463;&#20803;&#21644;&#21487;&#36866;&#24212;&#32467;&#26500;&#30340;X-Net
&lt;/p&gt;
&lt;p&gt;
A Novel Paradigm for Neural Computation: X-Net with Learnable Neurons and Adaptable Structure. (arXiv:2401.01772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Net&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#20132;&#26367;&#21453;&#21521;&#20256;&#25773;&#26426;&#21046;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23548;&#25968;&#20449;&#24687;&#21160;&#24577;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#24050;&#32463;&#28183;&#36879;&#21040;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#65292;&#20174;&#29983;&#29289;&#20449;&#24687;&#23398;&#21040;&#37329;&#34701;&#20998;&#26512;&#65292;&#22312;&#24403;&#20195;&#31185;&#23398;&#30740;&#31350;&#20013;&#24050;&#32463;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32593;&#32476;&#32467;&#26500;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#22266;&#26377;&#38480;&#21046;&#23548;&#33268;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;X-Net&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#35774;&#35745;&#30340;&#20132;&#26367;&#21453;&#21521;&#20256;&#25773;&#26426;&#21046;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23548;&#25968;&#20449;&#24687;&#21160;&#24577;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors. However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions. 1, The type of activation function is single and relatively fixed, which leads to poor "unit representation ability" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient. To address the aforementioned issues, this study proposes a novel neural network called X-Net. By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;FastPitch&#65292;&#19968;&#31181;&#33021;&#22815;&#22686;&#37327;&#29983;&#25104;&#39640;&#36136;&#37327;Mel&#22359;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#25913;&#36827;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#21450;&#22266;&#23450;&#22823;&#23567;&#30340;&#36807;&#21435;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#25512;&#29702;&#65292;&#23427;&#22312;&#23454;&#26102;&#35821;&#38899;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#30701;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.01755</link><description>&lt;p&gt;
&#22686;&#37327;&#24335;FastPitch&#65306;&#22522;&#20110;&#20998;&#22359;&#30340;&#39640;&#36136;&#37327;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Incremental FastPitch: Chunk-based High Quality Text to Speech. (arXiv:2401.01755v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;FastPitch&#65292;&#19968;&#31181;&#33021;&#22815;&#22686;&#37327;&#29983;&#25104;&#39640;&#36136;&#37327;Mel&#22359;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#25913;&#36827;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#21450;&#22266;&#23450;&#22823;&#23567;&#30340;&#36807;&#21435;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#25512;&#29702;&#65292;&#23427;&#22312;&#23454;&#26102;&#35821;&#38899;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#30701;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#22312;&#23454;&#26102;&#35821;&#38899;&#21512;&#25104;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#22810;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#24555;&#30340;&#21512;&#25104;&#36807;&#31243;&#12290;&#23613;&#31649;&#24182;&#34892;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#37117;&#26377;&#20248;&#21183;&#65292;&#20294;&#30001;&#20110;&#20854;&#23436;&#20840;&#24182;&#34892;&#30340;&#26550;&#26500;&#65288;&#22914;transformer&#65289;&#65292;&#23427;&#20204;&#22312;&#22686;&#37327;&#21512;&#25104;&#26041;&#38754;&#33258;&#28982;&#26080;&#27861;&#36866;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;FastPitch&#65292;&#23427;&#26159;&#19968;&#31181;&#25913;&#36827;&#26550;&#26500;&#30340;FastPitch&#21464;&#20307;&#65292;&#33021;&#22815;&#36890;&#36807;&#25913;&#36827;&#30340;&#20998;&#22359;FFT&#22359;&#36827;&#34892;&#22686;&#37327;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;Mel&#22359;&#65292;&#36890;&#36807;&#21463;&#38480;&#25509;&#21463;&#22495;&#30340;&#20998;&#22359;&#27880;&#24847;&#21147;&#25513;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#22266;&#23450;&#22823;&#23567;&#30340;&#36807;&#21435;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#33021;&#22815;&#20135;&#29983;&#19982;&#24182;&#34892;FastPitch&#30456;&#24403;&#30340;&#35821;&#38899;&#36136;&#37327;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#33879;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35821;&#38899;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#20302;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#21033;&#29992;AI&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#20998;&#20139;&#24179;&#21488;&#20013;&#20934;&#30830;&#26816;&#27979;&#21644;&#26631;&#35760;&#26426;&#23494;&#20449;&#24687;&#65292;&#24182;&#33258;&#21160;&#20462;&#22797;&#36825;&#20123;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20004;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#22522;&#20934;AI&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26426;&#21046;&#26469;&#20462;&#22797;&#21457;&#29616;&#30340;&#26426;&#23494;&#20449;&#24687;&#38382;&#39064;&#65292;&#20026;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#25171;&#24320;&#20102;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01754</link><description>&lt;p&gt;
&#20351;&#29992;AI/ML&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#20998;&#20139;&#24179;&#21488;&#20013;&#26597;&#25214;&#21644;&#20462;&#22797;&#20225;&#19994;&#26426;&#23494;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Using AI/ML to Find and Remediate Enterprise Secrets in Code &amp; Document Sharing Platforms. (arXiv:2401.01754v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01754
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#21033;&#29992;AI&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#20998;&#20139;&#24179;&#21488;&#20013;&#20934;&#30830;&#26816;&#27979;&#21644;&#26631;&#35760;&#26426;&#23494;&#20449;&#24687;&#65292;&#24182;&#33258;&#21160;&#20462;&#22797;&#36825;&#20123;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20004;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#22522;&#20934;AI&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26426;&#21046;&#26469;&#20462;&#22797;&#21457;&#29616;&#30340;&#26426;&#23494;&#20449;&#24687;&#38382;&#39064;&#65292;&#20026;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#25171;&#24320;&#20102;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21521;&#36719;&#20214;&#24320;&#21457;&#31038;&#21306;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;1&#65289;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#26631;&#35760;&#20195;&#30721;&#21644;&#24120;&#29992;&#25991;&#26723;&#20998;&#20139;&#24179;&#21488;&#20013;&#30340;&#26426;&#23494;&#20449;&#24687;&#65292;&#22914;Confluence&#65292;&#24182;&#19988;2&#65289;&#33258;&#21160;&#20462;&#22797;&#36825;&#20123;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#24314;&#35758;&#20351;&#29992;&#23494;&#30721;&#20445;&#38505;&#24211;&#21151;&#33021;&#65289;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#21551;&#21457;&#24335;&#21644;&#27491;&#21017;&#34920;&#36798;&#24335;&#65292;&#20294;&#22122;&#22768;&#24456;&#22823;&#65292;&#20174;&#32780;&#22686;&#21152;&#24320;&#21457;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#19979;&#19968;&#27493;&#26159;&#20462;&#25913;&#20195;&#30721;&#26412;&#36523;&#65292;&#20197;&#33258;&#21160;&#20462;&#22797;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#22522;&#20934;AI&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26426;&#21046;&#26469;&#20462;&#22797;&#22312;&#20195;&#30721;&#20013;&#21457;&#29616;&#30340;&#26426;&#23494;&#20449;&#24687;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new challenge to the software development community: 1) leveraging AI to accurately detect and flag up secrets in code and on popular document sharing platforms that frequently used by developers, such as Confluence and 2) automatically remediating the detections (e.g. by suggesting password vault functionality). This is a challenging, and mostly unaddressed task. Existing methods leverage heuristics and regular expressions, that can be very noisy, and therefore increase toil on developers. The next step modifying code itself - to automatically remediate a detection, is a complex task. We introduce two baseline AI models that have good detection performance and propose an automatic mechanism for remediating secrets found in code, opening up the study of this task to the wider community.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25163;&#65292;&#33021;&#22815;&#21152;&#36895;&#23558;&#26412;&#22320;&#24212;&#29992;&#31243;&#24207;&#36801;&#31227;&#21040;&#20113;&#31471;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#36801;&#31227;&#31574;&#30053;&#21644;&#26550;&#26500;&#22270;&#30340;&#26041;&#24335;&#24110;&#21161;&#29992;&#25143;&#36873;&#25321;&#21512;&#36866;&#30340;&#36801;&#31227;&#37197;&#32622;&#25991;&#20214;&#65292;&#36991;&#20813;&#25163;&#21160;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01753</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21152;&#36895;&#20113;&#36801;&#31227;&#30340;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Generative AI Assistant to Accelerate Cloud Migration. (arXiv:2401.01753v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01753
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25163;&#65292;&#33021;&#22815;&#21152;&#36895;&#23558;&#26412;&#22320;&#24212;&#29992;&#31243;&#24207;&#36801;&#31227;&#21040;&#20113;&#31471;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#36801;&#31227;&#31574;&#30053;&#21644;&#26550;&#26500;&#22270;&#30340;&#26041;&#24335;&#24110;&#21161;&#29992;&#25143;&#36873;&#25321;&#21512;&#36866;&#30340;&#36801;&#31227;&#37197;&#32622;&#25991;&#20214;&#65292;&#36991;&#20813;&#25163;&#21160;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#65292;&#20197;&#21152;&#24555;&#23558;&#26412;&#22320;&#24212;&#29992;&#31243;&#24207;&#36801;&#31227;&#21040;&#20113;&#31471;&#12290;&#20113;&#36801;&#31227;LLM&#25509;&#21463;&#29992;&#25143;&#36755;&#20837;&#65292;&#25351;&#23450;&#36801;&#31227;&#21442;&#25968;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#26550;&#26500;&#22270;&#30340;&#36801;&#31227;&#31574;&#30053;&#12290;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#36801;&#31227;LLM&#21487;&#20197;&#24110;&#21161;&#27809;&#26377;&#32463;&#39564;&#30340;&#29992;&#25143;&#25214;&#21040;&#21512;&#36866;&#30340;&#20113;&#36801;&#31227;&#37197;&#32622;&#25991;&#20214;&#65292;&#21516;&#26102;&#36991;&#20813;&#25163;&#21160;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a tool that leverages generative AI to accelerate the migration of on-premises applications to the cloud. The Cloud Migration LLM accepts input from the user specifying the parameters of their migration, and outputs a migration strategy with an architecture diagram. A user study suggests that the migration LLM can assist inexperienced users in finding the right cloud migration profile, while avoiding complexities of a manual approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#26412;&#26694;&#26550;&#8212;&#8212;&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;&#65288;TENet&#65289;&#65292;&#23427;&#19981;&#20165;&#33021;&#23436;&#25104;&#20219;&#21153;&#65292;&#36824;&#33021;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#26679;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#24378;&#35843;&#25972;&#20010;AI&#39046;&#22495;&#37117;&#24212;&#35813;&#37325;&#35270;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01732</link><description>&lt;p&gt;
&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Task and Explanation Network. (arXiv:2401.01732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#26412;&#26694;&#26550;&#8212;&#8212;&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;&#65288;TENet&#65289;&#65292;&#23427;&#19981;&#20165;&#33021;&#23436;&#25104;&#20219;&#21153;&#65292;&#36824;&#33021;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#26679;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#24378;&#35843;&#25972;&#20010;AI&#39046;&#22495;&#37117;&#24212;&#35813;&#37325;&#35270;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#27492;&#35770;&#25991;&#20013;&#25552;&#20986;&#65292;AI&#19981;&#20165;&#35201;&#23436;&#25104;&#20219;&#21153;&#65292;&#36824;&#35201;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#26679;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#26694;&#26550;&#8212;&#8212;&#20219;&#21153;&#21644;&#35299;&#37322;&#32593;&#32476;&#65288;TENet&#65289;&#65292;&#23427;&#23436;&#20840;&#25972;&#21512;&#20102;&#20219;&#21153;&#23436;&#25104;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#20010;AI&#39046;&#22495;&#37117;&#24212;&#35813;&#24378;&#35843;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability in deep networks has gained increased importance in recent years. We argue herein that an AI must be tasked not just with a task but also with an explanation of why said task was accomplished as such. We present a basic framework -- Task and Explanation Network (TENet) -- which fully integrates task completion and its explanation. We believe that the field of AI as a whole should insist -- quite emphatically -- on explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01728</link><description>&lt;p&gt;
Ravnest: &#24322;&#26500;&#35774;&#22791;&#19978;&#30340;&#20998;&#25955;&#24335;&#24322;&#27493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24322;&#24120;&#30340;&#27867;&#21270;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#36235;&#21183;&#39044;&#35745;&#23558;&#32487;&#32493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22686;&#22823;&#20351;&#24471;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#22312;&#36825;&#31181;&#35268;&#27169;&#19978;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#36830;&#25509;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#26469;&#23454;&#29616;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;&#36825;&#20123;&#38598;&#32676;&#21442;&#19982;$\textit{&#38646;&#27668;&#27873;&#24322;&#27493;&#27169;&#22411;&#24182;&#34892;}$&#35757;&#32451;&#65292;&#21033;&#29992;$\textit{&#24182;&#34892;&#22810;&#29615;&#20840;&#23616;&#27719;&#32858;}$&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#36890;&#20449;&#21644;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#33258;&#21160;&#21270;&#30340;&#26426;&#21046;&#35774;&#35745;&#65292;&#29992;&#20110;&#22312;Feed&#20013;&#38598;&#25104;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#12290;&#36825;&#31181;&#35774;&#35745;&#35299;&#20915;&#20102;&#24191;&#21578;&#25293;&#21334;&#19981;&#32771;&#34385;&#22806;&#37096;&#24615;&#21644;&#24191;&#21578;&#20998;&#37197;&#26080;&#27861;&#20445;&#25345;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01656</link><description>&lt;p&gt;
&#22312;Feed&#20013;&#38598;&#25104;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#30340;&#28145;&#24230;&#33258;&#21160;&#21270;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Automated Mechanism Design for Integrating Ad Auction and Allocation in Feed. (arXiv:2401.01656v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#33258;&#21160;&#21270;&#30340;&#26426;&#21046;&#35774;&#35745;&#65292;&#29992;&#20110;&#22312;Feed&#20013;&#38598;&#25104;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#12290;&#36825;&#31181;&#35774;&#35745;&#35299;&#20915;&#20102;&#24191;&#21578;&#25293;&#21334;&#19981;&#32771;&#34385;&#22806;&#37096;&#24615;&#21644;&#24191;&#21578;&#20998;&#37197;&#26080;&#27861;&#20445;&#25345;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21709;&#24212;&#27599;&#20010;&#29992;&#25143;&#30340;&#39029;&#38754;&#35831;&#27714;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#21576;&#29616;&#19968;&#20010;&#26377;&#24207;&#21015;&#34920;&#65292;&#20854;&#20013;&#21253;&#21547;&#20960;&#20010;&#26377;&#26426;&#29289;&#21697;&#21644;&#19968;&#20010;&#24191;&#21578;&#12290;&#36825;&#20010;&#21015;&#34920;&#26159;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#36807;&#31243;&#30340;&#32467;&#26524;&#65292;&#30452;&#25509;&#24433;&#21709;&#24179;&#21488;&#30340;&#24191;&#21578;&#25910;&#20837;&#21644;&#24635;&#20132;&#26131;&#39069;&#12290;&#24191;&#21578;&#25293;&#21334;&#30830;&#23450;&#26174;&#31034;&#30340;&#24191;&#21578;&#21644;&#30456;&#24212;&#30340;&#25903;&#20184;&#65292;&#32780;&#24191;&#21578;&#20998;&#37197;&#20915;&#23450;&#24191;&#21578;&#21644;&#26377;&#26426;&#29289;&#21697;&#30340;&#26174;&#31034;&#20301;&#32622;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23558;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#38754;&#20020;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#24191;&#21578;&#25293;&#21334;&#27809;&#26377;&#32771;&#34385;&#22806;&#37096;&#24615;&#65292;&#22914;&#23454;&#38469;&#26174;&#31034;&#20301;&#32622;&#21644;&#19978;&#19979;&#25991;&#23545;&#24191;&#21578;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#65307;2&#65289;&#24191;&#21578;&#20998;&#37197;&#21033;&#29992;&#25293;&#21334;&#33719;&#32988;&#24191;&#21578;&#30340;&#25903;&#20184;&#26469;&#21160;&#24577;&#30830;&#23450;&#26174;&#31034;&#20301;&#32622;&#65292;&#26410;&#33021;&#20445;&#25345;&#24191;&#21578;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#20351;&#29992;&#20256;&#32479;&#30340;&#24191;&#20041;&#20108;&#20215;(GSP)&#25293;&#21334;&#38454;&#27573;&#65292;&#24191;&#21578;&#20998;&#37197;&#38454;&#27573;&#26080;&#27861;&#32500;&#25345;&#28608;&#21169;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce platforms usually present an ordered list, mixed with several organic items and an advertisement, in response to each user's page view request. This list, the outcome of ad auction and allocation processes, directly impacts the platform's ad revenue and gross merchandise volume (GMV). Specifically, the ad auction determines which ad is displayed and the corresponding payment, while the ad allocation decides the display positions of the advertisement and organic items. The prevalent methods of segregating the ad auction and allocation into two distinct stages face two problems: 1) Ad auction does not consider externalities, such as the influence of actual display position and context on ad Click-Through Rate (CTR); 2) The ad allocation, which utilizes the auction-winning ad's payment to determine the display position dynamically, fails to maintain incentive compatibility (IC) for the advertisement. For instance, in the auction stage employing the traditional Generalized Secon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24314;&#31435;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#12289;&#21160;&#24577;&#25928;&#26524;&#12289;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.01651</link><description>&lt;p&gt;
AIGCBench&#65306;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24314;&#31435;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#12289;&#21160;&#24577;&#25928;&#26524;&#12289;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#35270;&#39057;&#29983;&#25104;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#32508;&#21512;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#21508;&#31181;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#65292;&#20027;&#35201;&#20851;&#27880;&#22270;&#20687;&#21040;&#35270;&#39057;&#65288;I2V&#65289;&#29983;&#25104;&#12290;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#22522;&#20934;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21253;&#25324;&#19968;&#20010;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#22312;&#30456;&#31561;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#32452;&#21512;&#22120;&#21644;GPT-4&#26469;&#21019;&#24314;&#20016;&#23500;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#22235;&#20010;&#32500;&#24230;&#65292;&#20197;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#12290;&#36825;&#20123;&#32500;&#24230;&#26159;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#65292;&#21160;&#24577;&#25928;&#26524;&#65292;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning field of Artificial Intelligence Generated Content (AIGC) is witnessing rapid advancements, particularly in video generation. This paper introduces AIGCBench, a pioneering comprehensive and scalable benchmark designed to evaluate a variety of video generation tasks, with a primary focus on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of existing benchmarks, which suffer from a lack of diverse datasets, by including a varied and open-domain image-text dataset that evaluates different state-of-the-art algorithms under equivalent conditions. We employ a novel text combiner and GPT-4 to create rich text prompts, which are then used to generate images via advanced Text-to-Image models. To establish a unified evaluation framework for video generation tasks, our benchmark includes 11 metrics spanning four dimensions to assess algorithm performance. These dimensions are control-video alignment, motion effects, temporal consistency, and video quality. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#24102;&#26377;&#20154;&#24037;&#26234;&#33021;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20316;&#20026;&#31034;&#20363;&#26469;&#35828;&#26126;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.01630</link><description>&lt;p&gt;
&#19968;&#20010;&#36866;&#29992;&#20110;&#24102;&#26377;&#20154;&#24037;&#26234;&#33021;&#32452;&#20214;&#30340;&#31995;&#32479;&#30340;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Cybersecurity Risk Analysis Framework for Systems with Artificial Intelligence Components. (arXiv:2401.01630v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#24102;&#26377;&#20154;&#24037;&#26234;&#33021;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20316;&#20026;&#31034;&#20363;&#26469;&#35828;&#26126;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12289;NIST&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#20197;&#21450;&#30456;&#20851;&#35268;&#33539;&#30340;&#24341;&#20837;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#23454;&#26045;&#35780;&#20272;&#24102;&#26377;&#20154;&#24037;&#26234;&#33021;&#32452;&#20214;&#30340;&#31995;&#32479;&#30340;&#26032;&#22411;&#39118;&#38505;&#20998;&#26512;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#27492;&#31867;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20851;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the European Union Artificial Intelligence Act, the NIST Artificial Intelligence Risk Management Framework, and related norms demands a better understanding and implementation of novel risk analysis approaches to evaluate systems with Artificial Intelligence components. This paper provides a cybersecurity risk analysis framework that can help assessing such systems. We use an illustrative example concerning automated driving systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#23427;&#20171;&#32461;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#26041;&#27861;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#20844;&#24179;&#24615;&#12289;&#20943;&#23569;&#20559;&#35265;&#21644;&#32500;&#25252;&#20262;&#29702;&#26631;&#20934;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01629</link><description>&lt;p&gt;
AI&#20013;&#30340;&#21512;&#25104;&#25968;&#25454;:&#25361;&#25112;&#65292;&#24212;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data in AI: Challenges, Applications, and Ethical Implications. (arXiv:2401.01629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#21512;&#25104;&#25968;&#25454;&#30340;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#23427;&#20171;&#32461;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#26041;&#27861;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#20844;&#24179;&#24615;&#12289;&#20943;&#23569;&#20559;&#35265;&#21644;&#32500;&#25252;&#20262;&#29702;&#26631;&#20934;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25253;&#21578;&#28145;&#20837;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#22810;&#26041;&#38754;&#38382;&#39064;&#65292;&#29305;&#21035;&#24378;&#35843;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#20559;&#35265;&#12290;&#23427;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26412;&#25253;&#21578;&#36824;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;&#19982;&#21512;&#25104;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#20262;&#29702;&#32771;&#34385;&#21644;&#27861;&#24459;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#30830;&#20445;&#20844;&#24179;&#24615;&#65292;&#20943;&#23569;&#20559;&#35265;&#21644;&#32500;&#25252;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#20262;&#29702;&#26631;&#20934;&#30340;&#26426;&#21046;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant. This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor. It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains. The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01626</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01626
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;GNN&#21487;&#20197;&#35299;&#20915;&#31038;&#20250;&#31185;&#23398;&#12289;&#21270;&#23398;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;GNN&#26550;&#26500;&#30340;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#33410;&#28857;&#25110;&#22270;&#20998;&#31867;&#31561;&#20219;&#21153;&#30340;&#23454;&#35777;&#24615;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#21017;&#23547;&#27714;&#25214;&#21040;&#20855;&#26377;&#29702;&#35770;&#29305;&#24615;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#30740;&#31350;&#20854;&#34920;&#36798;&#33021;&#21147;&#24182;&#35774;&#35745;&#26368;&#22823;&#21270;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#20851;&#20110;&#22914;&#20309;&#23450;&#20041;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#36824;&#27809;&#26377;&#20849;&#35782;&#65292;&#20294;&#21487;&#20197;&#20174;&#20960;&#20010;&#26377;&#24456;&#22909;&#21160;&#26426;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#12290;&#20063;&#35768;&#26368;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65292;&#23601;&#20687;MLP&#30340;&#36825;&#31181;&#24615;&#36136;&#19968;&#26679;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#20851;&#27880;&#30340;&#26159;GNN&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#20013;&#39118;&#38505;&#39044;&#27979;&#21644;&#39044;&#21518;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#26415;&#25551;&#36848;&#21644;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#65292;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#32771;&#23519;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23569;&#37327;&#26679;&#26412;&#21644;&#38142;&#24335;&#21551;&#21457;&#24335;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#26415;&#20013;&#39118;&#38505;&#20998;&#23618;&#21644;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#24635;&#32467;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.01620</link><description>&lt;p&gt;
&#26415;&#20013;&#39118;&#38505;&#39044;&#27979;&#21644;&#39044;&#21518;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Capabilities in Perioperative Risk Prediction and Prognostication. (arXiv:2401.01620v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01620
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#20013;&#39118;&#38505;&#39044;&#27979;&#21644;&#39044;&#21518;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#26415;&#25551;&#36848;&#21644;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#65292;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#32771;&#23519;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23569;&#37327;&#26679;&#26412;&#21644;&#38142;&#24335;&#21551;&#21457;&#24335;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#26415;&#20013;&#39118;&#38505;&#20998;&#23618;&#21644;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#24635;&#32467;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#29992;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-4 Turbo&#65292;&#33021;&#21542;&#20351;&#29992;&#25163;&#26415;&#30340;&#25551;&#36848;&#21644;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#65292;&#36827;&#34892;&#39118;&#38505;&#20998;&#23618;&#21644;&#39044;&#27979;&#26415;&#21518;&#32467;&#26524;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;8&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#20102;&#32771;&#23519;&#65306;ASA&#29983;&#29702;&#29366;&#24577;&#20998;&#31867;&#30340;&#39044;&#27979;&#12289;&#20303;&#38498;&#12289;&#37325;&#30151;&#30417;&#25252;&#23460;&#20837;&#38498;&#12289;&#38750;&#35745;&#21010;&#20837;&#38498;&#12289;&#20303;&#38498;&#27515;&#20129;&#12289;PACU&#31532;&#19968;&#38454;&#27573;&#25345;&#32493;&#26102;&#38388;&#12289;&#20303;&#38498;&#26102;&#38388;&#21644;&#37325;&#30151;&#30417;&#25252;&#23460;&#26102;&#38388;&#30340;&#39044;&#27979;&#12290;&#23569;&#37327;&#26679;&#26412;&#21644;&#38142;&#24335;&#21551;&#21457;&#24335;&#31574;&#30053;&#25913;&#21892;&#20102;&#20960;&#20010;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;ASA&#29983;&#29702;&#29366;&#24577;&#20998;&#31867;&#39044;&#27979;&#19978;&#36798;&#21040;&#20102;0.50&#30340;F1&#24471;&#20998;&#65292;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20837;&#38498;&#39044;&#27979;&#19978;&#36798;&#21040;&#20102;0.81&#30340;F1&#24471;&#20998;&#65292;&#22312;&#20303;&#38498;&#27515;&#20129;&#39044;&#27979;&#19978;&#36798;&#21040;&#20102;0.86&#30340;F1&#24471;&#20998;&#12290;&#25152;&#26377;&#25552;&#31034;&#31574;&#30053;&#22312;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26222;&#36941;&#36739;&#24046;&#12290;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#26415;&#20013;&#39118;&#38505;&#20998;&#23618;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate whether general-domain large language models such as GPT-4 Turbo can perform risk stratification and predict post-operative outcome measures using a description of the procedure and a patient's clinical notes derived from the electronic health record. We examine predictive performance on 8 different tasks: prediction of ASA Physical Status Classification, hospital admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1 duration, hospital duration, and ICU duration. Few-shot and chain-of-thought prompting improves predictive performance for several of the tasks. We achieve F1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU admission, and 0.86 for hospital mortality. Performance on duration prediction tasks were universally poor across all prompt strategies. Current generation large language models can assist clinicians in perioperative risk stratification on classification tasks and produce high-quality natural language summarie
&lt;/p&gt;</description></item><item><title>GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01614</link><description>&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#22914;&#26524;&#26377;&#22522;&#30784;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01614
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;GPT-4V(ision)&#21644;Gemini&#65292;&#24555;&#36895;&#25512;&#21160;&#20102;&#22810;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#36229;&#36234;&#20256;&#32479;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;GPT-4V&#36825;&#26679;&#30340;LMM&#20316;&#20026;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEEACT&#65292;&#19968;&#31181;&#21033;&#29992;LMM&#30340;&#21147;&#37327;&#36827;&#34892;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#26032;&#30340;MIND2WEB&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#38500;&#20102;&#23545;&#32531;&#23384;&#32593;&#31449;&#30340;&#26631;&#20934;&#31163;&#32447;&#35780;&#20272;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#22312;&#23454;&#26102;&#32593;&#31449;&#19978;&#36816;&#34892;&#32593;&#32476;&#20195;&#29702;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35780;&#20272;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4V&#22312;&#32593;&#39029;&#20195;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;-&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#25991;&#26412;&#35745;&#21010;&#25163;&#21160;&#22320;&#23454;&#26045;&#20026;&#32593;&#31449;&#19978;&#30340;&#34892;&#21160;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23436;&#25104;50%&#30340;&#20219;&#21153;&#12290;&#27492;&#32467;&#26524;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms
&lt;/p&gt;</description></item><item><title>PLLaMa&#26159;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#25968;&#25454;&#24211;&#22686;&#24378;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#20854;&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#65292;&#24182;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#30340;&#21512;&#20316;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01600</link><description>&lt;p&gt;
PLLaMa&#65306;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01600
&lt;/p&gt;
&lt;p&gt;
PLLaMa&#26159;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#25968;&#25454;&#24211;&#22686;&#24378;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#20854;&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#65292;&#24182;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#30340;&#21512;&#20316;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#19982;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#20132;&#20114;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26893;&#29289;&#31185;&#23398;&#31561;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PLLaMa&#65292;&#19968;&#31181;&#20174;LLaMa-2&#36827;&#21270;&#32780;&#26469;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21253;&#25324;&#36229;&#36807;150&#19975;&#31687;&#26893;&#29289;&#31185;&#23398;&#23398;&#26415;&#25991;&#31456;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#36827;&#34892;&#22686;&#24378;&#12290;&#36825;&#19968;&#21457;&#23637;&#26174;&#33879;&#20016;&#23500;&#20102;PLLaMa&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#27979;&#35797;&#20013;&#65292;&#28041;&#21450;&#19982;&#26893;&#29289;&#21644;&#20892;&#19994;&#30456;&#20851;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#26174;&#31034;PLLaMa&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26893;&#29289;&#31185;&#23398;&#30456;&#20851;&#20027;&#39064;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32452;&#24314;&#20102;&#19968;&#20010;&#22269;&#38469;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#65292;&#21253;&#25324;&#26893;&#29289;&#31185;&#23398;&#23478;&#12289;&#20892;&#19994;&#24037;&#31243;&#24072;&#21644;&#26893;&#29289;&#32946;&#31181;&#21592;&#12290;&#35813;&#22242;&#38431;&#22312;&#39564;&#35777;PLLaMa&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable capabilities in understanding and interacting with natural language across various sectors. However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields. This paper introduces PLLaMa, an open-source language model that evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising more than 1.5 million scholarly articles in plant science. This development significantly enriches PLLaMa with extensive knowledge and proficiency in plant and agricultural sciences. Our initial tests, involving specific datasets related to plants and agriculture, show that PLLaMa substantially improves its understanding of plant science-related topics. Moreover, we have formed an international panel of professionals, including plant scientists, agricultural engineers, and plant breeders. This team plays a crucial role in verifying the accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#24635;&#32467;&#28151;&#21512;&#30721;Hindi-English&#20020;&#24202;&#26597;&#35810;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24739;&#32773;&#21307;&#30103;&#29366;&#20917;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MedSumm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.01596</link><description>&lt;p&gt;
MedSumm&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#24635;&#32467;&#28151;&#21512;&#30721;Hindi-English&#20020;&#24202;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries. (arXiv:2401.01596v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#24635;&#32467;&#28151;&#21512;&#30721;Hindi-English&#20020;&#24202;&#26597;&#35810;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24739;&#32773;&#21307;&#30103;&#29366;&#20917;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MedSumm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#24635;&#32467;&#24739;&#32773;&#25552;&#20986;&#30340;&#21307;&#30103;&#38382;&#39064;&#23545;&#20110;&#25913;&#21892;&#21307;&#24739;&#20114;&#21160;&#21644;&#21307;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#30446;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#19978;&#65292;&#24573;&#35270;&#20102;&#35270;&#35273;&#32447;&#32034;&#30340;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#22312;&#21307;&#23398;&#38382;&#39064;&#24635;&#32467;&#30340;&#39046;&#22495;&#20013;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#38024;&#23545;&#28151;&#21512;&#30721;&#36755;&#20837;&#36827;&#34892;&#22810;&#27169;&#24577;&#21307;&#23398;&#38382;&#39064;&#24635;&#32467;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multimodal Medical Codemixed Question Summarization&#65288;MMCQS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;Hindi-English&#28151;&#21512;&#30721;&#21307;&#23398;&#26597;&#35810;&#21644;&#35270;&#35273;&#36741;&#21161;&#24037;&#20855;&#12290;&#36825;&#31181;&#25972;&#21512;&#20016;&#23500;&#20102;&#24739;&#32773;&#30340;&#21307;&#30103;&#29366;&#20917;&#30340;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MedSumm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#21644;VLMs&#30340;&#33021;&#21147;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;WiFi&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#21551;&#21457;&#30340;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#26465;&#20214;&#34920;&#26684;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CTGAN&#65289;&#29983;&#25104;&#20266;&#35013;&#25104;&#30495;&#23454;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23545;&#21512;&#25104;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35780;&#20272;&#21644;&#24615;&#33021;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01542</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;WiFi&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#21551;&#21457;&#30340;&#21311;&#21517;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning-Enabled Anonymization of OpenWiFi Data. (arXiv:2401.01542v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;WiFi&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#21551;&#21457;&#30340;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#26465;&#20214;&#34920;&#26684;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CTGAN&#65289;&#29983;&#25104;&#20266;&#35013;&#25104;&#30495;&#23454;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23545;&#21512;&#25104;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35780;&#20272;&#21644;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#25968;&#25454;&#36716;&#21457;&#32473;&#20854;&#20182;&#21487;&#33021;&#20351;&#29992;&#25968;&#25454;&#30340;&#26426;&#26500;&#20043;&#21069;&#65292;&#32593;&#32476;&#36816;&#33829;&#21830;&#25110;&#25968;&#25454;&#25152;&#26377;&#32773;&#36890;&#36807;&#21311;&#21517;&#21270;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#65292;&#25968;&#25454;&#21311;&#21517;&#21270;&#22686;&#21152;&#20102;&#25513;&#30422;&#24517;&#35201;&#25935;&#24863;&#20449;&#24687;&#30340;&#21487;&#33021;&#24615;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#21644;&#20449;&#24687;&#20002;&#22833;&#12290;OpenWiFi&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#35797;&#22270;&#33719;&#21462;&#25110;&#20102;&#35299;&#27969;&#37327;&#30340;&#20219;&#20309;&#25932;&#23545;&#26041;&#30340;&#25915;&#20987;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#25152;&#26377;&#32773;&#25152;&#25317;&#26377;&#30340;&#30693;&#35782;&#12290;&#24212;&#29992;&#26465;&#20214;&#34920;&#26684;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CTGAN&#65289;&#26469;&#35299;&#20915;&#21457;&#29616;&#23454;&#38469;&#27969;&#37327;&#20449;&#24687;&#30340;&#20960;&#29575;&#38382;&#39064;&#12290;CTGAN&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#65292;&#20266;&#35013;&#25104;&#30495;&#23454;&#25968;&#25454;&#65292;&#20294;&#36879;&#38706;&#20102;&#30495;&#23454;&#25968;&#25454;&#30340;&#38544;&#34255;&#20449;&#24687;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23545;&#21512;&#25104;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#28982;&#21518;&#27604;&#36739;&#20102;&#38024;&#23545;&#26080;&#30417;&#30563;&#32858;&#31867;&#39564;&#35777;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#31639;&#27861;&#65292;K-Means&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and protection through anonymization is a critical issue for network operators or data owners before it is forwarded for other possible use of data. With the adoption of Artificial Intelligence (AI), data anonymization augments the likelihood of covering up necessary sensitive information; preventing data leakage and information loss. OpenWiFi networks are vulnerable to any adversary who is trying to gain access or knowledge on traffic regardless of the knowledge possessed by data owners. The odds for discovery of actual traffic information is addressed by applied conditional tabular generative adversarial network (CTGAN). CTGAN yields synthetic data; which disguises as actual data but fostering hidden acute information of actual data. In this paper, the similarity assessment of synthetic with actual data is showcased in terms of clustering algorithms followed by a comparison of performance for unsupervised cluster validation metrics. A well-known algorithm, K-means outper
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#65292;&#32467;&#21512;&#20687;&#32032;&#32423;&#21644;&#20999;&#29255;&#32423;&#27880;&#37322;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#30002;&#29366;&#33146;&#32959;&#30244;&#65292;&#35299;&#20915;&#20102;&#30149;&#29702;&#35786;&#26029;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01496</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#20999;&#29255;&#22270;&#20687;: &#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#26497;&#21270;&#27169;&#24577;&#30340;&#30149;&#29702;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
From Pixel to Slide image: Polarization Modality-based Pathological Diagnosis Using Representation Learning. (arXiv:2401.01496v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#65292;&#32467;&#21512;&#20687;&#32032;&#32423;&#21644;&#20999;&#29255;&#32423;&#27880;&#37322;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#30002;&#29366;&#33146;&#32959;&#30244;&#65292;&#35299;&#20915;&#20102;&#30149;&#29702;&#35786;&#26029;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30002;&#29366;&#33146;&#30284;&#26159;&#26368;&#24120;&#35265;&#30340;&#20869;&#20998;&#27852;&#24694;&#24615;&#32959;&#30244;&#65292;&#20934;&#30830;&#21306;&#20998;&#33391;&#24615;&#21644;&#24694;&#24615;&#30002;&#29366;&#33146;&#32959;&#30244;&#23545;&#20110;&#21046;&#23450;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#30149;&#29702;&#23398;&#19978;&#65292;&#30001;&#20110;&#26679;&#26412;&#37319;&#38598;&#19981;&#24403;&#65292;&#30002;&#29366;&#33146;&#32959;&#30244;&#23384;&#22312;&#35786;&#26029;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#20351;&#29992;&#34920;&#31034;&#23398;&#20064;&#26469;&#38598;&#25104;&#20687;&#32032;&#32423;&#21644;&#20999;&#29255;&#32423;&#27880;&#37322;&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#30002;&#29366;&#33146;&#32959;&#30244;&#12290;&#35813;&#32467;&#26500;&#21253;&#25324;&#19968;&#20010;&#30149;&#29702;&#32467;&#26500;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#19982;&#30002;&#29366;&#33146;&#32959;&#30244;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#22359;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#21462;&#20687;&#32032;&#32423;&#27880;&#37322;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#26368;&#32456;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#20840;&#23616;&#32771;&#34385;&#20102;&#30149;&#29702;&#21306;&#22495;&#20013;&#19981;&#21516;&#22270;&#20687;&#22359;&#30340;&#20449;&#24687;&#65292;&#23398;&#20064;&#20102;&#27599;&#20010;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#31532;&#19977;&#38454;&#27573;&#65292;&#36890;&#36807;&#20999;&#29255;&#21306;&#22495;&#20013;&#30340;&#22270;&#20687;&#22359;&#30340;&#25152;&#26377;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thyroid cancer is the most common endocrine malignancy, and accurately distinguishing between benign and malignant thyroid tumors is crucial for developing effective treatment plans in clinical practice. Pathologically, thyroid tumors pose diagnostic challenges due to improper specimen sampling. In this study, we have designed a three-stage model using representation learning to integrate pixel-level and slice-level annotations for distinguishing thyroid tumors. This structure includes a pathology structure recognition method to predict structures related to thyroid tumors, an encoder-decoder network to extract pixel-level annotation information by learning the feature representations of image blocks, and an attention-based learning mechanism for the final classification task. This mechanism learns the importance of different image blocks in a pathological region, globally considering the information from each block. In the third stage, all information from the image blocks in a region
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PRFL&#30340;&#38544;&#31169;&#20445;&#25252;TFGC&#26694;&#26550;&#65292;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#36965;&#24863;&#30446;&#26631;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01493</link><description>&lt;p&gt;
&#33258;&#30001;&#21320;&#39184;&#20026;&#32852;&#37030;&#36965;&#24863;&#30446;&#26631;&#32454;&#31890;&#24230;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework. (arXiv:2401.01493v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PRFL&#30340;&#38544;&#31169;&#20445;&#25252;TFGC&#26694;&#26550;&#65292;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#36965;&#24863;&#30446;&#26631;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#30446;&#26631;&#30340;&#32454;&#31890;&#24230;&#20998;&#31867;&#23545;&#20891;&#20107;&#21644;&#27665;&#29992;&#39046;&#22495;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#30001;&#20110;&#20301;&#32622;&#24046;&#24322;&#12289;&#25968;&#25454;&#35268;&#27169;&#22686;&#38271;&#21644;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#20648;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#19981;&#21516;&#22320;&#21306;/&#22269;&#23478;&#30340;&#19981;&#21516;&#25968;&#25454;&#24211;&#20013;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#27861;&#24459;&#21644;&#22269;&#23478;&#23433;&#20840;&#38382;&#39064;&#38480;&#21046;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#20998;&#26512;&#36825;&#20123;&#25935;&#24863;&#30340;&#36965;&#24863;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#20302;&#36164;&#28304;&#30340;&#36965;&#24863;&#35774;&#22791;&#22312;&#22788;&#29702;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#26102;&#65292;&#20063;&#38754;&#20020;&#36890;&#20449;&#24320;&#38144;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;&#38544;&#31169;&#20445;&#25252;TFGC&#26694;&#26550;&#65292;&#31216;&#20026;PRFL&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20801;&#35768;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#32479;&#35745;&#24322;&#26500;&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65292;IID&#65289;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#31169;&#26377;&#25968;&#25454;&#30340;&#26412;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote Sensing Target Fine-grained Classification (TFGC) is of great significance in both military and civilian fields. Due to location differences, growth in data size, and centralized server storage constraints, these data are usually stored under different databases across regions/countries. However, privacy laws and national security concerns constrain researchers from accessing these sensitive remote sensing images for further analysis. Additionally, low-resource remote sensing devices encounter challenges in terms of communication overhead and efficiency when dealing with the ever-increasing data and model scales. To solve the above challenges, this paper proposes a novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed PRFL. The proposed framework allows each client to learn global and local knowledge to enhance the local representation of private data in environments with extreme statistical heterogeneity (non. Independent and Identically Distributed, IID). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#22238;&#24402;&#32593;&#32476;&#65288;ERN&#65289;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20351;ERN&#33021;&#22815;&#20174;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.01484</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#35268;&#33539;&#21270;&#30340;&#35777;&#25454;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Regularized Evidential Regression. (arXiv:2401.01484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#22238;&#24402;&#32593;&#32476;&#65288;ERN&#65289;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20351;ERN&#33021;&#22815;&#20174;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#22238;&#24402;&#32593;&#32476;&#65288;ERN&#65289;&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;Dempster-Shafer&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#30446;&#26631;&#24182;&#37327;&#21270;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#24213;&#23618;&#29702;&#35770;&#30340;&#25351;&#23548;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#29305;&#23450;&#30340;&#28608;&#27963;&#20989;&#25968;&#26469;&#24378;&#21046;&#38750;&#36127;&#20540;&#65292;&#36825;&#31181;&#32422;&#26463;&#38480;&#21046;&#20102;&#27169;&#22411;&#20174;&#25152;&#26377;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#21361;&#23475;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#38480;&#21046;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#20811;&#26381;&#23427;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#23398;&#20064;&#26679;&#26412;&#30340;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;ERN&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20010;&#32422;&#26463;&#12290;&#22522;&#20110;&#25105;&#20204;&#20998;&#26512;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#20351;ERN&#33021;&#22815;&#20174;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Evidential Regression Network (ERN) represents a novel approach that integrates deep learning with Dempster-Shafer's theory to predict a target and quantify the associated uncertainty. Guided by the underlying theory, specific activation functions must be employed to enforce non-negative values, which is a constraint that compromises model performance by limiting its ability to learn from all samples. This paper provides a theoretical analysis of this limitation and introduces an improvement to overcome it. Initially, we define the region where the models can't effectively learn from the samples. Following this, we thoroughly analyze the ERN and investigate this constraint. Leveraging the insights from our analysis, we address the limitation by introducing a novel regularization term that empowers the ERN to learn from the whole training set. Our extensive experiments substantiate our theoretical findings and demonstrate the effectiveness of the proposed solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.01482</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#24773;&#26223;&#19979;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#35774;&#35745;&#21644;&#29615;&#22659;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;&#36825;&#20123;&#36716;&#31227;&#19979;&#30340;&#23545;&#35937;&#27010;&#24565;&#65292;&#38656;&#35201;&#35843;&#25972;&#31867;&#21035;&#34920;&#31034;&#12290;&#22312;&#32570;&#20047;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#21033;&#29992;&#22320;&#29702;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#25551;&#36848;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#25506;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38598;&#25104;&#30693;&#35782;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#22312;&#19968;&#32452;&#28304;&#22320;&#29702;&#20301;&#32622;&#19978;&#35757;&#32451;&#30340;&#36719;&#25552;&#31034;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#38598;&#21512;&#12290;&#24403;&#20165;&#20381;&#36182;&#26469;&#33258;&#27431;&#27954;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#22312;DollarStreet&#19978;&#30340;&#22686;&#30410;&#36798;&#21040;&#20102;+2.8&#20010;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01470</link><description>&lt;p&gt;
&#39640;&#25928;&#35270;&#35273;Transformer&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#19978;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#36880;&#28176;&#20943;&#23569;&#20196;&#29260;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20551;&#35774;&#19968;&#20010;&#23618;&#20013;&#30340;&#20196;&#29260;&#20887;&#20313;&#24847;&#21619;&#30528;&#25152;&#26377;&#21518;&#32493;&#23618;&#20013;&#20063;&#26377;&#20887;&#20313;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#27491;&#30830;&#30340;&#65292;&#21363;&#19968;&#20010;&#23618;&#20013;&#22810;&#20313;&#30340;&#20196;&#29260;&#22312;&#21518;&#38754;&#30340;&#23618;&#20013;&#21487;&#20197;&#26159;&#26377;&#29992;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20196;&#29260;&#20998;&#24067;&#65292;&#21363;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#29992;&#26469;&#25511;&#21046;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#20196;&#29260;&#21033;&#29992;&#12290;&#20026;&#20102;&#25913;&#21892;&#20196;&#29260;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#26426;&#21046;&#65292;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#26377;&#21161;&#20110;&#21435;&#38500;&#22122;&#22768;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#38382;&#31572;&#24335;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#12289;RAG&#21644;&#26368;&#26032;&#30340;LLMs&#65292;&#25688;&#35201;&#26159;&#25552;&#21462;&#34987;&#19987;&#19994;&#20154;&#22763;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01469</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#38382;&#31572;&#24335;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation. (arXiv:2401.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#38382;&#31572;&#24335;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#12289;RAG&#21644;&#26368;&#26032;&#30340;LLMs&#65292;&#25688;&#35201;&#26159;&#25552;&#21462;&#34987;&#19987;&#19994;&#20154;&#22763;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#25688;&#35201;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#24739;&#32773;&#21644;&#21307;&#21153;&#20154;&#21592;&#30340;&#8220;&#23631;&#24149;&#26102;&#38388;&#8221;&#12290;&#36817;&#24180;&#26469;&#65292;EHRs&#30340;&#25688;&#35201;&#24050;&#32463;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#24471;&#36275;&#22815;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#32467;&#26524;&#19981;&#22815;&#28385;&#24847;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#22312;&#36755;&#20837;&#22823;&#23567;&#26041;&#38754;&#22686;&#21152;&#20102;&#20108;&#27425;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#25972;&#20010;EHR&#20869;&#23481;&#30340;&#25688;&#35201;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#12289;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#26368;&#26032;&#30340;LLMs&#26469;&#32531;&#35299;&#36825;&#20123;&#32570;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25688;&#35201;&#26159;&#25552;&#21462;&#34987;&#19987;&#19994;&#20154;&#22763;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarization of electronic health records (EHRs) can substantially minimize 'screen time' for both patients as well as medical personnel. In recent years summarization of EHRs have employed machine learning pipelines using state of the art neural models. However, these models have produced less than adequate results that are attributed to the difficulty of obtaining sufficient annotated data for training. Moreover, the requirement to consider the entire content of an EHR in summarization has resulted in poor performance due to the fact that attention mechanisms in modern large language models (LLMs) adds a quadratic complexity in terms of the size of the input. We propose here a method that mitigates these shortcomings by combining semantic search, retrieval augmented generation (RAG) and question-answering using the latest LLMs. In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs). Our approach is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#25490;&#21517;&#24322;&#24120;&#20540;&#30340;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23618;&#32423;&#32593;&#32476;&#21644;&#26497;&#20540;&#20998;&#26512;&#30340;&#26032;&#31639;&#27861;&#65292;&#22312;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#19987;&#23478;&#20204;&#20351;&#29992;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#21518;&#65292;&#21457;&#29616;&#20540;&#24471;&#35843;&#26597;&#30340;&#24322;&#24120;&#20540;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;9.1&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.01459</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#30340;&#24322;&#24120;&#20540;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Outlier Ranking in Large-Scale Public Health Streams. (arXiv:2401.01459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#25490;&#21517;&#24322;&#24120;&#20540;&#30340;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23618;&#32423;&#32593;&#32476;&#21644;&#26497;&#20540;&#20998;&#26512;&#30340;&#26032;&#31639;&#27861;&#65292;&#22312;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#19987;&#23478;&#20204;&#20351;&#29992;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#21518;&#65292;&#21457;&#29616;&#20540;&#24471;&#35843;&#26597;&#30340;&#24322;&#24120;&#20540;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;9.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#25511;&#21046;&#19987;&#23478;&#27599;&#22825;&#20250;&#26816;&#26597;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#22914;&#19982;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#25110;&#30142;&#30149;&#29190;&#21457;&#30456;&#20851;&#30340;&#24322;&#24120;&#20540;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#21482;&#33021;&#26816;&#26597;&#21040;&#30001;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#30340;&#21333;&#21464;&#37327;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#36820;&#22238;&#30340;&#23569;&#25968;&#20960;&#20010;&#26368;&#22823;&#32465;&#23450;&#24322;&#24120;&#20540;&#12290;&#20026;&#20102;&#24110;&#21161;&#19987;&#23478;&#20174;&#36825;&#20123;&#25104;&#21315;&#19978;&#19975;&#20010;&#32465;&#23450;&#30340;&#24322;&#24120;&#20540;&#20013;&#21306;&#20998;&#20986;&#26368;&#37325;&#35201;&#30340;&#24322;&#24120;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#23545;&#24212;&#29992;&#20110;&#35768;&#22810;&#25968;&#25454;&#27969;&#30340;&#20219;&#20309;&#21333;&#21464;&#37327;&#26041;&#27861;&#30340;&#36755;&#20986;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#20219;&#21153;&#25552;&#20986;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#22312;&#20351;&#29992;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#36827;&#34892;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#26102;&#65292;&#22312;&#20256;&#32479;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#25351;&#26631;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#33258;2023&#24180;4&#26376;&#20197;&#26469;&#65292;&#19987;&#23478;&#20204;&#24050;&#32463;&#20351;&#29992;&#25105;&#20204;&#30340;&#24320;&#28304;Python&#23454;&#29616;&#65292;&#24182;&#25253;&#21578;&#27604;&#20043;&#21069;&#30340;&#22522;&#32447;&#24555;&#20102;9.1&#20493;&#22320;&#21457;&#29616;&#20540;&#24471;&#35843;&#26597;&#30340;&#24322;&#24120;&#20540;&#12290;&#20854;&#20182;&#32452;&#32455;&#21487;&#20197;&#36731;&#26494;&#22320;&#38405;&#35835;&#21644;&#20351;&#29992;&#25105;&#20204;&#30340;&#23454;&#29616;&#26469;&#25552;&#39640;&#24322;&#24120;&#20540;&#30340;&#21457;&#29616;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disease control experts inspect public health data streams daily for outliers worth investigating, like those corresponding to data quality issues or disease outbreaks. However, they can only examine a few of the thousands of maximally-tied outliers returned by univariate outlier detection methods applied to large-scale public health data streams. To help experts distinguish the most important outliers from these thousands of tied outliers, we propose a new task for algorithms to rank the outputs of any univariate method applied to each of many streams. Our novel algorithm for this task, which leverages hierarchical networks and extreme value analysis, performed the best across traditional outlier detection metrics in a human-expert evaluation using public health data streams. Most importantly, experts have used our open-source Python implementation since April 2023 and report identifying outliers worth investigating 9.1x faster than their prior baseline. Other organizations can readil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#21457;&#33258;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#25805;&#20316;&#20013;&#26816;&#27979;&#24182;&#32416;&#27491;&#21333;&#20010;&#21644;&#22810;&#20010;&#27704;&#20037;&#21644;&#36719;&#38169;&#35823;&#65292;&#36866;&#29992;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.01458</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#21457;&#33258;&#27979;
&lt;/p&gt;
&lt;p&gt;
Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint. (arXiv:2401.01458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#21457;&#33258;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#25805;&#20316;&#20013;&#26816;&#27979;&#24182;&#32416;&#27491;&#21333;&#20010;&#21644;&#22810;&#20010;&#27704;&#20037;&#21644;&#36719;&#38169;&#35823;&#65292;&#36866;&#29992;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;NN-HA&#65289;&#19978;&#30340;&#22987;&#32456;&#24320;&#21551;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#24212;&#29992;&#37319;&#29992;&#21508;&#31181;&#23384;&#20648;&#25216;&#26415;&#12290;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26469;&#35828;&#65292;&#21487;&#38752;&#30340;&#25345;&#32493;&#25805;&#20316;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#22312;&#32447;&#25805;&#20316;&#20013;&#65292;NN&#22240;&#36752;&#23556;&#12289;&#32769;&#21270;&#21644;&#28909;&#25928;&#24212;&#31561;&#22240;&#32032;&#32780;&#23481;&#26131;&#20986;&#29616;&#21333;&#20010;&#21644;&#22810;&#20010;&#27704;&#20037;&#21644;&#36719;&#38169;&#35823;&#12290;&#26174;&#24335;&#30340;NN-HA&#27979;&#35797;&#26041;&#27861;&#19981;&#33021;&#26816;&#27979;&#21040;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#30636;&#24577;&#25925;&#38556;&#65292;&#23545;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#24212;&#29992;&#26469;&#35828;&#20063;&#19981;&#36866;&#29992;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;&#21644;&#23384;&#20648;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;NN&#22312;&#32447;&#25925;&#38556;&#29366;&#24577;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#8221;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#21452;&#22836;NN&#25299;&#25169;&#32467;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#21644;NN&#30340;&#20027;&#35201;&#39044;&#27979;&#12290;&#22312;&#22312;&#32447;&#25805;&#20316;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#21305;&#37197;&#19981;&#30830;&#23450;&#24615;&#25351;&#32441;&#65292;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#33258;&#27979;NN&#65292;&#21487;&#36798;&#21040;100
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) are increasingly used in always-on safety-critical applications deployed on hardware accelerators (NN-HAs) employing various memory technologies. Reliable continuous operation of NN is essential for safety-critical applications. During online operation, NNs are susceptible to single and multiple permanent and soft errors due to factors such as radiation, aging, and thermal effects. Explicit NN-HA testing methods cannot detect transient faults during inference, are unsuitable for always-on applications, and require extensive test vector generation and storage. Therefore, in this paper, we propose the \emph{uncertainty fingerprint} approach representing the online fault status of NN. Furthermore, we propose a dual head NN topology specifically designed to produce uncertainty fingerprints and the primary prediction of the NN in \emph{a single shot}. During the online operation, by matching the uncertainty fingerprint, we can concurrently self-test NNs with up to $100
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#24847;&#35782;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25509;&#25910;&#26426;&#30340;&#24402;&#19968;&#21270;&#22240;&#23376;&#26469;&#26368;&#23567;&#21270;&#24178;&#25200;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#32858;&#21512;&#23545;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01442</link><description>&lt;p&gt;
&#20855;&#26377;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#24847;&#35782;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity. (arXiv:2401.01442v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#24847;&#35782;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25509;&#25910;&#26426;&#30340;&#24402;&#19968;&#21270;&#22240;&#23376;&#26469;&#26368;&#23567;&#21270;&#24178;&#25200;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#32858;&#21512;&#23545;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#23454;&#26045;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26102;&#65292;&#21487;&#25193;&#23637;&#24615;&#20445;&#35777;&#21644;&#22788;&#29702;&#24178;&#25200;&#21644;&#35774;&#22791;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#36890;&#36807;&#31354;&#20013;&#35745;&#31639;&#39640;&#25928;&#21033;&#29992;&#21333;&#19968;&#26080;&#32447;&#36164;&#28304;&#30340;&#21487;&#25193;&#23637;&#20256;&#36755;&#26041;&#26696;&#12290;&#20026;&#20102;&#25552;&#20379;&#23545;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26799;&#24230;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20248;&#21270;&#25509;&#25910;&#26426;&#24402;&#19968;&#21270;&#22240;&#23376;&#65292;&#26368;&#23567;&#21270;&#20102;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#20960;&#20309;&#27169;&#22411;&#23545;&#22810;&#31751;&#26080;&#32447;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#32858;&#21512;&#20272;&#35745;&#30340;&#22343;&#26041;&#35823;&#24046;&#34920;&#24449;&#20026;&#32593;&#32476;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#24178;&#25200;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial. This work introduces a learning method designed to address these challenges, along with a scalable transmission scheme that efficiently uses a single wireless resource through over-the-air computation. To provide resistance against data heterogeneity, we employ gradient aggregations. Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors. For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters. We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2401.01426</link><description>&lt;p&gt;
&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#22312;&#35266;&#27979;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35745;&#31639;&#21487;&#36776;&#35782;&#22240;&#26524;&#26597;&#35810;&#30340;&#22768;&#38899;&#21644;&#23436;&#25972;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#23618;&#27425;&#30340;&#22240;&#26524;&#32467;&#26500;&#21644;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36739;&#20302;&#23618;&#27425;&#30340;&#23618;&#27425;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#22914;&#22270;&#20687;&#36825;&#26679;&#30340;&#39640;&#32500;&#21464;&#37327;&#26159;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;&#22914;&#20309;&#20934;&#30830;&#22320;&#20174;&#36825;&#26679;&#30340;&#39640;&#32500;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#29305;&#21035;&#26159;&#38543;&#30528;&#22270;&#20687;&#22522;&#27169;&#22411;&#30340;&#26368;&#36817;&#20852;&#36215;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22238;&#31572;&#24102;&#26377;&#36825;&#26679;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#35757;&#32451;&#31639;&#27861;&#65292;&#32473;&#23450;&#22240;&#26524;&#32467;&#26500;&#21644;&#39044;&#35757;&#32451;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#35745;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#35774;&#35745;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#33258;&#21160;&#36229;&#36710;&#21644;&#25442;&#36947;&#31574;&#30053;&#35268;&#21010;&#27169;&#22411;SwapTransformer&#12290;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#27604;&#36739;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01425</link><description>&lt;p&gt;
SwapTransformer&#65306;&#36890;&#36807;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#36895;&#20844;&#36335;&#36229;&#36710;&#31574;&#30053;&#35268;&#21010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset. (arXiv:2401.01425v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#35774;&#35745;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#33258;&#21160;&#36229;&#36710;&#21644;&#25442;&#36947;&#31574;&#30053;&#35268;&#21010;&#27169;&#22411;SwapTransformer&#12290;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#27604;&#36739;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#30340;&#39640;&#32423;&#20915;&#31574;&#38382;&#39064;&#65292;&#20363;&#22914;&#25442;&#36947;&#21644;&#36229;&#36710;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#29992;&#20110;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#33258;&#21160;&#36229;&#36710;&#21644;&#25442;&#36947;&#30340;&#20986;&#34892;&#36741;&#21161;&#21151;&#33021;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#25910;&#38598;&#20102;&#32422;900&#19975;&#20010;&#26679;&#26412;&#65292;&#21253;&#25324;&#36710;&#36947;&#22270;&#20687;&#21644;&#20854;&#20182;&#21160;&#24577;&#29289;&#20307;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;OSHA&#65288;&#27169;&#25311;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36229;&#36710;&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;SwapTransformer&#30340;&#26550;&#26500;&#65292;&#20316;&#20026;&#22312;OSHA&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#36741;&#21161;&#20219;&#21153;&#65292;&#22914;&#26410;&#26469;&#28857;&#21644;&#36710;&#36742;&#36317;&#31163;&#32593;&#32476;&#39044;&#27979;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#19982;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles. In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways. About 9 million samples including lane images and other dynamic objects are collected in simulation. This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge. To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset. Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment. The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment. We also demonstrate the performance of the model 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01405</link><description>&lt;p&gt;
&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Uniqueness of Donald Trump in Presidential Discourse. (arXiv:2401.01405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#19982;&#20854;&#20182;&#24635;&#32479;&#22312;&#28436;&#35762;&#20013;&#26159;&#21542;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#39118;&#26684;&#65311;&#22914;&#26524;&#26159;&#65292;&#26377;&#21738;&#20123;&#26041;&#38754;&#30340;&#19981;&#21516;&#65311;&#36825;&#20123;&#24046;&#24322;&#26159;&#21542;&#23616;&#38480;&#20110;&#20219;&#20309;&#21333;&#19968;&#30340;&#27807;&#36890;&#23186;&#20171;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#35010;&#24615;&#28436;&#35762;&#35789;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#27604;&#36739;&#25919;&#27835;&#23545;&#25163;&#35789;&#27719;&#29305;&#24449;&#30340;&#26694;&#26550;&#12290;&#23558;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#22810;&#31181;&#24635;&#32479;&#28436;&#35762;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#30456;&#24403;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#29305;&#26391;&#26222;&#30340;&#35762;&#35805;&#27169;&#24335;&#19982;&#36817;&#20195;&#21382;&#20219;&#20027;&#35201;&#24635;&#32479;&#20505;&#36873;&#20154;&#19981;&#21516;&#12290;&#19968;&#20123;&#26174;&#33879;&#30340;&#21457;&#29616;&#21253;&#25324;&#29305;&#26391;&#26222;&#20351;&#29992;&#29305;&#21035;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#38024;&#23545;&#20182;&#30340;&#25919;&#27835;&#23545;&#25163;&#65292;&#24182;&#19988;&#20182;&#37325;&#22797;&#24378;&#35843;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20182;&#30340;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#21152;&#29420;&#29305;&#65292;&#32780;&#20182;&#20204;&#30340;&#29420;&#29305;&#24615;&#20540;&#19982;&#27665;&#20027;&#20826;&#30456;&#23545;&#36739;&#25509;&#36817;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Donald Trump speak differently from other presidents? If so, in what ways? Are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for comparing the lexical features of political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Some notable findings include Trump's employment of particularly divisive and antagonistic language targeting of his political opponents and his patterns of repetition for emphasis. Furthermore, Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values are comparably closer to those of the Democrats. These differences hold across a variety of measurement 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#38271;&#36317;&#31163;&#31359;&#22681;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;ESP32-S3&#19982;&#23450;&#21521;&#21452;&#26497;&#22825;&#32447;&#21644;ESP32-S3&#19982;&#21360;&#21047;&#20498;F&#22825;&#32447;&#32467;&#21512;&#30340;&#20004;&#20010;&#26377;&#21069;&#26223;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#22312;WiFi-based HAR&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01388</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#38271;&#36317;&#31163;&#31359;&#22681;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition. (arXiv:2401.01388v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#38271;&#36317;&#31163;&#31359;&#22681;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;ESP32-S3&#19982;&#23450;&#21521;&#21452;&#26497;&#22825;&#32447;&#21644;ESP32-S3&#19982;&#21360;&#21047;&#20498;F&#22825;&#32447;&#32467;&#21512;&#30340;&#20004;&#20010;&#26377;&#21069;&#26223;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#22312;WiFi-based HAR&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20351;&#24471;&#22312;&#31354;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#20445;&#25345;&#35270;&#35273;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38750;&#25509;&#35302;&#24335;&#12289;&#38271;&#36317;&#31163;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21608;&#22260;&#23384;&#22312;&#35768;&#22810;&#25903;&#25345;WiFi&#30340;&#35774;&#22791;&#65292;&#20294;&#24456;&#23569;&#26377;&#35774;&#22791;&#21521;&#29992;&#25143;&#20844;&#24320;CSI&#65292;&#23548;&#33268;&#24863;&#30693;&#30828;&#20214;&#36873;&#25321;&#26377;&#38480;&#12290;Espressif ESP32&#30340;&#21464;&#20307;&#24050;&#32463;&#25104;&#20026;&#28508;&#22312;&#30340;&#20302;&#25104;&#26412;&#12289;&#26131;&#20110;&#37096;&#32626;&#30340;WiFi CSI-based HAR&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23545;&#22522;&#20110;&#22235;&#20010;ESP32-S3&#30340;2.4GHz&#23450;&#21521;&#22825;&#32447;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#25903;&#25345;&#38271;&#36317;&#31163;&#31359;&#22681;HAR&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#26377;&#21069;&#26223;&#30340;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#23558;ESP32-S3&#19982;&#23450;&#21521;&#21452;&#26497;&#22825;&#32447;&#30456;&#32467;&#21512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#32452;&#21512;&#26159;WiFi&#22522;&#30784;HAR&#20013;&#30340;&#39318;&#27425;&#28436;&#31034;&#12290;&#31532;&#20108;&#20010;&#31995;&#32479;&#20381;&#38752;ESP32-S3&#30340;&#20869;&#32622;&#21360;&#21047;&#20498;F&#22825;&#32447;&#65288;PIFA&#65289;&#24182;&#36890;&#36807;&#24179;&#38754;&#21453;&#23556;&#22120;&#23454;&#29616;&#26041;&#21521;&#24615;&#12290;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#65292;&#36825;&#20004;&#20010;&#31995;&#32479;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi Channel State Information (CSI)-based human activity recognition (HAR) enables contactless, long-range sensing in spatially constrained environments while preserving visual privacy. However, despite the presence of numerous WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of sensing hardware options. Variants of the Espressif ESP32 have emerged as potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for their ability to facilitate long-range through-wall HAR. Two promising systems are proposed, one of which combines the ESP32-S3 with a directional biquad antenna. This combination represents, to the best of our knowledge, the first demonstration of such a system in WiFi-based HAR. The second system relies on the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves directionality through a plane reflector. In a comprehensive evaluation 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36882;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TransGNN&#65289;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#35282;&#24230;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20960;&#20010;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01384</link><description>&lt;p&gt;
&#24378;&#20256;&#36882;&#20851;&#31995;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strong Transitivity Relations and Graph Neural Networks. (arXiv:2401.01384v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01384
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36882;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TransGNN&#65289;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#35282;&#24230;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20960;&#20010;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#20013;&#65292;&#23616;&#37096;&#37051;&#22495;&#22312;&#23884;&#20837;&#29983;&#25104;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33410;&#28857;&#24212;&#35813;&#20855;&#26377;&#31867;&#20284;&#20110;&#20854;&#37051;&#23621;&#30340;&#23884;&#20837;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23558;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#20174;&#38468;&#36817;&#37051;&#22495;&#25193;&#23637;&#21040;&#25972;&#20010;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36882;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#25193;&#23637;&#65292;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#22270;&#30340;&#20840;&#23616;&#30456;&#20284;&#24615;&#21644;&#23616;&#37096;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TransGNN&#65289;&#65292;&#23427;&#19981;&#20165;&#32771;&#34385;&#20102;&#23616;&#37096;&#33410;&#28857;&#30456;&#20284;&#24615;&#65292;&#36824;&#36890;&#36807;&#21306;&#20998;&#24378;&#20256;&#36882;&#20851;&#31995;&#21644;&#24369;&#20256;&#36882;&#20851;&#31995;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#32771;&#34385;&#20840;&#23616;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#23427;&#26174;&#33879;&#25913;&#21892;&#20102;&#20960;&#20010;&#30693;&#21517;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#33410;&#28857;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local neighborhoods play a crucial role in embedding generation in graph-based learning. It is commonly believed that nodes ought to have embeddings that resemble those of their neighbors. In this research, we try to carefully expand the concept of similarity from nearby neighborhoods to the entire graph. We provide an extension of similarity that is based on transitivity relations, which enables Graph Neural Networks (GNNs) to capture both global similarities and local similarities over the whole graph. We introduce Transitivity Graph Neural Network (TransGNN), which more than local node similarities, takes into account global similarities by distinguishing strong transitivity relations from weak ones and exploiting them. We evaluate our model over several real-world datasets and showed that it considerably improves the performance of several well-known GNN models, for tasks such as node classification.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.01383</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#32570;&#25968;&#25454;&#21644;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01383
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35782;&#21035;&#26089;&#26399;&#33041;&#36830;&#25509;&#24615;&#21457;&#23637;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#20102;&#35299;&#23156;&#20799;&#33041;&#32593;&#32476;&#22312;&#20986;&#29983;&#21518;&#30340;&#31532;&#19968;&#24180;&#20013;&#30340;&#22797;&#26434;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#19981;&#33021;&#27867;&#21270;&#21040;&#22810;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#36712;&#36857;&#23545;&#24212;&#20110;&#29305;&#23450;&#30340;&#25104;&#20687;&#27169;&#24577;&#25110;&#36830;&#25509;&#31867;&#22411;&#65288;&#20363;&#22914;T1-w MRI&#65289;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#21462;&#12290;&#31532;&#19977;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#21033;&#29992;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedGmTE-Net++&#65292;&#19968;&#31181;&#32852;&#37030;&#22270;&#24418;&#22810;&#36712;&#36857;&#28436;&#21270;&#32593;&#32476;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#25105;&#20204;&#22312;&#26377;&#38480;&#30340;&#21307;&#38498;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#20102;&#19981;&#21516;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#12290;&#32467;&#26524;&#21363;&#21487;&#25552;&#39640;&#27599;&#20010;&#21307;&#38498;&#26412;&#22320;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;FLBA&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.01377</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Few-shot Learning Suffer from Backdoor Attacks?. (arXiv:2401.01377v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;FLBA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#26469;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#30340;&#25915;&#20987;&#65292;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#24448;&#24448;&#36807;&#25311;&#21512;&#20110;&#33391;&#24615;&#29305;&#24449;&#25110;&#35302;&#21457;&#29305;&#24449;&#65292;&#23548;&#33268;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#33391;&#24615;&#20934;&#30830;&#29575;&#20043;&#38388;&#23384;&#22312;&#24456;&#38590;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#65292;&#25903;&#25345;&#38598;&#20013;&#30340;&#33039;&#26631;&#31614;&#25110;&#21487;&#35265;&#35302;&#21457;&#22120;&#24456;&#23481;&#26131;&#34987;&#21463;&#25915;&#20987;&#32773;&#26816;&#27979;&#21040;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#20284;&#20046;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#65288;FLBA&#65289;&#26469;&#23637;&#31034;&#23569;&#26679;&#26412;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of few-shot learning (FSL) has shown promising results in scenarios where training data is limited, but its vulnerability to backdoor attacks remains largely unexplored. We first explore this topic by first evaluating the performance of the existing backdoor attack methods on few-shot learning scenarios. Unlike in standard supervised learning, existing backdoor attack methods failed to perform an effective attack in FSL due to two main issues. Firstly, the model tends to overfit to either benign features or trigger features, causing a tough trade-off between attack success rate and benign accuracy. Secondly, due to the small number of training samples, the dirty label or visible trigger in the support set can be easily detected by victims, which reduces the stealthiness of attacks. It seemed that FSL could survive from backdoor attacks. However, in this paper, we propose the Few-shot Learning Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor attacks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01373</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01373
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#26816;&#27979;&#26159;&#21046;&#36896;&#19994;&#36136;&#37327;&#25511;&#21046;&#38454;&#27573;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#65292;&#24182;&#22312;&#32599;&#20271;&#29305;&#183;&#21338;&#19990;&#21046;&#36896;&#21378;&#29983;&#20135;&#30340;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#32452;&#20214;&#30340;&#30495;&#23454;&#32570;&#38519;&#26816;&#27979;&#24212;&#29992;&#20013;&#32771;&#23519;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;T-CNN&#22312;&#20943;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#19978;&#36816;&#34892;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31561;&#25928;CNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;T-CNN&#21487;&#20197;&#36890;&#36807;&#36136;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#19982;&#20256;&#32479;CNN&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#21482;&#26377;&#20854;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;T-CNN&#22823;&#22823;&#36229;&#36234;&#20102;&#20256;&#32479;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30340;&#32467;&#26524;&#65292;&#22312;&#24403;&#21069;&#21046;&#36896;&#24212;&#29992;&#20013;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4% to 19% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.
&lt;/p&gt;</description></item><item><title>RL-MPCA&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01369</link><description>&lt;p&gt;
RL-MPCA: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01369
&lt;/p&gt;
&lt;p&gt;
RL-MPCA&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#20174;&#22823;&#37327;&#20505;&#36873;&#39033;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#38543;&#30528;&#29992;&#25143;&#35831;&#27714;&#30340;&#22686;&#21152;&#21644;&#26381;&#21153;&#65288;&#25110;&#27169;&#22411;&#65289;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#19994;&#21153;&#25910;&#30410;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22312;&#38431;&#21015;&#25130;&#26029;&#22330;&#26223;&#20013;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#65288;&#21363;&#20998;&#37197;&#20505;&#36873;&#39033;&#30340;&#22823;&#23567;&#65289;&#65292;&#24182;&#23558;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#32422;&#26463;&#26465;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20123;&#30740;&#31350;&#38598;&#20013;&#20110;&#21333;&#38454;&#27573;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#38598;&#20013;&#20110;&#22810;&#38454;&#27573;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#20123;&#20851;&#20110;&#38431;&#21015;&#25130;&#26029;&#22330;&#26223;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#20854;&#20182;&#24773;&#26223;&#19979;&#65288;&#22914;&#26816;&#32034;&#36890;&#36947;&#36873;&#25321;&#21644;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#65289;&#26159;&#19981;&#25104;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#35831;&#27714;&#22312;&#19981;&#21516;&#38454;&#27573;&#20043;&#38388;&#30340;&#29366;&#24577;&#36716;&#31227;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiven
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#21518;&#32487;&#34920;&#31034;&#35757;&#32451;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#27169;&#25311;&#20301;&#32622;&#32454;&#32990;&#21160;&#24577;&#21644;&#35748;&#30693;&#22320;&#22270;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#20174;&#19968;&#31181;&#24418;&#24335;&#25512;&#26029;&#21040;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#23545;&#20110;&#25913;&#21892;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#29702;&#35299;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.01364</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#21518;&#32487;&#34920;&#31034;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#35748;&#30693;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations. (arXiv:2401.01364v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#21518;&#32487;&#34920;&#31034;&#35757;&#32451;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#27169;&#25311;&#20301;&#32622;&#32454;&#32990;&#21160;&#24577;&#21644;&#35748;&#30693;&#22320;&#22270;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#20174;&#19968;&#31181;&#24418;&#24335;&#25512;&#26029;&#21040;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#23545;&#20110;&#25913;&#21892;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#29702;&#35299;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#22320;&#22270;&#26159;&#20851;&#20110;&#22823;&#33041;&#22914;&#20309;&#26377;&#25928;&#22320;&#32452;&#32455;&#35760;&#24518;&#21644;&#25552;&#21462;&#19978;&#19979;&#25991;&#30340;&#19968;&#20010;&#25552;&#35758;&#27010;&#24565;&#12290;&#20869;&#38544;-&#28023;&#39532;&#22797;&#21512;&#20307;&#22312;&#24773;&#33410;&#21644;&#20851;&#31995;&#35760;&#24518;&#22788;&#29702;&#20197;&#21450;&#31354;&#38388;&#23548;&#33322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#34987;&#35748;&#20026;&#36890;&#36807;&#20301;&#32622;&#32454;&#32990;&#21644;&#32593;&#26684;&#32454;&#32990;&#24314;&#31435;&#35748;&#30693;&#22320;&#22270;&#12290;&#20026;&#20102;&#21033;&#29992;&#35748;&#30693;&#22320;&#22270;&#30340;&#26377;&#24076;&#26395;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#21518;&#32487;&#34920;&#31034;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#27169;&#25311;&#20301;&#32622;&#32454;&#32990;&#21160;&#24577;&#21644;&#35748;&#30693;&#22320;&#22270;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;&#22270;&#20687;&#21644;&#35789;&#23884;&#20837;&#32452;&#25104;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#12290;&#32593;&#32476;&#23398;&#20064;&#20102;&#26032;&#36755;&#20837;&#19982;&#35757;&#32451;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25104;&#21151;&#24314;&#31435;&#20102;&#35748;&#30693;&#22320;&#22270;&#30340;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#32593;&#32476;&#30340;&#39044;&#27979;&#21487;&#20197;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#20174;&#19968;&#31181;&#24418;&#24335;&#25512;&#26029;&#21040;&#21478;&#19968;&#31181;&#24418;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#33021;&#25104;&#20026;&#25913;&#36827;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20197;&#26356;&#22909;&#29702;&#35299;&#29615;&#22659;&#30340;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive maps are a proposed concept on how the brain efficiently organizes memories and retrieves context out of them. The entorhinal-hippocampal complex is heavily involved in episodic and relational memory processing, as well as spatial navigation and is thought to built cognitive maps via place and grid cells. To make use of the promising properties of cognitive maps, we set up a multi-modal neural network using successor representations which is able to model place cell dynamics and cognitive map representations. Here, we use multi-modal inputs consisting of images and word embeddings. The network learns the similarities between novel inputs and the training database and therefore the representation of the cognitive map successfully. Subsequently, the prediction of the network can be used to infer from one modality to another with over $90\%$ accuracy. The proposed method could therefore be a building block to improve current AI systems for better understanding of the environment
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#22609;&#36896;&#24847;&#35265;&#30340;&#36229;&#32423;&#20256;&#25773;&#32773;&#30340;&#37325;&#35201;&#35282;&#33394;&#65292;&#36890;&#36807;&#23545;&#36229;&#32423;&#20256;&#25773;&#32773;&#34892;&#20026;&#30340;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#32676;&#20307;&#21160;&#24577;&#21644;&#24847;&#35265;&#24418;&#25104;&#30340;&#26465;&#20214;&#30340;&#29702;&#35299;&#65292;&#23545;&#25913;&#36827;&#22312;&#32447;&#20132;&#27969;&#23433;&#20840;&#21644;&#31038;&#20132;&#24433;&#21709;&#21147;&#30340;&#35748;&#35782;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.01349</link><description>&lt;p&gt;
&#22312;&#32447;&#24847;&#35265;&#26497;&#21270;&#30340;&#20256;&#25773;&#35299;&#21078;&#65306;&#36229;&#32423;&#20256;&#25773;&#32773;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Anatomy Spread of Online Opinion Polarization: The Pivotal Role of Super-Spreaders in Social Networks. (arXiv:2401.01349v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#22609;&#36896;&#24847;&#35265;&#30340;&#36229;&#32423;&#20256;&#25773;&#32773;&#30340;&#37325;&#35201;&#35282;&#33394;&#65292;&#36890;&#36807;&#23545;&#36229;&#32423;&#20256;&#25773;&#32773;&#34892;&#20026;&#30340;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#32676;&#20307;&#21160;&#24577;&#21644;&#24847;&#35265;&#24418;&#25104;&#30340;&#26465;&#20214;&#30340;&#29702;&#35299;&#65292;&#23545;&#25913;&#36827;&#22312;&#32447;&#20132;&#27969;&#23433;&#20840;&#21644;&#31038;&#20132;&#24433;&#21709;&#21147;&#30340;&#35748;&#35782;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32593;&#32476;&#20013;&#22609;&#36896;&#24847;&#35265;&#30340;&#8220;&#36229;&#32423;&#20256;&#25773;&#32773;&#8221;&#30340;&#35282;&#33394;&#65292;&#21306;&#20998;&#20102;&#19977;&#31181;&#31867;&#22411;&#65306;A&#12289;B&#21644;C&#12290;A&#22411;&#22312;&#22609;&#36896;&#24847;&#35265;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#65292;B&#22411;&#21017;&#36215;&#21040;&#20102;&#24179;&#34913;A&#22411;&#30340;&#20316;&#29992;&#65292;C&#22411;&#21017;&#20687;&#23186;&#20307;&#19968;&#26679;&#65292;&#25552;&#20379;&#23458;&#35266;&#35266;&#28857;&#65292;&#24182;&#28508;&#22312;&#22320;&#35843;&#25511;A&#22411;&#21644;B&#22411;&#30340;&#24433;&#21709;&#21147;&#12290;&#30740;&#31350;&#20351;&#29992;&#32622;&#20449;&#31995;&#25968;&#21644;z&#20998;&#25968;&#26469;&#35843;&#26597;&#36229;&#32423;&#20256;&#25773;&#32773;&#30340;&#34892;&#20026;&#65292;&#30528;&#37325;&#32771;&#23519;&#24433;&#21709;&#32676;&#20307;&#21160;&#24577;&#21644;&#24847;&#35265;&#24418;&#25104;&#30340;&#26465;&#20214;&#65292;&#21253;&#25324;&#29615;&#22659;&#22240;&#32032;&#21644;&#38543;&#26102;&#38388;&#30340;&#36951;&#24536;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#25913;&#21892;&#22312;&#32447;&#20132;&#27969;&#23433;&#20840;&#21644;&#20102;&#35299;&#31038;&#20250;&#24433;&#21709;&#21147;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the role of 'superspreaders' in shaping opinions within networks, distinguishing three types: A, B, and C. Type A has a significant influence in shaping opinions, Type B acts as a counterbalance to A, and Type C functions like media, providing an objective viewpoint and potentially regulating A and B's influence. The research uses a confidence coefficient and z-score to survey superspreaders' behaviors, with a focus on the conditions affecting group dynamics and opinion formation, including environmental factors and forgetfulness over time. The findings offer insights for improving online communication security and understanding social influence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.01343</link><description>&lt;p&gt;
IoTGeM: &#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#32593;&#32476;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#25915;&#20987;&#26816;&#27979;&#30740;&#31350;&#65292;&#25152;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24314;&#27169;&#29289;&#32852;&#32593;&#32593;&#32476;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#27169;&#22411;&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24120;&#35265;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#20986;&#25903;&#25745;&#25915;&#20987;&#20934;&#30830;&#26816;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research on behaviour-based attack detection on networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited, and often not demonstrated. In this paper we present an approach for modelling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. First, we present an improved rolling window approach for feature extraction, and introduce a multi-step feature selection process that reduces overfitting. Second, we build and test models using isolated train and test datasets, thereby avoiding common data leaks that have limited the generalizability of previous models. Third, we rigorously evaluate our methodology using a diverse portfolio of machine learning models, evaluation metrics and datasets. Finally, we build confidence in the models by using explainable AI techniques, allowing us to identify the features that underlie accurate detection of attacks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.01262</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20844;&#24179;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#25307;&#32856;&#31561;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#20316;&#20026;&#19987;&#23478;&#31995;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#23548;&#24072;&#12290;&#30001;&#20110;NLP&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#28183;&#20837;NLP&#31995;&#32479;&#65292;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#27495;&#35270;&#23569;&#25968;&#32676;&#20307;&#25110;&#24341;&#21457;&#27861;&#24459;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;NLP&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#37319;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37327;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#22810;&#20301;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#35775;&#35848;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;NLP&#30340;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#21270;&#20026;18&#20010;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#20026;&#23454;&#26045;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.01078</link><description>&lt;p&gt;
&#36234;&#21335;&#35799;&#27468;&#29983;&#25104;&#19982;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#30340;&#36807;&#31243;&#21644;&#22686;&#24378;&#30340;&#20869;&#23481;&#25511;&#21046;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;GPT-3 Babbage&#21464;&#31181;&#65292;&#22312;&#36234;&#21335;&#35799;&#27468;&#30340;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#23454;&#29616;&#20102;0.8&#30340;&#33258;&#23450;&#20041;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#25913;&#20889;&#25104;&#27491;&#24120;&#25991;&#26412;&#25552;&#31034;&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#33719;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;0.718&#20998;&#25968;&#12290;&#36825;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#20197;&#32763;&#35793;&#21518;&#30340;&#35799;&#27468;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the "luc bat" genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.
&lt;/p&gt;</description></item><item><title>&#33041;&#26465;&#20214;&#22810;&#27169;&#24577;&#21512;&#25104;&#26159;&#19968;&#39033;&#21033;&#29992;&#33041;&#20449;&#21495;&#35299;&#30721;&#20026;&#30693;&#35273;&#20307;&#39564;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#23454;&#29992;&#30340;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#31995;&#32479;&#20197;&#21450;&#25581;&#31034;&#22823;&#33041;&#24863;&#30693;&#21644;&#29702;&#35299;&#22806;&#37096;&#21050;&#28608;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.00430</link><description>&lt;p&gt;
&#33041;&#26465;&#20214;&#22810;&#27169;&#24577;&#21512;&#25104;&#65306;&#19968;&#39033;&#35843;&#26597;&#19982;&#20998;&#31867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy. (arXiv:2401.00430v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00430
&lt;/p&gt;
&lt;p&gt;
&#33041;&#26465;&#20214;&#22810;&#27169;&#24577;&#21512;&#25104;&#26159;&#19968;&#39033;&#21033;&#29992;&#33041;&#20449;&#21495;&#35299;&#30721;&#20026;&#30693;&#35273;&#20307;&#39564;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#23454;&#29992;&#30340;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#31995;&#32479;&#20197;&#21450;&#25581;&#31034;&#22823;&#33041;&#24863;&#30693;&#21644;&#29702;&#35299;&#22806;&#37096;&#21050;&#28608;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26102;&#20195;&#65292;&#26465;&#20214;&#22810;&#27169;&#24577;&#21512;&#25104;&#25216;&#26415;&#65288;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#25991;&#26412;&#21040;&#35270;&#39057;&#12289;&#25991;&#26412;&#21040;&#38899;&#39057;&#31561;&#65289;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#20869;&#23481;&#12290;&#22810;&#27169;&#24577;&#21512;&#25104;&#25216;&#26415;&#30340;&#20851;&#38190;&#26159;&#24314;&#31435;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#20316;&#20026;&#22823;&#33041;&#35299;&#35835;&#22806;&#37096;&#20449;&#24687;&#30340;&#28508;&#22312;&#21453;&#26144;&#65292;&#33041;&#20449;&#21495;&#19982;&#21508;&#31181;&#22806;&#37096;&#27169;&#24577;&#20043;&#38388;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#19968;&#23545;&#22810;&#23545;&#24212;&#20851;&#31995;&#12290;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#20351;&#24471;&#33041;&#20449;&#21495;&#25104;&#20026;&#22810;&#27169;&#24577;&#20869;&#23481;&#21512;&#25104;&#30340;&#26377;&#24076;&#26395;&#30340;&#24341;&#23548;&#26465;&#20214;&#12290;&#33041;&#26465;&#20214;&#22810;&#27169;&#24577;&#21512;&#25104;&#25351;&#30340;&#26159;&#23558;&#33041;&#20449;&#21495;&#35299;&#30721;&#20026;&#30693;&#35273;&#20307;&#39564;&#65292;&#36825;&#23545;&#20110;&#21457;&#23637;&#23454;&#29992;&#30340;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#31995;&#32479;&#21644;&#25581;&#31034;&#22823;&#33041;&#24863;&#30693;&#21644;&#29702;&#35299;&#22806;&#37096;&#21050;&#28608;&#30340;&#22797;&#26434;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;AIGC&#30340;&#33041;&#26465;&#20214;&#22810;&#27169;&#24577;&#21512;&#25104;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Artificial Intelligence Generated Content (AIGC), conditional multimodal synthesis technologies (e.g., text-to-image, text-to-video, text-to-audio, etc) are gradually reshaping the natural content in the real world. The key to multimodal synthesis technology is to establish the mapping relationship between different modalities. Brain signals, serving as potential reflections of how the brain interprets external information, exhibit a distinctive One-to-Many correspondence with various external modalities. This correspondence makes brain signals emerge as a promising guiding condition for multimodal content synthesis. Brian-conditional multimodal synthesis refers to decoding brain signals back to perceptual experience, which is crucial for developing practical brain-computer interface systems and unraveling complex mechanisms underlying how the brain perceives and comprehends external stimuli. This survey comprehensively examines the emerging field of AIGC-based Brain-cond
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.16476</link><description>&lt;p&gt;
SVGDreamer&#65306;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#21487;&#32553;&#25918;&#30690;&#37327;&#22270;&#24418;&#65288;SVG&#65289;&#21512;&#25104;&#22312;&#22270;&#26631;&#35774;&#35745;&#21644;&#33609;&#22270;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#22270;&#24418;&#21512;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;SVGDreamer&#12290;SVGDreamer&#25972;&#21512;&#20102;&#19968;&#31181;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#65288;SIVE&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#21512;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#21069;&#26223;&#23545;&#35937;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#21487;&#32534;&#36753;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SIVE&#36807;&#31243;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22522;&#26412;&#20803;&#32032;&#25511;&#21046;&#21644;&#27880;&#24847;&#21147;&#25513;&#34109;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#21644;&#25805;&#20316;&#21508;&#20010;&#20803;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#65288;VPSD&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#20013;&#39068;&#33394;&#36807;&#39281;&#21644;&#12289;&#30690;&#37327;&#22522;&#20803;&#36807;&#24179;&#28369;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduce attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to tackle the challenges of color over-saturation, vector primitives over-smoothing, and limited result diversity in existing text-to-SVG generation methods. Furthermore, on the basis of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.16127</link><description>&lt;p&gt;
LLM-SAP: &#22823;&#35821;&#35328;&#27169;&#22411;&#24773;&#26223;&#24863;&#30693;&#30340;&#22522;&#20110;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-SAP: Large Language Model Situational Awareness Based Planning. (arXiv:2312.16127v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35780;&#20272;&#22522;&#20110;&#24773;&#26223;&#24863;&#30693;&#30340;&#32039;&#24613;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#65288;i&#65289;&#29992;&#20110;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#26032;&#22411;&#22522;&#20934;&#21644;&#25351;&#26631;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#36827;&#23637;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#28436;&#31034;&#20197;&#25552;&#31034;&#21644;&#22810;&#26234;&#33021;&#20307;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#25935;&#24863;&#35268;&#21010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23558;&#20854;&#32435;&#20837;&#21040;&#19968;&#20010;&#22788;&#22659;&#26234;&#33021;&#20307;&#21644;&#33258;&#21160;&#21270;&#35268;&#21010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22266;&#26377;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;-&#23613;&#31649;&#22312;&#27169;&#25311;&#39046;&#22495;&#30340;&#36827;&#27493;&#20013;&#65292;&#22914;&#20309;&#22312;&#27809;&#26377;&#29615;&#22659;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#19990;&#30028;&#29366;&#24577;&#26144;&#23556;&#21040;&#34892;&#21160;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36229;&#20986;&#33539;&#22260;&#65292;&#23545;&#20110;&#39564;&#35777;&#26041;&#27861;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#34920;&#26126;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#23545;&#25193;&#23637;&#35268;&#21010;&#35821;&#26009;&#24211;&#36827;&#34892;&#24494;&#35843;&#21644;&#38024;&#23545;&#24555;&#36895;&#28508;&#22312;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#36890;&#36807;&#20005;&#23494;&#30340;&#27604;&#36739;&#26469;&#30830;&#23454;&#22320;&#23637;&#31034;&#24403;&#21069;&#26041;&#27861;&#30340;&#25215;&#35834;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25512;&#21160;&#20102;&#23545;&#21487;&#38752;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMXRec&#26694;&#26550;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#21319;&#35299;&#37322;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20808;&#21069;&#30340;&#25512;&#33616;&#27169;&#22411;&#23494;&#20999;&#21327;&#20316;&#65292;&#22312;&#25512;&#33616;&#25928;&#26524;&#21644;&#35299;&#37322;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.15661</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21487;&#35299;&#37322;&#25512;&#33616;&#20013;&#30340;&#28508;&#21147;&#35299;&#38145;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Large Language Models for Explainable Recommendations. (arXiv:2312.15661v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMXRec&#26694;&#26550;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#21319;&#35299;&#37322;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20808;&#21069;&#30340;&#25512;&#33616;&#27169;&#22411;&#23494;&#20999;&#21327;&#20316;&#65292;&#22312;&#25512;&#33616;&#25928;&#26524;&#21644;&#35299;&#37322;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20851;&#20110;&#20026;&#20309;&#25512;&#33616;&#26576;&#20010;&#39033;&#30446;&#30340;&#29992;&#25143;&#21451;&#22909;&#35299;&#37322;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#24182;&#22312;&#20351;&#29992;&#22312;&#32447;&#26381;&#21153;&#26102;&#20419;&#36827;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23558;&#35299;&#37322;&#29983;&#25104;&#22120;&#26367;&#25442;&#20026;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#20135;&#29983;&#20309;&#31181;&#24433;&#21709;&#12290;&#25105;&#20204;&#33021;&#21542;&#26399;&#26395;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMXRec&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#21487;&#35299;&#37322;&#25512;&#33616;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#37322;&#36136;&#37327;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#24037;&#20316;&#19981;&#21516;&#65292;LLMXRec&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#23427;&#24378;&#35843;&#20808;&#21069;&#30340;&#25512;&#33616;&#27169;&#22411;&#19982;&#22522;&#20110;LLM&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#20043;&#38388;&#30340;&#23494;&#20999;&#21327;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#37319;&#29992;&#20960;&#31181;&#20851;&#38190;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21253;&#25324;&#21442;&#25968;&#35843;&#20248;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#25928;&#26524;&#21644;&#35299;&#37322;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating user-friendly explanations regarding why an item is recommended has become increasingly common, largely due to advances in language generation technology, which can enhance user trust and facilitate more informed decision-making when using online services. However, existing explainable recommendation systems focus on using small-size language models. It remains uncertain what impact replacing the explanation generator with the recently emerging large language models (LLMs) would have. Can we expect unprecedented results?  In this study, we propose LLMXRec, a simple yet effective two-stage explainable recommendation framework aimed at further boosting the explanation quality by employing LLMs. Unlike most existing LLM-based recommendation works, a key characteristic of LLMXRec is its emphasis on the close collaboration between previous recommender models and LLM-based explanation generators. Specifically, by adopting several key fine-tuning techniques, including parameter-eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#32034;&#24863;&#30693;&#30340;&#38598;&#25104;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#35770;&#35777;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#22411;&#22810;&#26679;&#24615;&#19979;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#24773;&#20917;&#19979;CEs&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2312.15097</link><description>&lt;p&gt;
&#36890;&#36807;&#35770;&#25454;&#38598;&#25104;&#23454;&#29616;&#27169;&#22411;&#22810;&#26679;&#24615;&#19979;&#30340;&#36861;&#32034;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#32034;&#24863;&#30693;&#30340;&#38598;&#25104;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#35770;&#35777;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#22411;&#22810;&#26679;&#24615;&#19979;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#24773;&#20917;&#19979;CEs&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22810;&#26679;&#24615;&#65288;MM&#65289;&#26159;&#25351;&#22312;&#35299;&#20915;&#21516;&#19968;&#39044;&#27979;&#20219;&#21153;&#26102;&#21487;&#20197;&#35757;&#32451;&#20986;&#22810;&#20010;&#24615;&#33021;&#30456;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;MM&#19979;&#33719;&#24471;&#30340;&#27169;&#22411;&#23545;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#21487;&#33021;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#24403;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#20026;&#21463;&#21040;&#27169;&#22411;&#39044;&#27979;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;&#36861;&#32034;&#24863;&#30693;&#30340;&#38598;&#25104;&#65292;&#24182;&#30830;&#23450;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#24212;&#35813;&#28385;&#36275;&#30340;&#20960;&#20010;&#29702;&#24819;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#38598;&#25104;&#26041;&#27861;&#22312;&#19981;&#21516;&#26041;&#24335;&#19979;&#33258;&#28982;&#25193;&#23637;&#20197;&#25552;&#20379;CEs&#26102;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35770;&#35777;&#38598;&#25104;&#65292;&#37319;&#29992;&#35745;&#31639;&#35770;&#35777;&#26041;&#27861;&#26469;&#30830;&#20445;CEs&#23545;MM&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36866;&#24212;&#21487;&#23450;&#21046;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#35770;&#35777;&#38598;&#25104;&#28385;&#36275;&#20102;&#36825;&#20123;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Multiplicity (MM) arises when multiple, equally performing machine learning models can be trained to solve the same prediction task. Recent studies show that models obtained under MM may produce inconsistent predictions for the same input. When this occurs, it becomes challenging to provide counterfactual explanations (CEs), a common means for offering recourse recommendations to individuals negatively affected by models' predictions. In this paper, we formalise this problem, which we name recourse-aware ensembling, and identify several desirable properties which methods for solving it should satisfy. We show that existing ensembling methods, naturally extended in different ways to provide CEs, fail to satisfy these properties. We then introduce argumentative ensembling, deploying computational argumentation to guarantee robustness of CEs to MM, while also accommodating customisable user preferences. We show theoretically and experimentally that argumentative ensembling satisfies
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#35843;&#25972;&#19982;&#22122;&#22768;&#39044;&#27979;&#38590;&#24230;&#30456;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.13307</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#27493;&#39588;&#37117;&#30456;&#31561;&#65306;&#36827;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#35843;&#25972;&#19982;&#22122;&#22768;&#39044;&#27979;&#38590;&#24230;&#30456;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#33021;&#65292;&#20855;&#26377;&#21435;&#22122;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#19978;&#37117;&#37319;&#29992;&#32479;&#19968;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#22122;&#22768;&#28508;&#22312;&#21464;&#21270;&#23548;&#33268;&#20102;&#35757;&#32451;&#20013;&#30340;&#20914;&#31361;&#65292;&#38480;&#21046;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;&#35757;&#32451;&#19968;&#20010;&#22522;&#30784;&#30340;&#21435;&#22122;&#27169;&#22411;&#26469;&#21253;&#25324;&#25152;&#26377;&#30340;&#26102;&#38388;&#27493;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#27493;&#20998;&#20026;&#19981;&#21516;&#30340;&#32452;&#65292;&#23545;&#27599;&#20010;&#32452;&#20869;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#36798;&#21040;&#19987;&#38376;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#65292;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#22122;&#22768;&#39044;&#27979;&#22256;&#38590;&#31243;&#24230;&#26159;&#19981;&#21516;&#30340;&#65292;&#25152;&#20197;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26679;&#30340;&#27169;&#22411;&#22823;&#23567;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#20449;&#22122;&#27604;&#26469;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#65292;&#20197;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#12290;&#27492;&#35843;&#25972;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#27969;&#31243;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable efficacy in various generative tasks with the predictive prowess of denoising model. Currently, these models employ a uniform denoising approach across all timesteps. However, the inherent variations in noisy latents at each timestep lead to conflicts during training, constraining the potential of diffusion models. To address this challenge, we propose a novel two-stage training strategy termed Step-Adaptive Training. In the initial stage, a base denoising model is trained to encompass all timesteps. Subsequently, we partition the timesteps into distinct groups, fine-tuning the model within each group to achieve specialized denoising capabilities. Recognizing that the difficulties of predicting noise at different timesteps vary, we introduce a diverse model size requirement. We dynamically adjust the model size for each timestep by estimating task difficulty based on its signal-to-noise ratio before fine-tuning. This adjustment is facilitat
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#38754;&#20020;&#24187;&#35273;&#12289;&#36807;&#26102;&#30693;&#35782;&#21644;&#38750;&#36879;&#26126;&#12289;&#19981;&#21487;&#36861;&#28335;&#30340;&#25512;&#29702;&#36807;&#31243;&#31561;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;RAG&#23558;LLMs&#33258;&#36523;&#30340;&#30693;&#35782;&#19982;&#24222;&#22823;&#12289;&#21160;&#24577;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32771;&#23519;&#20102;RAG&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;&#23427;&#35814;&#32454;&#23457;&#35270;&#20102;RAG&#26694;&#26550;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23884;&#20837;&#22312;RAG&#26694;&#26550;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
&lt;/p&gt;</description></item><item><title>EQ-Bench&#26159;&#19968;&#31181;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#21830;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#35282;&#33394;&#30340;&#24773;&#32490;&#29366;&#24577;&#24378;&#24230;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#24773;&#32490;&#21644;&#31038;&#20132;&#20132;&#20114;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#19982;&#20854;&#20182;&#32508;&#21512;&#22522;&#20934;&#30456;&#20851;&#24615;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2312.06281</link><description>&lt;p&gt;
EQ-Bench:&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#21830;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models. (arXiv:2312.06281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06281
&lt;/p&gt;
&lt;p&gt;
EQ-Bench&#26159;&#19968;&#31181;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#21830;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#35282;&#33394;&#30340;&#24773;&#32490;&#29366;&#24577;&#24378;&#24230;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#24773;&#32490;&#21644;&#31038;&#20132;&#20132;&#20114;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#19982;&#20854;&#20182;&#32508;&#21512;&#22522;&#20934;&#30456;&#20851;&#24615;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EQ-Bench&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24773;&#21830;&#26041;&#38754;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35201;&#27714;LLMs&#39044;&#27979;&#23545;&#35805;&#20013;&#35282;&#33394;&#30340;&#24773;&#32490;&#29366;&#24577;&#30340;&#24378;&#24230;&#26469;&#35780;&#20272;LLMs&#29702;&#35299;&#22797;&#26434;&#24773;&#32490;&#21644;&#31038;&#20132;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#22522;&#20934;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#19981;&#21516;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;EQ-Bench&#19982;&#32508;&#21512;&#22810;&#39046;&#22495;&#22522;&#20934;&#65288;&#22914;MMLU&#65289;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65288;r=0.97&#65289;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#21487;&#33021;&#25429;&#25417;&#21040;&#20102;&#24191;&#27867;&#26234;&#33021;&#30340;&#30456;&#20284;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20351;&#29992;60&#20010;&#33521;&#35821;&#38382;&#39064;&#20135;&#29983;&#39640;&#24230;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;https://github.com/EQ-bench/EQ-Bench&#19978;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#29992;&#20110;&#33258;&#21160;&#21270;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#65292;&#20197;&#21450;https://eqbench.com&#19978;&#30340;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs). We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;SkateboardAI&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26368;&#37239;&#30340;&#28369;&#26495;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31181;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#20248;&#31168;&#30340;AI&#20307;&#32946;&#35009;&#21028;&#12290;</title><link>http://arxiv.org/abs/2311.11467</link><description>&lt;p&gt;
SkateboardAI&#65306;&#26368;&#37239;&#30340;&#28369;&#26495;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SkateboardAI: The Coolest Video Action Recognition for Skateboarding. (arXiv:2311.11467v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11467
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SkateboardAI&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26368;&#37239;&#30340;&#28369;&#26495;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31181;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#20248;&#31168;&#30340;AI&#20307;&#32946;&#35009;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;2021&#24180;&#19996;&#20140;&#22885;&#36816;&#20250;&#26368;&#37239;&#30340;&#28369;&#26495;&#36816;&#21160;&#33410;&#30446;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#31574;&#21010;&#20102;&#21407;&#22987;&#30340;&#29616;&#23454;&#19990;&#30028;&#35270;&#39057;&#25968;&#25454;&#38598;&#8220;SkateboardAI&#8221;&#65292;&#24182;&#19988;&#33258;&#34892;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19981;&#21516;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35782;&#21035;&#19981;&#21516;&#30340;&#25216;&#24039;&#12290;&#23545;&#20110;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#21035;&#24212;&#29992;&#20102;&#65288;1&#65289;CNN&#21644;LSTM&#65307;&#65288;2&#65289;CNN&#21644;BiLSTM&#65307;&#65288;3&#65289;&#24102;&#26377;&#25928;&#27880;&#24847;&#26426;&#21046;&#30340;CNN&#21644;BiLSTM&#65307;&#65288;4&#65289;&#22522;&#20110;Transformer&#30340;&#21160;&#20316;&#35782;&#21035;&#27969;&#31243;&#12290;&#36716;&#31227;&#21040;&#22810;&#27169;&#24577;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#8220;SkateboardAI&#8221;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21452;&#27969;Inflated-3D&#26550;&#26500;&#65292;&#24182;&#23558;&#20854;&#19982;&#21333;&#27169;&#24577;&#24773;&#20917;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#26368;&#37239;&#30340;&#28369;&#26495;&#27604;&#36187;&#24320;&#21457;&#20986;&#19968;&#27454;&#20248;&#31168;&#30340;AI&#20307;&#32946;&#35009;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic Games, we are the first to curate the original real-world video datasets "SkateboardAI" in the wild, even self-design and implement diverse uni-modal and multi-modal video action recognition approaches to recognize different tricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM; (2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4) Transformer-based action recognition pipeline. Transferred to the multi-modal conditions, we investigated the two-stream Inflated-3D architecture on "SkateboardAI" datasets to compare its performance with uni-modal cases. In sum, our objective is developing an excellent AI sport referee for the coolest skateboarding competitions.
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>Othello&#26159;&#19990;&#30028;&#19978;&#26368;&#22797;&#26434;&#21644;&#21463;&#27426;&#36814;&#30340;&#28216;&#25103;&#20043;&#19968;&#65292;&#32463;&#36807;&#35745;&#31639;&#35777;&#26126;&#21452;&#26041;&#29609;&#23478;&#30340;&#23436;&#32654;&#28216;&#25103;&#23558;&#23548;&#33268;&#24179;&#23616;&#12290;</title><link>http://arxiv.org/abs/2310.19387</link><description>&lt;p&gt;
Othello&#24050;&#34987;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Othello is Solved. (arXiv:2310.19387v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19387
&lt;/p&gt;
&lt;p&gt;
Othello&#26159;&#19990;&#30028;&#19978;&#26368;&#22797;&#26434;&#21644;&#21463;&#27426;&#36814;&#30340;&#28216;&#25103;&#20043;&#19968;&#65292;&#32463;&#36807;&#35745;&#31639;&#35777;&#26126;&#21452;&#26041;&#29609;&#23478;&#30340;&#23436;&#32654;&#28216;&#25103;&#23558;&#23548;&#33268;&#24179;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Othello&#26159;&#19990;&#30028;&#19978;&#26368;&#22797;&#26434;&#21644;&#21463;&#27426;&#36814;&#30340;&#28216;&#25103;&#20043;&#19968;&#65292;&#23578;&#26410;&#22312;&#35745;&#31639;&#26426;&#19978;&#35299;&#20915;&#12290; Othello&#22823;&#32422;&#26377;10&#30340;58&#27425;&#26041;&#20010;&#21487;&#33021;&#30340;&#28216;&#25103;&#35760;&#24405;&#21644;10&#30340;28&#27425;&#26041;&#20010;&#21487;&#33021;&#30340;&#28216;&#25103;&#20301;&#32622;&#12290;&#35299;&#20915;Othello&#30340;&#25361;&#25112;&#65292;&#21363;&#30830;&#23450;&#27809;&#26377;&#21452;&#26041;&#29609;&#23478;&#29359;&#38169;&#35823;&#26102;&#30340;&#28216;&#25103;&#32467;&#26524;&#65292;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23459;&#24067;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#37324;&#31243;&#30865;&#65306;Othello&#29616;&#24050;&#35299;&#20915;&#12290;&#36890;&#36807;&#35745;&#31639;&#35777;&#26126;&#65292;&#22914;&#26524;&#21452;&#26041;&#29609;&#23478;&#37117;&#20197;&#23436;&#32654;&#30340;&#26041;&#24335;&#36827;&#34892;&#28216;&#25103;&#65292;&#32467;&#26524;&#23558;&#26159;&#24179;&#23616;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#25628;&#32034;&#25216;&#26415;&#26500;&#24314;&#20102;&#24378;&#22823;&#30340;Othello&#36719;&#20214;&#12290;&#35299;&#20915;&#19968;&#20010;&#28216;&#25103;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#36719;&#20214;&#33021;&#22815;&#23436;&#32654;&#22320;&#36827;&#34892;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The game of Othello is one of the world's most complex and popular games that has yet to be computationally solved. Othello has roughly ten octodecillion (10 to the 58th power) possible game records and ten octillion (10 to the 28th power) possible game positions. The challenge of solving Othello, determining the outcome of a game with no mistake made by either player, has long been a grand challenge in computer science. This paper announces a significant milestone: Othello is now solved. It is computationally proved that perfect play by both players lead to a draw. Strong Othello software has long been built using heuristically designed search techniques. Solving a game provides a solution that enables the software to play the game perfectly.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.06452</link><description>&lt;p&gt;
&#29702;&#35299;RLHF&#23545;LLM&#27867;&#21270;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#22914;OpenAI&#30340;ChatGPT&#25110;Anthropic&#30340;Claude&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#24320;&#21457;&#26041;&#38754;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#23545;RLHF&#36807;&#31243;&#20013;&#27599;&#20010;&#38454;&#27573;&#30340;&#21033;&#19982;&#24330;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#38454;&#27573;&#65288;&#21363;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22870;&#21169;&#24314;&#27169;&#21644;RLHF&#65289;&#22914;&#20309;&#24433;&#21709;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65306;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#24773;&#26223;&#30340;&#32972;&#26223;&#19979;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#36755;&#20986;&#22810;&#26679;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#21508;&#31181;&#29992;&#20363;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25688;&#35201;&#21644;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#20013;&#23545;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21518;&#32773;&#38750;&#24120;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
&lt;/p&gt;</description></item><item><title>GMMFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;&#12290;&#23427;&#36890;&#36807;&#38544;&#24335;&#24314;&#27169;&#21098;&#36753;&#34920;&#31034;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#21098;&#36753;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#35821;&#20041;&#24046;&#24322;&#23548;&#33268;&#30340;&#31232;&#30095;&#23884;&#20837;&#31354;&#38388;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05195</link><description>&lt;p&gt;
GMMFormer: &#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#29992;&#20110;&#39640;&#25928;&#30340;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval. (arXiv:2310.05195v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05195
&lt;/p&gt;
&lt;p&gt;
GMMFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;&#12290;&#23427;&#36890;&#36807;&#38544;&#24335;&#24314;&#27169;&#21098;&#36753;&#34920;&#31034;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;&#21098;&#36753;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#35821;&#20041;&#24046;&#24322;&#23548;&#33268;&#30340;&#31232;&#30095;&#23884;&#20837;&#31354;&#38388;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#20010;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#37096;&#20998;&#30456;&#20851;&#35270;&#39057;&#26816;&#32034;&#65288;PRVR&#65289;&#26088;&#22312;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21253;&#21547;&#30456;&#20851;&#29255;&#27573;&#30340;&#26410;&#21098;&#36753;&#35270;&#39057;&#12290;&#23545;&#20110;PRVR&#65292;&#21098;&#36753;&#24314;&#27169;&#23545;&#20110;&#25429;&#25417;&#25991;&#26412;&#21644;&#35270;&#39057;&#20043;&#38388;&#30340;&#37096;&#20998;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;PRVR&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#25195;&#25551;&#30340;&#21098;&#36753;&#26500;&#24314;&#26469;&#23454;&#29616;&#26174;&#24335;&#21098;&#36753;&#24314;&#27169;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#24687;&#20887;&#20313;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#23384;&#20648;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;PRVR&#26041;&#27861;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GMMFormer&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;Transformer&#65292;&#23427;&#38544;&#24335;&#22320;&#24314;&#27169;&#20102;&#21098;&#36753;&#34920;&#31034;&#12290;&#22312;&#24103;&#20132;&#20114;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#32422;&#26463;&#65292;&#20351;&#27599;&#20010;&#24103;&#19987;&#27880;&#20110;&#20854;&#30456;&#37051;&#24103;&#32780;&#19981;&#26159;&#25972;&#20010;&#35270;&#39057;&#12290;&#28982;&#21518;&#29983;&#25104;&#30340;&#34920;&#31034;&#23558;&#21253;&#21547;&#22810;&#23610;&#24230;&#30340;&#21098;&#36753;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#24335;&#21098;&#36753;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;PRVR&#26041;&#27861;&#24573;&#35270;&#20102;&#19982;&#21516;&#19968;&#35270;&#39057;&#30456;&#20851;&#30340;&#25991;&#26412;&#26597;&#35810;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#23548;&#33268;&#31232;&#30095;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a text query, partially relevant video retrieval (PRVR) seeks to find untrimmed videos containing pertinent moments in a database. For PRVR, clip modeling is essential to capture the partial relationship between texts and videos. Current PRVR methods adopt scanning-based clip construction to achieve explicit clip modeling, which is information-redundant and requires a large storage overhead. To solve the efficiency problem of PRVR methods, this paper proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models clip representations implicitly. During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. Then generated representations will contain multi-scale clip information, achieving implicit clip modeling. In addition, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04171</link><description>&lt;p&gt;
&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26088;&#22312;&#21457;&#29616;&#27450;&#35784;&#32773;&#36890;&#36807;&#30041;&#19979;&#20551;&#35780;&#35770;&#25110;&#36827;&#34892;&#24322;&#24120;&#20132;&#26131;&#27450;&#39575;&#20854;&#20182;&#29992;&#25143;&#12290;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#31867;&#21035;&#65288;&#27450;&#35784;&#25110;&#27491;&#24120;&#65289;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#23454;&#38469;&#19990;&#30028;&#22270;&#34920;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#32858;&#21512;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#20989;&#25968;&#20026;&#27599;&#20010;&#20851;&#31995;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#32858;&#21512;&#36807;&#31243;&#20013;&#37319;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#30340;&#21078;&#26512;&#65292;&#25581;&#31034;&#20102;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#20063;&#21457;&#29616;&#20102;&#19982;&#27492;&#31639;&#27861;&#30456;&#20851;&#30340;&#26356;&#39640;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#36798;&#21040;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.00488</link><description>&lt;p&gt;
&#20851;&#20110;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#30340;&#35760;&#24518;&#21644;&#38544;&#31169;&#39118;&#38505;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Memorization and Privacy Risks of Sharpness Aware Minimization. (arXiv:2310.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#30340;&#21078;&#26512;&#65292;&#25581;&#31034;&#20102;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#20063;&#21457;&#29616;&#20102;&#19982;&#27492;&#31639;&#27861;&#30456;&#20851;&#30340;&#26356;&#39640;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#36798;&#21040;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#35774;&#35745;&#23547;&#27714;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20248;&#21270;&#20013;&#26356;&#24179;&#22374;&#30340;&#26497;&#20540;&#30340;&#31639;&#27861;&#25104;&#20026;&#28966;&#28857;&#65292;&#22240;&#20026;&#26377;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#36825;&#20250;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#35270;&#35282;&#26469;&#21078;&#26512;&#36825;&#20123;&#24615;&#33021;&#25910;&#30410;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#30456;&#23545;&#20110;&#26222;&#36890;SGD&#65292;&#23547;&#27714;&#26356;&#24179;&#22374;&#26497;&#20540;&#30340;&#31639;&#27861;&#22312;&#21738;&#20123;&#25968;&#25454;&#28857;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#25152;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#29305;&#21035;&#26174;&#33879;&#65292;&#36825;&#38656;&#35201;&#35760;&#24518;&#12290;&#36825;&#19968;&#35748;&#35782;&#24110;&#21161;&#25105;&#20204;&#25581;&#31034;&#19982;SAM&#30456;&#20851;&#30340;&#26356;&#39640;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#36890;&#36807;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many recent works, there is an increased focus on designing algorithms that seek flatter optima for neural network loss optimization as there is empirical evidence that it leads to better generalization performance in many datasets. In this work, we dissect these performance gains through the lens of data memorization in overparameterized models. We define a new metric that helps us identify which data points specifically do algorithms seeking flatter optima do better when compared to vanilla SGD. We find that the generalization gains achieved by Sharpness Aware Minimization (SAM) are particularly pronounced for atypical data points, which necessitate memorization. This insight helps us unearth higher privacy risks associated with SAM, which we verify through exhaustive empirical evaluations. Finally, we propose mitigation strategies to achieve a more desirable accuracy vs privacy tradeoff.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#24555;&#36895;&#30340;&#21387;&#32553;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21387;&#32553;&#22495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#24103;&#37319;&#26679;&#21644;&#20887;&#20313;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12867</link><description>&lt;p&gt;
&#20934;&#30830;&#24555;&#36895;&#30340;&#21387;&#32553;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Accurate and Fast Compressed Video Captioning. (arXiv:2309.12867v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12867
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#24555;&#36895;&#30340;&#21387;&#32553;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21387;&#32553;&#22495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#24103;&#37319;&#26679;&#21644;&#20887;&#20313;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20174;&#35299;&#30721;&#35270;&#39057;&#20013;&#39318;&#20808;&#37319;&#26679;&#35270;&#39057;&#24103;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22788;&#29702;&#65288;&#20363;&#22914;&#65292;&#29305;&#24449;&#25552;&#21462;&#21644;&#23383;&#24149;&#27169;&#22411;&#23398;&#20064;&#65289;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25163;&#21160;&#24103;&#37319;&#26679;&#21487;&#33021;&#20250;&#24573;&#30053;&#35270;&#39057;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#37319;&#26679;&#24103;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#25512;&#29702;&#30340;&#25928;&#29575;&#20302;&#19979;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#21387;&#32553;&#22495;&#30340;&#19981;&#21516;&#35282;&#24230;&#30740;&#31350;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#30340;&#27969;&#27700;&#32447;&#20855;&#26377;&#22810;&#37325;&#20248;&#21183;&#65306;1&#65289;&#19982;&#35299;&#30721;&#35270;&#39057;&#30340;&#21407;&#22987;&#22270;&#20687;&#30456;&#27604;&#65292;&#30001;I-&#24103;&#12289;&#36816;&#21160;&#30690;&#37327;&#21644;&#27531;&#24046;&#26500;&#25104;&#30340;&#21387;&#32553;&#35270;&#39057;&#26356;&#20855;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19987;&#29992;&#27169;&#22411;&#35774;&#35745;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#25972;&#20010;&#35270;&#39057;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#37319;&#26679;&#65307;2&#65289;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#25928;&#29575;&#26356;&#39640;&#65292;&#22240;&#20026;&#22788;&#29702;&#30340;&#20449;&#24687;&#26356;&#23569;&#19988;&#26356;&#23569;&#20887;&#20313;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing video captioning approaches typically require to first sample video frames from a decoded video and then conduct a subsequent process (e.g., feature extraction and/or captioning model learning). In this pipeline, manual frame sampling may ignore key information in videos and thus degrade performance. Additionally, redundant information in the sampled frames may result in low efficiency in the inference of video captioning. Addressing this, we study video captioning from a different perspective in compressed domain, which brings multi-fold advantages over the existing pipeline: 1) Compared to raw images from the decoded video, the compressed video, consisting of I-frames, motion vectors and residuals, is highly distinguishable, which allows us to leverage the entire video for learning without manual sampling through a specialized model design; 2) The captioning model is more efficient in inference as smaller and less redundant information is processed. We propose a simple yet e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;CMAB&#28608;&#21169;&#65288;CACI&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#19968;&#20010;&#31934;&#32454;&#21010;&#20998;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#26377;&#25928;&#22320;&#28608;&#21169;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#39044;&#31639;&#30340;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2309.12113</link><description>&lt;p&gt;
&#21033;&#29992;&#26377;&#38480;&#39044;&#31639;&#28608;&#21169;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#30340;&#20247;&#24863;&#30693;&#65306;&#20174;&#31163;&#32447;&#21644;&#22312;&#32447;&#35270;&#35282;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives. (arXiv:2309.12113v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;CMAB&#28608;&#21169;&#65288;CACI&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#19968;&#20010;&#31934;&#32454;&#21010;&#20998;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#26377;&#25928;&#22320;&#28608;&#21169;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#39044;&#31639;&#30340;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#30340;&#25552;&#26696;&#36890;&#36807;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#26469;&#35299;&#20915;&#24037;&#20316;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26631;&#20934;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;CMAB&#65289;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#24037;&#20316;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#24403;&#24037;&#20316;&#32773;&#25968;&#37327;&#24040;&#22823;&#32780;&#39044;&#31639;&#26377;&#38480;&#26102;&#65292;&#21487;&#33021;&#26080;&#27861;&#23545;&#20010;&#20307;&#24037;&#20316;&#32773;&#36827;&#34892;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;&#30340;CMAB&#36890;&#24120;&#20551;&#35774;&#24037;&#20316;&#32773;&#22987;&#32456;&#30041;&#22312;&#31995;&#32479;&#20013;&#65292;&#32780;&#24037;&#20316;&#32773;&#21487;&#33021;&#22312;&#26102;&#38388;&#19978;&#21152;&#20837;&#25110;&#31163;&#24320;&#31995;&#32479;&#65292;&#22240;&#27492;&#22312;&#24037;&#20316;&#32773;&#31163;&#24320;&#21518;&#65292;&#25105;&#20204;&#23398;&#21040;&#30340;&#23545;&#20110;&#21333;&#20010;&#24037;&#20316;&#32773;&#30340;&#30693;&#35782;&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;CMAB&#28608;&#21169;&#65288;CACI&#65289;&#26426;&#21046;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#21033;&#29992;&#20102;&#19968;&#20010;&#31934;&#24515;&#21010;&#20998;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#20010;&#20307;&#24037;&#20316;&#32773;&#65292;&#20197;&#26377;&#25928;&#22320;&#28608;&#21169;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#39044;&#31639;&#30340;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the uncertainties of the workers can be addressed by the standard Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through a trade-off between exploration and exploitation, we may not have sufficient budget to enable the trade-off among the individual workers, especially when the number of the workers is huge while the budget is limited. Moreover, the standard CMAB usually assumes the workers always stay in the system, whereas the workers may join in or depart from the system over time, such that what we have learnt for an individual worker cannot be applied after the worker leaves. To address the above challenging issues, in this paper, we first propose an off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in leveraging the exploration-exploitation trade-off in a elaborately partitioned context space instead of the individual workers, to effectively incentivize the massive unknown workers with very limited budget. We also extend t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.08163</link><description>&lt;p&gt;
&#30740;&#31350;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#27979;&#37327;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models. (arXiv:2309.08163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08163
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#21457;&#23637;&#65292;&#21508;&#31181;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#30340;&#24515;&#29702;&#24037;&#20855;&#26469;&#37327;&#21270;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#20154;&#26684;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20851;&#20110;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#19977;&#20010;&#19981;&#21516;&#35770;&#25991;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;&#21516;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#25552;&#31034;&#23548;&#33268;&#20102;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#12290;&#36825;&#19968;&#31616;&#21333;&#27979;&#35797;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#24471;&#20998;&#21462;&#20915;&#20110;&#25552;&#31034;&#32773;&#30340;&#20027;&#35266;&#36873;&#25321;&#12290;&#30001;&#20110;&#25105;&#20204;&#19981;&#30693;&#36947;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#24471;&#20998;&#30340;&#30495;&#23454;&#20540;&#65292;&#22240;&#20026;&#27492;&#31867;&#38382;&#39064;&#27809;&#26377;&#27491;&#30830;&#31572;&#26696;&#65292;&#25152;&#20197;&#26080;&#27861;&#22768;&#26126;&#26576;&#20010;&#25552;&#31034;&#27604;&#20854;&#20182;&#25552;&#31034;&#26356;&#27491;&#30830;&#25110;&#26356;&#19981;&#27491;&#30830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#26684;&#36873;&#39033;&#39034;&#24207;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of "personality" of LLMs using personality self-assessment tests. In this paper, we take three such studies on personality measurement of LLMs that use personality self-assessment tests created to study human behavior. We use the prompts used in these three different papers to measure the personality of the same LLM. We find that all three prompts lead very different personality scores. This simple test reveals that personality self-assessment scores in LLMs depend on the subjective choice of the prompter. Since we don't know the ground truth value of personality scores for LLMs as there is no correct answer to such questions, there's no way of claiming if one prompt is more or less correct than the other. We then introduce the property of option order symmetry for persona
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#35206;&#30422;&#38598;&#21512;&#65288;MCS&#65289;&#27010;&#24565;&#65292;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#27169;&#25311;MCS&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#33258;&#32452;&#32455;&#22242;&#38431;&#21327;&#20316;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09595</link><description>&lt;p&gt;
&#26368;&#23567;&#35206;&#30422;&#38598;&#21512;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#33258;&#32452;&#32455;&#22242;&#38431;&#21327;&#20316;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents. (arXiv:2308.09595v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09595
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#35206;&#30422;&#38598;&#21512;&#65288;MCS&#65289;&#27010;&#24565;&#65292;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#27169;&#25311;MCS&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#33258;&#32452;&#32455;&#22242;&#38431;&#21327;&#20316;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21512;&#20316;&#20249;&#20276;&#21487;&#33021;&#37319;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#21512;&#20316;&#32422;&#23450;&#65292;&#19982;&#26410;&#30693;&#30340;&#20195;&#29702;&#21644;&#20154;&#31867;&#21512;&#20316;&#20855;&#26377;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#33258;&#32452;&#32455;&#22242;&#38431;&#21327;&#20316;&#65288;AHT&#65289;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#19982;&#36890;&#36807;&#26368;&#22823;&#21270;&#29305;&#23450;&#22810;&#26679;&#24615;&#24230;&#37327;&#33719;&#24471;&#30340;&#22810;&#26679;&#30340;&#38431;&#21451;&#31574;&#30053;&#32676;&#36827;&#34892;&#21512;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#24182;&#19981;&#24635;&#26159;&#22312;&#25152;&#26377;&#21512;&#20316;&#38382;&#39064;&#20013;&#26368;&#22823;&#21270;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26368;&#22823;&#21270;AHT&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#38656;&#35201;&#23427;&#27169;&#25311;&#26368;&#23567;&#35206;&#30422;&#38598;&#65288;MCS&#65289;&#20013;&#30340;&#31574;&#30053;&#65292;&#21363;&#23545;&#29615;&#22659;&#20013;&#30340;&#20219;&#20309;&#21512;&#20316;&#20249;&#20276;&#31574;&#30053;&#30340;&#26368;&#20248;&#21709;&#24212;&#31574;&#30053;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;L-BRDiv&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29983;&#25104;&#19968;&#32452;&#38431;&#21451;&#31574;&#30053;&#65292;&#29992;&#20110;AHT&#35757;&#32451;&#26102;&#40723;&#21169;&#20195;&#29702;&#27169;&#25311;MCS&#20013;&#30340;&#31574;&#30053;&#12290;L-BRDiv&#36890;&#36807;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#20849;&#21516;&#35757;&#32451;AHT&#35757;&#32451;&#30340;&#38431;&#21451;&#31574;&#30053;&#65292;&#20197;&#21450;&#36817;&#20284;AHT&#20195;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, prior heuristic-based diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05134</link><description>&lt;p&gt;
TIAM -- &#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#35780;&#20272;&#20854;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#28210;&#26579;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#37325;&#35201;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#31561;&#39069;&#22806;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#20174;&#38543;&#26426;&#36215;&#22987;&#28857;&#24320;&#22987;&#30340;&#65292;&#20294;&#36890;&#24120;&#19981;&#32771;&#34385;&#36825;&#19968;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#25552;&#31034;&#20013;&#25351;&#23450;&#30340;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#26356;&#22909;&#22320;&#25551;&#36848;&#23545;&#40784;&#24615;&#65292;&#21253;&#25324;&#25351;&#23450;&#23545;&#35937;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;T2I&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39069;&#22806;&#32467;&#26524;&#65292;&#21363;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
&lt;/p&gt;</description></item><item><title>LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17408</link><description>&lt;p&gt;
LMBot: &#23558;&#22270;&#24418;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17408
&lt;/p&gt;
&lt;p&gt;
LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#34892;&#20026;&#32773;&#20351;&#29992;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#26469;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25805;&#32437;&#33286;&#35770;&#65292;&#25512;&#29305;&#26426;&#22120;&#20154;&#30340;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#25512;&#29702;&#20381;&#36182;&#20110;&#36317;&#31163;&#30446;&#26631;&#29992;&#25143;&#22810;&#36339;&#30340;&#37051;&#23621;&#29992;&#25143;&#65292;&#24182;&#19988;&#33719;&#21462;&#37051;&#23621;&#29992;&#25143;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#19978;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#22270;&#24418;&#32467;&#26500;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;LMBot&#65292;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;&#20197;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;LMBot&#23545;&#22522;&#20110;&#22270;&#24418;&#21644;&#19981;&#20351;&#29992;&#22270;&#24418;&#30340;&#25968;&#25454;&#38598;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#19968;&#27573;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
As malicious actors employ increasingly advanced and widespread bots to disseminate misinformation and manipulate public opinion, the detection of Twitter bots has become a crucial task. Though graph-based Twitter bot detection methods achieve state-of-the-art performance, we find that their inference depends on the neighbor users multi-hop away from the targets, and fetching neighbors is time-consuming and may introduce bias. At the same time, we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment. Inspired by this finding, we propose a novel bot detection framework LMBot that distills the knowledge of graph neural networks (GNNs) into language models (LMs) for graph-less deployment in Twitter bot detection to combat the challenge of data dependency. Moreover, LMBot is compatible with graph-based and graph-less datasets. Specifically, we first represent each user as a tex
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;FlaSH&#65292;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#12290;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#25968;&#25454;&#37327;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16914</link><description>&lt;p&gt;
&#35745;&#31639;&#36741;&#21161;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#30340;&#36136;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Computationally Assisted Quality Control for Public Health Data Streams. (arXiv:2306.16914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16914
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;FlaSH&#65292;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#12290;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#25968;&#25454;&#37327;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65288;&#22914;COVID-19&#30149;&#20363;&#65289;&#38459;&#30861;&#20102;&#20844;&#20849;&#21355;&#29983;&#21033;&#30410;&#30456;&#20851;&#32773;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#12290;&#23454;&#26102;&#29983;&#25104;&#30340;&#35745;&#31639;&#26426;&#21015;&#34920;&#21487;&#20197;&#24110;&#21161;&#19987;&#23478;&#35780;&#23457;&#21592;&#35782;&#21035;&#25104;&#21315;&#19978;&#19975;&#20010;&#27599;&#26085;&#26356;&#26032;&#30340;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#26368;&#37325;&#35201;&#30340;&#24322;&#24120;&#25968;&#25454;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#25968;&#25454;&#37327;&#25110;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FlaSH&#65288;&#26071;&#26631;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#29992;&#25143;&#29992;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#36825;&#20123;&#32479;&#35745;&#29305;&#24615;&#12290;&#22312;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#20102;FlaSH&#21644;&#29616;&#26377;&#26041;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65289;&#65292;FlaSH&#36866;&#24212;&#20102;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#37327;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#22312;&#24179;&#22343;&#20934;&#30830;&#24230;&#19978;&#36798;&#21040;&#25110;&#36229;&#36807;&#65292;&#24182;&#19988;&#21487;&#20197;&#35782;&#21035;&#20986;&#24322;&#24120;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularities in public health data streams (like COVID-19 Cases) hamper data-driven decision-making for public health stakeholders. A real-time, computer-generated list of the most important, outlying data points from thousands of daily-updated public health data streams could assist an expert reviewer in identifying these irregularities. However, existing outlier detection frameworks perform poorly on this task because they do not account for the data volume or for the statistical properties of public health streams. Accordingly, we developed FlaSH (Flagging Streams in public Health), a practical outlier detection framework for public health data users that uses simple, scalable models to capture these statistical properties explicitly. In an experiment where human experts evaluate FlaSH and existing methods (including deep learning approaches), FlaSH scales to the data volume of this task, matches or exceeds these other methods in mean accuracy, and identifies the outlier points th
&lt;/p&gt;</description></item><item><title>LaDe&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26411;&#31471;&#37197;&#36865;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#25968;&#30334;&#19975;&#20010;&#26469;&#33258;&#20135;&#19994;&#30028;&#30340;&#21253;&#35065;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20449;&#24687;&#21644;&#22810;&#26679;&#24615;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.10675</link><description>&lt;p&gt;
LaDe: &#20135;&#19994;&#30028;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#26411;&#31471;&#37197;&#36865;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LaDe: The First Comprehensive Last-mile Delivery Dataset from Industry. (arXiv:2306.10675v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10675
&lt;/p&gt;
&lt;p&gt;
LaDe&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26411;&#31471;&#37197;&#36865;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#25968;&#30334;&#19975;&#20010;&#26469;&#33258;&#20135;&#19994;&#30028;&#30340;&#21253;&#35065;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20449;&#24687;&#21644;&#22810;&#26679;&#24615;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26411;&#31471;&#37197;&#36865;&#25968;&#25454;&#38598;&#23545;&#20110;&#29289;&#27969;&#12289;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30340;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36804;&#20170;&#24050;&#32463;&#24320;&#21457;&#20102;&#22823;&#37327;&#31639;&#27861;&#65292;&#20294;&#23578;&#26080;&#20844;&#35748;&#30340;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#26411;&#31471;&#37197;&#36865;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LaDe&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26411;&#31471;&#37197;&#36865;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#25968;&#30334;&#19975;&#20010;&#26469;&#33258;&#20135;&#19994;&#30028;&#30340;&#21253;&#35065;&#12290;LaDe&#20855;&#26377;&#19977;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;(1)&#22823;&#35268;&#27169;&#12290;&#23427;&#28041;&#21450;&#21040;6&#20010;&#26376;&#30340;&#30495;&#23454;&#36816;&#33829;&#20013;&#30340;10,677k&#20010;&#21253;&#35065;&#21644;21k&#20010;&#24555;&#36882;&#21592;&#12290;(2)&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#23427;&#25552;&#20379;&#21407;&#22987;&#21253;&#35065;&#20449;&#24687;&#65292;&#22914;&#20301;&#32622;&#21644;&#26102;&#38388;&#35201;&#27714;&#65292;&#20197;&#21450;&#20219;&#21153;&#20107;&#20214;&#20449;&#24687;&#65292;&#35760;&#24405;&#20102;&#24555;&#36882;&#21592;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#20219;&#21153;&#25509;&#21463;&#21644;&#23436;&#25104;&#12290;(3)&#22810;&#26679;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#22330;&#26223;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#21253;&#35065;&#30340;&#21462;&#36865;&#21644;&#26469;&#33258;&#22810;&#20010;&#22478;&#24066;&#30340;&#25968;&#25454;&#65292;&#27599;&#20010;&#22478;&#24066;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#26102;&#31354;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world last-mile delivery datasets are crucial for research in logistics, supply chain management, and spatio-temporal data mining. Despite a plethora of algorithms developed to date, no widely accepted, publicly available last-mile delivery dataset exists to support research in this field. In this paper, we introduce \texttt{LaDe}, the first publicly available last-mile delivery dataset with millions of packages from the industry. LaDe has three unique characteristics: (1) Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation. (2) Comprehensive information. It offers original package information, such as its location and time requirements, as well as task-event information, which records when and where the courier is while events such as task-accept and task-finish events happen. (3) Diversity. The dataset includes data from various scenarios, including package pick-up and delivery, and from multiple cities, each with its unique spatio-tem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20363;&#22914;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21270;&#23398;&#20998;&#23376;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#32467;&#26500;&#65292;&#20854;&#34892;&#20026;&#21160;&#24577;&#21464;&#21270;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#35745;&#31639;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#39640;&#26031;&#20998;&#24067;&#65292;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#20998;&#23376;&#25152;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#34920;&#38754;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35266;&#23519;&#21040;&#65292;&#21452;&#26354;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#23618;&#32467;&#26500;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#19988;&#26356;&#23481;&#26131;&#34987;&#25429;&#25417;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#21452;&#26354;&#23884;&#20837;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#21452;&#26354;&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#65292;&#21363;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EmotionGesture&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#29983;&#21160;&#22810;&#26679;&#30340;&#24773;&#24863;&#20849;&#35821;3D&#25163;&#21183;&#12290;&#36890;&#36807;&#24773;&#24863;-&#33410;&#22863;&#25366;&#25496;&#27169;&#22359;&#25552;&#21462;&#24773;&#24863;&#21644;&#38899;&#39057;&#33410;&#22863;&#29305;&#24449;&#65292;&#24182;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#65307;&#28982;&#21518;&#20351;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25552;&#31034;&#22120;&#20174;&#32473;&#23450;&#30340;&#21021;&#22987;&#23039;&#21183;&#29983;&#25104;&#26410;&#26469;&#30340;&#25163;&#21183;&#65292;&#23454;&#29616;&#31354;&#38388;-&#26102;&#38388;&#19968;&#33268;&#30340;&#23039;&#21183;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18891</link><description>&lt;p&gt;
EmotionGesture&#65306;&#38899;&#39057;&#39537;&#21160;&#30340;&#22810;&#26679;&#21270;&#24773;&#24863;&#20849;&#35821;3D&#25163;&#21183;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation. (arXiv:2305.18891v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EmotionGesture&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#29983;&#21160;&#22810;&#26679;&#30340;&#24773;&#24863;&#20849;&#35821;3D&#25163;&#21183;&#12290;&#36890;&#36807;&#24773;&#24863;-&#33410;&#22863;&#25366;&#25496;&#27169;&#22359;&#25552;&#21462;&#24773;&#24863;&#21644;&#38899;&#39057;&#33410;&#22863;&#29305;&#24449;&#65292;&#24182;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#65307;&#28982;&#21518;&#20351;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25552;&#31034;&#22120;&#20174;&#32473;&#23450;&#30340;&#21021;&#22987;&#23039;&#21183;&#29983;&#25104;&#26410;&#26469;&#30340;&#25163;&#21183;&#65292;&#23454;&#29616;&#31354;&#38388;-&#26102;&#38388;&#19968;&#33268;&#30340;&#23039;&#21183;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#29983;&#21160;&#22810;&#26679;&#30340;3D&#20849;&#35821;&#25163;&#21183;&#23545;&#20110;&#32473;&#34394;&#25311;&#35282;&#33394;&#27880;&#20837;&#29983;&#27668;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#25163;&#21183;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#35270;&#24773;&#24863;&#26159;&#30495;&#23454;&#20849;&#35821;&#25163;&#21183;&#29983;&#25104;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmotionGesture&#65292;&#19968;&#31181;&#20174;&#38899;&#39057;&#20013;&#21512;&#25104;&#29983;&#21160;&#22810;&#26679;&#30340;&#24773;&#24863;&#20849;&#35821;3D&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#12290;&#32771;&#34385;&#21040;&#24773;&#24863;&#24120;&#24120;&#19982;&#35821;&#38899;&#33410;&#22863;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#24773;&#24863;-&#33410;&#22863;&#25366;&#25496;&#27169;&#22359;&#65288;EBM&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#36716;&#24405;&#30340;&#35270;&#35273;-&#33410;&#22863;&#23545;&#40784;&#25552;&#21462;&#24773;&#24863;&#21644;&#38899;&#39057;&#33410;&#22863;&#29305;&#24449;&#65292;&#24182;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21021;&#22987;&#23039;&#21183;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25552;&#31034;&#22120;&#65288;STP&#65289;&#65292;&#20174;&#32473;&#23450;&#30340;&#21021;&#22987;&#23039;&#21183;&#29983;&#25104;&#26410;&#26469;&#30340;&#25163;&#21183;&#12290;STP&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#21021;&#22987;&#23039;&#21183;&#21644;&#26410;&#26469;&#25163;&#21183;&#20043;&#38388;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#65292;&#20174;&#32780;&#20135;&#29983;&#31354;&#38388;-&#26102;&#38388;&#19968;&#33268;&#30340;&#23039;&#21183;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating vivid and diverse 3D co-speech gestures is crucial for various applications in animating virtual avatars. While most existing methods can generate gestures from audio directly, they usually overlook that emotion is one of the key factors of authentic co-speech gesture generation. In this work, we propose EmotionGesture, a novel framework for synthesizing vivid and diverse emotional co-speech 3D gestures from audio. Considering emotion is often entangled with the rhythmic beat in speech audio, we first develop an Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features as well as model their correlation via a transcript-based visual-rhythm alignment. Then, we propose an initial pose based Spatial-Temporal Prompter (STP) to generate future gestures from the given initial poses. STP effectively models the spatial-temporal correlations between the initial poses and the future gestures, thus producing the spatial-temporal coherent pose prompt. Once we obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.09806</link><description>&lt;p&gt;
Logit-Q&#21160;&#21147;&#23398;&#23545;&#20110;&#38543;&#26426;&#22242;&#38431;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Logit-Q Dynamics for Efficient Learning in Stochastic Teams. (arXiv:2302.09806v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#19968;&#20010;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;Logit-Q&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Logit-Q&#21160;&#21147;&#23398;&#23545;&#32431;&#23450;&#24577;&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21160;&#21147;&#23398;&#22312;&#22870;&#21169;&#20989;&#25968;&#23548;&#33268;&#28508;&#22312;&#21338;&#24328;&#30340;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#28982;&#32780;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#29366;&#24577;&#36716;&#25442;&#36229;&#20986;&#38543;&#26426;&#22242;&#38431;&#12290;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#21160;&#21147;&#23398;&#19982;&#19968;&#20010;&#34394;&#26500;&#30340;&#22330;&#26223;&#36817;&#20284;&#65292;&#20854;&#20013;Q&#20989;&#25968;&#20272;&#35745;&#20165;&#22312;&#26377;&#38480;&#38271;&#24230;&#30340;&#32426;&#20803;&#20013;&#26159;&#23450;&#24577;&#30340;&#65292;&#20165;&#29992;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#35201;&#22330;&#26223;&#21644;&#34394;&#26500;&#22330;&#26223;&#20013;&#30340;&#21160;&#21147;&#23398;&#32806;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#36825;&#20004;&#20010;&#22330;&#26223;&#30001;&#20110;&#36880;&#27493;&#20943;&#23567;&#30340;&#27493;&#38271;&#32780;&#36234;&#26469;&#36234;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two logit-Q learning dynamics combining the classical and independent log-linear learning updates with an on-policy value iteration update for efficient learning in stochastic games. We show that the logit-Q dynamics presented reach (near) efficient equilibrium in stochastic teams. We quantify a bound on the approximation error. We also show the rationality of the logit-Q dynamics against agents following pure stationary strategies and the convergence of the dynamics in stochastic games where the reward functions induce potential games, yet only a single agent controls the state transitions beyond stochastic teams. The key idea is to approximate the dynamics with a fictional scenario where the Q-function estimates are stationary over finite-length epochs only for analysis. We then couple the dynamics in the main and fictional scenarios to show that these two scenarios become more and more similar across epochs due to the vanishing step size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2212.03932</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#34892;&#20026;&#31574;&#30053;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#65292;&#38656;&#35201;&#35780;&#20272;&#30446;&#26631;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#38656;&#35201;&#20351;&#29992;&#30001;&#34892;&#20026;&#31574;&#30053;&#37319;&#38598;&#30340;&#26679;&#26412;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#27010;&#29575;&#27604;&#20540;&#30340;&#20056;&#31215;&#32780;&#23548;&#33268;&#26041;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#28041;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#20986;&#29616;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28508;&#22312;&#29305;&#24449;&#20998;&#35299;&#20026;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#35299;&#34920;&#31034;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26131;&#20110;&#35299;&#37322;&#21644;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00086</link><description>&lt;p&gt;
&#35299;&#24320; (&#19981;)&#21487;&#25511;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangled (Un)Controllable Features. (arXiv:2211.00086v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28508;&#22312;&#29305;&#24449;&#20998;&#35299;&#20026;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#35299;&#34920;&#31034;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26131;&#20110;&#35299;&#37322;&#21644;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#39640;&#32500;&#29366;&#24577;&#30340;MDP&#32972;&#26223;&#19979;&#65292;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#22312;&#21407;&#22987;&#36755;&#20837;&#31354;&#38388;&#30340;&#21387;&#32553;&#12289;&#20302;&#32500;&#34920;&#31034;&#19978;&#36816;&#34892;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33719;&#24471;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#23398;&#20064;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#36890;&#24120;&#32570;&#20047;&#23545;&#19981;&#21516;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#28508;&#22312;&#29305;&#24449;&#20998;&#35299;&#20026;&#21487;&#25511;&#21644;&#19981;&#21487;&#25511;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20998;&#21306;&#34920;&#31034;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#29615;&#22659;&#20013;&#26159;&#26131;&#20110;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#22312;&#20998;&#31163;&#30340;&#21487;&#25511;&#28508;&#22312;&#20998;&#21306;&#20013;&#65292;&#33021;&#22815;&#22312;&#19968;&#32452;&#31243;&#24207;&#29983;&#25104;&#30340;&#36855;&#23467;&#29615;&#22659;&#20013;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of MDPs with high-dimensional states, downstream tasks are predominantly applied on a compressed, low-dimensional representation of the original input space. A variety of learning objectives have therefore been used to attain useful representations. However, these representations usually lack interpretability of the different features. We present a novel approach that is able to disentangle latent features into a controllable and an uncontrollable partition. We illustrate that the resulting partitioned representations are easily interpretable on three types of environments and show that, in a distribution of procedurally generated maze environments, it is feasible to interpretably employ a planning algorithm in the isolated controllable latent partition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;AIS&#25968;&#25454;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#26174;&#24335;&#32771;&#34385;&#24322;&#36136;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#21464;&#21387;&#22120;&#32593;&#32476;&#26469;&#39044;&#27979;&#33337;&#33334;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2109.03958</link><description>&lt;p&gt;
TrAISformer&#8212;&#8212;&#29992;&#20110;AIS&#36712;&#36857;&#39044;&#27979;&#30340;&#29983;&#25104;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
TrAISformer-A generative transformer for AIS trajectory prediction. (arXiv:2109.03958v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;AIS&#25968;&#25454;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#26174;&#24335;&#32771;&#34385;&#24322;&#36136;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#21464;&#21387;&#22120;&#32593;&#32476;&#26469;&#39044;&#27979;&#33337;&#33334;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33337;&#33334;&#22312;&#26410;&#26469;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#20301;&#32622;&#26159;&#35768;&#22810;&#28023;&#20107;&#24212;&#29992;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#34429;&#28982;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#65288;AIS&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#65292;&#20294;&#20351;&#29992;AIS&#25968;&#25454;&#26469;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#30340;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20063;&#26159;&#22914;&#27492;&#65292;&#22240;&#20026;&#36816;&#21160;&#25968;&#25454;&#26412;&#36136;&#19978;&#20855;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;AIS&#25968;&#25454;&#31163;&#25955;&#21270;&#39640;&#32500;&#34920;&#31034;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26174;&#24335;&#32771;&#34385;&#24322;&#36136;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#8212;&#8212;TrAISformer&#8212;&#8212;&#26159;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#22312;&#25152;&#25552;&#20986;&#30340;&#20016;&#23500;&#31354;&#38388;&#20013;&#25552;&#21462;AIS&#36712;&#36857;&#30340;&#38271;&#26399;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#20960;&#20010;&#23567;&#26102;&#21518;&#33337;&#33334;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#30495;&#23454;AIS&#25968;&#25454;&#19978;&#25253;&#21578;&#23454;&#39564;&#32467;&#26524;&#12290;TrAISformer&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of vessel positions at a specified point in the future is a fundamental aspect of many maritime applications. While Automatic Identification System (AIS) provides a rich source of information to enable this task, vessel trajectory forecasting using AIS data remains formidably challenging, even for modern machine learning/deep learning, because of the complexity and multimodality inherent in motion data. In this paper, we address these challenges by introducing a novel discrete, high-dimensional representation of AIS data and a new loss function to explicitly account for heterogeneity and multimodality. The proposed model -- referred to as TrAISformer -- is a modified transformer network that extracts long-term correlations of AIS trajectories in the proposed enriched space to forecast the positions of vessels several hours into the future. We report experimental results on publicly available, real AIS data. TrAISformer significantly outperforms state-of-the-art methods a
&lt;/p&gt;</description></item></channel></rss>