<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01476</link><description>&lt;p&gt;
&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#38656;&#35201;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#35299;&#20915;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23558;&#23545;&#31216;&#26680;&#24212;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#19979;&#30340;&#27880;&#24847;&#21147;&#26680;&#65307;&#28982;&#32780;&#65292;&#24573;&#30053;&#20102;&#27880;&#24847;&#21147;&#26680;&#26412;&#36136;&#19978;&#26159;&#19981;&#23545;&#31216;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25512;&#23548;&#20986;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;GP&#21518;&#39564;&#30340;&#22797;&#26434;&#24230;&#20173;&#28982;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464; &#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;KEP-SVGP&#65292;i&#65289;&#30001;&#20110;&#19982;&#27880;&#24847;&#21147;&#26680;&#30340;KSVD&#30456;&#23545;&#24212;&#30340;&#20004;&#32452;&#22855;&#24322;&#21521;&#37327;&#24341;&#23548;&#30340;SVGP&#23545;&#23436;&#20840;&#34920;&#24449;&#20102;&#19981;&#23545;&#31216;&#24615;&#65307;ii&#65289;&#20165;&#20351;&#29992;&#23569;&#37327;&#19982;KSVD&#30456;&#23545;&#24212;&#30340;&#20276;&#38543;&#29305;&#24449;&#20989;&#25968;&#65292;&#25512;&#23548;SVGP&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00195</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#22810;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multiple-policy Evaluation via Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#32473;&#23450;&#19968;&#32452; $K$ &#20010;&#30446;&#26631;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#20197;&#33267;&#23569; $1-\delta$ &#30340;&#27010;&#29575;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65288;&#26399;&#26395;&#24635;&#22870;&#21169;&#65289;&#36798;&#21040;&#31934;&#24230; $\epsilon$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20174;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#26469;&#21516;&#26102;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;$\mathrm{CAESAR}$ &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20197;&#38543;&#30528; $\tilde{O}(\frac{1}{\epsilon})$ &#32553;&#25918;&#30340;&#20302;&#35746;&#21333;&#37319;&#26679;&#22797;&#26434;&#24615;&#29575;&#20135;&#29983;&#30446;&#26631;&#31574;&#30053;&#30340;&#35775;&#38382;&#20998;&#24067;&#30340;&#31895;&#30053;&#20272;&#35745;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#36880;&#27493;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#26469;&#35745;&#31639;&#25152;&#26377;&#30446;&#26631;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00195v1 Announce Type: cross  Abstract: In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.18969</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models from Concept to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18969
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#26497;&#22823;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#26368;&#21021;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;GPT&#31995;&#21015;&#12290;&#36825;&#39033;&#25506;&#32034;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#39537;&#21160;&#24037;&#20855;&#22312;&#25913;&#38761;&#20256;&#32479;&#32534;&#30721;&#21644;&#38382;&#39064;&#35299;&#20915;&#31561;&#20219;&#21153;&#19978;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#36328;&#36234;&#19981;&#21516;&#34892;&#19994;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#24320;&#36767;&#26032;&#36335;&#24452;&#12290;&#20174;&#20195;&#30721;&#35299;&#37322;&#21644;&#22270;&#20687;&#25551;&#36848;&#21040;&#20419;&#36827;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#25645;&#24314;&#21644;&#25512;&#36827;&#35745;&#31639;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20307;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;</title><link>https://arxiv.org/abs/2403.07008</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20215;&#27491;&#30830;: &#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoEval Done Right: Using Synthetic Data for Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#20351;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21487;&#20197;&#20351;&#29992;AI&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#27492;&#31867;&#30446;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#37327;&#65292;&#36825;&#19968;&#36807;&#31243;&#31216;&#20026;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20559;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
&lt;/p&gt;</description></item><item><title>&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2403.02118</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#38754;&#21521;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#38544;&#24335;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Towards Implicit Prompt For Text-To-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26174;&#24335;&#25552;&#31034;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#25552;&#31034;&#65288;&#26263;&#31034;&#30446;&#26631;&#32780;&#19981;&#26126;&#30830;&#25552;&#21040;&#65289;&#12290;&#36825;&#20123;&#25552;&#31034;&#21487;&#33021;&#28040;&#38500;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24212;&#29992;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24403;&#19979;T2I&#27169;&#22411;&#26397;&#30528;&#38544;&#24335;&#25552;&#31034;&#30340;&#29616;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ImplicitBench&#30340;&#22522;&#20934;&#65292;&#24182;&#23545;&#27969;&#34892;&#30340;T2I&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25910;&#38598;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#36229;&#36807;2,000&#20010;&#38544;&#24335;&#25552;&#31034;&#65306;&#36890;&#29992;&#31526;&#21495;&#12289;&#21517;&#20154;&#38544;&#31169;&#21644;&#19981;&#23433;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20845;&#20010;&#30693;&#21517;T2I&#27169;&#22411;&#22312;&#36825;&#20123;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;T2I&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#21019;&#24314;&#21508;&#31181;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02118v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various targe
&lt;/p&gt;</description></item><item><title>&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.00993</link><description>&lt;p&gt;
&#35770;&#37096;&#20998;&#21487;&#35266;&#23519;&#24207;&#21015;&#22242;&#38431;&#21644;&#28216;&#25103;&#20013;&#20449;&#24687;&#32467;&#26500;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00993
&lt;/p&gt;
&lt;p&gt;
&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#20449;&#24687;&#32467;&#26500;&#25551;&#36848;&#20102;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#21051;&#20107;&#20214;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#24352;&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20855;&#26377;&#26126;&#30830;&#20449;&#24687;&#32467;&#26500;&#34920;&#31034;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00993v1 Announce Type: cross  Abstract: In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of reinforcement learning (e.g., MDPs, POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure.   In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving reinforcement learning problems. We propose novel reinforcement learning models with an explicit representation of information structure, capturing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18023</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Mirror Cognitive Language Processing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#35748;&#30693;&#20219;&#21153;&#20013;&#23454;&#29616;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;LLMs&#26159;&#20174;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#30340;&#22823;&#37327;&#25991;&#26412;&#20135;&#20986;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;LLMs&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65292;&#25110;LLMs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;LLMs&#34920;&#24449;&#21644;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#65292;&#20197;&#35780;&#20272;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#37319;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#26469;&#34913;&#37327;16&#31181;&#20027;&#27969;LLMs&#19982;&#22823;&#33041;fMRI&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#35268;&#27169;&#12289;&#23545;&#40784;&#35757;&#32451;&#12289;&#25351;&#23548;&#38468;&#21152;&#65289;&#23545;LLM-&#22823;&#33041;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#27491;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13448</link><description>&lt;p&gt;
ED-Copilot: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#36741;&#21161;&#20943;&#23569;&#24613;&#35786;&#31185;&#31561;&#24453;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24613;&#35786;&#31185;&#65288;ED&#65289;&#20013;&#65292;&#24739;&#32773;&#22312;&#35786;&#26029;&#21069;&#38656;&#35201;&#36827;&#34892;&#20998;&#35786;&#21644;&#22810;&#31181;&#23454;&#39564;&#23460;&#26816;&#27979;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23548;&#33268;&#24613;&#35786;&#31185;&#25317;&#25380;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;&#21307;&#30103;&#38169;&#35823;&#12289;&#20154;&#21592;&#26543;&#31469;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#26102;&#38388;&#65289;&#25104;&#26412;&#26377;&#25928;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21327;&#21161;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#25105;&#20204;&#19982;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#21512;&#20316;&#31574;&#21010;&#20102;MIMIC-ED-Assist&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24314;&#35758;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#24613;&#35786;&#31561;&#24453;&#26102;&#38388;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#24182;&#22312;&#27491;&#30830;&#39044;&#27979;&#35832;&#22914;&#27515;&#20129;&#20043;&#31867;&#20851;&#38190;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ED-Copilot&#65292;&#23427;&#20381;&#27425;&#24314;&#35758;&#24739;&#32773;&#29305;&#23450;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;ED-Copilot&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#24739;&#32773;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#24182;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.11639</link><description>&lt;p&gt;
&#20855;&#26377;Transformer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;Softmax&#27880;&#24847;&#21147;&#36866;&#24212;&#20989;&#25968;Lipschitz&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24615;&#26159;&#20854;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#22312;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#26576;&#20123;&#25968;&#25454;&#38544;&#24335;&#22320;&#34987;&#21576;&#29616;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#32972;&#26223;&#65292;&#24182;&#34987;&#35201;&#27714;&#22312;&#35813;&#32972;&#26223;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#24517;&#39035;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#32972;&#26223;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;Softmax&#27880;&#24847;&#21147;&#22312;&#19968;&#20010;ICL&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#32972;&#26223;&#37117;&#32534;&#30721;&#20102;&#19968;&#20010;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20064;&#19968;&#20010;&#31383;&#21475;&#65292;&#29992;&#20110;&#23454;&#29616;&#19968;&#20010;&#36866;&#24212;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31383;&#21475;&#38543;&#30528;Lipschitzness&#30340;&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#30340;&#22686;&#21152;&#32780;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#20302;&#31209;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#65292;&#27880;&#24847;&#21147;&#21333;&#20803;&#22312;&#25512;&#29702;&#20043;&#21069;&#23398;&#20250;&#20102;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#36866;&#24212;&#24615;&#20851;&#38190;&#22320;&#20381;&#36182;&#20110;softmax&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11639v1 Announce Type: new  Abstract: A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10958</link><description>&lt;p&gt;
&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;: &#36890;&#36807;&#23545;&#30456;&#21516;&#21644;&#19981;&#21516;&#25552;&#31034;&#30340;&#23545;&#27604;&#21709;&#24212;&#22686;&#24378;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#65292;&#23558;&#27169;&#22411;&#19982;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30452;&#25509;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;DPO&#36890;&#36807;&#20351;&#29992;&#20174;&#30456;&#21516;&#25552;&#31034;&#20013;&#27966;&#29983;&#30340;&#20559;&#22909;&#23545;&#26469;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DPO&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#20154;&#31867;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#23398;&#20064;&#24448;&#24448;&#28041;&#21450;&#23545;&#19981;&#20165;&#30456;&#21516;&#32780;&#19988;&#30456;&#20284;&#38382;&#39064;&#30340;&#23545;&#27604;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#12290;RPO&#26088;&#22312;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#12290;&#23427;&#24341;&#20837;&#20102;&#23545;&#27604;&#21152;&#26435;&#26426;&#21046;&#65292;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#21253;&#25324;&#25104;&#23545;&#21644;&#19981;&#25104;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08939</link><description>&lt;p&gt;
&#35770;&#25454;&#39034;&#24207;&#22312;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#36215;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Premise Order Matters in Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08939
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#33030;&#24369;&#24615;&#65306;&#23613;&#31649;&#36825;&#31181;&#39034;&#24207;&#19981;&#20250;&#25913;&#21464;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;LLMs&#23545;&#20110;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#33030;&#24369;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#35770;&#25454;&#39034;&#24207;&#19982;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#26102;&#65292;LLMs&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#35770;&#25454;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#65288;&#32780;&#19981;&#26159;&#38543;&#26426;&#39034;&#24207;&#65289;&#20250;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#23545;&#28436;&#32462;&#25512;&#29702;&#20013;&#35770;&#25454;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#35770;&#25454;&#39034;&#24207;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#36229;&#36807;30&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#22522;&#20110;GSM8K&#30340;&#22522;&#20934;&#27979;&#35797;R-GSM&#26469;&#30740;&#31350;&#39034;&#24207;&#25928;&#24212;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
&lt;/p&gt;</description></item><item><title>BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08219</link><description>&lt;p&gt;
BBox-Adapter: &#36731;&#37327;&#32423;&#36866;&#37197;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08219
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#21644;Gemini&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20219;&#21153;&#30340;&#35201;&#27714;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21442;&#25968;&#12289;&#23884;&#20837;&#21644;&#36755;&#20986;&#27010;&#29575;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#33021;&#36890;&#36807;&#23427;&#20204;&#30340;API&#26381;&#21153;&#36866;&#24212;&#36825;&#20123;&#40657;&#30418;LLMs&#65292;&#36825;&#24341;&#21457;&#20102;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BBox-Adapter&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#40657;&#30418;LLMs&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#12290;BBox-Adapter&#36890;&#36807;&#23558;&#30446;&#26631;&#25968;&#25454;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#28304;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#26469;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#26469;&#25552;&#39640;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#24809;&#32602;&#28304;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#23558;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#12289;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#30340;&#23454;&#26102;&#27491;&#26679;&#26412;&#37319;&#26679;&#19982;&#20808;&#21069;&#36866;&#24212;&#30340;&#36127;&#26679;&#26412;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BBox-Adapter&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#40657;&#30418;LLMs&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
&lt;/p&gt;</description></item><item><title>THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08191</link><description>&lt;p&gt;
THE COLOSSEUM&#65306;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#27867;&#21270;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08191
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#12289;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#35780;&#20272;&#26426;&#22120;&#20154;&#22312;&#19982;&#35757;&#32451;&#35774;&#32622;&#38750;&#24120;&#30456;&#20284;&#29978;&#33267;&#30456;&#21516;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;THE COLOSSEUM&#65292;&#20854;&#20013;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21487;&#20197;&#23545;&#27169;&#22411;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#36825;&#20123;&#24178;&#25200;&#21253;&#25324;&#29289;&#20307;&#12289;&#26700;&#38754;&#21644;&#32972;&#26223;&#30340;&#39068;&#33394;&#12289;&#32441;&#29702;&#21644;&#22823;&#23567;&#30340;&#21464;&#21270;&#65307;&#25105;&#20204;&#36824;&#25913;&#21464;&#20102;&#20809;&#29031;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#20351;&#29992;THE COLOSSEUM&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;4&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#22312;&#36825;&#20123;&#24178;&#25200;&#22240;&#32032;&#19979;&#19979;&#38477;&#20102;30-50%&#12290;&#24403;&#22810;&#20010;&#24178;&#25200;&#21516;&#26102;&#24212;&#29992;&#26102;&#65292;&#25104;&#21151;&#29575;&#19979;&#38477;&#33267;&#8805;75%&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#38899;&#20048;&#30340;&#29305;&#23450;&#23646;&#24615;&#32780;&#20445;&#25345;&#20854;&#20182;&#26041;&#38754;&#19981;&#21464;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39118;&#26684;&#21644;&#38899;&#33394;&#36716;&#25442;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#20197;&#21450;&#22312;&#23454;&#38469;&#38899;&#20048;&#32534;&#36753;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06178</link><description>&lt;p&gt;
MusicMagus: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#38899;&#20048;&#30340;&#29305;&#23450;&#23646;&#24615;&#32780;&#20445;&#25345;&#20854;&#20182;&#26041;&#38754;&#19981;&#21464;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39118;&#26684;&#21644;&#38899;&#33394;&#36716;&#25442;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#20197;&#21450;&#22312;&#23454;&#38469;&#38899;&#20048;&#32534;&#36753;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#36827;&#27493;&#20026;&#38899;&#20048;&#21019;&#36896;&#21147;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#38899;&#20048;&#29983;&#25104;&#36890;&#24120;&#28041;&#21450;&#36845;&#20195;&#30340;&#25913;&#36827;&#65292;&#22914;&#20309;&#32534;&#36753;&#29983;&#25104;&#30340;&#38899;&#20048;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32534;&#36753;&#36825;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#38899;&#20048;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#23646;&#24615;&#65288;&#22914;&#39118;&#26684;&#12289;&#24773;&#24863;&#21644;&#20048;&#22120;&#65289;&#30340;&#20462;&#25913;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#26041;&#38754;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#32534;&#36753;&#36716;&#21270;&#20026;&#28508;&#22312;&#31354;&#38388;&#25805;&#32437;&#65292;&#24182;&#28155;&#21152;&#39069;&#22806;&#30340;&#32422;&#26463;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#23427;&#21487;&#20197;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#20048;&#25193;&#25955;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#39118;&#26684;&#21644;&#38899;&#33394;&#36716;&#25442;&#35780;&#20272;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#38646;&#26679;&#26412;&#21644;&#26576;&#20123;&#30417;&#30563;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#23454;&#38469;&#38899;&#20048;&#32534;&#36753;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.04617</link><description>&lt;p&gt;
InfLLM: &#25581;&#31034;LLMs&#23545;&#20110;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26080;&#38656;&#35757;&#32451;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04617
&lt;/p&gt;
&lt;p&gt;
InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20855;&#26377;&#28459;&#38271;&#20256;&#36755;&#36755;&#20837;&#30340;&#29616;&#23454;&#24212;&#29992;&#30340;&#22522;&#30707;&#65292;&#22914;LLM&#39537;&#21160;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#21463;&#38480;&#26368;&#22823;&#38271;&#24230;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#26080;&#27861;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#22806;&#21644;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#28369;&#21160;&#27880;&#24847;&#21147;&#31383;&#21475;&#21644;&#20002;&#24323;&#36828;&#36317;&#31163;&#26631;&#35760;&#65292;&#20197;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#24207;&#21015;&#20869;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;InfLLM&#65292;&#26469;&#25581;&#31034;LLMs&#22788;&#29702;&#27969;&#24335;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InfLLM&#23558;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#23384;&#20648;&#21040;&#38468;&#21152;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#21046;&#26469;&#26597;&#25214;&#19982;&#27880;&#24847;&#35745;&#31639;&#30456;&#20851;&#30340;&#26631;&#35760;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;InfLLM&#20801;&#35768;LLMs&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#35821;&#20041;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
&lt;/p&gt;</description></item><item><title>&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03824</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21628;&#21505;
&lt;/p&gt;
&lt;p&gt;
A call for embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03824
&lt;/p&gt;
&lt;p&gt;
&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#36827;&#34892;&#23545;&#27604;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36328;&#36234;&#20102;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#23545;&#20855;&#35937;&#27010;&#24565;&#30340;&#28436;&#21464;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#21306;&#21035;&#20110;&#38745;&#24577;&#23398;&#20064;&#30340;&#32463;&#20856;&#33539;&#24335;&#12290;&#36890;&#36807;&#25193;&#22823;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24378;&#35843;&#30693;&#35273;&#12289;&#34892;&#21160;&#12289;&#35760;&#24518;&#21644;&#23398;&#20064;&#20316;&#20026;&#20855;&#35937;&#20195;&#29702;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35832;&#22914;&#21046;&#23450;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#29702;&#35770;&#21644;&#21019;&#26032;&#20808;&#36827;&#30828;&#20214;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#20026;&#26410;&#26469;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. High
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03686</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#22312;&#34164;&#21547;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minds versus Machines: Rethinking Entailment Verification with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25991;&#26412;&#29702;&#35299;&#20013;&#36827;&#34892;&#22823;&#37327;&#30340;&#25512;&#29702;&#20197;&#29702;&#35299;&#35770;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20154;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#36890;&#36807;&#32508;&#21512;&#31574;&#21010;&#30340;&#34164;&#21547;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#31867;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#31867;&#21035;&#65288;NLI&#12289;&#19978;&#19979;&#25991;QA&#21644;&#35299;&#37322;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#21477;&#21069;&#25552;&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;&#22797;&#26434;&#25512;&#29702;&#24773;&#20917;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;LLM&#22312;&#36328;&#25193;&#23637;&#19978;&#19979;&#25991;&#30340;&#22810;&#36339;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#38656;&#35201;&#31616;&#21333;&#28436;&#32462;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Flan-T5&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#24182;&#19982;GPT-4&#23218;&#32654;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#20379;&#34164;&#21547;&#39564;&#35777;&#20351;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03660</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#20986;&#29616;&#20102;&#36328;&#20219;&#21153;&#32447;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#24050;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#27969;&#36235;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20174;&#20844;&#20849;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#65288;CTL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#25105;&#20204;&#32447;&#24615;&#25554;&#20540;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#26435;&#37325;&#25554;&#20540;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#22823;&#33268;&#31561;&#20110;&#27599;&#23618;&#20013;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#29305;&#24449;&#30340;&#32447;&#24615;&#25554;&#20540;&#12290;&#36825;&#26679;&#30340;&#36328;&#20219;&#21153;&#32447;&#24615;&#22312;&#21516;&#34892;&#25991;&#29486;&#20013;&#23578;&#26410;&#34987;&#27880;&#24847;&#21040;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25903;&#25345;&#20174;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24320;&#22987;&#30340;&#24494;&#35843;&#27169;&#22411;&#19968;&#33268;&#20986;&#29616;CTL&#12290;&#25105;&#20204;&#25512;&#27979;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26412;&#36136;&#19978;&#26159;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#12289;&#21442;&#25968;&#20849;&#20139;&#31561;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02705</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#30340;&#34920;&#24449;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Representation Surgery for Multi-Task Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#23569;&#22810;&#20219;&#21153;&#27169;&#22411;&#21512;&#24182;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#38024;&#23545;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#20462;&#27491;&#65292;&#20197;&#25552;&#39640;&#21512;&#24182;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#23558;&#22810;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#39592;&#24178;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#25509;&#21512;&#24182;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25191;&#34892;MTL&#65292;&#32780;&#19981;&#26159;&#25910;&#38598;&#23427;&#20204;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;MTL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#30340;&#34920;&#31034;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#21512;&#24182;&#27169;&#22411;&#24448;&#24448;&#38754;&#20020;&#34920;&#31034;&#20559;&#24046;&#30340;&#22256;&#22659;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#24182;&#27169;&#22411;&#19982;&#20010;&#20307;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#20998;&#24067;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#21512;&#24182;MTL&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Surgery&#8221;&#30340;&#34920;&#24449;&#25163;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#21512;&#24182;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;Surgery&#8221;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20219;&#21153;&#19987;&#29992;&#27169;&#22359;&#65292;&#23427;&#20197;&#21512;&#24182;&#27169;&#22411;&#30340;&#34920;&#31034;&#20026;&#36755;&#20837;&#65292;&#24182;&#35797;&#22270;&#36755;&#20986;&#20854;&#20013;&#21253;&#21547;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation fr
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01922</link><description>&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning from Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#30528;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#24369;&#30417;&#30563;&#30340;&#21508;&#31181;&#22330;&#26223;&#21644;&#30001;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;GLWS&#65289;&#12290;GLWS&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#20844;&#24335;&#65292;&#28789;&#27963;&#22320;&#36866;&#24212;&#20102;&#21508;&#31181;&#24369;&#30417;&#30563;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#12289;&#32858;&#21512;&#32479;&#35745;&#12289;&#25104;&#23545;&#35266;&#23519;&#21644;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;NFA&#65289;&#20197;&#21450;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;EM&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#24120;&#25152;&#38656;&#30340;&#20108;&#27425;&#25110;&#38454;&#20056;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#20174;&#20219;&#24847;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20102;&#23545;&#23427;&#20204;&#36827;&#34892;NFA&#24314;&#27169;&#12290;GLWS&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;+
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.01821</link><description>&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#35299;&#37322;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Ecologically rational meta-learned inference explains human category learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ERMI&#27169;&#22411;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#24577;&#21512;&#29702;&#24615;&#26159;&#25351;&#20154;&#31867;&#20316;&#20026;&#36866;&#24212;&#29615;&#22659;&#30340;&#29702;&#24615;&#20027;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23450;&#20041;&#21738;&#20123;&#20219;&#21153;&#22312;&#29983;&#24577;&#19978;&#26159;&#26377;&#25928;&#30340;&#20197;&#21450;&#20026;&#36825;&#20123;&#20219;&#21153;&#24314;&#31435;&#21512;&#29702;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32479;&#35745;&#19968;&#33268;&#30340;&#35748;&#30693;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#31867;&#21035;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#25512;&#23548;&#36866;&#24212;&#36825;&#20123;&#20219;&#21153;&#30340;&#21512;&#29702;&#24615;&#20027;&#20307;&#26469;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#29983;&#24577;&#21512;&#29702;&#30340;&#20803;&#23398;&#20064;&#25512;&#26029;&#65288;ERMI&#65289;&#12290;ERMI&#22312;&#20004;&#20010;&#19981;&#21516;&#23454;&#39564;&#20013;&#20197;&#23450;&#37327;&#26041;&#24335;&#27604;&#20854;&#20182;&#19971;&#20010;&#35748;&#30693;&#27169;&#22411;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#23450;&#24615;&#19978;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#65306;&#65288;1&#65289;&#23427;&#21457;&#29616;&#20102;&#19982;&#20154;&#31867;&#21457;&#29616;&#22256;&#38590;&#30340;&#30456;&#21516;&#20219;&#21153;&#65292;&#65288;2&#65289;&#23427;&#21464;&#24471;&#26356;&#20381;&#36182;&#20110;&#22522;&#20110;&#26679;&#26412;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#36848;&#21644;&#35843;&#26597;&#30740;&#31350;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#23454;&#26045;&#38556;&#30861;&#65292;&#21457;&#29616;&#20102;13&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#20854;&#20013;&#65292;&#21512;&#35268;&#24615;&#21644;&#20919;&#38142;&#32593;&#32476;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#21644;DEMATEL&#26041;&#27861;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.01804</link><description>&lt;p&gt;
&#20919;&#38142;&#20013;&#29289;&#32852;&#32593;&#23454;&#26045;&#38556;&#30861;&#30340;&#20998;&#26512;&#65306;&#19968;&#31181;&#38598;&#25104;&#30340;ISM-MICMAC&#21644;DEMATEL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analysis of Internet of Things implementation barriers in the cold supply chain: an integrated ISM-MICMAC and DEMATEL approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#36848;&#21644;&#35843;&#26597;&#30740;&#31350;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#23454;&#26045;&#38556;&#30861;&#65292;&#21457;&#29616;&#20102;13&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#20854;&#20013;&#65292;&#21512;&#35268;&#24615;&#21644;&#20919;&#38142;&#32593;&#32476;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#21644;DEMATEL&#26041;&#27861;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29289;&#32852;&#32593;&#25216;&#26415;&#25972;&#21512;&#21040;&#20919;&#38142;&#20013;&#21487;&#20197;&#25552;&#39640;&#36879;&#26126;&#24230;&#12289;&#25928;&#29575;&#21644;&#36136;&#37327;&#65292;&#20248;&#21270;&#36816;&#33829;&#27969;&#31243;&#24182;&#25552;&#39640;&#29983;&#20135;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#29615;&#22659;&#19979;&#65292;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#38598;&#25104;&#21463;&#21040;&#29305;&#23450;&#30340;&#38556;&#30861;&#30340;&#38459;&#30861;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;&#29289;&#32852;&#32593;&#23454;&#26045;&#30340;&#30456;&#20851;&#25991;&#29486;&#36827;&#34892;&#32508;&#36848;&#65292;&#20849;&#21457;&#29616;&#20102;13&#20010;&#38556;&#30861;&#12290;&#35843;&#26597;&#25968;&#25454;&#32463;&#36807;&#20132;&#21449;&#39564;&#35777;&#20197;&#30830;&#20445;&#36136;&#37327;&#65292;&#24182;&#37319;&#29992;Cronbach's alpha&#27979;&#35797;&#26469;&#30830;&#20445;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#22312;&#31532;&#19968;&#38454;&#27573;&#24212;&#29992;&#35299;&#37322;&#24615;&#32467;&#26500;&#24314;&#27169;&#25216;&#26415;&#20197;&#35782;&#21035;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#20123;&#38556;&#30861;&#20013;&#65292;&#8220;&#21512;&#35268;&#24615;&#8221;&#21644;&#8220;&#20919;&#38142;&#32593;&#32476;&#8221;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#30340;&#39537;&#21160;&#21644;&#20381;&#36182;&#21147;&#37327;&#20803;&#32032;&#20998;&#31867;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#22312;&#26412;&#30740;&#31350;&#30340;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;DEMATEL&#26041;&#27861;&#30830;&#23450;&#20102;&#25152;&#35782;&#21035;&#38556;&#30861;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating Internet of Things (IoT) technology inside the cold supply chain can enhance transparency, efficiency, and quality, optimizing operating procedures and increasing productivity. The integration of IoT in this complicated setting is hindered by specific barriers that need a thorough examination. Prominent barriers to IoT implementation in the cold supply chain are identified using a two-stage model. After reviewing the available literature on the topic of IoT implementation, a total of 13 barriers were found. The survey data was cross-validated for quality, and Cronbach's alpha test was employed to ensure validity. This research applies the interpretative structural modeling technique in the first phase to identify the main barriers. Among those barriers, "regularity compliance" and "cold chain networks" are key drivers for IoT adoption strategies. MICMAC's driving and dependence power element categorization helps evaluate the barrier interactions. In the second phase of this
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.15741</link><description>&lt;p&gt;
SERNet-Former: &#24102;&#26377;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#65292;&#25913;&#21892;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#29575;&#38656;&#35201;&#35299;&#20915;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#25104;&#21151;&#21644;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#29420;&#29305;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#65288;AbGs&#65289;&#21644;&#27880;&#24847;&#21147;&#22686;&#24378;&#27169;&#22359;&#65288;AbMs&#65289;&#65292;&#30446;&#26631;&#26159;&#22312;&#32534;&#30721;&#22120;&#20013;&#23558;&#22522;&#20110;&#29305;&#24449;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#12290;&#21516;&#26102;&#65292;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37319;&#29992;&#20102;&#21463;&#21040;AbM&#21551;&#21457;&#30340;&#39069;&#22806;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;AfNs&#65289;&#12290;AfNs&#26088;&#22312;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37096;&#32626;&#39069;&#22806;&#30340;&#21367;&#31215;&#23618;&#65292;&#25913;&#21892;&#35821;&#20041;&#20449;&#24687;&#30340;&#36880;&#19968;&#36716;&#25442;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#32593;&#32476;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CamVid&#21644;Cityscapes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the efficiency of state-of-the-art methods in semantic segmentation requires overcoming the increasing computational cost as well as issues such as fusing semantic information from global and local contexts. Based on the recent success and problems that convolutional neural networks (CNNs) encounter in semantic segmentation, this research proposes an encoder-decoder architecture with a unique efficient residual network. Attention-boosting gates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to fuse the feature-based semantic information with the global context of the efficient residual network in the encoder. Respectively, the decoder network is developed with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve the efficiency in the one-to-one conversion of the semantic information by deploying additional convolution layers in the decoder part. Our network is tested on the challenging CamVid and Cityscapes dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.04234</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#36215;&#21040;&#20102;&#25913;&#36827;&#30340;&#20316;&#29992;&#65281;&#65288;arXiv&#65306;2312.04234v2 [cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22240;&#20854;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#32780;&#38395;&#21517;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Transformer&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#34920;&#31034;&#22312;&#21508;&#20010;&#23618;&#20043;&#38388;&#36235;&#20110;&#26080;&#27861;&#21306;&#20998;&#30340;&#20540;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;&#24182;&#20174;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#28388;&#27874;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GFSA&#65289;&#65292;&#20197;&#23398;&#20064;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#20854;&#22797;&#26434;&#24230;&#30053;&#39640;&#20110;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GFSA&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#27169;&#24335;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#20195;&#30721;&#20998;&#31867;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#25913;&#36827;&#20102;Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03940</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#38590;&#35270;&#22270;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#23545;&#22270;&#20687;&#36755;&#20837;&#30340;&#19981;&#21516;&#8220;&#35270;&#22270;&#8221;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#32780;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#23545;&#27492;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#22686;&#24378;&#27969;&#31243;&#20013;&#30340;&#25805;&#20316;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#65292;&#22914;&#38543;&#26426;&#35009;&#21098;&#25110;&#39068;&#33394;&#25197;&#26354;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#35270;&#22270;&#29983;&#25104;&#21450;&#20854;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22312;&#30446;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#20294;&#24378;&#22823;&#30340;&#8220;&#38590;&#35270;&#22270;&#36873;&#25321;&#8221;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#35270;&#22270;&#29983;&#25104;&#25193;&#23637;&#21040;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31574;&#30053;&#21253;&#25324;&#20197;&#19979;&#36845;&#20195;&#27493;&#39588;&#65306;1&#65289;&#38543;&#26426;&#36873;&#25321;&#22810;&#20010;&#35270;&#22270;&#24182;&#21019;&#24314;&#20004;&#20010;&#35270;&#22270;&#30340;&#37197;&#23545;&#65292;2&#65289;&#36827;&#34892;&#21521;&#21069;&#20256;&#36882;...
&lt;/p&gt;
&lt;p&gt;
Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.02932</link><description>&lt;p&gt;
&#23545;&#27668;&#20505;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#65292;&#20102;&#35299;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#36171;&#20104;&#20010;&#20154;&#21644;&#31038;&#21306;&#20943;&#32531;&#21644;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;LLM&#23545;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#22238;&#31572;&#30340;&#21576;&#29616;&#21644;&#35748;&#35782;&#19978;&#30340;&#36866;&#24403;&#24615;&#65292;&#20026;LLM&#29983;&#25104;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#35206;&#30422;&#20102;8&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;30&#20010;&#19981;&#21516;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20363;&#23376;&#65292;&#36825;&#20010;&#39046;&#22495;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;AI&#21487;&#20197;&#34917;&#20805;&#21644;&#25552;&#21319;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30417;&#30563;&#21327;&#35758;&#65292;&#21033;&#29992;AI&#36741;&#21161;&#24182;&#20381;&#38752;&#20855;&#26377;&#30456;&#20851;&#25945;&#32946;&#32972;&#26223;&#30340;&#35780;&#20272;&#21592;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;</title><link>http://arxiv.org/abs/2310.01616</link><description>&lt;p&gt;
&#22810;&#25209;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65306;&#23545;&#20110;&#32500;&#24230;&#30456;&#20851;&#30340;&#36866;&#24212;&#24615;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity. (arXiv:2310.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22914;&#26524;&#31639;&#27861;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;d&#20013;&#20351;&#29992;&#30340;&#29615;&#22659;&#26597;&#35810;&#27425;&#25968;n&#26159;&#22810;&#39033;&#24335;&#30340;&#65292;&#37027;&#20040;&#23427;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#12290;&#36866;&#24212;&#24615;&#26159;&#25351;&#26597;&#35810;&#34987;&#21457;&#36865;&#21644;&#21453;&#39304;&#34987;&#22788;&#29702;&#20197;&#26356;&#26032;&#26597;&#35810;&#31574;&#30053;&#30340;&#39057;&#29575;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;K&#20010;&#25209;&#27425;&#20013;&#21457;&#36865;&#26597;&#35810;&#65292;&#22312;&#27599;&#20010;&#25209;&#27425;&#20043;&#21518;&#22788;&#29702;&#21453;&#39304;&#24182;&#26356;&#26032;&#26597;&#35810;&#12290;&#36825;&#20010;&#27169;&#22411;&#21253;&#25324;&#25972;&#20010;&#36866;&#24212;&#24615;&#35889;&#65292;&#20174;&#38750;&#33258;&#36866;&#24212;&#30340;&#8220;&#31163;&#32447;&#8221;&#65288;K=1&#65289;&#21040;&#23436;&#20840;&#33258;&#36866;&#24212;&#65288;K=n&#65289;&#30340;&#22330;&#26223;&#65292;&#20197;&#21450;&#20013;&#38388;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#22312;d&#32500;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#26679;&#26412;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#24314;&#31435;&#20102;&#937;(log log d)&#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#38656;&#35201;&#30340;&#25209;&#27425;&#25968;K&#20855;&#26377; &#937;(log log d) &#30340;&#19979;&#30028;&#65292;&#20854;&#20013;n = O(poly(d))&#12290;
&lt;/p&gt;
&lt;p&gt;
We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that j
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16750</link><description>&lt;p&gt;
&#30524;&#20013;&#35760;&#24518;&#65306;&#25193;&#25955;&#27169;&#22411;&#21644;&#20851;&#32852;&#35760;&#24518;&#20043;&#38388;&#30340;&#31070;&#31192;&#30456;&#20284;&#20043;&#22788;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26368;&#36817;&#22312;&#35768;&#22810;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#25968;&#23398;&#25551;&#36848;&#26377;&#24456;&#22810;&#31181;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#20154;&#20204;&#24456;&#38590;&#23545;&#20854;&#24037;&#20316;&#21407;&#29702;&#36827;&#34892;&#31616;&#21333;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#21160;&#21147;&#31995;&#32479;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;DMs&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19982;&#20854;&#39640;&#24230;&#30456;&#20851;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#33021;&#37327;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#30340;&#25968;&#23398;&#32852;&#31995;&#12290;&#22522;&#20110;&#33021;&#37327;&#30340;AMs&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20854;&#34892;&#20026;&#19982;&#21435;&#22122;DMs&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#19968;&#20010;Lyapunov&#33021;&#37327;&#20989;&#25968;&#65292;&#22312;&#20854;&#19978;&#21487;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#20197;&#21435;&#22122;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#33021;&#37327;AMs&#30340;40&#24180;&#21382;&#21490;&#65292;&#20174;&#26368;&#21021;&#30340;Hopfield&#32593;&#32476;&#24320;&#22987;&#65292;&#24182;&#35752;&#35770;&#20102;&#36890;&#36807;&#25551;&#36848;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#31243;&#24230;&#25581;&#31034;&#20986;&#26469;&#30340;AMs&#21644;DMs&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12455</link><description>&lt;p&gt;
LongDocFACTScore: &#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#23454;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12455
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#20107;&#23454;&#19968;&#33268;&#24615;&#26159;&#29983;&#25104;&#24615;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#25688;&#35201;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;ROUGE&#24471;&#20998;&#65289;&#26080;&#27861;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26377;&#38480;&#21046;&#24615;&#30340;&#20196;&#29260;&#38480;&#21046;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#22312;&#24212;&#29992;&#20110;&#38271;&#25991;&#26723;&#25968;&#25454;&#38598;&#26102;&#26159;&#21542;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;LongDocFACTScore&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#24230;&#37327;&#26631;&#20934;&#25193;&#23637;&#21040;&#20219;&#24847;&#38271;&#24230;&#30340;&#25991;&#26723;&#12290;&#35813;&#26694;&#26550;&#22312;&#19982;&#20154;&#31867;&#20107;&#23454;&#19968;&#33268;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;MoE&#27169;&#22411;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#20998;&#26512;&#20248;&#21270;&#21442;&#25968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.15030</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;MoE&#27169;&#22411;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#20998;&#26512;&#20248;&#21270;&#21442;&#25968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#28608;&#27963;&#30340;&#24182;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;(&#19987;&#23478;)&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24310;&#36831;&#20851;&#38190;&#30340;&#36793;&#32536;&#22330;&#26223;&#20013;&#25552;&#20379;MoE&#27169;&#22411;&#30340;&#26381;&#21153;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#20102;MoE&#27169;&#22411;&#22312;&#36830;&#32493;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#19987;&#23478;&#28608;&#27963;&#30340;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#26102;&#38388;&#23616;&#37096;&#24615;&#12289;&#21487;&#20132;&#25442;&#24615;&#21644;&#21487;&#36339;&#36807;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PC-MoE&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36830;&#32493;MoE&#27169;&#22411;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#12290;PC-MoE&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#8220;&#21442;&#25968;&#22996;&#21592;&#20250;&#8221;&#65292;&#23427;&#26234;&#33021;&#22320;&#32500;&#25252;&#19968;&#37096;&#20998;&#37325;&#35201;&#30340;&#27491;&#22312;&#20351;&#29992;&#30340;&#19987;&#23478;&#65292;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#22522;&#20110;&#24615;&#33021;&#20998;&#26512;&#30340;&#22996;&#21592;&#20250;&#35268;&#21010;&#22120;&#65292;&#22312;&#32447;&#25214;&#21040;&#21442;&#25968;&#22996;&#21592;&#20250;&#30340;&#26368;&#20339;&#37197;&#32622;&#65292;&#24182;&#36827;&#34892;&#19987;&#23478;&#20132;&#25442;&#21644;&#35831;&#27714;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request 
&lt;/p&gt;</description></item><item><title>CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06213</link><description>&lt;p&gt;
CHGNN: &#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06213
&lt;/p&gt;
&lt;p&gt;
CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.
&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21487;&#20197;&#27169;&#25311;&#24212;&#29992;&#31243;&#24207;&#20013;&#21457;&#29616;&#30340;&#25968;&#25454;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36229;&#22270;&#23398;&#20064;&#30740;&#31350;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#25193;&#23637;&#21040;&#36229;&#22270;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#29305;&#24449;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;CHGNN&#65292;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;CHGNN&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#65292;&#37319;&#29992;&#33258;&#21160;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#23398;&#20064;&#26368;&#23567;&#20805;&#20998;&#35270;&#22270;&#30340;&#25200;&#21160;&#27010;&#29575;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;CHGNN&#21253;&#21547;&#19968;&#20010;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#65292;&#32771;&#34385;&#21040;&#36229;&#36793;&#30340;&#21516;&#36136;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#31532;&#19977;&#65292;CHGNN&#37197;&#22791;&#20102;&#19968;&#20010;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#35270;&#22270;&#29983;&#25104;&#22120;&#30340;&#30456;&#20284;&#24615;&#25439;&#22833;&#12289;&#33410;&#28857;&#20998;&#31867;&#25439;&#22833;&#21644;&#36229;&#36793;&#21516;&#36136;&#24615;&#25439;&#22833;&#65292;&#27880;&#20837;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.11749</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26410;&#30693;&#25968;&#37327;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#28304;&#20998;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#26410;&#30693;&#25968;&#37327;&#20449;&#21495;&#30340;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#36755;&#20986;&#19982;&#30446;&#26631;&#23545;&#40784;&#24341;&#36215;&#30340;&#25490;&#21015;&#38382;&#39064;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;&#26377;&#38745;&#38899;&#36890;&#36947;&#30340;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#28151;&#21512;&#30340;&#36752;&#23556;&#33337;&#22122;&#22768;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36798;&#21040;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09736</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24352;&#37327;&#21644;&#30697;&#38453;&#20302;&#31209;&#20540;&#20989;&#25968;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning. (arXiv:2201.09736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24352;&#37327;&#34920;&#31034;&#21644;PARAFAC&#20998;&#35299;&#30340;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#65288;VF&#65289;&#30340;&#36817;&#20284;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;VF&#20272;&#35745;&#22312;&#32500;&#24230;&#28798;&#38590;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20154;&#20204;&#37319;&#29992;&#20102;&#31616;&#27905;&#30340;&#21442;&#25968;&#27169;&#22411;&#26469;&#36817;&#20284;VF&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#19978;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#31616;&#27905;&#30340;&#38750;&#21442;&#25968;&#8221;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#31639;&#27861;&#20197;&#22312;&#32447;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#20272;&#35745;VF&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;VF&#24448;&#24448;&#26159;&#22810;&#32500;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#24352;&#37327;&#65288;&#22810;&#32500;&#25968;&#32452;&#65289;&#34920;&#31034;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;VF&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;PARAFAC&#20998;&#35299;&#26469;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#26080;&#27169;&#22411;&#30340;&#24352;&#37327;&#20302;&#31209;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-function (VF) approximation is a central problem in Reinforcement Learning (RL). Classical non-parametric VF estimation suffers from the curse of dimensionality. As a result, parsimonious parametric models have been adopted to approximate VFs in high-dimensional spaces, with most efforts being focused on linear and neural-network-based approaches. Differently, this paper puts forth a a \emph{parsimonious non-parametric} approach, where we use \emph{stochastic low-rank algorithms} to estimate the VF matrix in an online and model-free fashion. Furthermore, as VFs tend to be multi-dimensional, we propose replacing the classical VF matrix representation with a tensor (multi-way array) representation and, then, use the PARAFAC decomposition to design an online model-free tensor low-rank algorithm. Different versions of the algorithms are proposed, their complexity is analyzed, and their performance is assessed numerically using standardized RL environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20142;&#24230;&#30340;&#30333;&#30418;&#26080;&#38480;&#21046;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#22270;&#20687;&#30340;&#20142;&#24230;&#26469;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#23545;&#20154;&#31867;&#24863;&#30693;&#30340;&#24418;&#29366;&#21644;&#39068;&#33394;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.06070</link><description>&lt;p&gt;
ALA: &#20855;&#26377;&#33258;&#28982;&#24863;&#30693;&#30340;&#23545;&#25239;&#20142;&#24230;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ALA: Naturalness-aware Adversarial Lightness Attack. (arXiv:2201.06070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20142;&#24230;&#30340;&#30333;&#30418;&#26080;&#38480;&#21046;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#22270;&#20687;&#30340;&#20142;&#24230;&#26469;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#23545;&#20154;&#31867;&#24863;&#30693;&#30340;&#24418;&#29366;&#21644;&#39068;&#33394;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25581;&#31034;&#21644;&#20462;&#22797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#33030;&#24369;&#24615;&#26469;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25915;&#20987;&#26679;&#26412;&#30340;&#37096;&#20998;&#26159;&#21463;Lp&#33539;&#25968;&#38480;&#21046;&#30340;&#20960;&#20046;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#39057;&#23646;&#24615;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#26041;&#27861;&#36827;&#34892;&#38450;&#24481;&#65292;&#24182;&#19988;&#24456;&#38590;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#23454;&#29616;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#32570;&#38519;&#65292;&#26377;&#20123;&#24037;&#20316;&#25552;&#20986;&#20102;&#26080;&#38480;&#21046;&#25915;&#20987;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20196;&#20154;&#22833;&#26395;&#30340;&#26159;&#65292;&#36825;&#20123;&#31034;&#20363;&#36890;&#24120;&#30475;&#36215;&#26469;&#19981;&#33258;&#28982;&#65292;&#20250;&#24341;&#36215;&#35686;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#20142;&#24230;&#25915;&#20987;(ALA)&#65292;&#19968;&#31181;&#38024;&#23545;&#20462;&#25913;&#22270;&#20687;&#20142;&#24230;&#30340;&#30333;&#30418;&#26080;&#38480;&#21046;&#23545;&#25239;&#25915;&#20987;&#12290;&#23545;&#20110;&#20154;&#31867;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#30340;&#26679;&#26412;&#30340;&#24418;&#29366;&#21644;&#39068;&#33394;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#33719;&#24471;&#20855;&#26377;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#22312;&#20142;&#24230;&#21644;&#38452;&#24433;&#20851;&#31995;&#26041;&#38754;&#25552;&#20986;&#20102;&#26080;&#32422;&#26463;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most researchers have tried to enhance the robustness of DNNs by revealing and repairing the vulnerability of DNNs with specialized adversarial examples. Parts of the attack examples have imperceptible perturbations restricted by Lp norm. However, due to their high-frequency property, the adversarial examples can be defended by denoising methods and are hard to realize in the physical world. To avoid the defects, some works have proposed unrestricted attacks to gain better robustness and practicality. It is disappointing that these examples usually look unnatural and can alert the guards. In this paper, we propose Adversarial Lightness Attack (ALA), a white-box unrestricted adversarial attack that focuses on modifying the lightness of the images. The shape and color of the samples, which are crucial to human perception, are barely influenced. To obtain adversarial examples with a high attack success rate, we propose unconstrained enhancement in terms of the light and shade relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item></channel></rss>