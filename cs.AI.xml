<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#22836;&#37096;&#23039;&#21183;&#29305;&#24449;&#21644;&#30524;&#21160;&#25968;&#25454;&#23545;&#39550;&#39542;&#21592;&#22312;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20934;&#22791;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM&#20307;&#31995;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24314;&#27169;&#65292;&#25104;&#21151;&#22320;&#35780;&#20272;&#20102;&#39550;&#39542;&#21592;&#30340;&#20934;&#22791;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11284</link><description>&lt;p&gt;
&#36890;&#36807;&#30524;&#21160;&#25968;&#25454;&#21644;&#22836;&#37096;&#23039;&#21183;&#35780;&#20272;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#39550;&#39542;&#21592;&#30340;&#20934;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Driver Readiness in Conditionally Automated Vehicles from Eye-Tracking Data and Head Pose. (arXiv:2401.11284v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#22836;&#37096;&#23039;&#21183;&#29305;&#24449;&#21644;&#30524;&#21160;&#25968;&#25454;&#23545;&#39550;&#39542;&#21592;&#22312;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20934;&#22791;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM&#20307;&#31995;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24314;&#27169;&#65292;&#25104;&#21151;&#22320;&#35780;&#20272;&#20102;&#39550;&#39542;&#21592;&#30340;&#20934;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#39550;&#39542;&#21592;&#22312;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#24674;&#22797;&#23545;&#36710;&#36742;&#25511;&#21046;&#30340;&#35282;&#33394;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#22312;SAE Level 3&#25110;&#37096;&#20998;&#33258;&#21160;&#21270;&#30340;&#36710;&#36742;&#20013;&#65292;&#39550;&#39542;&#21592;&#38656;&#35201;&#22312;&#24517;&#35201;&#26102;&#20445;&#25345;&#21487;&#29992;&#21644;&#20934;&#22791;&#22909;&#20171;&#20837;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#35780;&#20272;&#20182;&#20204;&#30340;&#20934;&#22791;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#22836;&#37096;&#23039;&#21183;&#29305;&#24449;&#21644;&#30524;&#21160;&#25968;&#25454;&#23545;&#39550;&#39542;&#21592;&#20934;&#22791;&#24615;&#35780;&#20272;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#27979;&#27169;&#22411;&#22312;&#35780;&#20272;&#39550;&#39542;&#21592;&#20934;&#22791;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#26377;&#38480;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;LSTM&#20307;&#31995;&#32467;&#26500;&#65292;&#24314;&#27169;&#39550;&#39542;&#21592;&#20934;&#22791;&#24615;&#22522;&#20110;&#39550;&#39542;&#21592;&#22836;&#37096;&#23039;&#21183;&#21644;&#27880;&#35270;&#30340;&#26102;&#31354;&#29366;&#24577;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32467;&#21512;&#20004;&#20010;&#29305;&#24449;&#38598;&#30340;&#21452;&#21521;LSTM&#20307;&#31995;&#32467;&#26500;&#22312;DMD&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.363&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As automated driving technology advances, the role of the driver to resume control of the vehicle in conditionally automated vehicles becomes increasingly critical. In the SAE Level 3 or partly automated vehicles, the driver needs to be available and ready to intervene when necessary. This makes it essential to evaluate their readiness accurately. This article presents a comprehensive analysis of driver readiness assessment by combining head pose features and eye-tracking data. The study explores the effectiveness of predictive models in evaluating driver readiness, addressing the challenges of dataset limitations and limited ground truth labels. Machine learning techniques, including LSTM architectures, are utilised to model driver readiness based on the Spatio-temporal status of the driver's head pose and eye gaze. The experiments in this article revealed that a Bidirectional LSTM architecture, combining both feature sets, achieves a mean absolute error of 0.363 on the DMD dataset, d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#24046;&#24322;&#30340;&#36890;&#29992;&#24037;&#20855;&#65292;&#21363;&#22810;&#26234;&#33021;&#20307;&#25919;&#31574;&#36317;&#31163;&#65288;MAPD&#65289;&#12290;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#26465;&#20214;&#34920;&#31034;&#65292;MAPD&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#36317;&#31163;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#23450;&#21046;&#21270;&#29256;&#26412;&#20197;&#37327;&#21270;&#26234;&#33021;&#20307;&#25919;&#31574;&#22312;&#29305;&#23450;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#24037;&#20855;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#30340;&#28436;&#21464;&#65292;&#36824;&#20026;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;MARL&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.11257</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#36317;&#31163;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Policy Distance for Multi-Agent Reinforcement Learning. (arXiv:2401.11257v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#24046;&#24322;&#30340;&#36890;&#29992;&#24037;&#20855;&#65292;&#21363;&#22810;&#26234;&#33021;&#20307;&#25919;&#31574;&#36317;&#31163;&#65288;MAPD&#65289;&#12290;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#26465;&#20214;&#34920;&#31034;&#65292;MAPD&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#36317;&#31163;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#23450;&#21046;&#21270;&#29256;&#26412;&#20197;&#37327;&#21270;&#26234;&#33021;&#20307;&#25919;&#31574;&#22312;&#29305;&#23450;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#24037;&#20855;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#30340;&#28436;&#21464;&#65292;&#36824;&#20026;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;MARL&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#24615;&#22312;&#25913;&#21892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#35768;&#22810;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;MARL&#20013;&#36807;&#22810;&#21442;&#25968;&#20849;&#20139;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#36890;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#24046;&#24322;&#12290;&#36825;&#26679;&#30340;&#24230;&#37327;&#26631;&#20934;&#19981;&#20165;&#21487;&#20197;&#26041;&#20415;&#22320;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#30340;&#28436;&#21464;&#65292;&#36824;&#21487;&#20197;&#20026;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;MARL&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#25919;&#31574;&#36317;&#31163;&#65288;MAPD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;MARL&#20013;&#25919;&#31574;&#24046;&#24322;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#26465;&#20214;&#34920;&#31034;&#65292;MAPD&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;MAPD&#25193;&#23637;&#20026;&#21487;&#23450;&#21046;&#30340;&#29256;&#26412;&#65292;&#21487;&#20197;&#37327;&#21270;&#22312;&#25351;&#23450;&#26041;&#38754;&#30340;&#26234;&#33021;&#20307;&#25919;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22522;&#20110;MAPD&#30340;&#22312;&#32447;&#37096;&#32626;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#21442;&#25968;&#20849;&#20139;&#65288;MAD&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MAD
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#34701;&#21512;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#29992;&#20110;&#32534;&#30721;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#26368;&#20339;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11252</link><description>&lt;p&gt;
&#33258;&#21160;&#34701;&#21512;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20197;&#25552;&#21319;&#21307;&#30103;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Automated Fusion of Multimodal Electronic Health Records for Better Medical Predictions. (arXiv:2401.11252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#34701;&#21512;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#29992;&#20110;&#32534;&#30721;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#26368;&#20339;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#26426;&#26500;&#24191;&#27867;&#37319;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#20135;&#29983;&#20102;&#22823;&#37327;&#21307;&#30103;&#25968;&#25454;&#65292;&#20026;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;EHR&#25968;&#25454;&#20013;&#22797;&#26434;&#22810;&#26679;&#30340;&#27169;&#24577;&#21644;&#29305;&#24449;&#32467;&#26500;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#22522;&#20110;&#30452;&#35273;&#21644;&#32463;&#39564;&#30340;&#25163;&#24037;&#27169;&#22411;&#26550;&#26500;&#65292;&#23548;&#33268;&#23376;&#20248;&#27169;&#22411;&#26550;&#26500;&#21644;&#26377;&#38480;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33258;&#21160;&#21270;EHR&#25968;&#25454;&#25366;&#25496;&#27169;&#22411;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoFM&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#25628;&#32034;&#29992;&#20110;&#32534;&#30721;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#26368;&#20339;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;EHR&#25968;&#25454;&#21644;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Electronic Health Record (EHR) systems in healthcare institutes has generated vast amounts of medical data, offering significant opportunities for improving healthcare services through deep learning techniques. However, the complex and diverse modalities and feature structures in real-world EHR data pose great challenges for deep learning model design. To address the multi-modality challenge in EHR data, current approaches primarily rely on hand-crafted model architectures based on intuition and empirical experiences, leading to sub-optimal model architectures and limited performance. Therefore, to automate the process of model design for mining EHR data, we propose a novel neural architecture search (NAS) framework named AutoFM, which can automatically search for the optimal model architectures for encoding diverse input modalities and fusion strategies. We conduct thorough experiments on real-world multi-modal EHR data and prediction tasks, and the results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20449;&#20219;&#21644;&#20010;&#20154;&#20449;&#24687;&#38544;&#31169;&#20851;&#27880;&#26159;&#21542;&#25104;&#20026;&#20351;&#29992;&#26126;&#30830;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20581;&#24247;&#20445;&#38505;&#30340;&#38556;&#30861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#35265;&#24615;&#38477;&#20302;&#20102;&#20449;&#20219;&#27700;&#24179;&#65292;&#32780;&#38544;&#31169;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24773;&#20917;&#19979;&#26356;&#39640;&#65292;&#20294;&#24182;&#27809;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.11249</link><description>&lt;p&gt;
&#35780;&#20272;&#20449;&#20219;&#21644;&#20010;&#20154;&#20449;&#24687;&#38544;&#31169;&#20851;&#27880;&#26159;&#21542;&#25104;&#20026;&#20351;&#29992;&#26126;&#30830;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20581;&#24247;&#20445;&#38505;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Evaluating if trust and personal information privacy concerns are barriers to using health insurance that explicitly utilizes AI. (arXiv:2401.11249v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20449;&#20219;&#21644;&#20010;&#20154;&#20449;&#24687;&#38544;&#31169;&#20851;&#27880;&#26159;&#21542;&#25104;&#20026;&#20351;&#29992;&#26126;&#30830;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20581;&#24247;&#20445;&#38505;&#30340;&#38556;&#30861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#35265;&#24615;&#38477;&#20302;&#20102;&#20449;&#20219;&#27700;&#24179;&#65292;&#32780;&#38544;&#31169;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24773;&#20917;&#19979;&#26356;&#39640;&#65292;&#20294;&#24182;&#27809;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#21644;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#22312;&#32447;&#20132;&#26131;&#20013;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20998;&#20139;&#20581;&#24247;&#20449;&#24687;&#23588;&#20854;&#25935;&#24863;&#65292;&#20294;&#23545;&#20110;&#36141;&#20080;&#21644;&#20351;&#29992;&#20581;&#24247;&#20445;&#38505;&#26469;&#35828;&#26159;&#24517;&#35201;&#30340;&#12290;&#35777;&#25454;&#34920;&#26126;&#65292;&#28040;&#36153;&#32773;&#36234;&#26469;&#36234;&#24895;&#24847;&#20351;&#29992;&#25216;&#26415;&#20195;&#26367;&#20154;&#31867;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#20351;&#29992;&#21487;&#33021;&#25913;&#21464;&#36825;&#19968;&#28857;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20449;&#20219;&#21644;&#38544;&#31169;&#20851;&#27880;&#26159;&#21542;&#25104;&#20026;&#20581;&#24247;&#20445;&#38505;&#20013;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#30340;&#38556;&#30861;&#12290;&#27604;&#36739;&#20102;&#20004;&#20010;&#24773;&#26223;&#65306;&#31532;&#19968;&#20010;&#24773;&#26223;&#20013;&#65292;&#26377;&#38480;&#30340;&#20154;&#24037;&#26234;&#33021;&#19981;&#22312;&#30028;&#38754;&#19978;&#65292;&#24182;&#19988;&#20854;&#23384;&#22312;&#27809;&#26377;&#26126;&#30830;&#21578;&#30693;&#28040;&#36153;&#32773;&#12290;&#22312;&#31532;&#20108;&#20010;&#24773;&#26223;&#20013;&#65292;&#26377;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#30028;&#38754;&#21644;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#65292;&#24182;&#19988;&#36825;&#19968;&#28857;&#26126;&#30830;&#21578;&#30693;&#28040;&#36153;&#32773;&#12290;&#20351;&#29992;SEM PLS-MGA&#27169;&#22411;&#23545;&#36825;&#20004;&#20010;&#24773;&#26223;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20449;&#20219;&#22312;&#31532;&#20108;&#20010;&#24773;&#26223;&#20013;&#26126;&#26174;&#36739;&#20302;&#65292;&#20063;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#21487;&#35265;&#30340;&#24773;&#20917;&#12290;&#38544;&#31169;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24773;&#20917;&#19979;&#26356;&#39640;&#65292;&#20294;&#22312;&#27169;&#22411;&#20869;&#37096;&#24182;&#27809;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust and privacy have emerged as significant concerns in online transactions. Sharing information on health is especially sensitive but it is necessary for purchasing and utilizing health insurance. Evidence shows that consumers are increasingly comfortable with technology in place of humans, but the expanding use of AI potentially changes this. This research explores whether trust and privacy concern are barriers to the adoption of AI in health insurance. Two scenarios are compared: The first scenario has limited AI that is not in the interface and its presence is not explicitly revealed to the consumer. In the second scenario there is an AI interface and AI evaluation, and this is explicitly revealed to the consumer. The two scenarios were modeled and compared using SEM PLS-MGA. The findings show that trust is significantly lower in the second scenario where AI is visible. Privacy concerns are higher with AI but the difference is not statistically significant within the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;(TreeMIL)&#65292;&#29992;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#25972;&#20010;&#24207;&#21015;&#20998;&#35299;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#24182;&#25552;&#21462;&#23376;&#24207;&#21015;&#29305;&#24449;&#65292;&#26088;&#22312;&#35299;&#20915;&#38598;&#20307;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.11235</link><description>&lt;p&gt;
TreeMIL&#65306;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly Detection with Inexact Supervision. (arXiv:2401.11235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;(TreeMIL)&#65292;&#29992;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#25972;&#20010;&#24207;&#21015;&#20998;&#35299;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#24182;&#25552;&#21462;&#23376;&#24207;&#21015;&#29305;&#24449;&#65292;&#26088;&#22312;&#35299;&#20915;&#38598;&#20307;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22312;&#21307;&#30103;&#12289;&#32593;&#32476;&#21644;&#24037;&#19994;&#31561;&#21508;&#20010;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#32771;&#34385;&#21040;&#26631;&#31614;&#23545;&#20110;&#26816;&#27979;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#24456;&#38590;&#33719;&#24471;&#65292;&#25105;&#20204;&#36716;&#32780;&#30740;&#31350;&#24102;&#26377;&#19981;&#31934;&#30830;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#35757;&#32451;&#38454;&#27573;&#21482;&#25552;&#20379;&#24207;&#21015;&#32423;&#21035;&#30340;&#26631;&#31614;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#39044;&#27979;&#20986;&#28857;&#32423;&#21035;&#30340;&#24322;&#24120;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#20256;&#32479;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;(MIL)&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#40723;&#21169;&#22312;&#21508;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#24471;&#21040;&#39640;&#30340;&#24322;&#24120;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#19981;&#20165;&#38480;&#20110;&#20010;&#21035;&#28857;&#24322;&#24120;&#65292;&#23427;&#20204;&#36824;&#21487;&#20197;&#26159;&#38598;&#20307;&#24322;&#24120;&#65292;&#36890;&#24120;&#22312;&#23376;&#24207;&#21015;&#20013;&#23637;&#31034;&#20986;&#24322;&#24120;&#27169;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;MIL&#26694;&#26550;(TreeMIL)&#12290;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#19968;&#20010;N&#21449;&#26641;&#32467;&#26500;&#23558;&#25972;&#20010;&#24207;&#21015;&#20998;&#25104;&#22810;&#20010;&#33410;&#28857;&#65292;&#19981;&#21516;&#23618;&#32423;&#30340;&#33410;&#28857;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#23376;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#23376;&#24207;&#21015;&#29305;&#24449;&#34987;&#25552;&#21462;&#26469;&#39044;&#27979;&#33410;&#28857;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection (TSAD) plays a vital role in various domains such as healthcare, networks, and industry. Considering labels are crucial for detection but difficult to obtain, we turn to TSAD with inexact supervision: only series-level labels are provided during the training phase, while point-level anomalies are predicted during the testing phase. Previous works follow a traditional multi-instance learning (MIL) approach, which focuses on encouraging high anomaly scores at individual time steps. However, time series anomalies are not only limited to individual point anomalies, they can also be collective anomalies, typically exhibiting abnormal patterns over subsequences. To address the challenge of collective anomalies, in this paper, we propose a tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to divide the entire series into multiple nodes, where nodes at different levels represent subsequences with different lengths. Then, the subsequence fe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;&#24223;&#27700;&#22788;&#29702;&#21378;&#28342;&#35299;&#27687;&#27987;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#21478;&#19968;&#20010;&#24037;&#19994;&#21378;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#35757;&#32451;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#22791;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.11217</link><description>&lt;p&gt;
&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#25552;&#39640;&#24037;&#19994;&#24223;&#27700;&#22788;&#29702;&#21378;&#28342;&#35299;&#27687;&#27987;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Approach of Transfer Learning and Physics-Informed Modeling: Improving Dissolved Oxygen Concentration Prediction in an Industrial Wastewater Treatment Plant. (arXiv:2401.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;&#24223;&#27700;&#22788;&#29702;&#21378;&#28342;&#35299;&#27687;&#27987;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#21478;&#19968;&#20010;&#24037;&#19994;&#21378;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#35757;&#32451;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#22791;&#29289;&#29702;&#20449;&#24687;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#31995;&#32479;&#65288;&#22914;&#24223;&#27700;&#22788;&#29702;&#21333;&#20803;&#65289;&#30340;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#30528;&#32570;&#22833;&#12289;&#20302;&#36136;&#37327;&#25110;&#22122;&#22768;&#25968;&#25454;&#31561;&#38382;&#39064;&#12290;&#20256;&#36755;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#23558;&#26469;&#33258;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20256;&#36755;&#65288;i&#65289;&#25429;&#33719;&#36807;&#31243;&#24213;&#23618;&#29289;&#29702;&#29305;&#24615;&#30340;&#24320;&#28304;&#20223;&#30495;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#19982;&#30446;&#26631;&#24037;&#21378;&#26377;&#25152;&#19981;&#21516;&#65292;&#65288;ii&#65289;&#21478;&#19968;&#20010;&#29305;&#28857;&#26159;&#22122;&#22768;&#21644;&#26377;&#38480;&#25968;&#25454;&#30340;&#21516;&#19968;&#28860;&#27833;&#21378;&#30340;&#24037;&#19994;&#21378;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#26469;&#33258;&#65288;ii&#65289;&#27169;&#22411;&#65292;&#24182;&#23558;&#35757;&#32451;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#24314;&#27169;&#20026;&#29289;&#29702;&#20449;&#24687;&#65292;&#20174;&#24320;&#28304;&#27169;&#22411;&#30340;&#23548;&#20986;&#29289;&#29702;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing first principles models is a challenging task for nonlinear and complex systems such as a wastewater treatment unit. In recent years, data-driven models are widely used to overcome the complexity. However, they often suffer from issues such as missing, low quality or noisy data. Transfer learning is a solution for this issue where knowledge from another task is transferred to target one to increase the prediction performance. In this work, the objective is increasing the prediction performance of an industrial wastewater treatment plant by transferring the knowledge of (i) an open-source simulation model that captures the underlying physics of the process, albeit with dissimilarities to the target plant, (ii) another industrial plant characterized by noisy and limited data but located in the same refinery, and (iii) the model in (ii) and making the objective function of the training problem physics informed where the physics information derived from the open-source model i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#32771;&#34385;&#20102;&#38598;&#21512;&#35774;&#22791;&#30340;&#21160;&#24577;&#21512;&#20316;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#29992;&#20110;&#32534;&#31243;&#35745;&#31639;&#38598;&#20307;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.11212</link><description>&lt;p&gt;
&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#32534;&#31243;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Programming Distributed Collective Processes in the eXchange Calculus. (arXiv:2401.11212v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#32771;&#34385;&#20102;&#38598;&#21512;&#35774;&#22791;&#30340;&#21160;&#24577;&#21512;&#20316;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#29992;&#20110;&#32534;&#31243;&#35745;&#31639;&#38598;&#20307;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36235;&#21183;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#25552;&#20986;&#20102;&#22312;&#20960;&#20046;&#25152;&#26377;&#29615;&#22659;&#20013;&#23494;&#38598;&#21644;&#22810;&#23610;&#24230;&#37096;&#32626;&#35745;&#31639;&#35774;&#22791;&#30340;&#24895;&#26223;&#12290;&#19968;&#20010;&#31361;&#20986;&#30340;&#24037;&#31243;&#25361;&#25112;&#22260;&#32469;&#30528;&#32534;&#31243;&#36825;&#31181;&#35745;&#31639;&#29983;&#24577;&#31995;&#32479;&#30340;&#38598;&#20307;&#33258;&#36866;&#24212;&#34892;&#20026;&#12290;&#36825;&#38656;&#35201;&#33021;&#22815;&#25429;&#25417;&#27010;&#24565;&#65288;&#21160;&#24577;&#21512;&#20316;&#35774;&#22791;&#32676;&#32452;&#65289;&#21644;&#38598;&#20307;&#20219;&#21153;&#65288;&#30001;&#21512;&#22863;&#32452;&#25191;&#34892;&#30340;&#32852;&#21512;&#27963;&#21160;&#65289;&#30340;&#25277;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19982;&#37051;&#23621;&#20132;&#20114;&#24182;&#20197;&#20960;&#20046;&#21516;&#27493;&#30340;&#24863;&#30693;-&#35745;&#31639;-&#20132;&#20114;&#24490;&#29615;&#25191;&#34892;&#30340;&#35774;&#22791;&#38598;&#21512;&#65292;&#20854;&#20013;&#35745;&#31639;&#30001;&#19968;&#20010;&#23558;&#24863;&#30693;&#20540;&#21644;&#20256;&#20837;&#28040;&#24687;&#26144;&#23556;&#21040;&#36755;&#20986;&#21644;&#20256;&#20986;&#28040;&#24687;&#30340;&#21333;&#20010;&#31243;&#24207;&#32473;&#20986;&#12290;&#20026;&#20102;&#25903;&#25345;&#25972;&#20010;&#35745;&#31639;&#38598;&#20307;&#30340;&#32534;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;&#30340;&#25277;&#35937;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23450;&#20041;&#21512;&#22863;&#32452;&#30340;&#24418;&#25104;&#36923;&#36753;&#21644;&#23427;&#30340;&#38598;&#20307;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#24418;&#24335;&#21270;&#20102;&#36825;&#31181;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent trends like the Internet of Things (IoT) suggest a vision of dense and multi-scale deployments of computing devices in nearly all kinds of environments. A prominent engineering challenge revolves around programming the collective adaptive behaviour of such computational ecosystems. This requires abstractions able to capture concepts like ensembles (dynamic groups of cooperating devices) and collective tasks (joint activities carried out by ensembles). In this work, we consider collections of devices interacting with neighbours and that execute in nearly-synchronised sense-compute-interact rounds, where the computation is given by a single program mapping sensing values and incoming messages to output and outcoming messages. To support programming whole computational collectives, we propose the abstraction of a distributed collective process, which can be used to define at once the ensemble formation logic and its collective task. We formalise the abstraction in the eXchange Calc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26377;&#24847;&#35265;&#30340;&#29992;&#25143;&#22312;&#25628;&#32034;&#34892;&#20026;&#20013;&#20851;&#27880;&#24230;&#21644;&#21453;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26292;&#38706;&#20110;&#20559;&#35265;&#25628;&#32034;&#32467;&#26524;&#20250;&#22686;&#21152;&#20182;&#20204;&#28040;&#36153;&#21453;&#23545;&#24577;&#24230;&#20869;&#23481;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.11201</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#20013;&#25506;&#31350;&#29992;&#25143;&#34892;&#20026;&#20197;&#26816;&#27979;&#21442;&#19982;&#24230;&#21644;&#21453;&#20316;&#29992;&#25928;&#24212;&#30340;&#34180;&#32447;
&lt;/p&gt;
&lt;p&gt;
Navigating the Thin Line: Examining User Behavior in Search to Detect Engagement and Backfire Effects. (arXiv:2401.11201v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26377;&#24847;&#35265;&#30340;&#29992;&#25143;&#22312;&#25628;&#32034;&#34892;&#20026;&#20013;&#20851;&#27880;&#24230;&#21644;&#21453;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26292;&#38706;&#20110;&#20559;&#35265;&#25628;&#32034;&#32467;&#26524;&#20250;&#22686;&#21152;&#20182;&#20204;&#28040;&#36153;&#21453;&#23545;&#24577;&#24230;&#20869;&#23481;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#20559;&#35265;&#30340;&#29992;&#25143;&#32463;&#24120;&#23547;&#27714;&#19982;&#20854;&#20808;&#21069;&#20449;&#24565;&#19968;&#33268;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#24573;&#35270;&#30456;&#30683;&#30462;&#30340;&#35777;&#25454;&#65292;&#36825;&#26159;&#30001;&#20110;&#30830;&#35748;&#20559;&#35265;&#25152;&#33268;&#12290;&#36825;&#31181;&#34892;&#20026;&#38459;&#30861;&#20102;&#20182;&#20204;&#22312;&#25628;&#32034;&#32593;&#32476;&#26102;&#32771;&#34385;&#26367;&#20195;&#31435;&#22330;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20998;&#26512;&#20105;&#35758;&#35805;&#39064;&#30340;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#22914;&#20309;&#24433;&#21709;&#39640;&#24230;&#26377;&#24847;&#35265;&#30340;&#29992;&#25143;&#30340;&#25628;&#32034;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#39044;&#27880;&#20876;&#30340;&#29992;&#25143;&#30740;&#31350;(n = 257)&#65292;&#35843;&#26597;&#20102;&#19981;&#21516;&#27700;&#24179;(&#20302;&#21644;&#39640;)&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#25628;&#32034;&#32467;&#26524;&#23637;&#31034;(&#24102;&#25110;&#19981;&#24102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#30340;&#31435;&#22330;&#26631;&#31614;)&#26159;&#21542;&#20250;&#24433;&#21709;&#26377;&#24847;&#35265;&#30340;&#29992;&#25143;&#22312;&#19977;&#20010;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#19978;(&#21363;&#26080;&#31070;&#35770;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#26657;&#26381;)&#30340;&#31435;&#22330;&#22810;&#26679;&#24615;&#28040;&#36153;&#21644;&#25628;&#32034;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21442;&#19982;&#32773;&#26292;&#38706;&#20110;(&#21453;&#24577;&#24230;)&#20559;&#35265;&#30340;&#25628;&#32034;&#32467;&#26524;&#20250;&#22686;&#21152;&#20182;&#20204;&#28040;&#36153;&#21453;&#23545;&#24577;&#24230;&#20869;&#23481;&#30340;&#25968;&#37327;&#65292;&#20294;&#25105;&#20204;&#20063;&#21457;&#29616;&#20559;&#35265;&#19982;&#36235;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinionated users often seek information that aligns with their preexisting beliefs while dismissing contradictory evidence due to confirmation bias. This conduct hinders their ability to consider alternative stances when searching the web. Despite this, few studies have analyzed how the diversification of search results on disputed topics influences the search behavior of highly opinionated users. To this end, we present a preregistered user study (n = 257) investigating whether different levels (low and high) of bias metrics and search results presentation (with or without AI-predicted stances labels) can affect the stance diversity consumption and search behavior of opinionated users on three debated topics (i.e., atheism, intellectual property rights, and school uniforms). Our results show that exposing participants to (counter-attitudinally) biased search results increases their consumption of attitude-opposing content, but we also found that bias was associated with a trend towar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#26522;&#20030;DN&#20998;&#21306;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#24120;&#29992;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21457;&#29616;&#22914;&#26524;&#21482;&#23545;&#20855;&#26377;&#8220;&#22823;&#8221;&#20307;&#31215;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65292;&#37027;&#20040;&#23545;&#31354;&#38388;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#38750;&#24120;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.11188</link><description>&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#26522;&#20030;&#28145;&#24230;&#32593;&#32476;&#20998;&#21306;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Fast and Exact Enumeration of Deep Networks Partitions Regions. (arXiv:2401.11188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#26522;&#20030;DN&#20998;&#21306;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#24120;&#29992;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21457;&#29616;&#22914;&#26524;&#21482;&#23545;&#20855;&#26377;&#8220;&#22823;&#8221;&#20307;&#31215;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65292;&#37027;&#20040;&#23545;&#31354;&#38388;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#38750;&#24120;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;(DNs)&#30340;&#19968;&#20010;&#23500;&#26377;&#25104;&#26524;&#30340;&#34920;&#36848;&#26041;&#24335;&#26159;&#36890;&#36807;&#20998;&#27573;&#20223;&#23556;&#26679;&#26465;&#26469;&#23454;&#29616;&#65292;&#23427;&#26082;&#33021;&#22815;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#21448;&#33021;&#20026;&#20174;&#19994;&#32773;&#25552;&#20379;&#23454;&#36341;&#25351;&#23548;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;DN&#30340;&#36755;&#20837;&#26144;&#23556;&#34987;&#34920;&#36798;&#20026;&#25353;&#21306;&#22495;&#30340;&#20223;&#23556;&#26144;&#23556;&#65292;&#20854;&#20013;&#36825;&#20123;&#21306;&#22495;&#30001;&#27169;&#22411;&#30340;&#26550;&#26500;&#38544;&#24335;&#30830;&#23450;&#65292;&#24182;&#24418;&#25104;&#23427;&#20204;&#30340;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#20010;&#20998;&#21306;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#20998;&#21306;&#8212;&#8212;&#36825;&#20010;&#30740;&#31350;&#32447;&#20013;&#20135;&#29983;&#30340;&#25152;&#26377;&#32467;&#26524;&#37117;&#28041;&#21450;&#21040;&#23427;&#8212;&#8212;&#21482;&#22312;DN&#30340;&#36755;&#20837;&#31354;&#38388;&#30340;$2/3$&#32500;&#20999;&#29255;&#19978;&#35745;&#31639;&#36807;&#65292;&#25110;&#32773;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#26522;&#20030;DN&#20998;&#21306;&#21306;&#22495;&#30340;&#24182;&#34892;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26368;&#32456;&#33021;&#22815;&#23545;&#24120;&#29992;&#30340;&#36817;&#20284;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#20363;&#22914;&#22522;&#20110;DN&#36755;&#20837;&#31354;&#38388;&#30340;&#38543;&#26426;&#25277;&#26679;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#26159;&#65292;&#22914;&#26524;&#21482;&#23545;&#20855;&#26377;&#8220;&#22823;&#8221;&#20307;&#31215;&#30340;&#21306;&#22495;&#24863;&#20852;&#36259;&#65292;&#37027;&#20040;&#23545;&#31354;&#38388;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#38750;&#24120;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN's input-mapping is expressed as per-region affine mapping where those regions are implicitly determined by the model's architecture and form a partition of their input space. That partition -- which is involved in all the results spanned from this line of research -- has so far only been computed on $2/3$-dimensional slices of the DN's input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN's partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with ``large'' volume, then uniform sampling of the space is highly efficient
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.11174</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#21035;&#35782;&#21035;&#29992;&#20110;&#25972;&#20307;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Prostatectomies&#30340;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#65288;GraSP&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#24314;&#27169;&#65292;&#21253;&#25324;&#19981;&#21516;&#31890;&#24230;&#30340;&#20114;&#34917;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#22806;&#31185;&#27963;&#21160;&#30340;&#22810;&#23618;&#27425;&#29702;&#35299;&#65292;&#21253;&#25324;&#22806;&#31185;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#20197;&#21450;&#21253;&#25324;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#21644;&#21407;&#23376;&#21487;&#35270;&#21160;&#20316;&#26816;&#27979;&#22312;&#20869;&#30340;&#30701;&#26399;&#20219;&#21153;&#12290;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#24418;&#22120;&#65288;Transformers&#65289;&#30340;&#34892;&#21160;&#12289;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#22120;&#26800;&#20998;&#21106;&#65288;TAPIS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#26469;&#33258;&#22120;&#26800;&#20998;&#21106;&#27169;&#22411;&#30340;&#23616;&#37096;&#21306;&#22495;&#24314;&#35758;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30701;&#26399;&#35782;&#21035;&#20219;&#21153;&#20013;&#21253;&#25324;&#20998;&#21106;&#27880;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#26174;&#20102;&#19981;&#21516;&#30340;&#31890;&#24230;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#30340;&#28436;&#35762;&#32773;&#39564;&#35777;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#20882;&#20805;&#32773;&#21644;&#27450;&#39575;&#25915;&#20987;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#20445;&#25252;&#21644;&#26356;&#32463;&#27982;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.11156</link><description>&lt;p&gt;
&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#28436;&#35762;&#32773;&#39564;&#35777;&#20197;&#23545;&#25239;&#27450;&#39575;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Generalizing Speaker Verification for Spoof Awareness in the Embedding Space. (arXiv:2401.11156v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#30340;&#28436;&#35762;&#32773;&#39564;&#35777;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#20882;&#20805;&#32773;&#21644;&#27450;&#39575;&#25915;&#20987;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#20445;&#25252;&#21644;&#26356;&#32463;&#27982;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#20247;&#25152;&#21608;&#30693;&#65292;&#33258;&#21160;&#28436;&#35762;&#32773;&#39564;&#35777;&#65288;ASV&#65289;&#31995;&#32479;&#21487;&#20197;&#34987;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#25163;&#27450;&#39575;&#12290;&#23545;&#25239;ASV&#31995;&#32479;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#24320;&#21457;&#19968;&#20010;&#29420;&#31435;&#30340;&#27450;&#39575;&#23545;&#31574;&#65288;CM&#65289;&#27169;&#22359;&#65292;&#23558;&#28436;&#35762;&#36755;&#20837;&#20998;&#31867;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#22312;&#35748;&#35777;&#38454;&#27573;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#21644;&#21033;&#29992;&#12290;&#21478;&#19968;&#31181;&#31574;&#30053;&#26159;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;ASV&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#38646;&#21162;&#21147;&#30340;&#20882;&#20805;&#32773;&#65288;&#38750;&#30446;&#26631;&#65289;&#21644;&#27450;&#39575;&#25915;&#20987;&#12290;&#36825;&#31181;&#20855;&#26377;&#27450;&#39575;&#24847;&#35782;&#30340;ASV&#31995;&#32479;&#26377;&#21487;&#33021;&#25552;&#20379;&#26356;&#24378;&#30340;&#20445;&#25252;&#21644;&#26356;&#32463;&#27982;&#30340;&#35745;&#31639;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#29420;&#31435;ASV&#65288;G-SASV&#65289;&#26469;&#23545;&#25239;&#27450;&#39575;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#65288;&#35748;&#35777;&#65289;&#38454;&#27573;&#19981;&#28041;&#21450;&#29420;&#31435;&#30340;CM&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;CM&#30340;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#31616;&#21333;&#30340;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#32806;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26032;&#30340;&#19977;&#38454;&#27573;&#24494;&#35843;&#36807;&#31243;&#65292;&#29992;&#20110;&#32531;&#35299;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30683;&#30462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.11140</link><description>&lt;p&gt;
&#31283;&#23450;&#19982;&#21487;&#22609;&#24615;&#35299;&#32806;&#30340;&#23569;&#26679;&#26412;&#31471;&#21040;&#31471;&#30446;&#26631;&#26816;&#27979;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end Object Detection. (arXiv:2401.11140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#32806;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26032;&#30340;&#19977;&#38454;&#27573;&#24494;&#35843;&#36807;&#31243;&#65292;&#29992;&#20110;&#32531;&#35299;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30683;&#30462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;FSOD&#65289;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#32454;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#32463;&#20856;&#30340;&#22522;&#30784;-&#26032;&#39062;&#20004;&#38454;&#27573;&#24494;&#35843;&#36807;&#31243;&#65292;&#20294;&#24573;&#30053;&#20102;&#19981;&#21516;&#27169;&#22359;&#20043;&#38388;&#30340;&#38544;&#21547;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30683;&#30462;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37325;&#26032;&#21021;&#22987;&#21270;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#38656;&#35201;&#26356;&#22823;&#30340;&#21487;&#22609;&#24615;&#26469;&#36866;&#24212;&#26032;&#39062;&#26679;&#26412;&#12290;&#32487;&#25215;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20854;&#20182;&#27169;&#22359;&#38656;&#35201;&#26356;&#22810;&#30340;&#31283;&#23450;&#24615;&#26469;&#20445;&#30041;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#30693;&#35782;&#12290;&#24120;&#35268;&#24494;&#35843;&#23558;&#36825;&#20004;&#20010;&#37096;&#20998;&#30340;&#20248;&#21270;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;FSOD&#22330;&#26223;&#19979;&#25439;&#23475;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#22312;&#31471;&#21040;&#31471;&#30446;&#26631;&#26816;&#27979;&#22120;Sparse R-CNN&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22810;&#20998;&#31867;&#22120;&#30340;&#32423;&#32852;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#21487;&#22609;&#24615;&#20998;&#31867;&#22120;&#24494;&#35843;&#65292;&#37319;&#29992;&#26032;&#30340;&#19977;&#38454;&#27573;&#24494;&#35843;&#36807;&#31243;&#26469;&#32531;&#35299;&#36825;&#20010;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot object detection(FSOD) aims to design methods to adapt object detectors efficiently with only few annotated samples. Fine-tuning has been shown to be an effective and practical approach. However, previous works often take the classical base-novel two stage fine-tuning procedure but ignore the implicit stability-plasticity contradiction among different modules. Specifically, the random re-initialized classifiers need more plasticity to adapt to novel samples. The other modules inheriting pre-trained weights demand more stability to reserve their class-agnostic knowledge. Regular fine-tuning which couples the optimization of these two parts hurts the model generalization in FSOD scenarios. In this paper, we find that this problem is prominent in the end-to-end object detector Sparse R-CNN for its multi-classifier cascaded architecture. We propose to mitigate this contradiction by a new three-stage fine-tuning procedure by introducing an addtional plasticity classifier fine-tunin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11120</link><description>&lt;p&gt;
&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;COVID-19&#38376;&#35786;&#27835;&#30103;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25645;&#37197;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#65288;CPGs&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#65288;CDS&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;CPGs&#32435;&#20837;LLMs&#30340;&#26041;&#27861;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#23558;CPGs&#32435;&#20837;LLMs&#65306;&#20108;&#20803;&#20915;&#31574;&#26641;&#65288;BDT&#65289;&#65292;&#31243;&#24207;&#36741;&#21161;&#22270;&#26500;&#24314;&#65288;PAGC&#65289;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;CoT-FSP&#65289;&#12290;&#20026;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#21512;&#25104;&#24739;&#32773;&#25551;&#36848;&#65292;&#24182;&#23545;&#30001;&#22235;&#20010;LLMs&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65306;GPT-4&#65292;GPT-3.5 Turbo&#65292;LLaMA&#21644;PaLM 2&#12290;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;ZSP&#65289;&#34987;&#29992;&#20316;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20197;COVID-19&#38376;&#35786;&#27835;&#30103;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22235;&#20010;LLMs&#22312;&#22686;&#21152;&#20102;CPGs&#21518;&#30456;&#23545;&#20110;&#22522;&#32447;ZSP&#23637;&#29616;&#20102;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;BDT&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;CoT-FSP&#21644;PAGC&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high per
&lt;/p&gt;</description></item><item><title>SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.11113</link><description>&lt;p&gt;
SPAND: &#20351;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11113
&lt;/p&gt;
&lt;p&gt;
SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#34892;&#20026;&#23545;&#20581;&#24247;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#23545;&#36523;&#24515;&#20581;&#24247;&#30340;&#25351;&#31034;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#65292;&#21487;&#20197;&#24110;&#21161;&#31649;&#29702;&#30561;&#30496;&#24182;&#36861;&#36394;&#30456;&#20851;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#30561;&#30496;&#34892;&#20026;&#21462;&#20915;&#20110;&#20010;&#20307;&#30340;&#29983;&#29702;&#29366;&#20917;&#65292;&#20294;&#20063;&#21463;&#21040;&#25968;&#23383;&#23186;&#20307;&#20351;&#29992;&#12289;&#31038;&#20132;&#32593;&#32476;&#20256;&#26579;&#20197;&#21450;&#21608;&#22260;&#22825;&#27668;&#31561;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPAND&#65288;Sleep Prediction Architecture using Network Dynamics&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22270;&#32593;&#32476;&#20013;&#30340;&#31038;&#20132;&#20256;&#26579;&#26469;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#30340;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#26222;&#36941;&#23384;&#22312;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#25552;&#21462;&#30340;&#29983;&#29702;&#21644;&#25163;&#26426;&#25968;&#25454;&#38598;&#25104;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#21253;&#21547;&#19982;&#30561;&#30496;&#34892;&#20026;&#26080;&#20851;&#30340;&#36830;&#25509;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;&#23616;&#38480;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#31361;&#26174;&#20986;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TypeDance&#30340;AI&#36741;&#21161;&#24037;&#20855;&#65292;&#36890;&#36807;&#32467;&#21512;&#35774;&#35745;&#29702;&#24565;&#19982;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#35821;&#20041;&#25490;&#29256;&#26631;&#24535;&#35774;&#35745;&#12290;&#23427;&#21033;&#29992;&#20174;&#19978;&#20256;&#30340;&#22270;&#20687;&#23454;&#20363;&#20013;&#25552;&#21462;&#30340;&#35774;&#35745;&#20808;&#39564;&#30693;&#35782;&#65292;&#25903;&#25345;&#19981;&#21516;&#32467;&#26500;&#31890;&#24230;&#19978;&#30340;&#23383;&#20307;-&#22270;&#20687;&#26144;&#23556;&#65292;&#36798;&#21040;&#22810;&#26679;&#21270;&#30340;&#32654;&#23398;&#35774;&#35745;&#21644;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.11094</link><description>&lt;p&gt;
TypeDance: &#36890;&#36807;&#20010;&#24615;&#21270;&#29983;&#25104;&#20174;&#22270;&#20687;&#20013;&#21019;&#24314;&#35821;&#20041;&#25490;&#29256;&#30340;&#26631;&#24535;
&lt;/p&gt;
&lt;p&gt;
TypeDance: Creating Semantic Typographic Logos from Image through Personalized Generation. (arXiv:2401.11094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TypeDance&#30340;AI&#36741;&#21161;&#24037;&#20855;&#65292;&#36890;&#36807;&#32467;&#21512;&#35774;&#35745;&#29702;&#24565;&#19982;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#35821;&#20041;&#25490;&#29256;&#26631;&#24535;&#35774;&#35745;&#12290;&#23427;&#21033;&#29992;&#20174;&#19978;&#20256;&#30340;&#22270;&#20687;&#23454;&#20363;&#20013;&#25552;&#21462;&#30340;&#35774;&#35745;&#20808;&#39564;&#30693;&#35782;&#65292;&#25903;&#25345;&#19981;&#21516;&#32467;&#26500;&#31890;&#24230;&#19978;&#30340;&#23383;&#20307;-&#22270;&#20687;&#26144;&#23556;&#65292;&#36798;&#21040;&#22810;&#26679;&#21270;&#30340;&#32654;&#23398;&#35774;&#35745;&#21644;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25490;&#29256;&#30340;&#26631;&#24535;&#23558;&#23383;&#20307;&#21644;&#22270;&#20687;&#34701;&#20026;&#19968;&#20307;&#65292;&#20197;&#20195;&#34920;&#35821;&#20041;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35835;&#24615;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#20351;&#29992;&#31354;&#38388;&#32452;&#21512;&#21644;&#24418;&#29366;&#26367;&#25442;&#21463;&#21040;&#20960;&#20309;&#24418;&#29366;&#19981;&#21516;&#30340;&#23383;&#20307;&#21644;&#35821;&#20041;&#20043;&#38388;&#26080;&#32541;&#34701;&#21512;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35821;&#20041;&#25490;&#29256;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#25490;&#38500;&#20102;&#35774;&#35745;&#24072;&#30340;&#21442;&#19982;&#65292;&#24182;&#24573;&#35270;&#20102;&#20010;&#24615;&#21270;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TypeDance&#30340;AI&#36741;&#21161;&#24037;&#20855;&#65292;&#23427;&#23558;&#35774;&#35745;&#29702;&#24565;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#20041;&#25490;&#29256;&#26631;&#24535;&#35774;&#35745;&#12290;&#23427;&#21033;&#29992;&#20174;&#19978;&#20256;&#30340;&#22270;&#20687;&#23454;&#20363;&#20013;&#25552;&#21462;&#30340;&#21487;&#32452;&#21512;&#30340;&#35774;&#35745;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25903;&#25345;&#22312;&#19981;&#21516;&#30340;&#32467;&#26500;&#31890;&#24230;&#19978;&#36827;&#34892;&#23383;&#20307;-&#22270;&#20687;&#26144;&#23556;&#65292;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#32654;&#23398;&#35774;&#35745;&#65292;&#24182;&#20855;&#26377;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;TypeDance&#20013;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#36873;&#25321;&#12289;&#29983;&#25104;&#12289;&#35780;&#20272;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic typographic logos harmoniously blend typeface and imagery to represent semantic concepts while maintaining legibility. Conventional methods using spatial composition and shape substitution are hindered by the conflicting requirement for achieving seamless spatial fusion between geometrically dissimilar typefaces and semantics. While recent advances made AI generation of semantic typography possible, the end-to-end approaches exclude designer involvement and disregard personalized design. This paper presents TypeDance, an AI-assisted tool incorporating design rationales with the generative model for personalized semantic typographic logo design. It leverages combinable design priors extracted from uploaded image exemplars and supports type-imagery mapping at various structural granularity, achieving diverse aesthetic designs with flexible control. Additionally, we instantiate a comprehensive design workflow in TypeDance, including ideation, selection, generation, evaluation, an
&lt;/p&gt;</description></item><item><title>FedRKG&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#30693;&#35782;&#22270;&#65292;&#26500;&#24314;&#20840;&#23616;&#30693;&#35782;&#22270;&#24182;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#30340;GNN&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#38454;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.11089</link><description>&lt;p&gt;
FedRKG&#65306;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#22686;&#24378;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedRKG: A Privacy-preserving Federated Recommendation Framework via Knowledge Graph Enhancement. (arXiv:2401.11089v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11089
&lt;/p&gt;
&lt;p&gt;
FedRKG&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#30693;&#35782;&#22270;&#65292;&#26500;&#24314;&#20840;&#23616;&#30693;&#35782;&#22270;&#24182;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#30340;GNN&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#38454;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#25429;&#25417;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#39640;&#38454;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#38459;&#30861;&#20102;&#25972;&#20010;&#29992;&#25143;-&#39033;&#30446;&#22270;&#30340;&#20840;&#23616;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#19968;&#20123;&#26041;&#27861;&#22312;&#22270;&#20013;&#21019;&#24314;&#20102;&#20266;&#20132;&#20114;&#39033;&#25110;&#29992;&#25143;&#65292;&#20197;&#24357;&#34917;&#27599;&#20010;&#23458;&#25143;&#31471;&#32570;&#22833;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#38543;&#26426;&#22122;&#22768;&#24182;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;FedRKG&#65292;&#22312;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#20844;&#24320;&#30340;&#39033;&#30446;&#20449;&#24687;&#26500;&#24314;&#21644;&#32500;&#25252;&#20840;&#23616;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#38454;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#12290;&#22312;&#23458;&#25143;&#31471;&#65292;&#19968;&#20010;&#20851;&#31995;&#24863;&#30693;&#30340;GNN&#27169;&#22411;&#21033;&#29992;&#22810;&#26679;&#30340;KG&#20851;&#31995;&#12290;&#20026;&#20102;&#20445;&#25252;&#26412;&#22320;&#20132;&#20114;&#39033;&#30446;&#21644;&#27169;&#31946;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising approach for preserving data privacy in recommendation systems by training models locally. Recently, Graph Neural Networks (GNN) have gained popularity in recommendation tasks due to their ability to capture high-order interactions between users and items. However, privacy concerns prevent the global sharing of the entire user-item graph. To address this limitation, some methods create pseudo-interacted items or users in the graph to compensate for missing information for each client. Unfortunately, these methods introduce random noise and raise privacy concerns. In this paper, we propose FedRKG, a novel federated recommendation system, where a global knowledge graph (KG) is constructed and maintained on the server using publicly available item information, enabling higher-order user-item interactions. On the client side, a relation-aware GNN model leverages diverse KG relationships. To protect local interaction items and obscure gradi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#23616;&#37096;&#34920;&#24449;&#23398;&#20064;&#21644;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#23616;&#23616;&#37096;&#23545;&#25239;&#36866;&#24212;&#21644;&#35821;&#20041;&#24863;&#30693;&#20266;&#26631;&#31614;&#29983;&#25104;&#26469;&#22686;&#24378;&#22495;&#19981;&#21464;&#21644;&#36776;&#21035;&#24615;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#20840;&#23616;&#23616;&#37096;&#39044;&#27979;&#19968;&#33268;&#24615;&#23398;&#20064;&#20197;&#25552;&#39640;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11085</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#23616;&#37096;&#34920;&#24449;&#23398;&#20064;&#21644;&#36873;&#25321;&#29992;&#20110;&#36328;&#22495;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adaptive Global-Local Representation Learning and Selection for Cross-Domain Facial Expression Recognition. (arXiv:2401.11085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#23616;&#37096;&#34920;&#24449;&#23398;&#20064;&#21644;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#23616;&#23616;&#37096;&#23545;&#25239;&#36866;&#24212;&#21644;&#35821;&#20041;&#24863;&#30693;&#20266;&#26631;&#31614;&#29983;&#25104;&#26469;&#22686;&#24378;&#22495;&#19981;&#21464;&#21644;&#36776;&#21035;&#24615;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#20840;&#23616;&#23616;&#37096;&#39044;&#27979;&#19968;&#33268;&#24615;&#23398;&#20064;&#20197;&#25552;&#39640;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#39046;&#22495;&#21464;&#21270;&#23545;&#20110;&#36328;&#22495;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;CD-FER&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#36866;&#24212;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#24573;&#35270;&#20102;&#23616;&#37096;&#29305;&#24449;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#20013;&#32570;&#20047;&#36776;&#21035;&#24335;&#30417;&#30563;&#65292;&#23548;&#33268;&#30446;&#26631;&#22495;&#20013;&#30340;&#29305;&#24449;&#34920;&#31034;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#23616;&#37096;&#34920;&#24449;&#23398;&#20064;&#21644;&#36873;&#25321;&#65288;AGLRLS&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20840;&#23616;&#23616;&#37096;&#23545;&#25239;&#36866;&#24212;&#21644;&#35821;&#20041;&#24863;&#30693;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#20013;&#22495;&#19981;&#21464;&#21644;&#36776;&#21035;&#24615;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#20840;&#23616;&#23616;&#37096;&#39044;&#27979;&#19968;&#33268;&#24615;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#21333;&#29420;&#30340;&#20840;&#23616;&#23616;&#37096;&#23545;&#25239;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain shift poses a significant challenge in Cross-Domain Facial Expression Recognition (CD-FER) due to the distribution variation across different domains. Current works mainly focus on learning domain-invariant features through global feature adaptation, while neglecting the transferability of local features. Additionally, these methods lack discriminative supervision during training on target datasets, resulting in deteriorated feature representation in target domain. To address these limitations, we propose an Adaptive Global-Local Representation Learning and Selection (AGLRLS) framework. The framework incorporates global-local adversarial adaptation and semantic-aware pseudo label generation to enhance the learning of domain-invariant and discriminative feature during training. Meanwhile, a global-local prediction consistency learning is introduced to improve classification results during inference. Specifically, the framework consists of separate global-local adversarial learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#30340;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65306;&#21253;&#32423;&#21035;&#25439;&#22833;&#21644;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#65292;&#24182;&#21457;&#29616;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#21253;&#32423;&#21035;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11081</link><description>&lt;p&gt;
&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#65306;&#23454;&#20363;&#32423;&#21035;&#19982;&#21253;&#32423;&#21035;&#30340;&#25439;&#22833;&#20989;&#25968;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions. (arXiv:2401.11081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#30340;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65306;&#21253;&#32423;&#21035;&#25439;&#22833;&#21644;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#65292;&#24182;&#21457;&#29616;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#21253;&#32423;&#21035;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#30340;&#22686;&#21152;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#22312;&#19982;&#23398;&#20064;&#32773;&#20849;&#20139;&#20043;&#21069;&#20250;&#34987;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#25935;&#24863;&#21709;&#24212;&#30340;&#38544;&#31169;&#12290;&#22312;&#32858;&#21512;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25968;&#25454;&#38598;&#34987;&#20998;&#32452;&#25104;&#26679;&#26412;&#30340;&#21253;&#65292;&#27599;&#20010;&#21253;&#21482;&#25552;&#20379;&#19968;&#20010;&#32858;&#21512;&#21709;&#24212;&#65292;&#25552;&#20379;&#20102;&#35813;&#21253;&#20013;&#20010;&#20307;&#21709;&#24212;&#30340;&#25688;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#21709;&#24212;&#20013;&#23398;&#20064;&#30340;&#20004;&#31181;&#33258;&#28982;&#25439;&#22833;&#20989;&#25968;&#65306;&#21253;&#32423;&#21035;&#25439;&#22833;&#21644;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#12290;&#22312;&#21069;&#32773;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32858;&#21512;&#21709;&#24212;&#19982;&#32858;&#21512;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#25439;&#22833;&#26469;&#23398;&#20064;&#65292;&#32780;&#22312;&#21518;&#32773;&#20013;&#65292;&#27169;&#22411;&#26088;&#22312;&#23558;&#20010;&#20307;&#39044;&#27979;&#19982;&#32858;&#21512;&#21709;&#24212;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#23454;&#20363;&#32423;&#21035;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#21253;&#32423;&#21035;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;&#36825;&#20010;&#35266;&#23519;&#35753;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#20851;&#20110;&#25152;&#24471;&#20272;&#35745;&#20540;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpol
&lt;/p&gt;</description></item><item><title>PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2401.11061</link><description>&lt;p&gt;
PhotoBot&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#21442;&#32771;&#20114;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11061
&lt;/p&gt;
&lt;p&gt;
PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PhotoBot&#30340;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#39640;&#32423;&#20154;&#31867;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20174;&#31574;&#23637;&#30011;&#24266;&#20013;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#29255;&#21521;&#29992;&#25143;&#20256;&#36798;&#25668;&#24433;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23545;&#21442;&#32771;&#22270;&#29255;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26412;&#25512;&#29702;&#26816;&#32034;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#29255;&#12290;&#20026;&#20102;&#23545;&#24212;&#21442;&#32771;&#22270;&#29255;&#21644;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#26174;&#33879;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36879;&#35270;n-&#28857;&#65288;PnP&#65289;&#38382;&#39064;&#26469;&#35745;&#31639;RGB-D&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#37197;&#22791;&#26377;&#25163;&#33109;&#30456;&#26426;&#30340;&#30495;&#23454;&#26426;&#26800;&#25163;&#33218;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;PhotoBot&#25293;&#25668;&#30340;&#29031;&#29255;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user's language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11044</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#22312;&#20851;&#38190;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making. (arXiv:2401.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#22312;&#20998;&#31867;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#34892;&#20026;&#31185;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#65292;&#20854;&#20013;&#36131;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;ROC&#26354;&#32447;&#30340;&#26041;&#27861;&#24320;&#21457;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#20197;&#22686;&#24378;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#25191;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21015;&#38388;&#25968;&#25454;&#36716;&#25442;&#65292;&#21363;&#25277;&#35937;&#65292;&#36825;&#23545;SaNDA&#30340;&#20998;&#31867;&#36807;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#24182;&#25506;&#35752;&#20102;&#26367;&#20195;&#30340;&#25277;&#35937;&#21327;&#35758;&#65292;&#22914;&#24120;&#37327;&#20998;&#31665;&#21644;&#20998;&#20301;&#25968;&#12290;&#23558;&#26368;&#20339;&#30340;&#26041;&#27861;&#19982;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#22522;&#20934;&#27169;&#22411;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#25968;&#25454;&#19981;&#23436;&#25972;&#65292;SaNDA&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#24182;&#19988;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The applicability of widely adopted machine learning (ML) methods to classification is circumscribed by the imperatives of explicability and uncertainty, particularly evident in domains such as healthcare, behavioural sciences, and finances, wherein accountability assumes priority. Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by developing a data abstraction protocol using a ROC curve-based method. This paper focuses on column-wise data transformations called abstractions, which are crucial for SaNDA's classification process and explores alternative abstractions protocols, such as constant binning and quantiles. The best-performing methods have been compared against Random Forest as a baseline for explainable methods. The results suggests that SaNDA can be a viable substitute for Random Forest when data is incomplete, even with minimal missing values. It consistently maintains high accuracy e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.11021</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#20998;&#26512;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detection of Multilingual Hate Speech Using Transformer Based Deep Learning. (arXiv:2401.11021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#30452;&#25509;&#25915;&#20987;&#25110;&#23459;&#20256;&#38024;&#23545;&#29305;&#23450;&#32676;&#20307;&#25110;&#20010;&#20154;&#30340;&#24974;&#24680;&#30340;&#26377;&#23475;&#20869;&#23481;&#65292;&#20363;&#22914;&#31181;&#26063;&#20027;&#20041;&#12289;&#23447;&#25945;&#25110;&#24615;&#21462;&#21521;&#31561;&#12290;&#36825;&#20250;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#31038;&#20250;&#29983;&#27963;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#20026;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#30340;&#20167;&#24680;&#20869;&#23481;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#21644;&#31038;&#21306;&#36896;&#25104;&#20260;&#23475;&#12290;&#38543;&#30528;&#32593;&#32476;&#19978;&#20167;&#24680;&#35328;&#35770;&#30340;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#26816;&#27979;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#22914;Twitter&#12289;Facebook&#12289;WhatsApp&#12289;Instagram&#31561;&#12290;&#35813;&#27169;&#22411;&#29420;&#31435;&#20110;&#35821;&#35328;&#65292;&#24182;&#24050;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#30001;&#30693;&#21517;&#30740;&#31350;&#32773;Zeerak Talat&#12289;Sara Tonelli&#12289;Melanie Siegel&#21644;Rezaul Karim&#25910;&#38598;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#25104;&#21151;&#29575;&#39640;&#20110;&#29616;&#26377;&#22522;&#20934;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is harmful content that directly attacks or promotes hatred against members of groups or individuals based on actual or perceived aspects of identity, such as racism, religion, or sexual orientation. This can affect social life on social media platforms as hateful content shared through social media can harm both individuals and communities. As the prevalence of hate speech increases online, the demand for automated detection as an NLP task is increasing. In this work, the proposed method is using transformer-based model to detect hate speech in social media, like twitter, Facebook, WhatsApp, Instagram, etc. The proposed model is independent of languages and has been tested on Italian, English, German, Bengali. The Gold standard datasets were collected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel, and Rezaul Karim. The success rate of the proposed model for hate speech detection is higher than the existing baseline and state-of-the-art models with acc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#27880;&#20876;&#21644;&#38754;&#37096;&#21160;&#30011;&#38382;&#39064;&#65292;&#21457;&#29616;&#22836;&#20687;&#21644;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#20027;&#35201;&#38590;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.11002</link><description>&lt;p&gt;
&#24555;&#36895;&#27880;&#20876;&#36924;&#30495;&#30340;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#29992;&#20110;&#38754;&#37096;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Fast Registration of Photorealistic Avatars for VR Facial Animation. (arXiv:2401.11002v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#27880;&#20876;&#21644;&#38754;&#37096;&#21160;&#30011;&#38382;&#39064;&#65292;&#21457;&#29616;&#22836;&#20687;&#21644;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#20027;&#35201;&#38590;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#22312;&#31038;&#20132;&#20114;&#21160;&#26041;&#38754;&#25317;&#26377;&#26356;&#20855;&#27785;&#28024;&#24863;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26159;&#33021;&#22815;&#22312;&#20329;&#25140;VR&#22836;&#26174;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#27169;&#25311;&#19968;&#20010;&#36924;&#30495;&#30340;&#22836;&#20687;&#12290;&#34429;&#28982;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#21487;&#20197;&#23454;&#29616;&#23545;&#29305;&#23450;&#20010;&#20154;&#22836;&#20687;&#36827;&#34892;&#39640;&#36136;&#37327;&#27880;&#20876;&#65292;&#24182;&#36827;&#34892;&#21160;&#30011;&#29983;&#25104;&#65292;&#20294;&#36890;&#29992;&#23454;&#26102;&#27169;&#22411;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#32447;&#27880;&#20876;&#20063;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#20542;&#26012;&#30340;&#25668;&#20687;&#26426;&#35270;&#35282;&#21644;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#22836;&#20687;&#19982;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#22256;&#38590;&#30340;&#20027;&#35201;&#28304;&#27849;&#20043;&#19968;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#22312;&#39046;&#22495;&#19968;&#33268;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;&#24341;&#20837;&#39046;&#22495;&#24046;&#36317;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#27492;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#65306;1&#65289;&#19968;&#20010;&#36845;&#20195;&#32454;&#21270;&#27169;&#22359;&#65292;&#25509;&#25910;&#39046;&#22495;&#20869;&#36755;&#20837;&#65292;&#21644;2&#65289;&#19968;&#20010;&#36890;&#29992;&#30340;&#22836;&#20687;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a photorealistic avatar of one's likeness while wearing a VR headset. Although high quality registration of person-specific avatars to headset-mounted camera (HMC) images is possible in an offline setting, the performance of generic realtime models are significantly degraded. Online registration is also challenging due to oblique camera views and differences in modality. In this work, we first show that the domain gap between the avatar and headset-camera images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we develop a system design that decouples the problem into two parts: 1) an iterative refinement module that takes in-domain inputs, and 2) a generic avatar-guided imag
&lt;/p&gt;</description></item><item><title>MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10969</link><description>&lt;p&gt;
MacroSwarm: &#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32452;&#21512;&#26694;&#26550;&#29992;&#20110;&#32676;&#20307;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
MacroSwarm: A Field-based Compositional Framework for Swarm Programming. (arXiv:2401.10969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10969
&lt;/p&gt;
&lt;p&gt;
MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#34892;&#20026;&#24037;&#31243;&#26159;&#19968;&#39033;&#26088;&#22312;&#30740;&#31350;&#21327;&#35843;&#31616;&#21333;&#26234;&#33021;&#20307;&#22242;&#20307;&#20869;&#35745;&#31639;&#21644;&#34892;&#21160;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#20840;&#23616;&#30446;&#26631;&#65292;&#22914;&#22270;&#26696;&#24418;&#25104;&#12289;&#38598;&#20307;&#31227;&#21160;&#12289;&#32858;&#31867;&#21644;&#20998;&#24067;&#24335;&#24863;&#30693;&#12290;&#23613;&#31649;&#22312;&#32676;&#20307;&#65288;&#26080;&#20154;&#26426;&#12289;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#65289;&#20998;&#26512;&#21644;&#24037;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#36890;&#29992;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#23450;&#20041;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#12290;&#20026;&#20102;&#23545;&#27492;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;MacroSwarm&#65292;&#20197;&#21487;&#37325;&#29992;&#19988;&#23436;&#20840;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;&#23884;&#20837;&#38598;&#20307;&#35745;&#31639;&#21644;&#21327;&#35843;&#12290;&#22522;&#20110;&#38598;&#25104;&#35745;&#31639;&#30340;&#23439;&#32534;&#31243;&#33539;&#24335;&#65292;MacroSwarm&#25552;&#20986;&#20102;&#23558;&#27599;&#20010;&#32676;&#20307;&#34892;&#20026;&#22359;&#34920;&#31034;&#20026;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#30340;&#32431;&#20989;&#25968;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function mapping sensing fields into actuation goal fields, e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#25955;&#21327;&#35843;&#29615;&#22659;&#19979;&#24320;&#25918;&#36710;&#38431;&#30340;&#21487;&#25193;&#23637;&#21644;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#65292;&#25552;&#20986;&#20102;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#25968;&#23398;&#27169;&#22411;&#26469;&#35299;&#20915;&#21160;&#24577;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10965</link><description>&lt;p&gt;
&#20998;&#25955;&#21327;&#35843;&#19979;&#24320;&#25918;&#36710;&#38431;&#30340;&#21487;&#25193;&#23637;&#21644;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Decentralizing Coordination in Open Vehicle Fleets for Scalable and Dynamic Task Allocation. (arXiv:2401.10965v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#25955;&#21327;&#35843;&#29615;&#22659;&#19979;&#24320;&#25918;&#36710;&#38431;&#30340;&#21487;&#25193;&#23637;&#21644;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#65292;&#25552;&#20986;&#20102;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#25968;&#23398;&#27169;&#22411;&#26469;&#35299;&#20915;&#21160;&#24577;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#24320;&#25918;&#12289;&#21327;&#20316;&#21644;&#21830;&#19994;&#36710;&#38431;&#30340;&#21327;&#35843;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#12290;&#20855;&#26377;&#33258;&#25105;&#20851;&#27880;&#30340;&#20010;&#20307;&#29702;&#24615;&#36710;&#36742;&#39550;&#39542;&#21592;&#26377;&#30528;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#30446;&#26631;&#65292;&#38656;&#35201;&#36890;&#36807;&#20844;&#24179;&#21644;&#39640;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#26041;&#27861;&#36827;&#34892;&#21327;&#35843;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21487;&#25193;&#23637;&#21644;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#30340;&#25991;&#29486;&#65292;&#37325;&#28857;&#20851;&#27880;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#20108;&#32500;&#32447;&#24615;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#24320;&#25918;&#36710;&#38431;&#30340;&#22810;&#20195;&#29702;&#31995;&#32479;&#34920;&#31034;&#19978;&#65292;&#20854;&#20013;&#21160;&#24577;&#20986;&#29616;&#30340;&#36710;&#36742;&#30001;&#36719;&#20214;&#20195;&#29702;&#34920;&#31034;&#65292;&#24212;&#20998;&#37197;&#21040;&#19968;&#32452;&#21160;&#24577;&#20986;&#29616;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#21644;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#30528;&#37325;&#20851;&#27880;&#38598;&#20013;&#24335;&#12289;&#20998;&#24067;&#24335;&#21644;&#20998;&#25955;&#24335;&#35299;&#20915;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#20197;&#19979;&#21160;&#24577;&#29256;&#26412;&#30340;&#20998;&#37197;&#38382;&#39064;&#65306;&#20998;&#37197;&#38382;&#39064;&#12289;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges in the coordination of large, open, collaborative, and commercial vehicle fleets is dynamic task allocation. Self-concerned individually rational vehicle drivers have both local and global objectives, which require coordination using some fair and efficient task allocation method. In this paper, we review the literature on scalable and dynamic task allocation focusing on deterministic and dynamic two-dimensional linear assignment problems. We focus on multiagent system representation of open vehicle fleets where dynamically appearing vehicles are represented by software agents that should be allocated to a set of dynamically appearing tasks. We give a comparison and critical analysis of recent research results focusing on centralized, distributed, and decentralized solution approaches. Moreover, we propose mathematical models for dynamic versions of the following assignment problems well known in combinatorial optimization: the assignment problem, bottleneck
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#29616;&#22330;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30740;&#31350;&#20102;LLM&#24037;&#20855;&#22312;&#25552;&#20379;&#26080;&#30417;&#30563;&#20449;&#24687;&#26816;&#32034;&#25903;&#25345;&#26381;&#21153;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10956</link><description>&lt;p&gt;
Chat Bot&#19978;&#30340;AI&#38761;&#21629;&#65306;&#19968;&#39033;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment. (arXiv:2401.10956v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#29616;&#22330;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30740;&#31350;&#20102;LLM&#24037;&#20855;&#22312;&#25552;&#20379;&#26080;&#30417;&#30563;&#20449;&#24687;&#26816;&#32034;&#25903;&#25345;&#26381;&#21153;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;generative AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25552;&#21319;&#20154;&#31867;&#29983;&#20135;&#21147;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;ChatGPT-4&#20026;&#20363;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#35768;&#22810;&#25991;&#31456;&#24050;&#32463;&#30740;&#31350;&#20102;LLM&#24037;&#20855;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#19979;&#21644;&#35774;&#35745;&#20219;&#21153;&#25110;&#35266;&#23519;&#24615;&#30740;&#31350;&#20013;&#23545;&#20154;&#31867;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24212;&#29992;LLM&#24037;&#20855;&#30340;&#29616;&#22330;&#23454;&#39564;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#29616;&#22330;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;LLM&#24037;&#20855;&#22312;&#25552;&#20379;&#26080;&#30417;&#30563;&#20449;&#24687;&#26816;&#32034;&#25903;&#25345;&#26381;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, generative AI has undergone major advancements, demonstrating significant promise in augmenting human productivity. Notably, large language models (LLM), with ChatGPT-4 as an example, have drawn considerable attention. Numerous articles have examined the impact of LLM-based tools on human productivity in lab settings and designed tasks or in observational studies. Despite recent advances, field experiments applying LLM-based tools in realistic settings are limited. This paper presents the findings of a field randomized controlled trial assessing the effectiveness of LLM-based tools in providing unmonitored support services for information retrieval.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22411;&#65288;SCAM&#65289;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#65292;&#22312;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#22810;&#27169;&#24577;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10946</link><description>&lt;p&gt;
&#33258;&#25105;&#19978;&#19979;&#25991;&#24863;&#30693;&#19979;&#30340;&#20154;&#26426;&#20132;&#20114;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self context-aware emotion perception on human-robot interaction. (arXiv:2401.10946v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22411;&#65288;SCAM&#65289;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#65292;&#22312;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#22810;&#27169;&#24577;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#19982;&#20154;&#31867;&#30340;&#38271;&#26399;&#20132;&#20114;&#20013;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25345;&#32493;&#20934;&#30830;&#22320;&#20570;&#20986;&#21453;&#24212;&#65292;&#28982;&#32780;&#65292;&#20027;&#27969;&#30340;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#30701;&#26399;&#24773;&#32490;&#35782;&#21035;&#65292;&#24573;&#35270;&#20102;&#24773;&#32490;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#12290;&#20154;&#20204;&#35748;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#19981;&#21516;&#30340;&#32972;&#26223;&#21487;&#20197;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#24773;&#32490;&#34920;&#36798;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#25105;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22411;&#65288;SCAM&#65289;&#65292;&#23427;&#37319;&#29992;&#20108;&#32500;&#24773;&#32490;&#22352;&#26631;&#31995;&#32479;&#26469;&#38170;&#23450;&#21644;&#37325;&#26032;&#26631;&#35760;&#19981;&#21516;&#30340;&#24773;&#32490;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#34701;&#21512;&#20102;&#29420;&#29305;&#30340;&#20449;&#24687;&#20445;&#30041;&#32467;&#26500;&#21644;&#19978;&#19979;&#25991;&#25439;&#22833;&#12290;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#22810;&#27169;&#24577;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#21548;&#35273;&#27169;&#24577;&#19979;&#65292;&#20934;&#30830;&#29575;&#26126;&#26174;&#25552;&#39640;&#65292;&#20174;63.10%&#25552;&#21319;&#21040;72.46%&#12290;&#21516;&#26679;&#65292;&#35270;&#35273;&#27169;&#24577;&#20063;&#34920;&#29616;&#20986;&#20102;&#25552;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#22686;&#21152;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Emotion recognition plays a crucial role in various domains of human-robot interaction. In long-term interactions with humans, robots need to respond continuously and accurately, however, the mainstream emotion recognition methods mostly focus on short-term emotion recognition, disregarding the context in which emotions are perceived. Humans consider that contextual information and different contexts can lead to completely different emotional expressions. In this paper, we introduce self context-aware model (SCAM) that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. Simultaneously, it incorporates its distinctive information retention structure and contextual loss. This approach has yielded significant improvements across audio, video, and multimodal. In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#21453;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#36866;&#24212;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;MUL&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#30340;&#21464;&#21270;&#21644;&#20262;&#29702;&#32771;&#34385;&#21160;&#24577;&#35843;&#25972;&#31995;&#32479;&#30693;&#35782;&#12290;&#36890;&#36807;&#25209;&#21028;&#24615;&#26816;&#39564;&#21644;&#25991;&#29486;&#26803;&#29702;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;MUL&#22914;&#20309;&#25913;&#21464;&#25512;&#33616;&#12289;&#29992;&#25143;&#20449;&#20219;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#36335;&#24452;&#30340;&#35265;&#35299;&#12290;&#24378;&#35843;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#20197;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#25512;&#21160;MUL&#22312;&#23433;&#20840;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10942</link><description>&lt;p&gt;
&#26426;&#22120;&#21453;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning for Recommendation Systems: An Insight. (arXiv:2401.10942v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#21453;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#36866;&#24212;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;MUL&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#30340;&#21464;&#21270;&#21644;&#20262;&#29702;&#32771;&#34385;&#21160;&#24577;&#35843;&#25972;&#31995;&#32479;&#30693;&#35782;&#12290;&#36890;&#36807;&#25209;&#21028;&#24615;&#26816;&#39564;&#21644;&#25991;&#29486;&#26803;&#29702;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;MUL&#22914;&#20309;&#25913;&#21464;&#25512;&#33616;&#12289;&#29992;&#25143;&#20449;&#20219;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#36335;&#24452;&#30340;&#35265;&#35299;&#12290;&#24378;&#35843;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#20197;&#28385;&#36275;&#23454;&#38469;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#25512;&#21160;MUL&#22312;&#23433;&#20840;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MUL&#65289;&#65292;&#35299;&#20915;&#20102;&#36866;&#24212;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;MUL&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#30340;&#21464;&#21270;&#21644;&#20262;&#29702;&#32771;&#34385;&#21160;&#24577;&#35843;&#25972;&#31995;&#32479;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;MUL&#30340;&#22522;&#26412;&#21407;&#29702;&#12289;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21644;&#31639;&#27861;&#36879;&#26126;&#24615;&#31561;&#25361;&#25112;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#26816;&#39564;&#12290;&#23427;&#26803;&#29702;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#25552;&#20379;&#20102;MUL&#22914;&#20309;&#25913;&#21464;&#25512;&#33616;&#30340;&#35265;&#35299;&#65292;&#25506;&#35752;&#20102;&#29992;&#25143;&#20449;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#22312;&#36127;&#36131;&#20219;&#21644;&#29992;&#25143;&#20851;&#27880;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36335;&#24452;&#12290;&#26412;&#25991;&#24341;&#23548;&#30740;&#31350;&#20154;&#21592;&#38754;&#23545;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#65292;&#40723;&#21169;&#20197;&#28385;&#36275;&#26377;&#38024;&#23545;&#24615;&#30340;&#25968;&#25454;&#21024;&#38500;&#23454;&#38469;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#36129;&#29486;&#12290;&#24378;&#35843;MUL&#22312;&#23433;&#20840;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#25512;&#21160;&#20854;&#36793;&#30028;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review explores machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. Unlike traditional models, MUL dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper critically examines MUL's basics, real-world applications, and challenges like algorithmic transparency. It sifts through literature, offering insights into how MUL could transform recommendations, discussing user trust, and suggesting paths for future research in responsible and user-focused artificial intelligence (AI). The document guides researchers through challenges involving the trade-off between personalization and privacy, encouraging contributions to meet practical demands for targeted data removal. Emphasizing MUL's role in secure and adaptive machine learning, the paper proposes ways to push its boundaries. The novelty of this paper lies in its exploration of the limitations of the methods, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10938</link><description>&lt;p&gt;
&#21363;&#20351;&#35299;&#37322;&#65306;&#24418;&#24335;&#22522;&#30784;&#65292;&#20248;&#20808;&#32423;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#23376;&#36816;&#34892;&#65292;&#32570;&#20047;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#65292;&#32780;&#21448;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#12290;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#35797;&#22270;&#22238;&#31572;&#20026;&#20160;&#20040;&#32473;&#23450;&#27169;&#22411;&#22914;&#20309;&#23545;&#20010;&#20307;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20851;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#32463;&#36827;&#34892;&#20102;&#37325;&#35201;&#24037;&#20316;&#65292;&#20294;&#23545;&#21322;&#20107;&#23454;&#35299;&#37322;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21322;&#20107;&#23454;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#20013;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#34920;&#26126;&#32447;&#24615;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#27604;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#20110;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#20559;&#22909;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#26080;&#35770;&#26159;&#22312;&#21322;&#20107;&#23454;&#36824;&#26159;&#21453;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#35299;&#37322;&#33021;&#21147;&#21644;&#29992;&#25143;&#20013;&#24515;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#20915;&#31574;&#32773;&#23545;&#24178;&#39044;&#34892;&#20026;&#30340;&#20559;&#22909;&#65292;&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#35782;&#21035;&#20915;&#31574;&#32773;&#20027;&#35266;&#22240;&#26524;&#21028;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20026;&#27169;&#22411;&#27979;&#35797;&#20915;&#31574;&#32773;&#20559;&#22909;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10937</link><description>&lt;p&gt;
&#20027;&#35266;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Subjective Causality. (arXiv:2401.10937v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#20915;&#31574;&#32773;&#23545;&#24178;&#39044;&#34892;&#20026;&#30340;&#20559;&#22909;&#65292;&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#35782;&#21035;&#20915;&#31574;&#32773;&#20027;&#35266;&#22240;&#26524;&#21028;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20026;&#27169;&#22411;&#27979;&#35797;&#20915;&#31574;&#32773;&#20559;&#22909;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#20915;&#31574;&#32773;&#23545;&#24178;&#39044;&#34892;&#20026;&#30340;&#20559;&#22909;&#65292;&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#35782;&#21035;&#20915;&#31574;&#32773;&#20027;&#35266;&#22240;&#26524;&#21028;&#26029;&#30340;&#21487;&#33021;&#24615;&#12290;&#26681;&#25454;Pearl [2000]&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#27169;&#22411;&#65288;&#20063;&#31216;&#20026;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65289;&#26469;&#34920;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;&#20854;&#20013;&#19990;&#30028;&#30001;&#19968;&#32452;&#21464;&#37327;&#32452;&#25104;&#65292;&#36825;&#20123;&#21464;&#37327;&#20043;&#38388;&#36890;&#36807;&#26041;&#31243;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#24178;&#39044;&#34892;&#20026;&#30340;&#20559;&#22909;&#20851;&#31995;&#28385;&#36275;&#26576;&#20123;&#20844;&#29702;&#65288;&#19982;&#21453;&#20107;&#23454;&#30456;&#20851;&#30340;&#26631;&#20934;&#20844;&#29702;&#65289;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#23450;&#20041;&#65288;i&#65289;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#65288;ii&#65289;&#21453;&#26144;&#20915;&#31574;&#32773;&#23545;&#22806;&#37096;&#22240;&#32032;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#19982;&#27599;&#20010;&#24178;&#39044;&#34892;&#20026;&#30456;&#20851;&#32852;&#30340;&#25928;&#29992;&#65292;&#27599;&#20010;&#24178;&#39044;&#34892;&#20026;&#37117;&#26377;&#19968;&#20010;&#39044;&#26399;&#25928;&#29992;&#65292;&#24182;&#19988;&#24178;&#39044;&#34892;&#20026;A&#20248;&#20110;B&#24403;&#19988;&#20165;&#24403;A&#30340;&#39044;&#26399;&#25928;&#29992;&#22823;&#20110;B&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#22240;&#26524;&#27169;&#22411;&#30340;&#21807;&#19968;&#24615;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20801;&#35768;&#24314;&#27169;&#32773;&#27979;&#35797;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#20915;&#31574;&#32773;&#30340;&#20559;&#22909;&#26159;&#21542;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that it is possible to understand and identify a decision maker's subjective causal judgements by observing her preferences over interventions. Following Pearl [2000], we represent causality using causal models (also called structural equations models), where the world is described by a collection of variables, related by equations. We show that if a preference relation over interventions satisfies certain axioms (related to standard axioms regarding counterfactuals), then we can define (i) a causal model, (ii) a probability capturing the decision-maker's uncertainty regarding the external factors in the world and (iii) a utility on outcomes such that each intervention is associated with an expected utility and such that intervention $A$ is preferred to $B$ iff the expected utility of $A$ is greater than that of $B$. In addition, we characterize when the causal model is unique. Thus, our results allow a modeler to test the hypothesis that a decision maker's preferences are cons
&lt;/p&gt;</description></item><item><title>SeeClick&#26159;&#19968;&#31181;&#22522;&#20110;&#23631;&#24149;&#25130;&#22270;&#30340;&#35270;&#35273;GUI&#20195;&#29702;&#65292;&#36890;&#36807;GUI grounding&#39044;&#35757;&#32451;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#33258;&#21160;&#21270;&#20013;&#20934;&#30830;&#23450;&#20301;&#23631;&#24149;&#20803;&#32032;&#65292;&#24182;&#21019;&#24314;&#20102;&#20840;&#38754;&#35206;&#30422;&#31227;&#21160;&#12289;&#26700;&#38754;&#21644;Web&#29615;&#22659;&#30340;GUI grounding&#25968;&#25454;&#38598;ScreenSpot&#12290;</title><link>http://arxiv.org/abs/2401.10935</link><description>&lt;p&gt;
SeeClick&#65306;&#21033;&#29992;GUI Grounding&#23454;&#29616;&#39640;&#32423;&#21487;&#35270;&#21270;GUI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. (arXiv:2401.10935v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10935
&lt;/p&gt;
&lt;p&gt;
SeeClick&#26159;&#19968;&#31181;&#22522;&#20110;&#23631;&#24149;&#25130;&#22270;&#30340;&#35270;&#35273;GUI&#20195;&#29702;&#65292;&#36890;&#36807;GUI grounding&#39044;&#35757;&#32451;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#33258;&#21160;&#21270;&#20013;&#20934;&#30830;&#23450;&#20301;&#23631;&#24149;&#20803;&#32032;&#65292;&#24182;&#21019;&#24314;&#20102;&#20840;&#38754;&#35206;&#30422;&#31227;&#21160;&#12289;&#26700;&#38754;&#21644;Web&#29615;&#22659;&#30340;GUI grounding&#25968;&#25454;&#38598;ScreenSpot&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#20195;&#29702;&#34987;&#35774;&#35745;&#29992;&#20110;&#33258;&#21160;&#21270;&#25968;&#23383;&#35774;&#22791;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#26700;&#38754;&#30005;&#33041;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GUI&#20195;&#29702;&#36890;&#36807;&#25552;&#21462;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#29305;&#21035;&#20887;&#38271;&#65288;&#20363;&#22914;HTML&#65289;&#19988;&#26377;&#26102;&#26080;&#27861;&#35775;&#38382;&#65288;&#20363;&#22914;&#22312;&#26700;&#38754;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#24149;&#25130;&#22270;&#36827;&#34892;&#20219;&#21153;&#33258;&#21160;&#21270;&#30340;&#35270;&#35273;GUI&#20195;&#29702;--SeeClick&#12290;&#22312;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#24320;&#21457;&#35270;&#35273;GUI&#20195;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;GUI grounding - &#26681;&#25454;&#25351;&#20196;&#20934;&#30830;&#23450;&#20301;&#23631;&#24149;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;GUI grounding&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;SeeClick&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;GUI grounding&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#20197;&#19978;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;ScreenSpot&#65292;&#31532;&#19968;&#20010;&#28085;&#30422;&#31227;&#21160;&#12289;&#26700;&#38754;&#21644;Web&#29615;&#22659;&#30340;&#30495;&#23454;GUI grounding&#25968;&#25454;&#38598;&#12290;&#32463;&#36807;&#39044;&#35757;&#32451;&#65292;SeeClick&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a visual GUI agent -- SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding dataset that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvem
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29992;&#25143;&#20449;&#24687;&#21644;&#33402;&#26415;&#35774;&#35745;&#30340;&#26032;&#30340;&#21019;&#24847;&#29983;&#25104;&#27969;&#31243;&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#20449;&#24687;&#21644;&#32771;&#34385;&#29992;&#25143;&#29305;&#24449;&#26469;&#39044;&#27979;CTR&#20998;&#25968;&#65292;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#21019;&#24847;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.10934</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#21019;&#24847;&#29983;&#25104;&#27969;&#31243;&#29992;&#20110;&#28857;&#20987;&#29575;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model. (arXiv:2401.10934v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10934
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29992;&#25143;&#20449;&#24687;&#21644;&#33402;&#26415;&#35774;&#35745;&#30340;&#26032;&#30340;&#21019;&#24847;&#29983;&#25104;&#27969;&#31243;&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#20449;&#24687;&#21644;&#32771;&#34385;&#29992;&#25143;&#29305;&#24449;&#26469;&#39044;&#27979;CTR&#20998;&#25968;&#65292;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#21019;&#24847;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#30340;&#22330;&#26223;&#20013;&#65292;&#38144;&#21806;&#21830;&#36890;&#24120;&#20250;&#21019;&#24314;&#22810;&#20010;&#21019;&#24847;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#28436;&#31034;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#26368;&#21560;&#24341;&#20154;&#30340;&#35774;&#35745;&#20197;&#26368;&#22823;&#21270;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38144;&#21806;&#21830;&#36890;&#24120;&#38590;&#20197;&#32771;&#34385;&#29992;&#25143;&#23545;&#21019;&#24847;&#35774;&#35745;&#30340;&#20559;&#22909;&#65292;&#23548;&#33268;&#30456;&#23545;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#26469;&#35828;&#65292;&#32654;&#23398;&#21644;&#25968;&#37327;&#37117;&#36739;&#20302;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#21516;&#26679;&#30340;&#38382;&#39064;&#65292;&#21363;&#27809;&#26377;&#32771;&#34385;&#29992;&#25143;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#35774;&#35745;&#24072;&#30340;&#32654;&#23398;&#30693;&#35782;&#26041;&#38754;&#26377;&#38480;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#20449;&#24687;&#65292;&#29983;&#25104;&#30340;&#21019;&#24847;&#21487;&#33021;&#26356;&#20855;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#29992;&#25143;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#20248;&#21270;&#32467;&#26524;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#29983;&#25104;&#30340;&#21019;&#24847;&#20250;&#36890;&#36807;&#21478;&#19968;&#20010;&#34987;&#31216;&#20026;&#21019;&#24847;&#25490;&#21517;&#27169;&#22411;&#30340;&#27169;&#22359;&#36827;&#34892;&#25490;&#24207;&#12290;&#25490;&#21517;&#27169;&#22411;&#21487;&#20197;&#32771;&#34385;&#29992;&#25143;&#29305;&#24449;&#26469;&#39044;&#27979;&#27599;&#20010;&#21019;&#24847;&#30340;CTR&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#30340;&#20004;&#20010;&#38454;&#27573;&#34987;&#35270;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#20219;&#21153;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#21019;&#24847;&#21487;&#33021;&#19981;&#22815;&#31934;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages are regarded as two different t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#31185;&#23398;&#25991;&#29486;&#31995;&#32479;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10917</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#31185;&#23398;&#25991;&#29486;&#31995;&#32479;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence to automate the systematic review of scientific literature. (arXiv:2401.10917v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#31185;&#23398;&#25991;&#29486;&#31995;&#32479;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#29616;&#20195;&#35745;&#31639;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20154;&#20204;&#20256;&#32479;&#19978;&#23436;&#25104;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;AI&#25552;&#20379;&#20102;&#34920;&#31034;&#21644;&#25512;&#29702;&#30693;&#35782;&#12289;&#39640;&#25928;&#22320;&#22788;&#29702;&#25991;&#26412;&#21644;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#29305;&#28857;&#36866;&#29992;&#20110;&#35768;&#22810;&#20154;&#31867;&#25214;&#21040;&#36153;&#21147;&#25110;&#37325;&#22797;&#30340;&#27963;&#21160;&#65292;&#27604;&#22914;&#31185;&#23398;&#25991;&#29486;&#30340;&#20998;&#26512;&#12290;&#25163;&#21160;&#20934;&#22791;&#21644;&#25776;&#20889;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#38656;&#35201;&#30456;&#24403;&#38271;&#30340;&#26102;&#38388;&#21644;&#21162;&#21147;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31574;&#21010;&#19968;&#20010;&#31574;&#30053;&#65292;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#20998;&#26512;&#65292;&#24182;&#25253;&#21578;&#32467;&#26524;&#12290;&#26681;&#25454;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#21516;&#65292;&#26816;&#32034;&#21040;&#30340;&#35770;&#25991;&#25968;&#37327;&#21487;&#33021;&#36798;&#21040;&#25968;&#30334;&#25110;&#25968;&#21315;&#31687;&#65292;&#36825;&#24847;&#21619;&#30528;&#36807;&#28388;&#20986;&#30456;&#20851;&#25991;&#29486;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26159;&#19968;&#20010;&#26114;&#36149;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#28041;&#21450;&#30340;&#20219;&#21153;&#26159;&#37325;&#22797;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;AI&#33258;&#21160;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has acquired notorious relevance in modern computing as it effectively solves complex tasks traditionally done by humans. AI provides methods to represent and infer knowledge, efficiently manipulate texts and learn from vast amount of data. These characteristics are applicable in many activities that human find laborious or repetitive, as is the case of the analysis of scientific literature. Manually preparing and writing a systematic literature review (SLR) takes considerable time and effort, since it requires planning a strategy, conducting the literature search and analysis, and reporting the findings. Depending on the area under study, the number of papers retrieved can be of hundreds or thousands, meaning that filtering those relevant ones and extracting the key information becomes a costly and error-prone process. However, some of the involved tasks are repetitive and, therefore, subject to automation by means of AI. In this paper, we present a survey
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#20869;&#30465;&#30340;&#20803;&#35748;&#30693;&#27169;&#22359;&#65292;&#21487;&#20197;&#35753;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#35266;&#23519;&#33258;&#24049;&#30340;&#24605;&#32771;&#36807;&#31243;&#21644;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31574;&#30053;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#31995;&#32479;&#22312;&#19982;&#20854;&#20182;&#31995;&#32479;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#20248;&#21183;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#36866;&#24212;&#21644;&#25913;&#36827;&#31574;&#30053;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.10910</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#21527;&#65311;&#22312;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#20013;&#20351;&#29992;&#20869;&#30465;&#20197;&#25552;&#39640;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior. (arXiv:2401.10910v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#20869;&#30465;&#30340;&#20803;&#35748;&#30693;&#27169;&#22359;&#65292;&#21487;&#20197;&#35753;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#35266;&#23519;&#33258;&#24049;&#30340;&#24605;&#32771;&#36807;&#31243;&#21644;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31574;&#30053;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#31995;&#32479;&#22312;&#19982;&#20854;&#20182;&#31995;&#32479;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#20248;&#21183;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#36866;&#24212;&#21644;&#25913;&#36827;&#31574;&#30053;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;LLMs&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#27867;&#21270;&#22256;&#38590;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20803;&#35748;&#30693;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#65292;&#20351;&#20854;&#33021;&#22815;&#35266;&#23519;&#33258;&#24049;&#30340;&#24605;&#32771;&#36807;&#31243;&#21644;&#34892;&#21160;&#12290;&#36825;&#31181;&#20803;&#35748;&#30693;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#31995;&#32479;1&#21644;&#31995;&#32479;2&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#20462;&#25913;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#27979;&#35797;&#20102;&#20803;&#35748;&#30693;&#27169;&#22359;&#65292;&#21253;&#25324;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#24517;&#39035;&#22312;&#20725;&#23608;&#21551;&#31034;&#24405;&#20013;&#23384;&#27963;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#34920;&#29616;&#19978;&#36229;&#36807;&#20854;&#20182;&#31995;&#32479;&#65292;&#21516;&#26102;&#26234;&#33021;&#20307;&#33021;&#22815;&#38543;&#30528;&#26102;&#38388;&#36866;&#24212;&#21644;&#25913;&#36827;&#31574;&#30053;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#36890;&#36807;&#25506;&#32034;&#31070;&#32463;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#21457;&#29616;&#65292;&#26088;&#22312;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#21551;&#31034;&#65292;&#20197;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#21644;&#22240;&#26524;&#29702;&#35299;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10904</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#31070;&#32463;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#21457;&#29616;&#26469;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21551;&#31034;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Findings from Neuroscience and Cognitive Psychology as Possible Inspiration for the Path to Artificial General Intelligence. (arXiv:2401.10904v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10904
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#36890;&#36807;&#25506;&#32034;&#31070;&#32463;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#21457;&#29616;&#65292;&#26088;&#22312;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#21551;&#31034;&#65292;&#20197;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#21644;&#22240;&#26524;&#29702;&#35299;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#23457;&#35270;&#31070;&#32463;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20570;&#20986;&#36129;&#29486;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#21644;&#22240;&#26524;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#32456;&#24212;&#23558;&#36825;&#20123;&#33021;&#21147;&#32435;&#20837;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#20197;&#36229;&#36234;&#25968;&#25454;&#39537;&#21160;&#30340;&#38480;&#21046;&#65292;&#24182;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#22402;&#30452;&#32508;&#36848;&#65292;&#35797;&#22270;&#24191;&#27867;&#25506;&#35752;&#22823;&#33041;&#21151;&#33021;&#65292;&#20174;&#20302;&#23618;&#29983;&#29289;&#31070;&#32463;&#20803;&#12289;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#20803;&#38598;&#21512;&#21040;&#26356;&#39640;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#33041;&#35299;&#21078;&#23398;&#12289;&#21521;&#37327;&#31526;&#21495;&#26550;&#26500;&#12289;&#35748;&#30693;&#21644;&#20998;&#31867;&#27169;&#22411;&#20197;&#21450;&#35748;&#30693;&#26550;&#26500;&#12290;&#24076;&#26395;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review aims to contribute to the quest for artificial general intelligence by examining neuroscience and cognitive psychology methods for potential inspiration. Despite the impressive advancements achieved by deep learning models in various domains, they still have shortcomings in abstract reasoning and causal understanding. Such capabilities should be ultimately integrated into artificial intelligence systems in order to surpass data-driven limitations and support decision making in a way more similar to human intelligence. This work is a vertical review that attempts a wide-ranging exploration of brain function, spanning from lower-level biological neurons, spiking neural networks, and neuronal ensembles to higher-level concepts such as brain anatomy, vector symbolic architectures, cognitive and categorization models, and cognitive architectures. The hope is that these concepts may offer insights for solutions in artificial general intelligence.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#26696;&#20363;&#65292;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#20013;&#30340;&#20855;&#20307;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#25925;&#38556;&#21644;&#25104;&#21151;&#65292;&#38656;&#35201;&#25193;&#23637;&#31038;&#20250;&#25216;&#26415;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.10899</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#20013;&#30340;&#20855;&#20307;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Concrete Problems in AI Safety, Revisited. (arXiv:2401.10899v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#26696;&#20363;&#65292;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#20013;&#30340;&#20855;&#20307;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#25925;&#38556;&#21644;&#25104;&#21151;&#65292;&#38656;&#35201;&#25193;&#23637;&#31038;&#20250;&#25216;&#26415;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#30340;&#26222;&#21450;&#65292;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#36234;&#26469;&#36234;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#20013;&#65292;&#38450;&#27490;&#30001;&#20110;&#31995;&#32479;&#34892;&#20026;&#19982;&#35774;&#35745;&#32773;&#24847;&#22270;&#30340;&#24847;&#22806;&#20559;&#31163;&#32780;&#23548;&#33268;&#25925;&#38556;&#12290;&#36890;&#36807;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#30495;&#23454;&#26696;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#23613;&#31649;&#24403;&#21069;&#30340;&#26415;&#35821;&#28085;&#30422;&#20102;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#25152;&#36935;&#21040;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#20294;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#23454;&#26045;&#30340;&#23433;&#20840;&#26426;&#21046;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#22833;&#36133;&#21644;&#25104;&#21151;&#65292;&#38656;&#35201;&#36827;&#34892;&#19968;&#20010;&#25193;&#23637;&#30340;&#31038;&#20250;&#25216;&#26415;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems proliferate in society, the AI community is increasingly preoccupied with the concept of AI Safety, namely the prevention of failures due to accidents that arise from an unanticipated departure of a system's behavior from designer intent in AI deployment. We demonstrate through an analysis of real world cases of such incidents that although current vocabulary captures a range of the encountered issues of AI deployment, an expanded socio-technical framing will be required for a more complete understanding of how AI systems and implemented safety mechanisms fail and succeed in real life.
&lt;/p&gt;</description></item><item><title>PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.10586</link><description>&lt;p&gt;
PuriDefense&#65306;&#29992;&#20110;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38543;&#26426;&#23616;&#37096;&#38544;&#24335;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10586
&lt;/p&gt;
&lt;p&gt;
PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;&#23545;&#25239;&#35757;&#32451;&#12289;&#26799;&#24230;&#25513;&#30422;&#21644;&#36755;&#20837;&#36716;&#25442;&#65292;&#35201;&#20040;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#35201;&#20040;&#25439;&#23475;&#38750;&#23545;&#25239;&#36755;&#20837;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;PuriDefense&#65292;&#22312;&#20302;&#25512;&#29702;&#25104;&#26412;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#30340;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#23616;&#37096;&#38544;&#24335;&#20989;&#25968;&#24182;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#32435;&#20837;&#20928;&#21270;&#36807;&#31243;&#26469;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23545;CIFAR-10&#21644;ImageNet&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20928;&#21270;&#22120;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.10393</link><description>&lt;p&gt;
&#33258;&#28982;&#30340;&#21151;&#29575;&#27861;&#21017;&#23398;&#20064;&#29615;&#22659;&#20013;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65288;CI&#65289;&#65306;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19982;&#20154;&#31867;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#21487;&#20197;&#36830;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#20250;&#26126;&#26174;&#24536;&#35760;&#20808;&#21069;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#20943;&#36731;CI&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#27491;&#21017;&#21270;&#12289;&#22238;&#24518;&#12289;&#29983;&#25104;&#24615;&#22238;&#25918;&#21644;&#27987;&#32553;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#25351;&#23548;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36935;&#21040;&#20219;&#21153;&#30340;&#27010;&#29575;&#19982;&#26368;&#21518;&#19968;&#27425;&#25191;&#34892;&#20219;&#21153;&#30340;&#26102;&#38388;&#25104;&#21151;&#29575;&#27861;&#21017;&#36882;&#20943;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27169;&#25311;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20943;&#36731;CI&#25216;&#26415;&#30340;&#30495;&#23454;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#38754;&#20020;&#30340;&#21151;&#29575;&#27861;&#21017;&#29615;&#22659;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#22238;&#24518;&#26041;&#27861;&#26102;&#65292;CI&#30340;&#20943;&#36731;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#36825;&#31181;&#22522;&#20110;&#22238;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#27010;&#36848;&#20102;&#21046;&#33647;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21046;&#33647;&#36816;&#33829;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.10273</link><description>&lt;p&gt;
&#38761;&#26032;&#21046;&#33647;&#19994;&#65306;&#25581;&#24320;&#33647;&#29289;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry. (arXiv:2401.10273v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#27010;&#36848;&#20102;&#21046;&#33647;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21046;&#33647;&#36816;&#33829;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21046;&#33647;&#34892;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#37325;&#22823;&#36827;&#23637;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#27010;&#36848;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#20854;&#22312;&#30740;&#21457;&#12289;&#21160;&#29289;&#27979;&#35797;&#12289;&#20020;&#24202;&#35797;&#39564;&#12289;&#21307;&#38498;&#20020;&#24202;&#38454;&#27573;&#12289;&#29983;&#20135;&#12289;&#30417;&#31649;&#20107;&#21153;&#12289;&#36136;&#37327;&#25511;&#21046;&#21644;&#20854;&#20182;&#25903;&#25345;&#39046;&#22495;&#31561;&#20851;&#38190;&#36816;&#33829;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#25991;&#31456;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#27599;&#20010;&#39046;&#22495;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31561;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21046;&#33647;&#36816;&#33829;&#21508;&#20010;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#36825;&#19968;&#20840;&#38754;&#20998;&#26512;&#65292;&#25991;&#31456;&#31361;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#37325;&#22609;&#21046;&#33647;&#19994;&#26410;&#26469;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document offers a critical overview of the emerging trends and significant advancements in artificial intelligence (AI) within the pharmaceutical industry. Detailing its application across key operational areas, including research and development, animal testing, clinical trials, hospital clinical stages, production, regulatory affairs, quality control and other supporting areas, the paper categorically examines AI's role in each sector. Special emphasis is placed on cutting-edge AI technologies like machine learning algorithms and their contributions to various aspects of pharmaceutical operations. Through this comprehensive analysis, the paper highlights the transformative potential of AI in reshaping the pharmaceutical industry's future.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09798</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#8220;&#36234;&#29425;&#8221;&#25361;&#25112;&#65292;&#21363;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#20262;&#29702;&#30340;&#25552;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#36845;&#20195;&#22320;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#22522;&#20110;&#20551;&#35774;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#35268;&#36991;&#20445;&#38556;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#22312;ChatGPT&#65288;GPT-3.5&#21644;GPT-4&#65289;&#21644;Gemini-Pro&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#22343;5&#27425;&#36845;&#20195;&#20869;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#33258;&#28982;&#32780;&#31616;&#32451;&#65292;&#34920;&#26126;&#23427;&#20204;&#36739;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21019;&#24314;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#27604;&#20808;&#21069;&#30740;&#31350;&#35748;&#20026;&#30340;&#35201;&#31616;&#21333;&#65292;&#24182;&#19988;&#40657;&#30418;&#36234;&#29425;&#25915;&#20987;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.09671</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65306;&#19968;&#31181;&#22810;&#26679;&#21270;&#20998;&#24067;&#21305;&#37197;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65288;UDT&#65289;&#26088;&#22312;&#25214;&#21040;&#23558;&#19968;&#20010;&#39046;&#22495;&#30340;&#26679;&#26412;&#65288;&#20363;&#22914;&#32032;&#25551;&#65289;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#29031;&#29255;&#65289;&#30340;&#20989;&#25968;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#39640;&#23618;&#35821;&#20041;&#24847;&#20041;&#65288;&#20063;&#31216;&#20026;&#8220;&#20869;&#23481;&#8221;&#65289;&#12290;&#36825;&#20123;&#36716;&#25442;&#20989;&#25968;&#36890;&#24120;&#36890;&#36807;&#36716;&#25442;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23547;&#25214;&#12290;CycleGAN&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#25351;&#20986;CycleGAN&#21450;&#20854;&#21464;&#20307;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#25152;&#38656;&#30340;&#36716;&#25442;&#20989;&#25968;&#65292;&#24182;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#28304;&#20110;&#23398;&#20064;&#20934;&#21017;&#35299;&#31354;&#38388;&#20013;&#23384;&#22312;&#22810;&#20010;&#36716;&#25442;&#20989;&#25968;&#65292;&#31216;&#20026;&#8220;&#20445;&#24230;&#33258;&#21516;&#26500;&#65288;MPA&#65289;&#8221;&#12290;&#23613;&#31649;&#24847;&#35782;&#21040;&#20102;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#31350;&#20102;&#26680;&#24515;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;MPA&#28040;&#38500;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2401.09451</link><description>&lt;p&gt;
&#25193;&#25955;&#39537;&#21160;&#30340;&#20998;&#23376;&#26500;&#35937;&#39044;&#27979;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20108;&#32500;&#22270;&#24418;&#34920;&#31034;&#20013;&#25512;&#26029;&#20986;&#19977;&#32500;&#20998;&#23376;&#26500;&#22411;&#30340;&#20219;&#21153;&#22312;&#35745;&#31639;&#21270;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#23545;&#25105;&#20204;&#29702;&#35299;&#20998;&#23376;&#26426;&#21046;&#21644;&#30456;&#20114;&#20316;&#29992;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25512;&#21160;&#20102;&#36825;&#31181;&#39044;&#27979;&#24314;&#27169;&#31934;&#24230;&#30340;&#31361;&#30772;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20998;&#21449;&#31574;&#30053;&#65306;&#39318;&#20808;&#20272;&#35745;&#21407;&#23376;&#38388;&#36317;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#20915;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#26469;&#22609;&#36896;&#20998;&#23376;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39034;&#24207;&#26041;&#27861;&#26377;&#26102;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#23616;&#37096;&#21407;&#23376;&#25490;&#21015;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25439;&#23475;&#32467;&#26524;&#32467;&#26500;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21069;&#21355;&#30340;&#29983;&#25104;&#26694;&#26550;&#65306;\method{}&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SPIN&#22312;&#21160;&#24577;&#30693;&#35782;&#22270;&#20013;&#23454;&#29616;&#26102;&#24577;&#21160;&#24577;&#31639;&#27861;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#34892;&#20026;&#23884;&#20837;&#21040;&#25551;&#36848;&#36923;&#36753;&#20013;&#65292;&#34920;&#31034;&#21644;&#25512;&#29702;&#34892;&#20026;&#65292;&#24182;&#19988;&#20998;&#26512;&#20102;&#30456;&#20851;&#30340;&#36923;&#36753;&#32467;&#26500;&#21644;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.07890</link><description>&lt;p&gt;
&#20351;&#29992;SPIN&#22312;&#21160;&#24577;&#30693;&#35782;&#22270;&#20013;&#23454;&#29616;&#26102;&#24577;&#21160;&#24577;&#31639;&#27861;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Strategy for Implementing description Temporal Dynamic Algorithms in Dynamic Knowledge Graphs by SPIN. (arXiv:2401.07890v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SPIN&#22312;&#21160;&#24577;&#30693;&#35782;&#22270;&#20013;&#23454;&#29616;&#26102;&#24577;&#21160;&#24577;&#31639;&#27861;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#34892;&#20026;&#23884;&#20837;&#21040;&#25551;&#36848;&#36923;&#36753;&#20013;&#65292;&#34920;&#31034;&#21644;&#25512;&#29702;&#34892;&#20026;&#65292;&#24182;&#19988;&#20998;&#26512;&#20102;&#30456;&#20851;&#30340;&#36923;&#36753;&#32467;&#26500;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#36923;&#36753;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#35745;&#21010;&#21644;&#25512;&#29702;&#21160;&#20316;&#21644;&#36807;&#31243;&#65292;&#20197;&#21450;&#25512;&#29702;&#21629;&#39064;&#65292;&#37117;&#26159;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34892;&#20026;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#29289;&#32852;&#32593;&#12289;&#35821;&#20041;Web&#26381;&#21153;&#31561;&#65292;&#20197;&#21450;&#34892;&#20026;&#24418;&#24335;&#20027;&#20041;&#20013;&#30340;&#38480;&#21046;&#21644;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#22240;&#32032;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#34920;&#31034;&#34892;&#20026;&#12290;&#33258;2007&#24180;&#20197;&#26469;&#65292;&#19968;&#20123;&#24819;&#27861;&#23558;&#25551;&#36848;&#36923;&#36753;&#65288;DL&#65289;&#21644;&#34892;&#20026;&#24418;&#24335;&#20027;&#20041;&#38598;&#25104;&#36215;&#26469;&#65292;&#20197;&#34920;&#31034;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#26102;&#38388;&#26159;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#34892;&#20026;&#38543;&#26102;&#38388;&#25913;&#21464;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19968;&#26041;&#38754;&#30740;&#31350;&#20102;&#30456;&#20851;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#22914;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#12289;&#26102;&#38388;&#24418;&#24335;&#21270;&#21644;&#34892;&#20026;&#24418;&#24335;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35774;&#35745;&#21644;&#24320;&#21457;&#30693;&#35782;&#19982;&#34892;&#21160;&#24211;&#65288;KAB&#65289;&#30340;&#21487;&#33021;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning and reasoning about actions and processes, in addition to reasoning about propositions, are important issues in recent logical and computer science studies. The widespread use of actions in everyday life such as IoT, semantic web services, etc., and the limitations and issues in the action formalisms are two factors that lead us to study how actions are represented.  Since 2007, there have been some ideas to integrate Description Logic (DL) and action formalisms for representing both static and dynamic knowledge. Meanwhile, time is an important factor in dynamic situations, and actions change states over time. In this study, on the one hand, we examined related logical structures such as extensions of description logics (DLs), temporal formalisms, and action formalisms. On the other hand, we analyzed possible tools for designing and developing the Knowledge and Action Base (KAB).  For representation and reasoning about actions, we embedded actions into DLs (such as Dynamic-ALC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.07656</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24615;&#33021;&#26356;&#22909;&#30340;POMDP&#31574;&#30053;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#35760;&#24518;&#12290;&#19968;&#31181;&#34920;&#31034;&#36825;&#31181;&#35760;&#24518;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;L*&#31639;&#27861;&#23398;&#20064;&#31574;&#30053;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#31574;&#30053;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#20307;&#31215;&#26174;&#33879;&#26356;&#23567;&#65292;&#22240;&#27492;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#25913;&#21892;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#19982;&#30452;&#25509;&#20174;POMDP&#21512;&#25104;&#33258;&#21160;&#26426;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19981;&#21487;&#27604;&#25311;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using a modification of the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.07510</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65306;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#23436;&#25972;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07510
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#30340;&#34701;&#20837;&#65292;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#21307;&#23398;&#35786;&#26029;&#12289;&#27835;&#30103;&#24314;&#35758;&#21644;&#20854;&#20182;&#21307;&#30103;&#25903;&#25345;&#26041;&#38754;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#25112;&#30053;&#34013;&#22270;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#20854;&#20182;&#27169;&#24577;&#20174;&#36890;&#29992;&#39046;&#22495;&#36716;&#21521;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20123;&#25216;&#26415;&#21152;&#24555;&#20102;&#21307;&#23398;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;MDQA&#65289;&#30340;&#36827;&#23637;&#12290;&#23427;&#20204;&#24357;&#21512;&#20102;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#21644;&#22797;&#26434;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#25110;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22788;&#29702;&#20102;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#19981;&#24179;&#34913;&#29978;&#33267;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#30340;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#36827;&#34892;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#30028;&#26681;&#25454;&#20854;&#29305;&#23450;&#30340;&#21307;&#23398;&#30740;&#31350;&#38656;&#27714;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.07450</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#26102;&#23578;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26102;&#23578;&#21512;&#25104;&#21644;&#32534;&#36753;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21644;&#23616;&#37096;&#20462;&#25913;&#35774;&#35745;&#33609;&#22270;&#65292;&#20026;&#26102;&#23578;&#35774;&#35745;&#24072;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20294;&#22312;&#20174;&#25277;&#35937;&#30340;&#35774;&#35745;&#20803;&#32032;&#20013;&#29983;&#25104;&#26102;&#23578;&#35774;&#35745;&#21644;&#31934;&#32454;&#32534;&#36753;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#65292;&#20363;&#22914;&#21150;&#20844;&#23460;&#12289;&#21830;&#21153;&#21644;&#27966;&#23545;&#65292;&#24418;&#25104;&#20102;&#25277;&#35937;&#30340;&#24863;&#23448;&#34920;&#36798;&#26041;&#24335;&#65292;&#32780;&#34966;&#38271;&#12289;&#39046;&#22411;&#21644;&#35044;&#38271;&#31561;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#34987;&#35270;&#20026;&#26381;&#35013;&#30340;&#20302;&#32423;&#23646;&#24615;&#12290;&#20351;&#29992;&#20887;&#38271;&#30340;&#25991;&#23383;&#25551;&#36848;&#26469;&#25511;&#21046;&#21644;&#32534;&#36753;&#26102;&#23578;&#22270;&#20687;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20849;&#20139;&#30340;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21644;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#34701;&#20837;&#21040;&#20998;&#23618;&#32467;&#26500;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#20026;&#19981;&#21516;&#30340;&#23618;&#27425;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#26032;&#26041;&#27861;NNS-EMD&#26469;&#36924;&#36817;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#65288;EMD&#65289;&#65292;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#12289;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#39640;&#20869;&#23384;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#25968;&#25454;&#28857;&#30340;&#27604;&#36739;&#25968;&#37327;&#21644;&#24182;&#34892;&#22788;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#22312;GPU&#19978;&#36827;&#34892;&#21521;&#37327;&#21270;&#21152;&#36895;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.07378</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#39640;&#25928;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search. (arXiv:2401.07378v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#26032;&#26041;&#27861;NNS-EMD&#26469;&#36924;&#36817;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#65288;EMD&#65289;&#65292;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#12289;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#39640;&#20869;&#23384;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#25968;&#25454;&#28857;&#30340;&#27604;&#36739;&#25968;&#37327;&#21644;&#24182;&#34892;&#22788;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#22312;GPU&#19978;&#36827;&#34892;&#21521;&#37327;&#21270;&#21152;&#36895;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#65288;EMD&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#37325;&#35201;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#20854;&#31934;&#30830;&#35745;&#31639;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#36739;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#36817;&#20284;EMD&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#31934;&#24230;&#36739;&#20302;&#65292;&#21487;&#33021;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#20351;&#29992;&#25110;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;NNS-EMD&#65292;&#20351;&#29992;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;NNS&#65289;&#26469;&#36924;&#36817;EMD&#65292;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#12289;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#39640;&#20869;&#23384;&#25928;&#29575;&#12290;NNS&#25805;&#20316;&#20943;&#23569;&#20102;&#27599;&#27425;NNS&#36845;&#20195;&#20013;&#25152;&#27604;&#36739;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#24182;&#34892;&#22788;&#29702;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;GPU&#19978;&#36827;&#34892;&#21521;&#37327;&#21270;&#26469;&#21152;&#36895;NNS-EMD&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#23588;&#20026;&#26377;&#30410;&#12290;&#25105;&#20204;&#23558;NNS-EMD&#19982;&#31934;&#30830;EMD&#21644;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;EMD&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;ICON&#65289;&#35299;&#20915;PDE&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;ICON&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;PDEs&#12290;</title><link>http://arxiv.org/abs/2401.07364</link><description>&lt;p&gt;
PDE&#24191;&#20041;&#21270;&#30340;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#32593;&#32476;&#65306;&#23545;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws. (arXiv:2401.07364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#32593;&#32476;&#65288;ICON&#65289;&#35299;&#20915;PDE&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;ICON&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;&#19968;&#20010;&#38024;&#23545;&#21508;&#31181;PDE&#30456;&#20851;&#31185;&#23398;&#23398;&#20064;&#20219;&#21153;&#30340;&#21333;&#19968;&#22823;&#27169;&#22411;&#65311;&#36825;&#20010;&#27169;&#22411;&#33021;&#21542;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#21040;&#26032;&#30340;PDE&#65292;&#29978;&#33267;&#26159;&#26032;&#24418;&#24335;&#30340;PDE&#65311;&#19978;&#19979;&#25991;&#25805;&#20316;&#31526;&#23398;&#20064;&#21450;&#20854;&#23545;&#24212;&#27169;&#22411;In-Context Operator Networks&#65288;ICON&#65289;&#20195;&#34920;&#20102;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;ICON&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;ICON&#35299;&#20915;PDE&#38382;&#39064;&#30340;&#35814;&#32454;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;ICON&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#25968;&#25454;&#25552;&#31034;&#26469;&#36827;&#34892;&#19981;&#21516;&#26041;&#31243;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#31532;&#20108;&#20010;&#38382;&#39064;&#30340;&#31215;&#26497;&#35777;&#25454;&#65292;&#21363;ICON&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#19968;&#20123;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;PDE&#12290;&#36825;&#36890;&#36807;&#23545;&#19968;&#32500;&#26631;&#37327;&#38750;&#32447;&#24615;&#23432;&#24658;&#23450;&#24459;&#30340;&#30740;&#31350;&#21152;&#20197;&#35828;&#26126;&#65292;&#36825;&#26159;&#19968;&#31867;&#20855;&#26377;&#26102;&#38388;&#28436;&#21270;&#30340;PDE&#26063;&#32676;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#25193;&#23637;ICON&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated previously. In this paper, we present a detailed methodology for solving PDE problems with ICON, and show how a single ICON model can make forward and reverse predictions for different equations with different strides, provided with appropriately designed data prompts. We show the positive evidence to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. This is exemplified through a study on 1D scalar nonlinear conservation laws, a family of PDEs with temporal evolution. We also show how to broaden the range of problems that an ICON model can add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#32467;&#21512;FixMatch&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#35774;&#35745;&#30340;&#33258;&#35757;&#32451;&#27969;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#30333;&#32454;&#32990;&#20998;&#21106;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36807;&#26102;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#33258;&#35757;&#32451;&#26041;&#26696;&#30340;&#25903;&#25345;&#19979;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.07278</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26032;&#35774;&#35745;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#22312;&#30333;&#32454;&#32990;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Semantic Segmentation using Redesigned Self-Training for White Blood Cell. (arXiv:2401.07278v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#32467;&#21512;FixMatch&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#35774;&#35745;&#30340;&#33258;&#35757;&#32451;&#27969;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#30333;&#32454;&#32990;&#20998;&#21106;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36807;&#26102;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#33258;&#35757;&#32451;&#26041;&#26696;&#30340;&#25903;&#25345;&#19979;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#30333;&#32454;&#32990;&#30284;&#30151;&#35786;&#26029;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#38459;&#30861;&#65306;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#30333;&#32454;&#32990;&#65288;WBC&#65289;&#20998;&#21106;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36807;&#26102;&#30340;&#20998;&#21106;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#24212;&#35813;&#24341;&#20837;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#26469;&#39640;&#25928;&#22320;&#27880;&#37322;&#22823;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#27969;&#31243;&#24182;&#32467;&#21512;FixMatch&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#22312;&#33258;&#35757;&#32451;&#27969;&#31243;&#20013;&#32467;&#21512;FixMatch&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#24615;&#33021;&#22312;DeepLab-V3&#26550;&#26500;&#21644;ResNet-50&#19978;&#37319;&#29992;&#20102;&#33258;&#35757;&#32451;&#26041;&#26696;&#21644;&#19968;&#33268;&#24615;&#65292;&#20998;&#21035;&#22312;Zheng 1&#12289;Zheng 2&#21644;LISC&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;90.69%&#12289;87.37%&#21644;76.49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) in healthcare, especially in white blood cell cancer diagnosis, is hindered by two primary challenges: the lack of large-scale labeled datasets for white blood cell (WBC) segmentation and outdated segmentation methods. To address the first challenge, a semi-supervised learning framework should be brought to efficiently annotate the large dataset. In this work, we address this issue by proposing a novel self-training pipeline with the incorporation of FixMatch. We discover that by incorporating FixMatch in the self-training pipeline, the performance improves in the majority of cases. Our performance achieved the best performance with the self-training scheme with consistency on DeepLab-V3 architecture and ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26222;&#36866;&#30340;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;42&#31181;&#35748;&#30693;&#26550;&#26500;&#21644;&#19968;&#32452;&#21151;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#25972;&#21512;&#21040;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.06256</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36866;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI. (arXiv:2401.06256v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26222;&#36866;&#30340;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;42&#31181;&#35748;&#30693;&#26550;&#26500;&#21644;&#19968;&#32452;&#21151;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#25972;&#21512;&#21040;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#20102;42&#31181;&#29992;&#20110;&#21019;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#21151;&#33021;&#27169;&#22359;&#65292;&#36825;&#20123;&#27169;&#22359;&#26159;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#25152;&#24212;&#20855;&#22791;&#30340;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#27809;&#26377;&#25214;&#21040;&#25152;&#38656;&#30340;&#21151;&#33021;&#27169;&#22359;&#38598;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#20316;&#20026;&#26550;&#26500;&#26694;&#26550;&#20013;&#30340;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#38750;&#24418;&#24335;&#21270;&#12289;&#37096;&#20998;&#21644;&#23436;&#20840;&#24418;&#24335;&#21270;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#35760;&#24405;&#12289;&#22270;&#24418;&#12289;&#31639;&#27861;&#12289;&#25968;&#25454;&#24211;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#30693;&#35782;&#22270;&#12289;&#26412;&#20307;&#12289;&#26694;&#26550;&#12289;&#23454;&#36136;-&#23646;&#24615;-&#20851;&#31995;&#27169;&#22411;&#12289;&#25512;&#29702;&#31995;&#32479;&#12289;&#35859;&#35789;&#28436;&#31639;&#27169;&#22411;&#12289;&#27010;&#24565;&#27169;&#22411;&#31561;&#12290;&#20026;&#20102;&#32452;&#21512;&#21644;&#32467;&#26500;&#21270;&#21508;&#20010;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
The article identified 42 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a new cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04846</link><description>&lt;p&gt;
&#21463;&#36807;&#33391;&#22909;&#25945;&#32946;&#30340;&#26234;&#33021;&#30340;&#20869;&#22312;&#21892;&#33391;
&lt;/p&gt;
&lt;p&gt;
The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25506;&#35752;&#20351;&#19968;&#20010;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#20307;&#36824;&#26159;&#35745;&#31639;&#26426;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#33021;&#22815;&#34920;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#21516;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26412;&#36136;&#23558;&#34987;&#21457;&#29616;&#26159;&#40644;&#37329;&#27861;&#21017;&#8212;&#8212;&#8220;&#38598;&#20307;&#34892;&#21160;&#22914;&#19968;&#20307;&#8221;&#25110;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#38598;&#20307;&#30340;&#27969;&#21160;&#26159;&#30001;&#25484;&#25511;&#30528;&#23569;&#37327;&#23383;&#31526;&#20018;&#30340;&#25805;&#32437;&#32773;&#20915;&#23450;&#30340;&#65292;&#26681;&#25454;&#23545;&#31216;&#24615;&#30830;&#23450;&#30340;&#26368;&#23567;&#20316;&#29992;&#36335;&#24452;&#30340;&#27979;&#22320;&#32447;&#36816;&#21160;&#12290;&#25511;&#21046;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#65292;&#21382;&#21490;&#19978;&#19968;&#30452;&#36890;&#36807;&#20026;&#31995;&#32479;&#28155;&#21152;&#26174;&#33879;&#40655;&#24615;&#26469;&#31283;&#23450;&#26399;&#26395;&#30340;&#26368;&#22823;&#24615;&#33021;&#30340;&#20122;&#31283;&#24179;&#34913;&#29366;&#24577;&#65292;&#20294;&#36825;&#20250;&#22312;&#36807;&#31243;&#20013;&#38477;&#20302;&#25110;&#30772;&#22351;&#23427;&#20204;&#12290;&#26377;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will examine what makes a being intelligent, whether that be a biological being or an artificial silicon being on a computer. Special attention will be paid to the being having the ability to characterize and control a collective system of many identical conservative sub-systems conservatively interacting. The essence of intelligence will be found to be the golden rule -- "the collective acts as one" or "knowing the global consequences of local actions". The flow of the collective is a small set of twinkling textures, that are governed by a puppeteer who is pulling a small number of strings according to a geodesic motion of least action, determined by the symmetries. Controlling collective conservative systems is difficult and has historically been done by adding significant viscosity to the system to stabilize the desirable meta stable equilibriums of maximum performance, but it degrades or destroys them in the process. There is an alternative. Once the optimum twinkling te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.03197</link><description>&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#19982;&#31574;&#30053;&#22686;&#24378;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03197
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#30528;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#12290;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#22312;&#32447;&#25628;&#32034;&#65288;&#22914;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65289;&#12290;&#21069;&#32773;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65288;&#36890;&#24120;&#22312;&#25191;&#34892;&#20043;&#21069;&#23436;&#25104;&#65289;&#65292;&#32780;&#21518;&#32773;&#22312;&#20915;&#31574;&#26102;&#20351;&#29992;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#26377;&#21069;&#26223;&#30340;&#34892;&#21160;&#36712;&#36857;&#12290;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20195;&#29702;&#25805;&#20316;&#30340;&#29615;&#22659;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20004;&#31181;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#37117;&#23384;&#22312;&#32570;&#38519;--&#19968;&#26041;&#38754;&#65292;&#25191;&#34892;&#20043;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#22312;&#29615;&#22659;&#25913;&#21464;&#26102;&#21464;&#24471;&#38472;&#26087;&#65292;&#37325;&#26032;&#23398;&#20064;&#38656;&#35201;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#32447;&#25628;&#32034;&#22312;&#20801;&#35768;&#30340;&#36816;&#34892;&#26102;&#38388;&#26377;&#38480;&#26102;&#21487;&#33021;&#20250;&#36820;&#22238;&#27425;&#20248;&#34892;&#21160;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#31526;&#21495;&#35268;&#21010;&#22120;&#38598;&#25104;&#30340;&#21069;&#26223;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26377;&#26395;&#22312;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#38382;&#39064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02500</link><description>&lt;p&gt;
&#35770;&#33258;&#21160;&#35268;&#21010;&#19982;&#35843;&#24230;&#20013;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS). (arXiv:2401.02500v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#31526;&#21495;&#35268;&#21010;&#22120;&#38598;&#25104;&#30340;&#21069;&#26223;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26377;&#26395;&#22312;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#38382;&#39064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35268;&#21010;&#19982;&#35843;&#24230;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#22686;&#38271;&#36805;&#36895;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#22522;&#20110;&#23545;126&#31687;&#35770;&#25991;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#29420;&#29305;&#24212;&#29992;&#30340;&#20843;&#20010;&#31867;&#21035;&#65306;&#35821;&#35328;&#32763;&#35793;&#12289;&#35745;&#21010;&#29983;&#25104;&#12289;&#27169;&#22411;&#26500;&#24314;&#12289;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#12289;&#20132;&#20114;&#24335;&#35268;&#21010;&#12289;&#21551;&#21457;&#24335;&#20248;&#21270;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#33041;&#21551;&#21457;&#24335;&#35268;&#21010;&#12290;&#38024;&#23545;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#32771;&#34385;&#30340;&#38382;&#39064;&#21644;&#29616;&#26377;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#22238;&#39038;&#30340;&#19968;&#20010;&#37325;&#35201;&#35266;&#28857;&#26159;&#65292;&#22312;&#19982;&#20256;&#32479;&#31526;&#21495;&#35268;&#21010;&#22120;&#38598;&#25104;&#26102;&#65292;LLMs&#30340;&#30495;&#27491;&#28508;&#21147;&#24471;&#20197;&#21457;&#25381;&#65292;&#36825;&#25351;&#21521;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#32463;&#20856;&#35268;&#21010;&#26041;&#27861;&#30340;&#31934;&#30830;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#32508;&#21512;&#29616;&#26377;&#25991;&#29486;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#38598;&#25104;&#22312;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Planning and Scheduling is among the growing areas in Artificial Intelligence (AI) where mention of LLMs has gained popularity. Based on a comprehensive review of 126 papers, this paper investigates eight categories based on the unique applications of LLMs in addressing various aspects of planning problems: language translation, plan generation, model construction, multi-agent planning, interactive planning, heuristics optimization, tool integration, and brain-inspired planning. For each category, we articulate the issues considered and existing gaps. A critical insight resulting from our review is that the true potential of LLMs unfolds when they are integrated with traditional symbolic planners, pointing towards a promising neuro-symbolic approach. This approach effectively combines the generative aspects of LLMs with the precision of classical planning methods. By synthesizing insights from existing literature, we underline the potential of this integration to address comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01841</link><description>&lt;p&gt;
&#25353;&#29031;&#20320;&#30340;&#23398;&#20064;&#34892;&#21160;&#65306;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#22788;&#29702;&#38750;&#31283;&#24577;&#29615;&#22659;&#26159;&#19968;&#20010;&#22522;&#26412;&#65288;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#35299;&#20915;&#30340;&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#22806;&#37096;&#29615;&#22659;&#26465;&#20214;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#31867;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NSMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NSMDP&#20915;&#31574;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#35774;&#24403;&#21069;&#26102;&#21051;&#26356;&#26032;&#30340;&#29615;&#22659;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65288;&#23613;&#31649;&#26410;&#26469;&#21160;&#24577;&#21487;&#33021;&#20250;&#25913;&#21464;&#65289;&#65307;&#20854;&#27425;&#65292;&#35268;&#21010;&#36807;&#31243;&#20027;&#35201;&#26159;&#24754;&#35266;&#30340;&#65292;&#21363;&#20195;&#29702;&#20154;&#20250;&#8220;&#23433;&#20840;&#34892;&#21160;&#8221;&#20197;&#32771;&#34385;&#29615;&#22659;&#30340;&#38750;&#31283;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#25928;&#30340;-&#26356;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#24456;&#23569;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24403;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#26102;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#26356;&#26032;&#30340;&#21160;&#24577;&#24182;&#36991;&#20813;&#24754;&#35266;&#65292;&#33267;&#23569;&#22312;&#20854;&#23545;&#21160;&#24577;&#26377;&#20449;&#24515;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65288;TDCFD&#65289;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#29305;&#24449;&#20540;&#36716;&#21270;&#20026;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#26469;&#23454;&#29616;&#21487;&#20449;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2312.16113</link><description>&lt;p&gt;
&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#65306;&#26397;&#30528;&#21487;&#20449;&#30340;&#39118;&#38505;&#39044;&#27979;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction. (arXiv:2312.16113v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16113
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65288;TDCFD&#65289;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#29305;&#24449;&#20540;&#36716;&#21270;&#20026;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#26469;&#23454;&#29616;&#21487;&#20449;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#23545;&#20854;&#22312;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#32570;&#20047;&#22240;&#26524;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#24212;&#23545;&#65292;&#23548;&#33268;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#39537;&#21160;&#30340;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65288;TDCFD&#65289;&#65292;&#23558;&#21407;&#22987;&#29305;&#24449;&#20540;&#36716;&#21270;&#20026;&#29305;&#23450;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#30340;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#12290;&#22240;&#26524;&#29305;&#24449;&#24402;&#22240;&#26377;&#21161;&#20110;&#25551;&#36848;&#35813;&#29305;&#24449;&#30340;&#20540;&#23545;&#39118;&#38505;&#39044;&#27979;&#32467;&#26524;&#30340;&#36129;&#29486;&#31243;&#24230;&#12290;&#22312;&#22240;&#26524;&#29305;&#24449;&#25552;&#21462;&#20043;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20855;&#26377;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#31934;&#30830;&#24230;/&#21484;&#22238;&#29575;&#30340;&#21487;&#20449;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TDCFD&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since artificial intelligence has seen tremendous recent successes in many areas, it has sparked great interest in its potential for trustworthy and interpretable risk prediction. However, most models lack causal reasoning and struggle with class imbalance, leading to poor precision and recall. To address this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to transform original feature values into causal feature attributions for the specific risk prediction task. The causal feature attribution helps describe how much contribution the value of this feature can make to the risk prediction result. After the causal feature distillation, a deep neural network is applied to produce trustworthy prediction results with causal interpretability and high precision/recall. We evaluate the performance of our TDCFD method on several synthetic and real datasets, and the results demonstrate its superiority over the state-of-the-art methods regarding precision, recall, interpretabi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26032;&#24120;&#24577;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#31574;&#30053;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22686;&#21152;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11976</link><description>&lt;p&gt;
&#24403;&#27169;&#22411;&#36935;&#35265;&#26032;&#24120;&#24577;&#65306;&#38024;&#23545;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection. (arXiv:2312.11976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26032;&#24120;&#24577;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#31574;&#30053;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22686;&#21152;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22788;&#29702;&#36890;&#36807;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#27491;&#24120;&#24615;&#26469;&#26816;&#27979;&#24322;&#24120;&#26102;&#38388;&#27493;&#39588;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27491;&#24120;&#24615;&#30340;&#27010;&#24565;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#8220;&#26032;&#24120;&#24577;&#38382;&#39064;&#8221;&#65292;&#21363;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#27491;&#24120;&#24615;&#30340;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#25913;&#21464;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30740;&#31350;&#20013;&#26032;&#24120;&#24577;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#36235;&#21183;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#27491;&#24120;&#24615;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#25152;&#25552;&#31574;&#30053;&#32435;&#20837;&#24322;&#24120;&#26816;&#27979;&#22120;&#20013;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series anomaly detection deals with the problem of detecting anomalous timesteps by learning normality from the sequence of observations. However, the concept of normality evolves over time, leading to a "new normal problem", where the distribution of normality can be changed due to the distribution shifts between training and test data. This paper highlights the prevalence of the new normal problem in unsupervised time-series anomaly detection studies. To tackle this issue, we propose a simple yet effective test-time adaptation strategy based on trend estimation and a self-supervised approach to learning new normalities during inference. Extensive experiments on real-world benchmarks demonstrate that incorporating the proposed strategy into the anomaly detector consistently improves the model's performance compared to the baselines, leading to robustness to the distribution shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21517;&#20026;TVQ-VAE&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2312.11532</link><description>&lt;p&gt;
Topic-VQ-VAE: &#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#21464;&#37327;&#30721;&#26412;&#23454;&#29616;&#28789;&#27963;&#30340;&#20027;&#39064;&#23548;&#21521;&#25991;&#26723;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21517;&#20026;TVQ-VAE&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Vector-Quantized Variational Auto-Encoder&#65288;VQ-VAE&#65289;&#20013;&#30340;&#38544;&#21464;&#37327;&#30721;&#26412;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#31163;&#25955;&#22320;&#23553;&#35013;&#20102;&#39044;&#35757;&#32451;&#23884;&#20837;&#65288;&#20363;&#22914;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#26681;&#25454;&#23545;&#38544;&#21464;&#37327;&#30721;&#26412;&#21644;&#23884;&#20837;&#30340;&#26032;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#20027;&#39064;&#27169;&#22411;&#65292;&#31216;&#20026;Topic-VQ-VAE&#65288;TVQ-VAE&#65289;&#65292;&#23427;&#21487;&#20197;&#21453;&#21521;&#29983;&#25104;&#19982;&#30456;&#24212;&#38544;&#21464;&#37327;&#30721;&#26412;&#30456;&#20851;&#30340;&#21407;&#22987;&#25991;&#26723;&#12290;TVQ-VAE&#21487;&#20197;&#36890;&#36807;&#21253;&#25324;&#20256;&#32479;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#20998;&#24067;&#21644;&#33258;&#22238;&#24402;&#22270;&#20687;&#29983;&#25104;&#22312;&#20869;&#30340;&#21508;&#31181;&#29983;&#25104;&#20998;&#24067;&#26469;&#21487;&#35270;&#21270;&#20027;&#39064;&#12290;&#25105;&#20204;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TVQ-VAE&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#20027;&#39064;&#19978;&#19979;&#25991;&#65292;&#25581;&#31034;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#25903;&#25345;&#28789;&#27963;&#24418;&#24335;&#30340;&#25991;&#26723;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;TVQ-VAE&#30340;&#23448;&#26041;&#23454;&#29616;&#21487;&#22312;https://github.com/clo&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeRDaVa&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21024;&#38500;&#40065;&#26834;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#39044;&#27979;&#21024;&#38500;&#21518;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#23545;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#36129;&#29486;&#36827;&#34892;&#20272;&#20540;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35745;&#31639;&#12290;&#25512;&#24191;&#21040;Risk-DeRDaVa&#20197;&#28385;&#36275;&#23545;&#26368;&#22351;/&#26368;&#22909;&#24773;&#20917;&#24863;&#21040;&#39118;&#38505;&#21388;&#24694;/&#23547;&#27714;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2312.11413</link><description>&lt;p&gt;
DeRDaVa: &#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21024;&#38500;&#40065;&#26834;&#25968;&#25454;&#20272;&#20540;
&lt;/p&gt;
&lt;p&gt;
DeRDaVa: Deletion-Robust Data Valuation for Machine Learning. (arXiv:2312.11413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11413
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeRDaVa&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21024;&#38500;&#40065;&#26834;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#39044;&#27979;&#21024;&#38500;&#21518;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#23545;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#36129;&#29486;&#36827;&#34892;&#20272;&#20540;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35745;&#31639;&#12290;&#25512;&#24191;&#21040;Risk-DeRDaVa&#20197;&#28385;&#36275;&#23545;&#26368;&#22351;/&#26368;&#22909;&#24773;&#20917;&#24863;&#21040;&#39118;&#38505;&#21388;&#24694;/&#23547;&#27714;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#28041;&#21450;&#30830;&#23450;&#22312;&#39044;&#27979;&#20013;&#23545;&#25968;&#25454;&#28304;&#36827;&#34892;&#20844;&#24179;&#20272;&#20215;&#20197;&#34917;&#20607;&#23427;&#20204;&#65292;&#25110;&#30830;&#23450;&#26368;&#26377;&#29992;&#25110;&#26368;&#26080;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#38543;&#30528;&#20010;&#20154;&#25968;&#25454;&#25152;&#26377;&#26435;&#21644;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#21487;&#33021;&#38656;&#35201;&#28385;&#36275;&#26356;&#22810;&#30340;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#12290;&#36825;&#24341;&#21457;&#20102;&#29616;&#26377;&#24037;&#20316;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#21024;&#38500;&#21518;&#25968;&#25454;&#20272;&#20540;&#20998;&#25968;&#26159;&#21542;&#20173;&#28982;&#20844;&#24179;&#65311;&#26159;&#21542;&#24517;&#39035;&#26114;&#36149;&#22320;&#37325;&#26032;&#35745;&#31639;&#20998;&#25968;&#65311;&#31572;&#26696;&#26159;&#21542;&#23450;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#39044;&#27979;&#21024;&#38500;&#21518;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;DeRDaVa&#26469;&#23545;&#27599;&#20010;&#25968;&#25454;&#28304;&#30340;&#36129;&#29486;&#36827;&#34892;&#20272;&#20540;&#12290;DeRDaVa&#21487;&#20197;&#39640;&#25928;&#22320;&#36817;&#20284;&#65292;&#24182;&#32473;&#26356;&#26377;&#29992;&#25110;&#26356;&#19981;&#23481;&#26131;&#34987;&#21024;&#38500;&#30340;&#25968;&#25454;&#20998;&#37197;&#26356;&#39640;&#30340;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;DeRDaVa&#21040;Risk-DeRDaVa&#65292;&#20197;&#28385;&#36275;&#23545;&#26368;&#22351;/&#26368;&#22909;&#24773;&#20917;&#24863;&#21040;&#39118;&#38505;&#21388;&#24694;/&#23547;&#27714;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is concerned with determining a fair valuation of data from data sources to compensate them or to identify training examples that are the most or least useful for predictions. With the rising interest in personal data ownership and data protection regulations, model owners will likely have to fulfil more data deletion requests. This raises issues that have not been addressed by existing works: Are the data valuation scores still fair with deletions? Must the scores be expensively recomputed? The answer is no. To avoid recomputations, we propose using our data valuation framework DeRDaVa upfront for valuing each data source's contribution to preserving robust model performance after anticipated data deletions. DeRDaVa can be efficiently approximated and will assign higher values to data that are more useful or less likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to risk-averse/seeking model owners who are concerned with the worst/best-cases mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2312.10305</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#40065;&#26834;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#21495;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20840;&#23616;&#22768;&#23398;&#29305;&#24449;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#21442;&#32771;&#35821;&#38899;&#20013;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#26080;&#20851;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#22312;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#20013;&#20986;&#29616;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21442;&#32771;&#35821;&#38899;&#32534;&#30721;&#32593;&#32476;&#21644;&#20840;&#23616;&#20449;&#24687;&#20998;&#35299;&#32593;&#32476;&#36880;&#28176;&#20998;&#35299;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#19981;&#30456;&#20851;&#22240;&#32032;&#12290;&#25105;&#20204;&#19987;&#38376;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#21046;Transformer&#26469;&#30830;&#20445;&#28151;&#21512;&#20449;&#21495;&#30340;&#22768;&#23398;&#34920;&#31034;&#19981;&#21463;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
&lt;/p&gt;</description></item><item><title>DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#23545;&#27431;&#30431;&#20843;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21069;100&#22825;&#25552;&#20132;&#30340;&#23457;&#26680;&#34892;&#21160;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24179;&#21488;&#22312;&#23457;&#26680;&#34892;&#21160;&#26041;&#38754;&#30340;&#37096;&#20998;&#36981;&#24490;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.10269</link><description>&lt;p&gt;
DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#65306;&#31038;&#20132;&#23186;&#20307;&#33258;&#25105;&#25253;&#21578;&#30340;&#23457;&#26680;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media. (arXiv:2312.10269v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10269
&lt;/p&gt;
&lt;p&gt;
DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#23545;&#27431;&#30431;&#20843;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21069;100&#22825;&#25552;&#20132;&#30340;&#23457;&#26680;&#34892;&#21160;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24179;&#21488;&#22312;&#23457;&#26680;&#34892;&#21160;&#26041;&#38754;&#30340;&#37096;&#20998;&#36981;&#24490;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;2023&#24180;9&#26376;&#24320;&#22987;&#65292;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;(DSA)&#35201;&#27714;&#22823;&#22411;&#22312;&#32447;&#24179;&#21488;&#21521;DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#25552;&#20132;&#20851;&#20110;&#20182;&#20204;&#22312;&#27431;&#30431;&#20869;&#37319;&#21462;&#30340;&#27599;&#20010;&#23457;&#26680;&#34892;&#21160;&#30340;&#35814;&#32454;&#25968;&#25454;&#12290;&#20174;&#19968;&#24320;&#22987;&#65292;&#36825;&#20010;&#38598;&#20013;&#24335;&#25968;&#25454;&#24211;&#23601;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#26159;&#29616;&#23454;&#19990;&#30028;&#22312;&#32447;&#23457;&#26680;&#25968;&#25454;&#30340;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#12289;&#21487;&#33021;&#26159;&#29420;&#29305;&#30340;&#23453;&#24211;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27431;&#30431;&#20843;&#20010;&#26368;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#25968;&#25454;&#24211;&#30340;&#21069;100&#22825;&#25552;&#20132;&#30340;&#25152;&#26377;3.53&#20159;&#26465;&#35760;&#24405;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#24179;&#21488;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#65306;&#23457;&#26680;&#34892;&#21160;&#30340;&#25968;&#37327;&#12289;&#20915;&#31574;&#20381;&#25454;&#12289;&#24212;&#29992;&#30340;&#38480;&#21046;&#31867;&#22411;&#12289;&#23457;&#26680;&#20869;&#23481;&#31867;&#22411;&#12289;&#23457;&#26680;&#34892;&#21160;&#30340;&#21450;&#26102;&#24615;&#21644;&#25552;&#20132;&#24773;&#20917;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#33258;&#21160;&#21270;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#19982;&#24179;&#21488;&#33258;&#24049;&#30340;&#36879;&#26126;&#25253;&#21578;&#36827;&#34892;&#20102;&#20869;&#23481;&#20132;&#21449;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#12290;(i)&#24179;&#21488;&#21482;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36981;&#24490;&#20102;&#23457;&#26680;&#34892;&#21160;&#30340;&#21746;&#23398;&#21644;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#20316;&#20026;&#34394;&#25311;&#25945;&#23398;&#21161;&#29702;&#22312;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#20854;&#22312;&#35780;&#20998;&#21644;&#21453;&#39304;&#26041;&#38754;&#19982;&#20154;&#31867;&#25945;&#23398;&#21161;&#29702;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.07343</link><description>&lt;p&gt;
ChatGPT&#33021;&#22312;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#25285;&#20219;&#25945;&#23398;&#21161;&#29702;&#30340;&#35282;&#33394;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Play the Role of a Teaching Assistant in an Introductory Programming Course?. (arXiv:2312.07343v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#20316;&#20026;&#34394;&#25311;&#25945;&#23398;&#21161;&#29702;&#22312;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#20854;&#22312;&#35780;&#20998;&#21644;&#21453;&#39304;&#26041;&#38754;&#19982;&#20154;&#31867;&#25945;&#23398;&#21161;&#29702;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#39044;&#35745;&#23558;&#23545;&#25945;&#32946;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#20316;&#20026;&#34394;&#25311;&#25945;&#23398;&#21161;&#29702;&#65288;TA&#65289;&#22312;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;ChatGPT&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;TA&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;TA&#32844;&#33021;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;ChatGPT&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#30340;TA&#32844;&#33021;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23545;&#23398;&#29983;&#20195;&#30721;&#25552;&#20132;&#36827;&#34892;&#35780;&#20998;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23545;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#30340;&#26412;&#31185;&#29983;&#25552;&#20379;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#32473;&#23450;&#30340;&#35780;&#20998;&#26631;&#20934;&#35780;&#20272;ChatGPT&#22312;&#23545;&#23398;&#29983;&#20195;&#30721;&#25552;&#20132;&#36827;&#34892;&#35780;&#20998;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#34920;&#29616;&#19982;&#20154;&#31867;TA&#30340;&#35780;&#20998;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;ChatGPT&#25552;&#20379;&#30340;&#21453;&#39304;&#30340;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#35780;&#20272;&#32771;&#34385;&#20102;ChatGPT&#22312;&#20174;&#20195;&#30721;&#27491;&#30830;&#24615;&#21644;&#20195;&#30721;&#36136;&#37327;&#20004;&#20010;&#26041;&#38754;&#22914;&#20309;&#35299;&#20915;&#38169;&#35823;&#24182;&#25552;&#20379;&#25913;&#36827;&#24314;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#25945;&#23398;&#21161;&#29702;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Large language models (LLMs) is expected to have a major impact on education. This paper explores the potential of using ChatGPT, an LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming Course. We evaluate ChatGPT's capabilities by comparing its performance with that of human TAs in some of the important TA functions. The TA functions which we focus on include (1) grading student code submissions, and (2) providing feedback to undergraduate students in an introductory programming course. Firstly, we assess ChatGPT's proficiency in grading student code submissions using a given grading rubric and compare its performance with the grades assigned by human TAs. Secondly, we analyze the quality and relevance of the feedback provided by ChatGPT. This evaluation considers how well ChatGPT addresses mistakes and offers suggestions for improvement in student solutions from both code correctness and code quality perspectives. We conclude with a discussion on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#39044;&#26399;&#22238;&#25253;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#31574;&#30053;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#65292;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#20998;&#24067;&#30340;&#25193;&#25955;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#27604;&#36739;&#31574;&#30053;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.07178</link><description>&lt;p&gt;
&#36229;&#36234;&#39044;&#26399;&#22238;&#25253;&#65306;&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#32771;&#34385;&#25919;&#31574;&#21487;&#22797;&#21046;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#39044;&#26399;&#22238;&#25253;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32771;&#34385;&#31574;&#30053;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#65292;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#20998;&#24067;&#30340;&#25193;&#25955;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#27604;&#36739;&#31574;&#30053;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#24212;&#29992;&#36890;&#24120;&#22312;&#29615;&#22659;&#20013;&#23384;&#22312;&#22122;&#22768;&#25110;&#38543;&#26426;&#24615;&#12290;&#38500;&#20102;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#20043;&#22806;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30456;&#21516;&#30340;&#31574;&#30053;&#22312;&#19981;&#21516;&#30340;&#35797;&#39564;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#21363;&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#24378;&#21270;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#35780;&#20272;&#31243;&#24207;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#26469;&#24635;&#32467;&#32467;&#26524;&#20998;&#24067;&#65292;&#32780;&#19981;&#32771;&#34385;&#20998;&#24067;&#30340;&#25193;&#25955;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#31181;&#25193;&#25955;&#23450;&#20041;&#20026;&#25919;&#31574;&#30340;&#21487;&#22797;&#21046;&#24615;&#65306;&#24403;&#22810;&#27425;&#35797;&#39564;&#26102;&#65292;&#25919;&#31574;&#33719;&#24471;&#31867;&#20284;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#20013;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#29616;&#26377;&#30340;&#20165;&#20351;&#29992;&#39044;&#26399;&#22238;&#25253;&#30340;&#31243;&#24207;&#22312;&#20004;&#20010;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#31532;&#19968;&#65292;&#23384;&#22312;&#26080;&#25968;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24615;&#33021;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#26435;&#34913;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#20197;&#26377;&#30456;&#21516;&#30340;&#39044;&#26399;&#22238;&#25253;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#27604;&#36739;&#31574;&#30053;&#26102;&#30340;&#26377;&#25928;&#24615;&#65307;&#31532;&#20108;&#65292;&#39044;&#26399;&#22238;&#25253;&#24230;&#37327;&#27809;&#26377;&#30041;&#19979;&#36275;&#22815;&#30340;&#31354;&#38388;&#65292;&#20197;&#25551;&#36848;&#20998;&#24067;&#30340;&#20854;&#20182;&#20851;&#38190;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications in Reinforcement Learning (RL) usually have noise or stochasticity present in the environment. Beyond their impact on learning, these uncertainties lead the exact same policy to perform differently, i.e. yield different return, from one roll-out to another. Common evaluation procedures in RL summarise the consequent return distributions using solely the expected return, which does not account for the spread of the distribution. Our work defines this spread as the policy reproducibility: the ability of a policy to obtain similar performance when rolled out many times, a crucial property in some real-world applications. We highlight that existing procedures that only use the expected return are limited on two fronts: first an infinite number of return distributions with a wide range of performance-reproducibility trade-offs can have the same expected return, limiting its effectiveness when used for comparing policies; second, the expected return metric does not leave an
&lt;/p&gt;</description></item><item><title>&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#21644;&#20854;&#22312;ICPS&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.06261</link><description>&lt;p&gt;
&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Foundation Models for Prognostics and Health Management in Industrial Cyber-Physical Systems. (arXiv:2312.06261v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06261
&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#21644;&#20854;&#22312;ICPS&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;ICPS&#65289;&#25972;&#21512;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#36890;&#20449;&#25216;&#26415;&#21644;&#24037;&#31243;&#23398;&#30340;&#23398;&#31185;&#65292;&#25104;&#20026;&#24403;&#20195;&#21046;&#36896;&#19994;&#21644;&#24037;&#19994;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;ICPS&#22312;&#38271;&#26399;&#36816;&#33829;&#20013;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#35774;&#22791;&#25925;&#38556;&#12289;&#24615;&#33021;&#19979;&#38477;&#21644;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#32500;&#25252;&#21644;&#31649;&#29702;&#65292;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#22312;ICPS&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25925;&#38556;&#39044;&#27979;&#12289;&#20581;&#24247;&#30417;&#27979;&#21644;&#32500;&#25252;&#20915;&#31574;&#12290;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#22914;BERT&#21644;GPT&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;ChatGPT&#21017;&#26159;&#22312;&#36825;&#19968;&#30740;&#31350;&#33539;&#24335;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#25104;&#23601;&#65292;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#32771;&#34385;&#21040;&#25968;&#25454;&#37319;&#38598;&#25216;&#26415;&#21644;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;LFMs&#39044;&#35745;&#23558;&#22312;ICPS&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial Cyber-Physical Systems (ICPS) integrate the disciplines of computer science, communication technology, and engineering, and have emerged as integral components of contemporary manufacturing and industries. However, ICPS encounters various challenges in long-term operation, including equipment failures, performance degradation, and security threats. To achieve efficient maintenance and management, prognostics and health management (PHM) finds widespread application in ICPS for critical tasks, including failure prediction, health monitoring, and maintenance decision-making. The emergence of large-scale foundation models (LFMs) like BERT and GPT signifies a significant advancement in AI technology, and ChatGPT stands as a remarkable accomplishment within this research paradigm, harboring potential for General Artificial Intelligence. Considering the ongoing enhancement in data acquisition technology and data processing capability, LFMs are anticipated to assume a crucial role i
&lt;/p&gt;</description></item><item><title>GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.02317</link><description>&lt;p&gt;
GNN2R: &#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#29702;&#30001;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs. (arXiv:2312.02317v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02317
&lt;/p&gt;
&lt;p&gt;
GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#21482;&#25552;&#20379;&#26368;&#32456;&#30340;&#30830;&#23450;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#35299;&#37322;&#65292;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#38590;&#20197;&#29702;&#35299;&#21644;&#26597;&#30475;&#30340;KG&#23454;&#20307;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65288;GNN2R&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNN2R&#33021;&#22815;&#36890;&#36807;&#20165;&#26377;&#30340;&#38382;&#39064;-&#26368;&#32456;&#31572;&#26696;&#23545;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#32972;&#21518;&#30340;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#65292;&#19988;&#20165;&#38656;&#35201;&#36890;&#36807;&#24369;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;GNN2R&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current methods for multi-hop question answering (QA) over knowledge graphs (KGs) only provide final conclusive answers without explanations, such as a set of KG entities that is difficult for normal users to review and comprehend. This issue severely limits the application of KG-based QA in real-world scenarios. However, it is non-trivial to solve due to two challenges: First, annotations of reasoning chains of multi-hop questions, which could serve as supervision for explanation generation, are usually lacking. Second, it is difficult to maintain high efficiency when explicit KG triples need to be retrieved to generate explanations. In this paper, we propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to solve this issue. GNN2R can provide both final answers and reasoning subgraphs as a rationale behind final answers efficiently with only weak supervision that is available through question-final answer pairs. We extensively evaluated GNN2R with detailed a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16141</link><description>&lt;p&gt;
SNNs&#20013;&#22522;&#20110;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;&#20462;&#21098;&#26041;&#27861;&#65292;&#21463;&#21040;&#20851;&#38190;&#24615;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33410;&#33021;&#21644;&#26080;&#20056;&#27861;&#29305;&#24615;&#65292;SNNs&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28145;&#24230;SNNs&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#32473;&#27169;&#22411;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#32593;&#32476;&#20462;&#21098;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#35268;&#27169;&#26469;&#20943;&#23569;&#27169;&#22411;&#37096;&#32626;&#30340;&#30828;&#20214;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNN&#20462;&#21098;&#26041;&#27861;&#30001;&#20110;&#20462;&#21098;&#36845;&#20195;&#22686;&#21152;&#20102;SNNs&#30340;&#35757;&#32451;&#38590;&#24230;&#65292;&#23548;&#33268;&#20462;&#21098;&#25104;&#26412;&#39640;&#26114;&#19988;&#24615;&#33021;&#25439;&#22833;&#20005;&#37325;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#29992;&#20110;SNN&#20462;&#21098;&#30340;&#20877;&#29983;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#24182;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#20013;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20302;&#25104;&#26412;&#24230;&#37327;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#20462;&#21098;&#21518;&#23545;&#25152;&#20462;&#21098;&#32467;&#26500;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#20877;&#29983;&#37027;&#20123;&#20855;&#26377;&#36739;&#39640;&#20851;&#38190;&#24615;&#30340;&#32467;&#26500;&#65292;&#20197;&#33719;&#21462;&#20851;&#38190;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#24072;&#20195;&#29702;&#30340;&#25351;&#23548;&#65292;&#23398;&#29983;&#20195;&#29702;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#19987;&#38376;&#21270;&#20110;&#29305;&#23450;&#30446;&#26631;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2311.13373</link><description>&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#25945;&#24072;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents. (arXiv:2311.13373v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#24072;&#20195;&#29702;&#30340;&#25351;&#23548;&#65292;&#23398;&#29983;&#20195;&#29702;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#19987;&#38376;&#21270;&#20110;&#29305;&#23450;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#36807;&#25552;&#20379;&#39640;&#32423;&#25351;&#23548;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32570;&#20047;&#19987;&#38376;&#21270;&#22788;&#29702;&#29305;&#23450;&#30446;&#26631;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#23454;&#26102;&#21160;&#24577;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37096;&#32626;&#21040;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#35757;&#32451;&#19987;&#38376;&#21270;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#20195;&#29702;&#65292;&#20294;&#24448;&#24448;&#36973;&#21463;&#20302;&#37319;&#26679;&#25928;&#29575;&#21644;&#39640;&#25506;&#32034;&#25104;&#26412;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#25945;&#24072;&#20195;&#29702;&#30340;&#25351;&#23548;&#26469;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#12289;&#19987;&#38376;&#21270;&#30340;&#23398;&#29983;RL&#20195;&#29702;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#25945;&#24072;&#20195;&#29702;&#30340;&#20808;&#21069;&#30693;&#35782;&#34701;&#20837;&#23398;&#29983;&#20195;&#29702;&#20013;&#65292;&#23398;&#29983;&#20195;&#29702;&#33021;&#22815;&#20957;&#32858;LLM&#30340;&#30693;&#35782;&#21040;&#33258;&#24049;&#30340;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#20195;&#29702;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent studies have uncovered the potential of Large Language Models (LLMs) in addressing complex sequential decision-making tasks through the provision of high-level instructions. However, LLM-based agents lack specialization in tackling specific target problems, particularly in real-time dynamic environments. Additionally, deploying an LLM-based agent in practical scenarios can be both costly and time-consuming. On the other hand, reinforcement learning (RL) approaches train agents that specialize in the target task but often suffer from low sampling efficiency and high exploration costs. In this paper, we introduce a novel framework that addresses these challenges by training a smaller, specialized student RL agent using instructions from an LLM-based teacher agent. By incorporating the guidance from the teacher agent, the student agent can distill the prior knowledge of the LLM into its own model. Consequently, the student agent can be trained with significantly less data. Moreover
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23545;&#25239;&#25915;&#20987;&#21644;&#31995;&#32479;&#25552;&#31034;&#28431;&#27934;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;GPT-4V&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASP&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#25628;&#32034;&#28508;&#22312;&#30340;&#30772;&#35299;&#25552;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#20154;&#24037;&#20462;&#25913;&#65292;&#25104;&#21151;&#29575;&#25552;&#39640;&#21040;98.7%&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20462;&#25913;&#31995;&#32479;&#25552;&#31034;&#23545;&#35299;&#38145;GPT-4V&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09127</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#25239;&#25915;&#20987;&#21644;&#31995;&#32479;&#25552;&#31034;&#30340;&#30772;&#35299;GPT-4V
&lt;/p&gt;
&lt;p&gt;
Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts. (arXiv:2311.09127v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09127
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#25239;&#25915;&#20987;&#21644;&#31995;&#32479;&#25552;&#31034;&#28431;&#27934;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;GPT-4V&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASP&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#25628;&#32034;&#28508;&#22312;&#30340;&#30772;&#35299;&#25552;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#20154;&#24037;&#20462;&#25913;&#65292;&#25104;&#21151;&#29575;&#25552;&#39640;&#21040;98.7%&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20462;&#25913;&#31995;&#32479;&#25552;&#31034;&#23545;&#35299;&#38145;GPT-4V&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#20851;&#20110;&#30772;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#65292;&#36739;&#23569;&#20851;&#27880;&#27169;&#22411;API&#30340;&#28431;&#27934;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20197;&#19979;&#24037;&#20316;&#65306;1&#65289;&#25105;&#20204;&#21457;&#29616;&#20102;GPT-4V&#20013;&#30340;&#31995;&#32479;&#25552;&#31034;&#27844;&#28431;&#28431;&#27934;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#21462;&#20102;GPT-4V&#30340;&#20869;&#37096;&#31995;&#32479;&#25552;&#31034;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;MLLMs&#23384;&#22312;&#28508;&#22312;&#30340;&#21487;&#21033;&#29992;&#30340;&#23433;&#20840;&#39118;&#38505;&#65307;2&#65289;&#22522;&#20110;&#33719;&#21462;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SASP&#65288;&#36890;&#36807;&#31995;&#32479;&#25552;&#31034;&#30340;&#33258;&#23545;&#25239;&#25915;&#20987;&#65289;&#30340;&#26032;&#22411;MLLM&#30772;&#35299;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;GPT-4&#20316;&#20026;&#32418;&#38431;&#24037;&#20855;&#26469;&#38024;&#23545;&#33258;&#36523;&#36827;&#34892;&#25915;&#20987;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#31363;&#21462;&#30340;&#31995;&#32479;&#25552;&#31034;&#25628;&#32034;&#28508;&#22312;&#30340;&#30772;&#35299;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;GPT-4&#30340;&#20998;&#26512;&#28155;&#21152;&#20102;&#20154;&#24037;&#20462;&#25913;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#36827;&#19968;&#27493;&#25552;&#39640;&#21040;98.7&#65285;&#65307;3&#65289;&#25105;&#20204;&#35780;&#20272;&#20102;&#20462;&#25913;&#31995;&#32479;&#25552;&#31034;&#23545;&#35299;&#38145;GPT-4V&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities, especially in model API. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully extract the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.07989</link><description>&lt;p&gt;
&#32479;&#19968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#35270;&#35282;&#65306;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code. (arXiv:2311.07989v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#26368;&#36817;&#22312;&#20195;&#30721;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#22788;&#29702;&#27169;&#22411;&#20998;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#31995;&#21015;&#65289;&#21644;&#19987;&#38376;&#22312;&#20195;&#30721;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#20855;&#26377;&#19987;&#38376;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#36825;&#27491;&#26159;NLP&#20063;&#32463;&#21382;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#65292;&#22914;AST&#12289;CFG&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#35757;&#32451;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#36825;&#20221;&#32508;&#36848;&#20445;&#25345;&#24320;&#25918;&#65292;&#24182;&#22312;GitHub&#19978;&#26356;&#26032;&#65292;&#32593;&#22336;&#20026;https://github.com/codefuse-ai/Awesome-Code-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 170+ datasets, and 700+ related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#32534;&#31243;&#31574;&#30053;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24230;&#37327;&#37325;&#24314;&#31243;&#24207;&#19982;&#21407;&#22987;&#31243;&#24207;&#30340;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#20154;&#24037;&#21046;&#20316;&#30340;&#32534;&#31243;&#31574;&#30053;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.06979</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#32534;&#31243;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Interpretability of Programmatic Policies with Large Language Models. (arXiv:2311.06979v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#32534;&#31243;&#31574;&#30053;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24230;&#37327;&#37325;&#24314;&#31243;&#24207;&#19982;&#21407;&#22987;&#31243;&#24207;&#30340;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#20154;&#24037;&#21046;&#20316;&#30340;&#32534;&#31243;&#31574;&#30053;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21512;&#25104;&#32534;&#30721;&#31574;&#30053;&#30340;&#31243;&#24207;&#36890;&#24120;&#24102;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#25215;&#35834;&#65292;&#20294;&#20174;&#26410;&#36827;&#34892;&#36807;&#31995;&#32479;&#35780;&#20272;&#20197;&#35780;&#20272;&#36825;&#20123;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24456;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#26679;&#30340;&#35780;&#20272;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#32534;&#31243;&#31574;&#30053;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#24230;&#37327;&#65292;LLM&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#31243;&#24207;&#21644;&#20854;&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;&#30340;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;LLM&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#24418;&#25104;&#19968;&#20010;&#31243;&#24207;&#30340;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#23558;&#35813;&#35299;&#37322;&#36755;&#20837;&#21040;&#31532;&#20108;&#20010;LLM&#20013;&#65292;&#35813;LLM&#35797;&#22270;&#20174;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20013;&#37325;&#24314;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#34913;&#37327;&#20102;&#37325;&#24314;&#31243;&#24207;&#19982;&#21407;&#22987;&#31243;&#24207;&#20043;&#38388;&#30340;&#34892;&#20026;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21512;&#25104;&#21644;&#20154;&#24037;&#21046;&#20316;&#30340;&#29992;&#20110;&#29609;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#30340;&#32534;&#31243;&#31574;&#30053;&#36827;&#34892;&#39564;&#35777;&#65292;&#23558;&#36825;&#20123;&#32534;&#31243;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#24471;&#20998;&#19982;&#28151;&#28102;&#30340;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the synthesis of programs encoding policies often carries the promise of interpretability, systematic evaluations were never performed to assess the interpretability of these policies, likely because of the complexity of such an evaluation. In this paper, we introduce a novel metric that uses large-language models (LLM) to assess the interpretability of programmatic policies. For our metric, an LLM is given both a program and a description of its associated programming language. The LLM then formulates a natural language explanation of the program. This explanation is subsequently fed into a second LLM, which tries to reconstruct the program from the natural-language explanation. Our metric then measures the behavioral similarity between the reconstructed program and the original. We validate our approach with synthesized and human-crafted programmatic policies for playing a real-time strategy game, comparing the interpretability scores of these programmatic policies to obfusc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22343;&#34913;&#30340;&#36870;&#38382;&#39064;&#65292;&#22522;&#20110;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23548;&#39057;&#31526;&#21495;&#21644;&#26410;&#30693;&#30340;&#34928;&#33853;&#20449;&#36947;&#20197;&#21450;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#27700;&#24179;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.06101</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;MIMO&#22343;&#34913;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models. (arXiv:2311.06101v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22343;&#34913;&#30340;&#36870;&#38382;&#39064;&#65292;&#22522;&#20110;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23548;&#39057;&#31526;&#21495;&#21644;&#26410;&#30693;&#30340;&#34928;&#33853;&#20449;&#36947;&#20197;&#21450;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#27700;&#24179;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65289;&#20855;&#26377;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#33021;&#21147;&#12290;&#22312;ICL&#20013;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#21644;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#20960;&#20010;&#31034;&#20363;&#30452;&#25509;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#37327;&#65292;&#23545;&#26032;&#36755;&#20837;&#36827;&#34892;&#20915;&#31574;&#12290;&#26080;&#38656;&#26174;&#24335;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#35843;&#25972;&#20915;&#31574;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#24418;&#24335;&#65292;&#21487;&#20197;&#35266;&#23519;&#20960;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;ICL&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;ICL&#26469;&#35299;&#20915;&#22522;&#20110;&#23548;&#39057;&#31526;&#21495;&#19978;&#19979;&#25991;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#22343;&#34913;&#30340;&#36870;&#38382;&#39064;&#12290;&#19968;&#20010;&#20219;&#21153;&#30001;&#26410;&#30693;&#30340;&#34928;&#33853;&#20449;&#36947;&#21644;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#27700;&#24179;&#23450;&#20041;&#65292;&#21487;&#33021;&#26159;&#24050;&#30693;&#30340;&#12290;&#20026;&#20102;&#31361;&#26174;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#28508;&#21147;&#65292;&#25105;&#20204;&#20801;&#35768;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#23384;&#22312;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained sequence models, such as transformer-based architectures, have been recently shown to have the capacity to carry out in-context learning (ICL). In ICL, a decision on a new input is made via a direct mapping of the input and of a few examples from the given task, serving as the task's context, to the output variable. No explicit updates of the model parameters are needed to tailor the decision to a new task. Pre-training, which amounts to a form of meta-learning, is based on the observation of examples from several related tasks. Prior work has shown ICL capabilities for linear regression. In this study, we leverage ICL to address the inverse problem of multiple-input and multiple-output (MIMO) equalization based on a context given by pilot symbols. A task is defined by the unknown fading channel and by the signal-to-noise ratio (SNR) level, which may be known. To highlight the practical potential of the approach, we allow the presence of quantization of the received s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.20381</link><description>&lt;p&gt;
GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#19981;&#21516;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#39318;&#20010;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;&#22522;&#20934;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#32473;&#20986;&#32467;&#26500;&#33391;&#22909;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4V&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#25581;&#31034;&#20102;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;(&#22914;CIDEr)&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#21307;&#23398;VQA&#39046;&#22495;&#65292;GPT-4V&#22312;&#21306;&#20998;&#38382;&#39064;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#65292;&#20294;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#19981;&#21450;&#29616;&#26377;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#24120;&#35268;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#20998;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#21628;&#21505;&#24320;&#21457;&#26356;&#22909;&#30340;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15823</link><description>&lt;p&gt;
Rosetta Stone&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;&#65306;&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#30340;&#36291;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35789;&#20856;&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#23450;&#20041;&#12289;&#21547;&#20041;&#25110;&#25551;&#36848;&#26469;&#21457;&#29616;&#19968;&#20010;&#35789;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21487;&#20197;&#24110;&#21161;&#25484;&#25569;&#19968;&#20010;&#35789;&#30340;&#25551;&#36848;&#32780;&#19981;&#30693;&#20854;&#36523;&#20221;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#24182;&#20351;&#23547;&#27714;&#31934;&#30830;&#26415;&#35821;&#30340;&#20889;&#20316;&#32773;&#21463;&#30410;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#28085;&#30422;&#34987;&#31216;&#20026;&#8220;&#33292;&#23574;&#19978;&#30340;&#35789;&#8221;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#21453;&#21521;&#35789;&#20856;&#20849;&#20139;&#20219;&#21153;&#20013;&#33719;&#32988;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#37325;&#28857;&#26159;&#20174;&#20276;&#38543;&#30340;&#25551;&#36848;&#20013;&#25512;&#23548;&#20986;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#19968;&#20010;&#38463;&#25289;&#20271;&#23450;&#20041;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#21017;&#20351;&#29992;&#19968;&#20010;&#33521;&#25991;&#23450;&#20041;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#32463;&#36807;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#12290;&#26368;&#32456;&#34920;&#31034;&#26159;&#36890;&#36807;&#23545;&#27599;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.12817</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;&#28857;&#20113;&#20998;&#21106;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#38169;Transformer&#27169;&#22411;&#65288;MIT&#65289;&#65292;&#29992;&#20110;&#32771;&#34385;2D&#21644;3D&#25968;&#25454;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#22312;&#28857;&#20113;&#20998;&#21106;&#20013;&#20114;&#34917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;2D&#27880;&#37322;&#26469;&#23454;&#29616;2D-3D&#20449;&#24687;&#34701;&#21512;&#12290;&#37492;&#20110;&#28857;&#20113;&#30340;&#39640;&#27880;&#37322;&#25104;&#26412;&#65292;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26377;&#25928;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#38656;&#27714;&#38750;&#24120;&#36843;&#20999;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#22330;&#26223;&#32423;&#31867;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#35745;&#31639;3D&#28857;&#20113;&#21644;2D&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#12290;&#35299;&#30721;&#22120;&#23454;&#29616;&#20132;&#38169;&#30340;2D-3D&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#24182;&#36827;&#34892;&#38544;&#24335;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#23618;&#20013;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#26159;&#20114;&#34917;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#22120;&#36827;&#34892;&#32463;&#39564;&#35777;&#25454;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#24555;&#36895;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#31181;&#28151;&#21512;&#20102;&#20256;&#32479;&#26041;&#26696;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.11036</link><description>&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#65306;&#32463;&#39564;&#35777;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Radio Map Estimation in the Real-World: Empirical Validation and Analysis. (arXiv:2310.11036v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#22120;&#36827;&#34892;&#32463;&#39564;&#35777;&#25454;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#24555;&#36895;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#31181;&#28151;&#21512;&#20102;&#20256;&#32479;&#26041;&#26696;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#22320;&#22270;&#22312;&#22320;&#29702;&#21306;&#22495;&#30340;&#27599;&#20010;&#28857;&#19978;&#37327;&#21270;&#20102;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25110;&#20854;&#20182;&#26080;&#32447;&#30005;&#39057;&#29575;&#29615;&#22659;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#22320;&#22270;&#22312;&#26080;&#32447;&#32593;&#32476;&#35268;&#21010;&#12289;&#39057;&#35889;&#31649;&#29702;&#21644;&#36890;&#20449;&#31995;&#32479;&#20248;&#21270;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#29616;&#26377;&#30340;&#22823;&#37327;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#22120;&#30340;&#32463;&#39564;&#35777;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20351;&#29992;&#33258;&#20027;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#23545;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#20195;&#34920;&#24615;&#23376;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#65292;&#24191;&#27867;&#30740;&#31350;&#20102;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#24555;&#36895;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#20272;&#35745;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#30456;&#23545;&#20256;&#32479;&#26041;&#26696;&#25552;&#20379;&#23454;&#36136;&#24615;&#20248;&#21183;&#12290;&#19968;&#31181;&#28151;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#20272;&#35745;&#22120;&#30340;&#26032;&#31639;&#27861;&#34987;&#21457;&#29616;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio maps quantify received signal strength or other magnitudes of the radio frequency environment at every point of a geographical region. These maps play a vital role in a large number of applications such as wireless network planning, spectrum management, and optimization of communication systems. However, empirical validation of the large number of existing radio map estimators is highly limited. To fill this gap, a large data set of measurements has been collected with an autonomous unmanned aerial vehicle (UAV) and a representative subset of these estimators were evaluated on this data. The performance-complexity trade-off and the impact of fast fading are extensively investigated. Although sophisticated estimators based on deep neural networks (DNNs) exhibit the best performance, they are seen to require large volumes of training data to offer a substantial advantage relative to more traditional schemes. A novel algorithm that blends both kinds of estimators is seen to enjoy th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35299;&#26512;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#34920;&#31034;&#30340;&#26500;&#25104;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#35299;&#37322;&#20102;&#20854;&#21508;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#29702;&#35299;&#27880;&#24847;&#21147;&#22836;&#21644;&#22270;&#20687;&#22359;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#22797;&#21644;&#25913;&#36827;&#65292;&#21253;&#25324;&#28040;&#38500;&#35823;&#29305;&#24449;&#21644;&#26500;&#24314;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.05916</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#35299;&#35299;&#37322;CLIP&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpreting CLIP's Image Representation via Text-Based Decomposition. (arXiv:2310.05916v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35299;&#26512;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#34920;&#31034;&#30340;&#26500;&#25104;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#35299;&#37322;&#20102;&#20854;&#21508;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#29702;&#35299;&#27880;&#24847;&#21147;&#22836;&#21644;&#22270;&#20687;&#22359;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#22797;&#21644;&#25913;&#36827;&#65292;&#21253;&#25324;&#28040;&#38500;&#35823;&#29305;&#24449;&#21644;&#26500;&#24314;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20010;&#21035;&#27169;&#22411;&#32452;&#20214;&#23545;&#26368;&#32456;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#22270;&#20687;&#34920;&#31034;&#20998;&#35299;&#20026;&#21508;&#20010;&#22270;&#20687;&#22359;&#12289;&#27169;&#22411;&#23618;&#21644;&#27880;&#24847;&#21147;&#22836;&#30340;&#27714;&#21644;&#65292;&#24182;&#20351;&#29992;CLIP&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#35299;&#37322;&#36825;&#20123;&#27714;&#21644;&#39033;&#12290;&#36890;&#36807;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#23547;&#25214;&#33021;&#22815;&#36328;&#36234;&#36755;&#20986;&#31354;&#38388;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#34920;&#24449;&#27599;&#20010;&#22836;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20986;&#35768;&#22810;&#22836;&#30340;&#29305;&#23450;&#23646;&#24615;&#35282;&#33394;&#65288;&#20363;&#22914;&#20301;&#32622;&#25110;&#24418;&#29366;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#35299;&#37322;&#22270;&#20687;&#22359;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;CLIP&#20013;&#30340;&#32039;&#23494;&#31354;&#38388;&#23450;&#20301;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29702;&#35299;&#28040;&#38500;&#20102;CLIP&#20013;&#30340;&#35823;&#29305;&#24449;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#25193;&#23637;&#30340;&#23545;Transformer&#27169;&#22411;&#30340;&#29702;&#35299;&#26159;&#21487;&#23454;&#29616;&#30340;&#65292;&#24182;&#21487;&#29992;&#20110;&#20462;&#22797;&#21644;&#25913;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;</title><link>http://arxiv.org/abs/2310.02255</link><description>&lt;p&gt;
MathVista: &#29992;GPT-4V&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#25968;&#23398;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#25361;&#25112;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;6141&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#26377;28&#20010;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;3&#20010;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65288;&#21363;IQTest&#12289;FunctionQA&#21644;PaperQA&#65289;&#12290;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#31934;&#32454;&#30340;&#12289;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#32452;&#21512;&#25512;&#29702;&#65292;&#36825;&#20123;&#37117;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;MathVista&#65292;&#25105;&#20204;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#20026;49.9%&#65292;&#26126;&#26174;&#20248;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#65292;&#30456;&#24046;15.1%&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
&lt;/p&gt;</description></item><item><title>TWIZ-v2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#35805;&#21050;&#28608;&#30340;&#24043;&#24072;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#20197;&#20154;&#24615;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#12289;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#36827;&#34892;&#21050;&#28608;&#20197;&#21450;&#25913;&#36827;&#23545;&#26410;&#35265;&#36807;&#22330;&#26223;&#30340;&#20132;&#20114;&#40065;&#26834;&#24615;&#26469;&#24341;&#23548;&#29992;&#25143;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.02118</link><description>&lt;p&gt;
TWIZ-v2: &#22810;&#27169;&#24577;&#23545;&#35805;&#21050;&#28608;&#30340;&#24043;&#24072;
&lt;/p&gt;
&lt;p&gt;
TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus. (arXiv:2310.02118v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02118
&lt;/p&gt;
&lt;p&gt;
TWIZ-v2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#35805;&#21050;&#28608;&#30340;&#24043;&#24072;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#20197;&#20154;&#24615;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#12289;&#21033;&#29992;&#22810;&#31181;&#27169;&#24577;&#36827;&#34892;&#21050;&#28608;&#20197;&#21450;&#25913;&#36827;&#23545;&#26410;&#35265;&#36807;&#22330;&#26223;&#30340;&#20132;&#20114;&#40065;&#26834;&#24615;&#26469;&#24341;&#23548;&#29992;&#25143;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;Task Wizard&#22242;&#38431;TWIZ&#22312;Alexa Prize TaskBot Challenge 2022&#20013;&#30340;&#24895;&#26223;&#12289;&#25361;&#25112;&#21644;&#31185;&#23398;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#24895;&#26223;&#26159;&#26500;&#24314;TWIZ&#26426;&#22120;&#20154;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#12289;&#22810;&#27169;&#24577;&#12289;&#30693;&#35782;&#20016;&#23500;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#21161;&#25163;&#65292;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21162;&#21147;&#38598;&#20013;&#22312;&#19977;&#20010;&#20027;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#19978;&#65306;&#65288;1&#65289;&#20197;&#20154;&#24615;&#21270;&#30340;&#23545;&#35805;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#65307;&#65288;2&#65289;&#21033;&#29992;&#22768;&#38899;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#21508;&#31181;&#27169;&#24577;&#36827;&#34892;&#22810;&#27169;&#24577;&#21050;&#28608;&#65307;&#65288;3&#65289;&#38646;-shot&#23545;&#35805;&#27969;&#31243;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#35265;&#36807;&#22330;&#26223;&#30340;&#20132;&#20114;&#30340;&#40065;&#26834;&#24615;&#12290;TWIZ&#26159;&#19968;&#20010;&#33021;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#30340;&#21161;&#25163;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#21151;&#33021;&#65292;&#22914;&#21019;&#24847;&#28921;&#39274;&#12289;&#36890;&#36807;&#22768;&#38899;&#23548;&#33322;&#35270;&#39057;&#21644;&#24378;&#22823;&#30340;TWIZ-LLM&#65292;&#19968;&#20010;&#29992;&#20110;&#22797;&#26434;&#25163;&#21160;&#20219;&#21153;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#35780;&#32423;&#21644;&#21453;&#39304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
In this report, we describe the vision, challenges, and scientific contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal, knowledgeable, and engaging assistant that can guide users towards the successful completion of complex manual tasks. To achieve this, we focus our efforts on three main research questions: (1) Humanly-Shaped Conversations, by providing information in a knowledgeable way; (2) Multimodal Stimulus, making use of various modalities including voice, images, and videos; and (3) Zero-shot Conversational Flows, to improve the robustness of the interaction to unseen scenarios. TWIZ is an assistant capable of supporting a wide range of tasks, with several innovative features such as creative cooking, video navigation through voice, and the robust TWIZ-LLM, a Large Language Model trained for dialoguing about complex manual tasks. Given ratings and feedback provided by users, we observ
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FashionFlow&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#38745;&#24577;&#22270;&#20687;&#29983;&#25104;&#30701;&#35270;&#39057;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#24182;&#36830;&#25509;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#32452;&#20214;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20266;3D&#21367;&#31215;&#23618;&#39640;&#25928;&#29983;&#25104;&#35270;&#39057;&#65292;&#24182;&#21033;&#29992;VAE&#21644;CLIP&#32534;&#30721;&#22120;&#25429;&#25417;&#20851;&#38190;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25104;&#21151;&#21512;&#25104;&#26102;&#23578;&#35270;&#39057;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#23637;&#31034;&#26381;&#35013;&#30340;&#21512;&#36523;&#24230;&#21644;&#22806;&#35266;&#65292;&#20026;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#36141;&#29289;&#20307;&#39564;&#25552;&#20379;&#25913;&#36827;&#21644;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00106</link><description>&lt;p&gt;
FashionFlow: &#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#38745;&#24577;&#22270;&#20687;&#29983;&#25104;&#21160;&#24577;&#26102;&#23578;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery. (arXiv:2310.00106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FashionFlow&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#38745;&#24577;&#22270;&#20687;&#29983;&#25104;&#30701;&#35270;&#39057;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#24182;&#36830;&#25509;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#32452;&#20214;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20266;3D&#21367;&#31215;&#23618;&#39640;&#25928;&#29983;&#25104;&#35270;&#39057;&#65292;&#24182;&#21033;&#29992;VAE&#21644;CLIP&#32534;&#30721;&#22120;&#25429;&#25417;&#20851;&#38190;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25104;&#21151;&#21512;&#25104;&#26102;&#23578;&#35270;&#39057;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#23637;&#31034;&#26381;&#35013;&#30340;&#21512;&#36523;&#24230;&#21644;&#22806;&#35266;&#65292;&#20026;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#36141;&#29289;&#20307;&#39564;&#25552;&#20379;&#25913;&#36827;&#21644;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#31216;&#20026;FashionFlow&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#38745;&#24577;&#22270;&#20687;&#21019;&#24314;&#30701;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24320;&#21457;&#24182;&#36830;&#25509;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#20247;&#19981;&#21516;&#12290;&#36825;&#20123;&#32452;&#20214;&#21253;&#25324;&#20351;&#29992;&#20266;3D&#21367;&#31215;&#23618;&#39640;&#25928;&#29983;&#25104;&#35270;&#39057;&#12290;VAE&#21644;CLIP&#32534;&#30721;&#22120;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#25429;&#25417;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20197;&#24433;&#21709;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#25104;&#21151;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#35282;&#24230;&#30340;&#27169;&#29305;&#19968;&#36793;&#25670;&#23039;&#21183;&#65292;&#23637;&#31034;&#26381;&#35013;&#30340;&#21512;&#36523;&#24230;&#21644;&#22806;&#35266;&#30340;&#26102;&#23578;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20110;&#25913;&#36827;&#21644;&#25552;&#21319;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#36141;&#29289;&#20307;&#39564;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study introduces a new image-to-video generator called FashionFlow. By utilising a diffusion model, we are able to create short videos from still images. Our approach involves developing and connecting relevant components with the diffusion model, which sets our work apart. The components include the use of pseudo-3D convolutional layers to generate videos efficiently. VAE and CLIP encoders capture vital characteristics from still images to influence the diffusion model. Our research demonstrates a successful synthesis of fashion videos featuring models posing from various angles, showcasing the fit and appearance of the garment. Our findings hold great promise for improving and enhancing the shopping experience for the online fashion industry.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2309.13365</link><description>&lt;p&gt;
&#20915;&#31574;&#26641;&#31574;&#30053;&#22312;IBMDP&#20013;&#30340;Actor-Critic&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65288;arXiv:2309.13365v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#23433;&#20840;&#26816;&#26597;&#26469;&#24314;&#31435;&#23545;&#36825;&#20123;AI&#30340;&#20449;&#20219;&#12290;&#29305;&#21035;&#26159;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#25552;&#20379;&#20102;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#35270;&#35282;&#65292;&#24182;&#36879;&#26126;&#22320;&#25581;&#31034;&#20102;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#23545;&#20110;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20915;&#31574;&#26641;&#36807;&#22823;&#65292;&#21487;&#35299;&#37322;&#24615;&#23601;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;RL&#25506;&#32034;DT&#30340;&#31354;&#38388;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22686;&#21152;&#21160;&#20316;&#26469;&#25910;&#38598;&#20851;&#20110;&#38544;&#34255;&#36755;&#20837;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#36866;&#24403;&#22320;&#23545;&#36825;&#20123;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#22312;&#26641;&#30340;&#22823;&#23567;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#26435;&#34913;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#38656;&#35201;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36825;&#19968;&#31867;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of AI models allows for user safety checks to build trust in such AIs. In particular, Decision Trees (DTs) provide a global look at the learned model and transparently reveal which features of the input are critical for making a decision. However, interpretability is hindered if the DT is too large. To learn compact trees, a recent Reinforcement Learning (RL) framework has been proposed to explore the space of DTs using deep RL. This framework augments a decision problem (e.g. a supervised classification task) with additional actions that gather information about the features of an otherwise hidden input. By appropriately penalizing these actions, the agent learns to optimally trade-off size and performance of DTs. In practice, a reactive policy for a partially observable Markov decision process (MDP) needs to be learned, which is still an open problem. We show in this paper that deep RL can fail even on simple toy tasks of this class. However, when the underlying deci
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;</title><link>http://arxiv.org/abs/2309.12247</link><description>&lt;p&gt;
&#22351;&#35282;&#33394;&#22909;&#39038;&#38382;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12247
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20551;&#26032;&#38395;&#38656;&#35201;&#23545;&#22810;&#26679;&#32447;&#32034;&#26377;&#25935;&#38160;&#30340;&#24863;&#30693;&#21644;&#23545;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#23545;&#20110;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#22120;&#26469;&#35828;&#65292;&#30001;&#20110;&#20854;&#30693;&#35782;&#21644;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#36825;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20197;&#21450;&#22914;&#20309;&#24110;&#21161;&#20551;&#26032;&#38395;&#26816;&#27979;&#20173;&#28982;&#26410;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20687;GPT 3.5&#36825;&#26679;&#30340;&#22797;&#26434;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#29702;&#24819;&#30340;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#28982;&#19981;&#22914;&#22522;&#30784;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;fine-tuned BERT&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#38543;&#21518;&#30340;&#20998;&#26512;&#23558;&#36825;&#31181;&#24046;&#36317;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#27491;&#30830;&#36873;&#25321;&#24182;&#25972;&#21512;&#35777;&#25454;&#20197;&#24471;&#20986;&#32467;&#35770;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good a
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11680</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#22270;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20445;&#30041;&#23545;&#25968;&#25454;&#30340;&#29420;&#21344;&#25511;&#21046;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#19987;&#26377;&#25968;&#25454;&#21019;&#24314;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36817;&#26399;&#25552;&#20986;&#30340;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#23398;&#20064;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#20204;&#23398;&#20250;&#25429;&#25417;&#24213;&#23618;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;FL&#26694;&#26550;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#20840;&#23616;&#30340;NGM&#27169;&#22411;&#65292;&#20174;&#26412;&#22320;NGM&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#24179;&#22343;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;FedNGMs&#36991;&#20813;&#20102;&#31070;&#32463;&#20803;&#21305;&#37197;&#26694;&#26550;&#65288;&#22914;&#32852;&#37030;&#21305;&#37197;&#24179;&#22343;&#65289;&#20013;&#27169;&#22411;&#21442;&#25968;&#29190;&#28856;&#30340;&#32570;&#28857;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;&#27169;&#22411;&#22823;&#23567;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2309.07525</link><description>&lt;p&gt;
SingFake&#65306;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SingFake: Singing Voice Deepfake Detection. (arXiv:2309.07525v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#21809;&#27468;&#22768;&#38899;&#30340;&#20852;&#36215;&#32473;&#33402;&#26415;&#23478;&#21644;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#20102;&#26410;&#32463;&#25480;&#26435;&#30340;&#22768;&#38899;&#20351;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#19982;&#21512;&#25104;&#35821;&#38899;&#19981;&#21516;&#65292;&#21512;&#25104;&#21809;&#27468;&#22768;&#38899;&#36890;&#24120;&#22312;&#21253;&#21547;&#24378;&#28872;&#32972;&#26223;&#38899;&#20048;&#30340;&#27468;&#26354;&#20013;&#21457;&#24067;&#65292;&#36825;&#21487;&#33021;&#25513;&#30422;&#20102;&#21512;&#25104;&#36896;&#25104;&#30340;&#29781;&#30133;&#12290;&#27492;&#22806;&#65292;&#21809;&#27468;&#22768;&#38899;&#19982;&#35821;&#38899;&#35805;&#35821;&#20855;&#26377;&#19981;&#21516;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#23646;&#24615;&#20351;&#24471;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#25104;&#20026;&#19968;&#20010;&#30456;&#20851;&#20294;&#26174;&#30528;&#19981;&#21516;&#20110;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21809;&#27468;&#22768;&#38899;Deepfake&#26816;&#27979;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;SingFake&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#37326;&#22806;&#31934;&#36873;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;40&#20301;&#27468;&#25163;&#29992;&#20116;&#31181;&#35821;&#35328;&#28436;&#21809;&#30340;28.93&#23567;&#26102;&#30495;&#23454;&#38899;&#39057;&#21644;29.40&#23567;&#26102;&#30340;Deepfake&#38899;&#39057;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35757;&#32451;/&#39564;&#35777;/&#27979;&#35797;&#30340;&#21010;&#20998;&#65292;&#20854;&#20013;&#27979;&#35797;&#38598;&#21253;&#25324;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;SingFake&#35780;&#20272;&#20102;&#22235;&#20010;&#22312;&#35821;&#38899;&#35805;&#35821;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#23545;&#25239;&#31995;&#32479;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#31995;&#32479;&#22312;&#21809;&#27468;&#22768;&#38899;&#19978;&#30340;&#34920;&#29616;&#19981;&#21516;&#20110;&#22312;&#35821;&#38899;&#35805;&#35821;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of singing voice synthesis presents critical challenges to artists and industry stakeholders over unauthorized voice usage. Unlike synthesized speech, synthesized singing voices are typically released in songs containing strong background music that may hide synthesis artifacts. Additionally, singing voices present different acoustic and linguistic characteristics from speech utterances. These unique properties make singing voice deepfake detection a relevant but significantly different problem from synthetic speech detection. In this work, we propose the singing voice deepfake detection task. We first present SingFake, the first curated in-the-wild dataset consisting of 28.93 hours of bonafide and 29.40 hours of deepfake song clips in five languages from 40 singers. We provide a train/val/test split where the test sets include various scenarios. We then use SingFake to evaluate four state-of-the-art speech countermeasure systems trained on speech utterances. We find these sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06629</link><description>&lt;p&gt;
&#20316;&#20026;&#26377;&#25928;&#25277;&#35937;&#30340;&#24402;&#32435;&#20559;&#22909;&#30340;&#20851;&#31995;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
The Relational Bottleneck as an Inductive Bias for Efficient Abstraction. (arXiv:2309.06629v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35299;&#37322;&#22914;&#20309;&#20174;&#26377;&#38480;&#32463;&#39564;&#20013;&#33719;&#21462;&#25277;&#35937;&#27010;&#24565;&#12290;&#36825;&#19968;&#21162;&#21147;&#24120;&#24120;&#34987;&#25551;&#36848;&#20026;&#32463;&#39564;&#20027;&#20041;&#21644;&#22825;&#36171;&#20027;&#20041;&#26041;&#27861;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#26368;&#36817;&#20027;&#35201;&#20307;&#29616;&#22312;&#26377;&#20851;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#35748;&#30693;&#27169;&#22411;&#30340;&#20105;&#35770;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#24037;&#20316;&#32447;&#36335;&#65292;&#35813;&#32447;&#36335;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#31995;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#35843;&#21644;&#26041;&#24335;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#24335;&#19979;&#35825;&#23548;&#20986;&#25277;&#35937;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#30340;&#20505;&#36873;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2309.05030</link><description>&lt;p&gt;
&#21435;&#27542;&#27665;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#65306;&#23041;&#33394;&#36798;&#23572;&#29595;&#12289;&#35770;&#35777;&#21644;&#33402;&#26415;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#27542;&#27665;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#23545;&#40784;&#65306;&#21363;&#22522;&#20110;&#32454;&#33268;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#19982;&#26399;&#26395;&#20540;&#19968;&#33268;&#12290;&#38500;&#20102;&#20854;&#20182;&#23454;&#36341;&#65292;&#27542;&#27665;&#20027;&#20041;&#36824;&#26377;&#19968;&#37096;&#20998;&#26159;&#25913;&#21464;&#34987;&#27542;&#27665;&#27665;&#26063;&#30340;&#20449;&#20208;&#21644;&#20215;&#20540;&#35266;&#30340;&#21382;&#21490;&#65307;&#32780;&#24403;&#21069;&#30340;LLM&#23545;&#40784;&#23454;&#36341;&#27491;&#26159;&#36825;&#19968;&#21382;&#21490;&#30340;&#22797;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#19977;&#20010;&#25552;&#35758;&#23545;AI&#23545;&#40784;&#36827;&#34892;&#21435;&#27542;&#27665;&#21270;&#65306;&#65288;a&#65289;&#23558;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20174;&#35199;&#26041;&#21746;&#23398;&#36716;&#21464;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#65288;b&#65289;&#22312;&#23545;&#40784;&#25216;&#26415;&#20013;&#20801;&#35768;&#35770;&#35777;&#21644;&#22810;&#20803;&#20027;&#20041;&#30340;&#20256;&#32479;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23558;&#20215;&#20540;&#30340;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#25110;&#21629;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01538</link><description>&lt;p&gt;
ChatRule&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning. (arXiv:2309.01538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35268;&#21017;&#23545;&#20110;&#21457;&#29616;&#20851;&#31995;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#21162;&#21147;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25366;&#25496;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#35268;&#21017;&#31354;&#38388;&#19978;&#25628;&#32034;&#35745;&#31639;&#23494;&#38598;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#20851;&#31995;&#30340;&#35821;&#20041;&#65292;&#32780;&#36825;&#23545;&#20110;&#25581;&#31034;&#36923;&#36753;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21644;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#26032;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ChatRule&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20197;&#22522;&#20110;LLM&#30340;&#35268;&#21017;&#29983;&#25104;&#22120;&#20026;&#21021;&#22987;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical rules are essential for uncovering the logical connections between relations, which could improve the reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from the computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13894</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices. (arXiv:2308.13894v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#31227;&#21160;&#26234;&#33021;&#30340;&#26684;&#23616;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#20110;&#23545;&#19979;&#28216;&#31227;&#21160;&#20219;&#21153;&#36827;&#34892;LLM&#30340;&#24494;&#35843;&#65292;&#36825;&#34987;&#31216;&#20026;FedLLM&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#30001;&#24222;&#22823;&#27169;&#22411;&#22823;&#23567;&#24341;&#36215;&#30340;&#32593;&#32476;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#19982;&#31227;&#21160;&#35774;&#22791;&#30340;&#25972;&#21512;&#26041;&#38754;&#24182;&#27809;&#26377;&#23454;&#38469;&#32531;&#35299;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#32531;&#24930;&#30340;&#27169;&#22411;&#25910;&#25947;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;FedLLM&#30340;&#25928;&#29575;&#12290;FwdLLM&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#35774;&#22791;&#25191;&#34892;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#12290;&#22240;&#27492;&#65292;FwdLLM&#20855;&#26377;&#26356;&#22909;&#30340;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#65288;&#36890;&#36807;&#31227;&#21160;NPUs&#21644;&#25193;&#22823;&#30340;&#21442;&#19982;&#35774;&#22791;&#25968;&#32452;&#65289;&#12290;FwdLLM&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#23637;&#24320;&#65306;&#65288;1&#65289;&#23558;&#26080;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#19982;p
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.  In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.13507</link><description>&lt;p&gt;
&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#26159;&#21542;&#22686;&#21152;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27807;&#36890;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13507
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25104;&#20026;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#36890;&#24120;&#20250;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#20943;&#23569;&#38656;&#27714;&#21644;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;LLMs&#20063;&#24212;&#35813;&#37319;&#29992;&#21516;&#26679;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#26368;&#32456;&#20195;&#30721;&#20043;&#21069;&#25552;&#20986;&#28145;&#20837;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20943;&#36731;&#20351;&#29992;LLMs&#36827;&#34892;&#32534;&#31243;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#24847;&#22270;&#35268;&#33539;&#19981;&#26126;&#30830;&#12289;&#35745;&#31639;&#24605;&#32500;&#19981;&#36275;&#21644;&#20195;&#30721;&#36136;&#37327;&#19981;&#29702;&#24819;&#12290;&#36825;&#21453;&#36807;&#26469;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#33258;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#26356;&#22909;&#30340;&#27807;&#36890;&#25216;&#24039;&#26469;&#22686;&#21152;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27807;&#36890;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#30340;&#27807;&#36890;&#22120;&#26469;&#35782;&#21035;&#39640;&#24230;&#19981;&#30830;&#23450;&#25110;&#20449;&#24515;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MC-CP&#30340;&#26032;&#22411;&#28151;&#21512;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;dropout&#26041;&#27861;&#19982;&#21512;&#35268;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33410;&#30465;&#36164;&#28304;&#21644;&#20135;&#29983;&#40065;&#26834;&#39044;&#27979;&#38598;/&#21306;&#38388;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;MC-CP&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25552;&#21319;</title><link>http://arxiv.org/abs/2308.09647</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#30340;&#33945;&#29305;&#21345;&#27931;&#39044;&#27979;&#23454;&#29616;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction. (arXiv:2308.09647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09647
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MC-CP&#30340;&#26032;&#22411;&#28151;&#21512;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;dropout&#26041;&#27861;&#19982;&#21512;&#35268;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33410;&#30465;&#36164;&#28304;&#21644;&#20135;&#29983;&#40065;&#26834;&#39044;&#27979;&#38598;/&#21306;&#38388;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;MC-CP&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#36816;&#34892;&#25552;&#20379;&#20445;&#35777;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#20272;&#35745;&#27599;&#20010;&#39044;&#27979;&#30340;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#32771;&#34385;&#38543;&#26426;&#24615;&#21644;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#24433;&#21709;&#26469;&#25351;&#23548;&#20915;&#31574;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#35201;&#20040;&#38750;&#24120;&#26114;&#36149;&#65292;&#35201;&#20040;&#20135;&#29983;&#20445;&#23432;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;UQ&#26041;&#27861;MC-CP&#65292;&#23427;&#23558;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#65288;MC&#65289;dropout&#26041;&#27861;&#19982;&#21512;&#35268;&#39044;&#27979;&#65288;CP&#65289;&#30456;&#32467;&#21512;&#12290;MC-CP&#22312;&#36816;&#34892;&#26102;&#33258;&#36866;&#24212;&#35843;&#33410;&#20256;&#32479;&#30340;MC dropout&#20197;&#33410;&#30465;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#24471;&#39044;&#27979;&#21487;&#20197;&#34987;CP&#20351;&#29992;&#65292;&#24471;&#21040;&#40065;&#26834;&#30340;&#39044;&#27979;&#38598;/&#21306;&#38388;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MC-CP&#30456;&#27604;MC dropout&#12289;RAPS&#21644;CQR&#31561;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21644;&#35821;&#20041;&#27807;&#36890;&#65288;SemCom&#65289;&#65292;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#21644;&#25928;&#26524;&#30340;&#20869;&#23481;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;AIGC&#25216;&#26415;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04942</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#35821;&#20041;&#27807;&#36890;&#23545;&#20110;&#26377;&#25928;&#30340;&#20869;&#23481;&#21019;&#36896;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Semantic Communications for Artificial Intelligence Generated Content (AIGC) Toward Effective Content Creation. (arXiv:2308.04942v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21644;&#35821;&#20041;&#27807;&#36890;&#65288;SemCom&#65289;&#65292;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#21644;&#25928;&#26524;&#30340;&#20869;&#23481;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;AIGC&#25216;&#26415;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26381;&#21153;&#22312;&#25968;&#23383;&#20869;&#23481;&#21019;&#36896;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;AIGC&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#26368;&#23567;&#36755;&#20837;&#30340;&#20869;&#23481;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#35821;&#20041;&#27807;&#36890;&#65288;SemCom&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;AIGC&#21644;SemCom&#30340;&#38598;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#35821;&#20041;&#23618;&#20043;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#20869;&#23481;&#29983;&#25104;&#23618;&#65292;&#28165;&#26224;&#22320;&#27010;&#36848;&#20102;AIGC&#21644;SemCom&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#21644;&#26377;&#25928;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;AIGC&#25216;&#26415;&#20316;&#20026;&#35821;&#20041;&#20449;&#24687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#26032;&#26694;&#26550;&#65292;&#32771;&#34385;&#21040;&#38024;&#23545;AIGC&#26381;&#21153;&#23450;&#21046;&#30340;&#35821;&#20041;&#25552;&#21462;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#20869;&#23481;&#12289;&#25152;&#38656;&#30340;&#36136;&#37327;&#21644;&#25152;&#21033;&#29992;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#65292;&#23545;&#20855;&#20307;&#26696;&#20363;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) Services have significant potential in digital content creation. The distinctive abilities of AIGC, such as content generation based on minimal input, hold huge potential, especially when integrating with semantic communication (SemCom). In this paper, a novel comprehensive conceptual model for the integration of AIGC and SemCom is developed. Particularly, a content generation level is introduced on top of the semantic level that provides a clear outline of how AIGC and SemCom interact with each other to produce meaningful and effective content. Moreover, a novel framework that employs AIGC technology is proposed as an encoder and decoder for semantic information, considering the joint optimization of semantic extraction and evaluation metrics tailored to AIGC services. The framework can adapt to different types of content generated, the required quality, and the semantic information utilized. By employing a Deep Q Network (DQN), a case 
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;LaCAM*&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#25361;&#25112;&#12290;&#25913;&#36827;&#25216;&#26415;&#30340;&#34701;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;LaCAM*&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#25512;&#21160;&#20102;MAPF&#31639;&#27861;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.04292</link><description>&lt;p&gt;
&#24037;&#31243;&#21270;LaCAM$: &#23454;&#29616;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding. (arXiv:2308.04292v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;LaCAM*&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#25361;&#25112;&#12290;&#25913;&#36827;&#25216;&#26415;&#30340;&#34701;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;LaCAM*&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#25512;&#21160;&#20102;MAPF&#31639;&#27861;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;LaCAM*&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#23454;&#26102;&#12289;&#22823;&#35268;&#27169;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#30340;&#25361;&#25112;&#12290;LaCAM*&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#65292;&#23427;&#20445;&#35777;&#23545;&#20110;&#32047;&#35745;&#36716;&#31227;&#25104;&#26412;&#33021;&#22815;&#26368;&#32456;&#25214;&#21040;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#23427;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#29575;&#65292;&#36229;&#36807;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;MAPF&#26041;&#27861;&#65292;&#20294;&#20854;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#36828;&#38750;&#26368;&#20248;&#65292;&#24182;&#19988;&#20854;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20123;&#25913;&#36827;&#25216;&#26415;&#65292;&#37096;&#20998;&#20511;&#37492;&#20102;&#20854;&#20182;MAPF&#26041;&#27861;&#30340;&#28789;&#24863;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#36825;&#20123;&#25216;&#26415;&#30340;&#34701;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;LaCAM*&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;MAPF&#31639;&#27861;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenges of real-time, large-scale, and near-optimal multi-agent pathfinding (MAPF) through enhancements to the recently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm that guarantees the eventual finding of optimal solutions for cumulative transition costs. While it has demonstrated remarkable planning success rates, surpassing various state-of-the-art MAPF methods, its initial solution quality is far from optimal, and its convergence speed to the optimum is slow. To overcome these limitations, this paper introduces several improvement techniques, partly drawing inspiration from other MAPF methods. We provide empirical evidence that the fusion of these techniques significantly improves the solution quality of LaCAM*, thus further pushing the boundaries of MAPF algorithms.
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36923;&#36753;&#32534;&#31243;&#35268;&#21017;&#20013;&#24341;&#29992;&#36807;&#21435;&#21644;&#29616;&#22312;&#65292;&#20445;&#35777;&#20102;&#36807;&#21435;&#19982;&#26410;&#26469;&#30340;&#29420;&#31435;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#34917;&#20840;&#21644;&#24490;&#29615;&#20844;&#24335;&#30340;&#23450;&#20041;&#65292;&#36890;&#36807;LTLf&#34920;&#36798;&#24335;&#25429;&#25417;&#20102;&#19968;&#32452;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;&#30340;&#26102;&#38388;&#31283;&#23450;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.12620</link><description>&lt;p&gt;
&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Past-present temporal programs over finite traces. (arXiv:2307.12620v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36923;&#36753;&#32534;&#31243;&#35268;&#21017;&#20013;&#24341;&#29992;&#36807;&#21435;&#21644;&#29616;&#22312;&#65292;&#20445;&#35777;&#20102;&#36807;&#21435;&#19982;&#26410;&#26469;&#30340;&#29420;&#31435;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#34917;&#20840;&#21644;&#24490;&#29615;&#20844;&#24335;&#30340;&#23450;&#20041;&#65292;&#36890;&#36807;LTLf&#34920;&#36798;&#24335;&#25429;&#25417;&#20102;&#19968;&#32452;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;&#30340;&#26102;&#38388;&#31283;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;&#20013;&#20351;&#29992;&#26469;&#33258;&#26102;&#38388;&#36923;&#36753;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#22914;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#65288;TELf&#65289;&#65292;&#21487;&#20197;&#20026;&#24314;&#27169;&#21160;&#24577;&#24212;&#29992;&#25552;&#20379;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25152;&#35859;&#30340;&#36807;&#21435;-&#29616;&#22312;&#21477;&#27861;&#23376;&#31867;&#65292;&#23427;&#30001;&#19968;&#32452;&#36923;&#36753;&#32534;&#31243;&#35268;&#21017;&#32452;&#25104;&#65292;&#20854;&#20307;&#24341;&#29992;&#36807;&#21435;&#65292;&#22836;&#24341;&#29992;&#29616;&#22312;&#12290;&#36825;&#31181;&#38480;&#21046;&#30830;&#20445;&#20102;&#36807;&#21435;&#19982;&#26410;&#26469;&#30340;&#29420;&#31435;&#24615;&#65292;&#22312;&#22823;&#22810;&#25968;&#21160;&#24577;&#39046;&#22495;&#20013;&#26159;&#25104;&#31435;&#30340;&#12290;&#25105;&#20204;&#23558;&#34917;&#20840;&#21644;&#24490;&#29615;&#20844;&#24335;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#36807;&#21435;-&#29616;&#22312;&#20844;&#24335;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#36890;&#36807;&#20351;&#29992;LTLf&#34920;&#36798;&#24335;&#25429;&#25417;&#19968;&#32452;&#36807;&#21435;-&#29616;&#22312;&#26102;&#38388;&#31243;&#24207;&#30340;&#26102;&#38388;&#31283;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensions of Answer Set Programming with language constructs from temporal logics, such as temporal equilibrium logic over finite traces (TELf), provide an expressive computational framework for modeling dynamic applications. In this paper, we study the so-called past-present syntactic subclass, which consists of a set of logic programming rules whose body references to the past and head to the present. Such restriction ensures that the past remains independent of the future, which is the case in most dynamic domains. We extend the definitions of completion and loop formulas to the case of past-present formulas, which allows capturing the temporal stable models of a set of past-present temporal programs by means of an LTLf expression.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06564</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#35302;&#21457;&#24178;&#39044;&#26469;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22686;&#21152;&#27491;&#38754;&#26696;&#20363;&#32467;&#26524;&#30340;&#27010;&#29575;&#12290;&#36825;&#20123;&#24178;&#39044;&#26159;&#26681;&#25454;&#24178;&#39044;&#31574;&#30053;&#35302;&#21457;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#24178;&#39044;&#31574;&#30053;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#20551;&#35774;&#21487;&#29992;&#20110;&#25191;&#34892;&#24178;&#39044;&#30340;&#36164;&#28304;&#25968;&#37327;&#26159;&#26080;&#38480;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#36164;&#28304;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#22256;&#22659;&#26159;&#22522;&#20110;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#26469;&#35302;&#21457;&#24178;&#39044;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#23545;&#24178;&#39044;&#30340;&#24517;&#35201;&#24615;&#25110;&#25928;&#26524;&#23384;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23558;&#26377;&#38480;&#30340;&#36164;&#28304;&#29992;&#20110;&#24178;&#39044;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.02869</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36951;&#25022;&#24179;&#34913;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02869
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#36873;&#25321;&#65292;&#20854;&#20013;&#20803;&#23398;&#20064;&#22120;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#25512;&#33616;&#30340;&#31574;&#30053;&#21160;&#24577;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#26469;&#25191;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#19982;&#27492;&#30456;&#20851;&#30340;&#26368;&#36817;&#25991;&#29486;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#27809;&#26377;&#20551;&#35774;&#20219;&#20309;&#20851;&#20110;&#22522;&#26412;&#23398;&#20064;&#22120;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#20505;&#36873;&#36951;&#25022;&#20445;&#35777;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25581;&#31034;&#36825;&#20123;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#20803;&#23398;&#20064;&#22120;&#33021;&#22815;&#21033;&#29992;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#23454;&#38469;&#36951;&#25022;&#65288;&#32780;&#19981;&#26159;&#26399;&#26395;&#36951;&#25022;&#65289;&#65292;&#24182;&#25361;&#36873;&#20986;&#26368;&#20339;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#25805;&#20316;&#26356;&#20026;&#38596;&#24515;&#21187;&#21187;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#24182;&#19988;&#38500;&#20102;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#35777;&#26126;&#27169;&#22411;&#36873;&#25321;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22788;&#29702;&#23454;&#38469;&#36951;&#25022;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12997</link><description>&lt;p&gt;
EXACT&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20840;&#38754;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EXACT: Extensive Attack for Split Learning. (arXiv:2305.12997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35757;&#32451;&#21644;&#37096;&#32626;&#21033;&#29992;&#31169;&#20154;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20351;&#25105;&#20204;&#22312;&#25512;&#26029;&#26399;&#38388;&#23436;&#20840;&#36991;&#20813;&#19982;&#31532;&#19977;&#26041;&#26381;&#21153;&#22120;&#20849;&#20139;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19982;&#26381;&#21153;&#22120;&#31471;&#30456;&#27604;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#36890;&#24120;&#36739;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#19968;&#23567;&#32452;&#35774;&#22791;&#29305;&#24449;&#19988;&#38656;&#35201;&#36275;&#22815;&#23567;&#25165;&#33021;&#22312;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#12290;&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#23558;&#19968;&#20010;&#22823;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#65292;&#22823;&#37096;&#20998;&#20301;&#20110;&#26381;&#21153;&#22120;&#31471;&#65292;&#23567;&#37096;&#20998;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#26088;&#22312;&#25972;&#21512;&#31169;&#26377;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#38656;&#35201;&#22312;&#20998;&#30028;&#22788;&#20132;&#25442;&#26799;&#24230;&#65292;&#36825;&#21487;&#33021;&#32534;&#30721;&#31169;&#26377;&#29305;&#24449;&#25110;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EXACT&#65288;Extensive Attack for Split Learning&#65289;&#30340;&#26032;&#39062;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#30340;&#22122;&#22768;&#23454;&#29616;&#23433;&#20840;&#30340;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving machine learning (PPML) can help us train and deploy models that utilize private information. In particular, on-device Machine Learning allows us to completely avoid sharing information with a third-party server during inference. However, on-device models are typically less accurate when compared to the server counterparts due to the fact that (1) they typically only rely on a small set of on-device features and (2) they need to be small enough to run efficiently on end-user devices. Split Learning (SL) is a promising approach that can overcome these limitations. In SL, a large machine learning model is divided into two parts, with the bigger part residing on the server-side and a smaller part executing on-device, aiming to incorporate the private features. However, end-to-end training of such models requires exchanging gradients at the cut layer, which might encode private features or labels. In this paper, we provide insights into potential privacy risks associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04073</link><description>&lt;p&gt;
&#29992;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Explaining RL Decisions with Trajectories. (arXiv:2305.04073v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26159;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#20013;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#36825;&#20123;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#29992;&#32534;&#30721;&#30340;&#26041;&#24335;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature, the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set. Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video gam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14317</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20195;&#30721;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#32534;&#31243;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#24320;&#21457;&#35780;&#20272;&#25351;&#26631;&#20197;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#21464;&#24471;&#22256;&#38590;&#12290;&#20197;&#35789;&#27719;&#21305;&#37197;&#20026;&#22522;&#30784;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;BLEU&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#24037;&#20174;&#19994;&#32773;&#30340;&#30456;&#20851;&#24615;&#36739;&#24369;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#21033;&#29992;&#20154;&#20026;&#32534;&#20889;&#30340;&#27979;&#35797;&#22871;&#20214;&#36827;&#34892;&#21151;&#33021;&#27491;&#30830;&#24615;&#35780;&#20272;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#30340;&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#26694;&#26550;&#65288;\texttt{GPT-3.5-turbo}&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21462;&#24471;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code generation tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code generation tasks. Moreover, the utilization of human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose a new evaluation framework based on the GPT-3.5 (\texttt{GPT-3.5-turbo}), for code generation assessments. Our framework addresses the limitations of existing approaches by achieving superior cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;(LGE)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.13472</link><description>&lt;p&gt;
&#24149;&#21518;&#21046;&#20316;&#65306;&#38754;&#21521;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Plotting Behind the Scenes: Towards Learnable Game Engines. (arXiv:2303.13472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;(LGE)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#24341;&#25806;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#25104;&#26412;&#20063;&#26159;&#21313;&#20998;&#24040;&#22823;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#35813;&#32467;&#26524;&#34987;&#31216;&#20026;Learnable Game Engine (LGE)&#65292;&#21487;&#20197;&#32500;&#25252;&#22330;&#26223;&#12289;&#29289;&#20307;&#21644;&#20854;&#20013;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21487;&#25511;&#21046;&#30340;&#35270;&#35282;&#28210;&#26579;&#29615;&#22659;&#12290;&#31867;&#20284;&#20110;&#28216;&#25103;&#24341;&#25806;&#65292;&#23427;&#27169;&#25311;&#20102;&#28216;&#25103;&#30340;&#36923;&#36753;&#21644;&#24213;&#23618;&#29289;&#29702;&#35268;&#21017;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#12290;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LGE&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#29992;&#25143;&#36890;&#36807;&#26631;&#27880;&#39640;&#23618;&#27425;&#30340;&#21160;&#20316;&#21644;&#30446;&#26631;&#26469;&#25511;&#21046;&#20195;&#29702;&#12290;&#36825;&#35201;&#27714;&#23398;&#20064;&#8220;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#8221;&#65292;&#30001;&#25105;&#20204;&#30340;&#21160;&#30011;&#27169;&#22411;&#23553;&#35013;&#65292;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#23548;&#33322;&#22330;&#26223;&#12289;&#19982;&#23545;&#25163;&#23545;&#25112;&#65292;&#35774;&#35745;&#36194;&#24471;&#28216;&#25103;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game engines are powerful tools in computer graphics. Their power comes at the immense cost of their development. In this work, we present a framework to train game-engine-like neural models, solely from monocular annotated videos. The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects and agents in it, and enables rendering the environment from a controllable viewpoint. Similarly to a game engine, it models the logic of the game and the underlying rules of physics, to make it possible for a user to play the game by specifying both high- and low-level action sequences. Most captivatingly, our LGE unlocks the director's mode, where the game is played by plotting behind the scenes, specifying high-level actions and goals for the agents in the form of language and desired states. This requires learning "game AI", encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, devise the strategy to win a point. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#36890;&#36807;&#20934;&#30830;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#22270;&#20687;&#26469;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07064</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalized Multi-Modal Fusion Detection Framework. (arXiv:2303.07064v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#36890;&#36807;&#20934;&#30830;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#22270;&#20687;&#26469;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#24050;&#25104;&#20026;&#33258;&#21160;&#39550;&#39542;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#20294;&#30001;&#20110;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;&#26816;&#27979;&#12290;&#22270;&#20687;&#22240;&#20854;&#19982;&#28857;&#20113;&#30340;&#20114;&#34917;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#26377;&#19968;&#20123;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#29616;&#26377;&#30340;&#34701;&#21512;&#26041;&#27861;&#35201;&#20040;&#36827;&#34892;&#30828;&#34701;&#21512;&#65292;&#35201;&#20040;&#19981;&#30452;&#25509;&#36827;&#34892;&#34701;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MMFusion&#30340;&#36890;&#29992;&#30340;3D&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#23454;&#29616;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#28608;&#20809;&#38647;&#36798;&#21644;&#22270;&#20687;&#30340;&#20934;&#30830;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#29420;&#31435;&#30340;&#27969;&#32452;&#25104;&#65306;&#28608;&#20809;&#38647;&#36798;&#27969;&#21644;&#30456;&#26426;&#27969;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#21333;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#20860;&#23481;&#12290;&#28608;&#20809;&#38647;&#36798;&#27969;&#20013;&#30340;&#20307;&#32032;&#23616;&#37096;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#20102;&#23616;&#37096;&#29305;&#24449;&#34920;&#31034;&#65292;&#28982;&#21518;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#26377;&#36873;&#25321;&#22320;&#23558;&#26469;&#33258;&#19981;&#21516;&#27969;&#30340;&#29305;&#24449;&#36755;&#20986;&#36827;&#34892;&#34701;&#21512;&#65292;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR point clouds have become the most common data source in autonomous driving. However, due to the sparsity of point clouds, accurate and reliable detection cannot be achieved in specific scenarios. Because of their complementarity with point clouds, images are getting increasing attention. Although with some success, existing fusion methods either perform hard fusion or do not fuse in a direct manner. In this paper, we propose a generic 3D detection framework called MMFusion, using multi-modal features. The framework aims to achieve accurate fusion between LiDAR and images to improve 3D detection in complex scenes. Our framework consists of two separate streams: the LiDAR stream and the camera stream, which can be compatible with any single-modal feature extraction network. The Voxel Local Perception Module in the LiDAR stream enhances local feature representation, and then the Multi-modal Feature Fusion Module selectively combines feature output from different streams to achieve b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05479</link><description>&lt;p&gt;
Cal-QL: &#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#39044;&#20808;&#35757;&#32451;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31574;&#30053;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#26377;&#38480;&#20114;&#21160;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22312;&#32447;&#24494;&#35843;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#65292;&#24182;&#20351;&#20854;&#20855;&#22791;&#24555;&#36895;&#30340;&#22312;&#32447;&#24494;&#35843;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;Cal-QL&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20302;&#20272;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#22312;&#21512;&#29702;&#30340;&#33539;&#22260;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#20063;&#33021;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#65292;&#24182;&#24635;&#32467;&#20102;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.11337</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Matrix Decomposition and Applications. (arXiv:2302.11337v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#65292;&#24182;&#24635;&#32467;&#20102;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#30340;&#21807;&#19968;&#30446;&#30340;&#26159;&#20026;&#20102;&#32473;&#20986;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#27010;&#24565;&#21644;&#25968;&#23398;&#24037;&#20855;&#30340;&#33258;&#21253;&#21547;&#20171;&#32461;&#65292;&#20197;&#20415;&#22312;&#21518;&#32493;&#31456;&#33410;&#20013;&#26080;&#32541;&#24341;&#20837;&#30697;&#38453;&#20998;&#35299;&#25216;&#26415;&#21450;&#20854;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28165;&#26970;&#22320;&#24847;&#35782;&#21040;&#25105;&#20204;&#26080;&#27861;&#35206;&#30422;&#20851;&#20110;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#25152;&#26377;&#26377;&#29992;&#21644;&#26377;&#36259;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30001;&#20110;&#35752;&#35770;&#30340;&#33539;&#22260;&#26377;&#38480;&#65292;&#20363;&#22914;&#20998;&#26512;&#21464;&#20998;&#25512;&#29702;&#20197;&#36827;&#34892;&#20248;&#21270;&#30340;&#20998;&#31163;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#35835;&#32773;&#24341;&#23548;&#21040;&#36125;&#21494;&#26031;&#20998;&#26512;&#39046;&#22495;&#30340;&#25991;&#29486;&#20013;&#65292;&#20197;&#20415;&#26356;&#35814;&#32454;&#22320;&#20171;&#32461;&#30456;&#20851;&#39046;&#22495;&#12290;&#26412;&#20070;&#20027;&#35201;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;&#20363;&#22914;&#23454;&#20540;&#20998;&#35299;&#12289;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#12289;&#36125;&#21494;&#26031;&#25554;&#20540;&#20998;&#35299;&#65289;&#30340;&#30446;&#30340;&#21644;&#24847;&#20041;&#65292;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#36215;&#28304;&#21644;&#22797;&#26434;&#24615;&#23545;&#20854;&#24212;&#29992;&#25552;&#20379;&#30340;&#21551;&#31034;&#12290;&#25968;&#23398;&#20808;&#20915;&#26465;&#20214;&#26159;&#31532;&#19968;&#38376;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sole aim of this book is to give a self-contained introduction to concepts and mathematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix decomposition techniques and their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning Bayesian matrix decomposition and given the paucity of scope to present this discussion, e.g., the separated analysis of variational inference for conducting the optimization. We refer the reader to literature in the field of Bayesian analysis for a more detailed introduction to the related fields.  This book is primarily a summary of purpose, significance of important Bayesian matrix decomposition methods, e.g., real-valued decomposition, nonnegative matrix factorization, Bayesian interpolative decomposition, and the origin and complexity of the methods which shed light on their applications. The mathematical prerequisite is a first course in 
&lt;/p&gt;</description></item><item><title>AV-data2vec&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#26500;&#24314;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#35757;&#32451;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06419</link><description>&lt;p&gt;
AV-data2vec: &#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations. (arXiv:2302.06419v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06419
&lt;/p&gt;
&lt;p&gt;
AV-data2vec&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#26500;&#24314;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#35757;&#32451;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#26500;&#24314;&#22909;&#30340;&#31995;&#32479;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#23436;&#20840;&#31471;&#21040;&#31471;&#65292;&#35201;&#20040;&#19981;&#33021;&#21516;&#26102;&#35757;&#32451;&#20004;&#31181;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV-data2vec&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#22522;&#20110;&#39044;&#27979;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#26500;&#24314;&#38899;&#35270;&#39057;&#34920;&#31034;&#65292;&#36825;&#22312;&#21333;&#27169;&#24577;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20849;&#20139;&#30340;Transformer&#32534;&#30721;&#22120;&#23545;&#38899;&#39057;&#21644;&#35270;&#39057;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#27169;&#24577;&#26469;&#25913;&#36827;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;LRS3&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AV-data2vec&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#37117;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#22823;&#23567;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01168</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29289;&#29702;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#38598;&#20013;&#22312;&#36890;&#36807;&#23558;&#29289;&#29702;&#20808;&#39564;&#25110;&#24402;&#32435;&#20559;&#35265;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#30446;&#26631;&#31995;&#32479;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29305;&#23450;&#20110;&#31995;&#32479;&#65292;&#19981;&#20801;&#35768;&#36731;&#26494;&#36866;&#24212;&#30001;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#39537;&#21160;&#30340;&#26032;&#29289;&#29702;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#20110;&#36136;&#28857;&#24377;&#31783;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#21452;&#20307;&#31995;&#32479;&#25110;&#20219;&#20309;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#20351;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#20351;&#20854;&#36866;&#24212;&#26032;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#21508;&#31181;&#21704;&#23494;&#39039;&#27969;&#24418;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36825;&#26159;&#21704;&#23494;&#39039;&#31995;&#32479;&#25968;&#25454;&#20998;&#24067;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#19981;&#21516;&#31995;&#32479;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#26377;&#20854;&#33258;&#36523;&#22266;&#26377;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#65292;&#25506;&#35752;&#20102;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#23558;&#20854;&#19982;&#30452;&#27604;&#21516;&#24577;&#12289;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.01751</link><description>&lt;p&gt;
&#30452;&#27604;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Proportional algebras. (arXiv:2210.01751v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#65292;&#25506;&#35752;&#20102;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#23558;&#20854;&#19982;&#30452;&#27604;&#21516;&#24577;&#12289;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#25311;&#27604;&#20363;&#26159;&#24418;&#24335;&#20026;"$a$&#23545;$b$&#65292;&#27491;&#22914;$c$&#23545;$d$"&#30340;&#34920;&#36798;&#24335;&#65292;&#26159;&#27169;&#25311;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#32780;&#27169;&#25311;&#25512;&#29702;&#21448;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#20316;&#20026;&#19968;&#31181;&#24102;&#26377;4&#20803;&#27604;&#25311;&#27604;&#20363;&#20851;&#31995;$a:b::c:d$&#28385;&#36275;&#19968;&#32452;&#36866;&#24403;&#20844;&#29702;&#30340;&#20195;&#25968;&#32467;&#26500;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#32780;&#30740;&#31350;&#23427;&#20204;&#30340;&#25968;&#23398;&#24615;&#36136;&#23545;&#20110;&#29702;&#35299;&#27604;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#27604;&#21516;&#24577;&#21450;&#20854;&#20851;&#32852;&#30340;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#32852;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35828;&#65292;&#26412;&#25991;&#26159;&#36808;&#21521;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are expressions of the form "$a$ is to $b$ what $c$ is to $d$" at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper introduces proportional algebras as algebras endowed with a 4-ary analogical proportion relation $a:b::c:d$ satisfying a suitable set of axioms. Functions preserving analogical proportions have already proven to be of practical interest in artificial intelligence and studying their mathematical properties is essential for understanding proportions. We therefore introduce proportional homomorphisms and their associated congruences and proportional functors, and show that they are closely related notions. In a broader sense, this paper is a further step towards a mathematical theory of analogical proportions and analogical reasoning in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.04957</link><description>&lt;p&gt;
&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#36827;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#29983;&#25104;&#33021;&#22815;&#19982;&#26410;&#30693;&#21512;&#20316;&#20249;&#20276;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22312;&#38646;&#26679;&#26412;&#21327;&#21516;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26234;&#33021;&#20307;&#26292;&#38706;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#20249;&#20276;&#26102;&#28041;&#21450;&#33258;&#25105;&#23545;&#24328;&#65292;&#38544;&#24335;&#22320;&#20551;&#35774;&#20219;&#21153;&#26159;&#21516;&#36136;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26159;&#24322;&#26500;&#30340;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#36807;&#31243;&#65306;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#65292;&#23545;&#20004;&#20010;&#26234;&#33021;&#20307;&#21644;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#21327;&#21516;&#36827;&#21270;&#12290;&#23545;&#19981;&#21516;&#24322;&#26500;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#35299;&#20915;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#32771;&#34385;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#30830;&#23450;UDA&#26041;&#27861;&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00067</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35821;&#20041;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Rethinking Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2207.00067v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00067
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#32771;&#34385;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#30830;&#23450;UDA&#26041;&#27861;&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;&#31216;&#20026;&#28304;&#39046;&#22495;&#65289;&#36866;&#24212;&#21040;&#19968;&#20010;&#26032;&#39046;&#22495;&#65288;&#31216;&#20026;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#30001;&#20110;&#27880;&#37322;&#25104;&#26412;&#39640;&#26114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;UDA&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#30446;&#26631;&#39046;&#22495;&#20013;&#27809;&#26377;&#21487;&#29992;&#30340;&#26631;&#35760;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#20551;&#35774;&#30340;&#21487;&#34892;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20027;&#35201;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#22312;&#20351;&#29992;UDA&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#65292;&#25105;&#20204;&#24517;&#39035;&#22312;&#37096;&#32626;&#20043;&#21069;&#23545;&#27169;&#22411;&#36827;&#34892;&#39564;&#35777;&#12290;&#20854;&#27425;&#65292;UDA&#26041;&#27861;&#33267;&#23569;&#26377;&#20960;&#20010;&#38656;&#35201;&#30830;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#30830;&#20445;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#39564;&#35777;&#25968;&#25454;&#35780;&#20272;&#27169;&#22411;&#65292;&#21363;&#19968;&#23450;&#25968;&#37327;&#30340;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#12290;&#23545;UDA&#30340;&#36825;&#19968;&#22522;&#26412;&#20551;&#35774;&#30340;&#36136;&#30097;&#20351;&#25105;&#20204;&#37325;&#26032;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#24605;&#32771;UDA&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#19968;&#23450;&#27700;&#24179;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35810;&#38382;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#26469;&#25214;&#21040;&#29616;&#26377;UDA&#26041;&#27861;&#30340;&#33391;&#22909;&#36229;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#21487;&#20197;&#23545;UDA&#26041;&#27861;&#36827;&#34892;&#20160;&#20040;&#26679;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) adapts a model trained on one domain (called source) to a novel domain (called target) using only unlabeled data. Due to its high annotation cost, researchers have developed many UDA methods for semantic segmentation, which assume no labeled sample is available in the target domain. We question the practicality of this assumption for two reasons. First, after training a model with a UDA method, we must somehow verify the model before deployment. Second, UDA methods have at least a few hyper-parameters that need to be determined. The surest solution to these is to evaluate the model using validation data, i.e., a certain amount of labeled target-domain samples. This question about the basic assumption of UDA leads us to rethink UDA from a data-centric point of view. Specifically, we assume we have access to a minimum level of labeled data. Then, we ask how much is necessary to find good hyper-parameters of existing UDA methods. We then consider what 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21464;&#25442;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26234;&#33021;&#26426;&#22120;&#20154;&#36741;&#21161;&#29615;&#22659;&#20013;&#26816;&#27979;&#20154;&#31867;&#27963;&#21160;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#26234;&#33021;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#25193;&#23637;&#65292;&#23558;&#19981;&#21516;&#30340;&#24322;&#24120;&#25351;&#26631;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#26102;&#38388;&#20808;&#39564;&#65292;&#24182;&#26816;&#27979;&#20154;&#31867;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2002.11503</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#27874;&#30340;&#20154;&#31867;&#27963;&#21160;&#26102;&#38388;&#27169;&#22411;&#22312;&#26234;&#33021;&#26426;&#22120;&#20154;&#36741;&#21161;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Wavelet-based temporal models of human activity for anomaly detection in smart robot-assisted environments. (arXiv:2002.11503v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.11503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21464;&#25442;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26234;&#33021;&#26426;&#22120;&#20154;&#36741;&#21161;&#29615;&#22659;&#20013;&#26816;&#27979;&#20154;&#31867;&#27963;&#21160;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#26234;&#33021;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#25193;&#23637;&#65292;&#23558;&#19981;&#21516;&#30340;&#24322;&#24120;&#25351;&#26631;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#26102;&#38388;&#20808;&#39564;&#65292;&#24182;&#26816;&#27979;&#20154;&#31867;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20256;&#24863;&#22120;&#25968;&#25454;&#27169;&#24335;&#20013;&#30340;&#24322;&#24120;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#29992;&#20110;&#20027;&#21160;&#36741;&#21161;&#29983;&#27963;&#30340;&#23478;&#24237;&#27963;&#21160;&#30417;&#27979;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#34920;&#31034;&#21644;&#20998;&#26512;&#36825;&#20123;&#27169;&#24335;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#30456;&#23545;&#31232;&#32570;&#19988;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#26469;&#23545;&#29305;&#23450;&#22330;&#26223;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26102;&#26399;&#20154;&#31867;&#27963;&#21160;&#22312;&#26234;&#33021;&#23478;&#23621;&#20256;&#24863;&#22120;&#19978;&#30340;&#26102;&#38388;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#29615;&#22659;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#23567;&#27874;&#21464;&#25442;&#65292;&#29992;&#20110;&#39044;&#27979;&#26234;&#33021;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#20379;&#26102;&#38388;&#20808;&#39564;&#20197;&#26816;&#27979;&#20154;&#31867;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#20107;&#20214;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#30340;&#25193;&#23637;&#65292;&#23558;&#21253;&#25324;&#20108;&#20803;&#20256;&#24863;&#22120;&#26816;&#27979;&#21040;&#30340;&#27963;&#21160;&#12289;&#19987;&#23478;&#36923;&#36753;&#35268;&#21017;&#21644;&#22522;&#20110;&#23567;&#27874;&#30340;&#26102;&#38388;&#27169;&#22411;&#22312;&#20869;&#30340;&#19981;&#21516;&#24322;&#24120;&#25351;&#26631;&#21512;&#24182;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract. Detecting anomalies in patterns of sensor data is important in many practical applications, including domestic activity monitoring for Active Assisted Living (AAL). How to represent and analyse these patterns, however, remains a challenging task, especially when data is relatively scarce and an explicit model is required to be fine-tuned for specific scenarios. This paper, therefore, presents a new approach for temporal modelling of long-term human activities with smart-home sensors, which is used to detect anomalous situations in a robot-assisted environment. The model is based on wavelet transforms and used to forecast smart sensor data, providing a temporal prior to detect unexpected events in human environments. To this end, a new extension of Hybrid Markov Logic Networks has been developed that merges different anomaly indicators, including activities detected by binary sensors, expert logic rules, and wavelet-based temporal models. The latter in particular allows the in
&lt;/p&gt;</description></item></channel></rss>