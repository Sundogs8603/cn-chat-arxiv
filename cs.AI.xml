<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#22330;&#26223;&#20013;&#25554;&#20837;&#29616;&#23454;&#23039;&#24577;&#30340;&#20154;&#65292;&#20197;&#21450;&#21512;&#25104;&#33021;&#22815;&#22312;&#22330;&#26223;&#20013;&#19982;&#20154;&#31867;&#36827;&#34892;&#33258;&#28982;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14406</link><description>&lt;p&gt;
&#23558;&#20154;&#32622;&#20110;&#20854;&#22330;&#26223;&#20013;&#65306;&#32771;&#34385;&#21487;&#20379;&#24615;&#30340;&#20154;&#31867;&#25554;&#20837;
&lt;/p&gt;
&lt;p&gt;
Putting People in Their Place: Affordance-Aware Human Insertion into Scenes. (arXiv:2304.14406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#22330;&#26223;&#20013;&#25554;&#20837;&#29616;&#23454;&#23039;&#24577;&#30340;&#20154;&#65292;&#20197;&#21450;&#21512;&#25104;&#33021;&#22815;&#22312;&#22330;&#26223;&#20013;&#19982;&#20154;&#31867;&#36827;&#34892;&#33258;&#28982;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25512;&#26029;&#22330;&#26223;&#21487;&#20379;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#22312;&#22330;&#26223;&#20013;&#25554;&#20837;&#20154;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#26377;&#26631;&#35760;&#21306;&#22495;&#30340;&#22330;&#26223;&#22270;&#20687;&#21644;&#19968;&#20010;&#20154;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20154;&#25554;&#20837;&#21040;&#22330;&#26223;&#20013;&#65292;&#24182;&#23562;&#37325;&#22330;&#26223;&#21487;&#20379;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#22330;&#26223;&#19978;&#19979;&#25991;&#25512;&#26029;&#20986;&#19968;&#32452;&#29616;&#23454;&#30340;&#23039;&#24577;&#65292;&#37325;&#26032;&#35843;&#25972;&#21442;&#32771;&#20154;&#30340;&#23039;&#24577;&#65292;&#24182;&#35843;&#21644;&#26500;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#22312;&#35270;&#39057;&#21098;&#36753;&#20013;&#37325;&#26032;&#23039;&#24577;&#20154;&#31867;&#26469;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#35774;&#32622;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#21253;&#21547; 240 &#19975;&#20010;&#35270;&#39057;&#21098;&#36753;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20250;&#20135;&#29983;&#22810;&#26679;&#30340;&#21512;&#29702;&#23039;&#24577;&#65292;&#21516;&#26102;&#23562;&#37325;&#22330;&#26223;&#19978;&#19979;&#25991;&#12290;&#37492;&#20110;&#23398;&#20064;&#21040;&#30340;&#20154; - &#22330;&#26223;&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#22312;&#27809;&#26377;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24863;&#24212;&#21040;&#30495;&#23454;&#30340;&#20154;&#21644;&#22330;&#26223;&#65292;&#20197;&#21450;&#21551;&#29992;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;&#23450;&#37327;&#35780;&#20272;&#26174;&#31034;&#20986;&#65292;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#25104;&#20102;&#26356;&#30495;&#23454;&#30340;&#20154;&#31867;&#22806;&#35266;&#21644;&#26356;&#33258;&#28982;&#30340;&#20154; - &#22330;&#26223;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re-pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hallucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. A quantitative evaluation shows that our method synthesizes more realistic human appearance and more natural human-scene interactions than prior work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2304.14382</link><description>&lt;p&gt;
&#27169;&#25311;&#24418;&#24335;&#36716;&#25442;&#22120;&#29992;&#20110;&#23569;&#26679;&#26412;3D&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14382
&lt;/p&gt;
&lt;p&gt;
"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#19968;&#32452;&#26377;&#26631;&#35760;&#30340;&#32467;&#26500;&#21270;3D&#22330;&#26223;&#20013;&#26174;&#24335;&#22320;&#32534;&#30721;&#39046;&#22495;&#30693;&#35782;&#65288;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#37096;&#20998;&#65289;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#23545;3D&#29289;&#20307;&#22330;&#26223;&#36827;&#34892;&#20998;&#21106;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#21450;&#20854;&#30456;&#24212;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#35843;&#21046;&#26426;&#21046;&#20026;&#36755;&#20837;&#22330;&#26223;&#39044;&#27979;&#31867;&#20284;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#22330;&#26223;&#26144;&#23556;&#21040;&#37096;&#20998;&#20998;&#21106;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26816;&#32034;&#30340;&#35760;&#24518;&#36827;&#34892;&#26465;&#20214;&#25511;&#21046;&#65292;&#39044;&#27979;&#28151;&#21512;&#21305;&#37197;&#26816;&#32034;&#35760;&#24518;&#30340;&#32467;&#26500;&#21512;&#25104;&#12290;&#22312;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#20013;&#65292;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#34987;&#19968;&#33268;&#22320;&#22788;&#29702;&#65292;&#36890;&#36807;&#23545;&#36866;&#24403;&#30340;&#35760;&#24518;&#38598;&#36827;&#34892;&#26465;&#20214;&#35859;&#35789;&#65292;&#26080;&#35770;&#26159;&#20174;&#21333;&#20010;&#12289;&#23569;&#25968;&#36824;&#26159;&#35768;&#22810;&#23384;&#20648;&#23454;&#20363;&#20013;&#32487;&#25215;&#30456;&#20284;&#30340;&#35299;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#22312;&#35768;&#22810;&#26679;&#26412;&#24773;&#20917;&#19979;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.14364</link><description>&lt;p&gt;
CONSCENDI: &#19968;&#31181;&#21453;&#23545;&#27604;&#19988;&#22330;&#26223;&#24341;&#23548;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20026;&#34394;&#25311;&#21161;&#25163;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-4&#31561;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#34394;&#25311;&#21161;&#25163;&#24212;&#36816;&#32780;&#29983;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#23458;&#25143;&#30340;&#20855;&#20307;&#29992;&#20363;&#36827;&#34892;&#23450;&#21046;&#65292;&#20294;&#30830;&#20445;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#20165;&#31526;&#21512;&#25552;&#31034;&#25351;&#20196;&#20013;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24072;&#36890;&#24120;&#20351;&#29992;&#21478;&#19968;&#20010;&#31216;&#20026;&#38450;&#25252;&#26639;&#27169;&#22411;&#30340;&#27169;&#22411;&#26469;&#39564;&#35777;&#20195;&#29702;&#36755;&#20986;&#26159;&#21542;&#19982;&#20854;&#35268;&#21017;&#21644;&#32422;&#26463;&#23545;&#40784;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#33976;&#39311;&#26041;&#27861;&#26469;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20351;&#29992;GPT-4&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;CONSCENDI&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20250;&#29983;&#25104;&#19968;&#32452;&#36829;&#21453;&#35268;&#21017;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#21015;&#20030;&#20102;&#36829;&#21453;&#35268;&#21017;&#30340;&#22810;&#26679;&#21270;&#39640;&#32423;&#26041;&#24335;&#12290;&#36825;&#31181;&#22330;&#26223;&#24341;&#23548;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#23427;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#20351;&#29992;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14339</link><description>&lt;p&gt;
MarsEclipse&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#22810;&#35821;&#35328;&#21644;&#22810;&#26631;&#31614;&#26694;&#26550;&#26816;&#27979;&#21450;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#20351;&#29992;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#20219;&#21153;3&#30340;&#23376;&#20219;&#21153;2&#19978;&#36827;&#34892;&#26694;&#26550;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35774;&#32622;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23448;&#26041;&#27979;&#35797;&#38598;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#23545;&#20110;&#25105;&#20204;&#26377;&#35757;&#32451;&#25968;&#25454;&#24182;&#33021;&#36827;&#34892;&#24494;&#35843;&#30340;&#20845;&#31181;&#35821;&#35328;&#20013;&#30340;&#20116;&#31181;&#35821;&#35328;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20197;&#21450;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20195;&#30721;&#21487;&#22312;https://github.com/QishengL/SemEval2023&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing Detection. We used a multi-label contrastive loss for fine-tuning large pre-trained language models in a multi-lingual setting, achieving very competitive results: our system was ranked first on the official test set and on the official shared task leaderboard for five of the six languages for which we had training data and for which we could perform fine-tuning. Here, we describe our experimental setup, as well as various ablation studies. The code of our system is available at https://github.com/QishengL/SemEval2023
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-nudging&#23457;&#35745;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;&#20351;&#29992;nudges&#30340;AI&#31995;&#32479;&#20445;&#25345;&#36947;&#24503;&#24815;&#24615;&#21644;&#20013;&#31435;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39118;&#38505;&#32531;&#35299;&#21644;&#24378;&#21270;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.14338</link><description>&lt;p&gt;
&#37319;&#29992;AI-Nudging&#23545;&#20799;&#31461;&#36827;&#34892;&#23457;&#35745;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Audit Framework for Adopting AI-Nudging on Children. (arXiv:2304.14338v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-nudging&#23457;&#35745;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;&#20351;&#29992;nudges&#30340;AI&#31995;&#32479;&#20445;&#25345;&#36947;&#24503;&#24815;&#24615;&#21644;&#20013;&#31435;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39118;&#38505;&#32531;&#35299;&#21644;&#24378;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;AI-nudging&#30340;&#23457;&#35745;&#26694;&#26550;&#12290;&#19982;&#25991;&#29486;&#20013;&#36890;&#24120;&#35752;&#35770;&#30340;&#38745;&#24577;"nudging"&#24418;&#24335;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#19968;&#31181;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#12289;&#21160;&#24577;&#21453;&#39304;&#21644;&#30028;&#38754;&#30340;nudging&#31867;&#22411;&#12290;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;AI-nudging&#12290;&#35813;&#23457;&#35745;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#30830;&#20445;&#20351;&#29992;nudges&#30340;AI&#31995;&#32479;&#36890;&#36807;&#36981;&#23432;&#23457;&#35745;&#30340;&#24314;&#35758;&#12289;&#35201;&#27714;&#25110;&#24314;&#35758;&#65288;&#25442;&#21477;&#35805;&#35828;&#65292;&#23457;&#35745;&#30340;&#26631;&#20934;&#65289;&#20445;&#25345;&#19968;&#31181;&#36947;&#24503;&#24815;&#24615;&#21644;&#20013;&#31435;&#24615;&#12290;&#22312;&#24847;&#22806;&#30340;&#36127;&#38754;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#35745;&#24314;&#35758;&#35774;&#32622;&#39118;&#38505;&#32531;&#35299;&#26426;&#21046;&#12290;&#22312;&#24847;&#22806;&#30340;&#27491;&#38754;&#24433;&#21709;&#24773;&#20917;&#19979;&#65292;&#23427;&#24314;&#35758;&#19968;&#20123;&#24378;&#21270;&#26426;&#21046;&#12290;&#35813;&#30740;&#31350;&#30001;IBM-Notre Dame Tech Ethics Lab&#36190;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is an audit framework for AI-nudging. Unlike the static form of nudging usually discussed in the literature, we focus here on a type of nudging that uses large amounts of data to provide personalized, dynamic feedback and interfaces. We call this AI-nudging (Lanzing, 2019, p. 549; Yeung, 2017). The ultimate goal of the audit outlined here is to ensure that an AI system that uses nudges will maintain a level of moral inertia and neutrality by complying with the recommendations, requirements, or suggestions of the audit (in other words, the criteria of the audit). In the case of unintended negative consequences, the audit suggests risk mitigation mechanisms that can be put in place. In the case of unintended positive consequences, it suggests some reinforcement mechanisms. Sponsored by the IBM-Notre Dame Tech Ethics Lab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ChatGPT&#29983;&#25104;&#21644;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#29305;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#19979;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14334</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#29983;&#25104;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;ZeroShotDataAug
&lt;/p&gt;
&lt;p&gt;
ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT. (arXiv:2304.14334v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ChatGPT&#29983;&#25104;&#21644;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#29305;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#19979;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#38754;&#21521;&#29305;&#23450;&#20219;&#21153;&#30340;ChatGPT&#25552;&#31034;&#65292;&#25105;&#20204;&#20248;&#20110;&#29616;&#26377;&#30340;&#27492;&#31867;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#35780;&#20272;ChatGPT&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#21644;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the use of data obtained from prompting a large generative language model, ChatGPT, to generate synthetic training data with the aim of augmenting data in low resource scenarios. We show that with appropriate task-specific ChatGPT prompts, we outperform the most popular existing approaches for such data augmentation. Furthermore, we investigate methodologies for evaluating the similarity of the augmented data generated from ChatGPT with the aim of validating and assessing the quality of the data generated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#35821;&#22312;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#20013;&#30340;&#32534;&#30721;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#20173;&#26410;&#30830;&#23450;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.14333</link><description>&lt;p&gt;
&#25104;&#35821;&#12289;&#25506;&#27979;&#21644;&#21361;&#38505;&#30340;&#20107;&#29289;&#65306;&#22522;&#20110;&#32467;&#26500;&#25506;&#27979;&#30340;&#35789;&#21521;&#37327;&#25104;&#35821;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#35821;&#22312;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#20013;&#30340;&#32534;&#30721;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#20173;&#26410;&#30830;&#23450;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25104;&#35821;&#20449;&#24687;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#35789;&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#33521;&#35821;&#21160;&#35789;&#22797;&#21512;&#35789;&#35821;&#65288;MWE&#65289;&#25968;&#25454;&#38598;&#26469;&#36866;&#24212;&#25506;&#27979;&#26694;&#26550;&#65292;&#24182;&#23545;&#38745;&#24577;&#65288;GloVe&#65289;&#21644;&#19978;&#19979;&#25991;&#65288;BERT&#65289;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#25506;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20197;&#19981;&#21516;&#31243;&#24230;&#32534;&#30721;&#20102;&#19968;&#20123;&#25104;&#35821;&#20449;&#24687;&#65292;&#20294;&#23545;&#20110;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#32473;&#20986;&#20102;&#20914;&#31361;&#30340;&#35777;&#25454;&#65292;&#36825;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#25152;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25913;&#36827;&#20854;&#36866;&#29992;&#24615;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#37325;&#35201;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#36923;&#36753;&#8212;&#8212;Standpoint EL+&#65292;&#20801;&#35768;&#20844;&#29702;&#21542;&#23450;&#12289;&#35282;&#33394;&#38142;&#20844;&#29702;&#12289;&#33258;&#29615;&#31561;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#28385;&#36275;&#24615;&#26816;&#27979;&#28436;&#32462;&#28436;&#31639;&#27861;&#26469;&#23454;&#29616;&#23427;&#65292;&#36825;&#20010;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14323</link><description>&lt;p&gt;
&#25512;&#21160;&#21487;&#34892;&#30340;&#22810;&#35282;&#24230;&#25512;&#29702;&#36793;&#30028;&#65306;&#19968;&#31181;&#38754;&#21521;STANDPOINT EL + &#30340;&#28436;&#32462;&#28436;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+. (arXiv:2304.14323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#36923;&#36753;&#8212;&#8212;Standpoint EL+&#65292;&#20801;&#35768;&#20844;&#29702;&#21542;&#23450;&#12289;&#35282;&#33394;&#38142;&#20844;&#29702;&#12289;&#33258;&#29615;&#31561;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#28385;&#36275;&#24615;&#26816;&#27979;&#28436;&#32462;&#28436;&#31639;&#27861;&#26469;&#23454;&#29616;&#23427;&#65292;&#36825;&#20010;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Standpoint EL&#26159;&#27969;&#34892;&#30340;&#25551;&#36848;&#36923;&#36753;EL&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#20801;&#35768;&#30456;&#23545;&#20110;&#19981;&#21516;&#30340;&#35266;&#28857;&#25110;&#35282;&#24230;&#38598;&#25104;&#39046;&#22495;&#30693;&#35782;&#30340;&#32508;&#21512;&#34920;&#31034;&#12290;&#26377;&#21033;&#30340;&#26159;&#65292;&#20854;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#22312;P&#26102;&#38388;&#20869;&#65292;&#20351;&#20854;&#25104;&#20026;&#22823;&#35268;&#27169;&#30693;&#35782;&#38598;&#25104;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#31181;&#24418;&#24335;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21040;&#36798;&#19968;&#20010;&#25193;&#23637;&#36923;&#36753;&#65292;&#31216;&#20026;Standpoint EL+&#65292;&#20801;&#35768;&#20844;&#29702;&#21542;&#23450;&#12289;&#35282;&#33394;&#38142;&#20844;&#29702;&#12289;&#33258;&#29615;&#31561;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#28385;&#36275;&#24615;&#26816;&#27979;&#28436;&#32462;&#28436;&#31639;&#27861;&#23454;&#29616;&#30340;&#65292;&#21516;&#26102;&#35299;&#20915;&#23454;&#29992;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20854;&#28436;&#32462;&#35268;&#21017;&#30340;&#21407;&#22411;Datalog&#23454;&#29616;&#26469;&#23637;&#31034;&#25105;&#20204;&#28436;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standpoint EL is a multi-modal extension of the popular description logic EL that allows for the integrated representation of domain knowledge relative to diverse standpoints or perspectives. Advantageously, its satisfiability problem has recently been shown to be in PTime, making it a promising framework for large-scale knowledge integration.  In this paper, we show that we can further push the expressivity of this formalism, arriving at an extended logic, called Standpoint EL+, which allows for axiom negation, role chain axioms, self-loops, and other features, while maintaining tractability. This is achieved by designing a satisfiability-checking deduction calculus, which at the same time addresses the need for practical algorithms. We demonstrate the feasibility of our calculus by presenting a prototypical Datalog implementation of its deduction rules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14317</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20195;&#30721;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; GPT-3.5 &#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#32534;&#31243;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#24320;&#21457;&#35780;&#20272;&#25351;&#26631;&#20197;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#21464;&#24471;&#22256;&#38590;&#12290;&#20197;&#35789;&#27719;&#21305;&#37197;&#20026;&#22522;&#30784;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;BLEU&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#19982;&#20154;&#24037;&#20174;&#19994;&#32773;&#30340;&#30456;&#20851;&#24615;&#36739;&#24369;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#21033;&#29992;&#20154;&#20026;&#32534;&#20889;&#30340;&#27979;&#35797;&#22871;&#20214;&#36827;&#34892;&#21151;&#33021;&#27491;&#30830;&#24615;&#35780;&#20272;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#30340;&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#26694;&#26550;&#65288;\texttt{GPT-3.5-turbo}&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21462;&#24471;&#26356;&#22909;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code generation tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code generation tasks. Moreover, the utilization of human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose a new evaluation framework based on the GPT-3.5 (\texttt{GPT-3.5-turbo}), for code generation assessments. Our framework addresses the limitations of existing approaches by achieving superior cor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21452;&#23618;&#22810;&#26426;&#22120;&#20154;&#25552;&#36135;&#21644;&#37197;&#36865;&#65288;DD-MAPD&#65289;&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#28041;&#21450;&#26426;&#22120;&#20154;&#30340;&#36135;&#26550;&#31227;&#21160;&#65292;&#20197;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;MAPF-DECOMP&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27492;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14309</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20179;&#24211;&#20013;&#30340;&#22810;&#26426;&#22120;&#20154;&#36135;&#26550;&#37325;&#25490;&#38382;&#39064;&#65306;&#21452;&#23618;&#22810;&#26426;&#22120;&#20154;&#25552;&#36135;&#21644;&#37197;&#36865;
&lt;/p&gt;
&lt;p&gt;
Double-Deck Multi-Agent Pickup and Delivery: Multi-Robot Rearrangement in Large-Scale Warehouses. (arXiv:2304.14309v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21452;&#23618;&#22810;&#26426;&#22120;&#20154;&#25552;&#36135;&#21644;&#37197;&#36865;&#65288;DD-MAPD&#65289;&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#28041;&#21450;&#26426;&#22120;&#20154;&#30340;&#36135;&#26550;&#31227;&#21160;&#65292;&#20197;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;MAPF-DECOMP&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27492;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#21452;&#23618;&#22810;&#26426;&#22120;&#20154;&#25552;&#36135;&#21644;&#37197;&#36865;&#65288;DD-MAPD&#65289;&#65292;&#23427;&#27169;&#25311;&#20102;&#33258;&#21160;&#21270;&#20179;&#24211;&#20013;&#30340;&#22810;&#26426;&#22120;&#20154;&#36135;&#26550;&#37325;&#25490;&#38382;&#39064;&#12290;DD-MAPD&#36890;&#36807;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#36135;&#26550;&#19979;&#31227;&#21160;&#25110;&#20030;&#36215;&#24182;&#23558;&#36135;&#26550;&#36865;&#24448;&#20219;&#24847;&#20301;&#32622;&#25913;&#21464;&#20179;&#24211;&#24067;&#23616;&#65292;&#25193;&#23637;&#20102;Multi-Agent Pickup and Delivery&#65288;MAPD&#65289;&#21644;Multi-Agent Path Finding&#65288;MAPF&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#27714;&#35299;DD-MAPD&#26159;NP&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;DD-MAPD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAPF-DECOMP&#65292;&#36825;&#26159;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;DD-MAPD&#23454;&#20363;&#20998;&#35299;&#25104;&#29992;&#20110;&#21327;&#35843;&#36135;&#26550;&#36712;&#36857;&#30340;MAPF&#23454;&#20363;&#21644;&#29992;&#20110;&#35745;&#31639;&#20195;&#29702;&#36335;&#24452;&#30340;&#20855;&#26377;&#20219;&#21153;&#20381;&#36182;&#24615;&#30340;&#21518;&#32493;MAPD&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#25913;&#36827;MAPF-DECOMP&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;MAPF-DECOMP&#38024;&#23545;&#38750;&#24120;&#35268;DD-MAPD&#23454;&#20363;&#65288;&#19968;&#20010;&#29616;&#23454;&#30340;DD-MAPD&#23454;&#20363;&#23376;&#38598;&#65289;&#23436;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;MAPF-DECOMP&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#26377;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new problem formulation, Double-Deck Multi-Agent Pickup and Delivery (DD-MAPD), which models the multi-robot shelf rearrangement problem in automated warehouses. DD-MAPD extends both Multi-Agent Pickup and Delivery (MAPD) and Multi-Agent Path Finding (MAPF) by allowing agents to move beneath shelves or lift and deliver a shelf to an arbitrary location, thereby changing the warehouse layout. We show that solving DD-MAPD is NP-hard. To tackle DD-MAPD, we propose MAPF-DECOMP, an algorithmic framework that decomposes a DD-MAPD instance into a MAPF instance for coordinating shelf trajectories and a subsequent MAPD instance with task dependencies for computing paths for agents. We also present an optimization technique to improve the performance of MAPF-DECOMP and demonstrate how to make MAPF-DECOMP complete for well-formed DD-MAPD instances, a realistic subclass of DD-MAPD instances. Our experimental results demonstrate the efficiency and effectiveness of MAPF-DECOMP, with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#40657;&#26263;&#29615;&#22659;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#20026;&#20102;&#25233;&#21046;&#20302;&#20809;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#22122;&#22768;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#19979;&#37319;&#26679;&#23618;&#12289;&#24179;&#28369;&#23450;&#21521;&#21367;&#31215;&#22359;&#21644;&#25200;&#21160;&#25233;&#21046;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#39640;&#20301;&#28145;&#30340;RAW&#22270;&#20687;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26356;&#20016;&#23500;&#30340;&#22330;&#26223;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.14298</link><description>&lt;p&gt;
&#40657;&#26263;&#29615;&#22659;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Instance Segmentation in the Dark. (arXiv:2304.14298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#40657;&#26263;&#29615;&#22659;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#20026;&#20102;&#25233;&#21046;&#20302;&#20809;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#22122;&#22768;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#19979;&#37319;&#26679;&#23618;&#12289;&#24179;&#28369;&#23450;&#21521;&#21367;&#31215;&#22359;&#21644;&#25200;&#21160;&#25233;&#21046;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#39640;&#20301;&#28145;&#30340;RAW&#22270;&#20687;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26356;&#20016;&#23500;&#30340;&#22330;&#26223;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#20027;&#35201;&#36866;&#29992;&#20110;&#39640;&#21487;&#35265;&#24230;&#30340;&#36755;&#20837;&#65292;&#20294;&#22312;&#26497;&#20302;&#20809;&#29615;&#22659;&#19979;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#40657;&#26263;&#20013;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#20171;&#32461;&#20102;&#20960;&#31181;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#20809;&#25512;&#26029;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#20302;&#20809;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#20250;&#24341;&#20837;&#39640;&#39057;&#25200;&#21160;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;&#20026;&#20102;&#25233;&#21046;&#36825;&#31181;&#8220;&#29305;&#24449;&#22122;&#22768;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#33258;&#36866;&#24212;&#21152;&#26435;&#19979;&#37319;&#26679;&#23618;&#12289;&#24179;&#28369;&#23450;&#21521;&#21367;&#31215;&#22359;&#21644;&#25200;&#21160;&#25233;&#21046;&#23398;&#20064;&#12290;&#36825;&#20123;&#32452;&#20214;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19979;&#37319;&#26679;&#21644;&#21367;&#31215;&#25805;&#20316;&#20013;&#30340;&#29305;&#24449;&#22122;&#22768;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#25239;&#25200;&#21160;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#20301;&#28145;&#30340;RAW&#22270;&#20687;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26356;&#20016;&#23500;&#30340;&#22330;&#26223;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
Existing instance segmentation techniques are primarily tailored for high-visibility inputs, but their performance significantly deteriorates in extremely low-light environments. In this work, we take a deep look at instance segmentation in the dark and introduce several techniques that substantially boost the low-light inference accuracy. The proposed method is motivated by the observation that noise in low-light images introduces high-frequency disturbances to the feature maps of neural networks, thereby significantly degrading performance. To suppress this ``feature noise", we propose a novel learning method that relies on an adaptive weighted downsampling layer, a smooth-oriented convolutional block, and disturbance suppression learning. These components effectively reduce feature noise during downsampling and convolution operations, enabling the model to learn disturbance-invariant features. Furthermore, we discover that high-bit-depth RAW images can better preserve richer scene i
&lt;/p&gt;</description></item><item><title>InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14293</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14293
&lt;/p&gt;
&lt;p&gt;
InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#24182;&#33021;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23427;&#20204;&#30340;&#29983;&#25104;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#32422;&#26463;&#35843;&#33410;&#30340;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;InstructCTG&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32422;&#26463;&#28436;&#31034;&#26469;&#32435;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#31995;&#21015;&#29616;&#25104;&#30340;NLP&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#21462;&#33258;&#28982;&#25991;&#26412;&#30340;&#28508;&#22312;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#24418;&#25104;&#24369;&#30417;&#30563;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#28155;&#21152;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#25551;&#36848;&#21644;&#23569;&#37327;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#32435;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#25628;&#32034;&#25110;&#24471;&#20998;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;InstructCTG &#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#21306;&#20998;&#34892;&#26143;&#20940;&#21644;&#35823;&#21028;&#12290;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#26469;&#20445;&#30041;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14283</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#21306;&#20998;&#34892;&#26143;&#20940;&#21644;&#35823;&#21028;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals. (arXiv:2304.14283v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#21306;&#20998;&#34892;&#26143;&#20940;&#21644;&#35823;&#21028;&#12290;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#26469;&#20445;&#30041;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20687;TESS&#36825;&#26679;&#30340;&#31354;&#38388;&#20219;&#21153;&#25552;&#20379;&#20102;&#22823;&#37327;&#24517;&#39035;&#39640;&#25928;&#12289;&#31995;&#32479;&#22320;&#20998;&#26512;&#30340;&#20809;&#21464;&#26354;&#32447;&#25968;&#25454;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#24050;&#34987;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#20505;&#36873;&#22806;&#34892;&#26143;&#30340;&#20940;&#21464;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;CNN&#20855;&#26377;&#19968;&#20123;&#32570;&#38519;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#38656;&#35201;&#35768;&#22810;&#23618;&#26469;&#25429;&#33719;&#24207;&#21015;&#25968;&#25454;&#65288;&#20363;&#22914;&#20809;&#21464;&#26354;&#32447;&#65289;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#32593;&#32476;&#21464;&#24471;&#36807;&#20110;&#24222;&#22823;&#65292;&#26368;&#32456;&#21464;&#24471;&#19981;&#23454;&#29992;&#12290;&#33258;&#27880;&#24847;&#26426;&#21046;&#26159;&#19968;&#31181;DL&#25216;&#26415;&#65292;&#35797;&#22270;&#27169;&#20223;&#26377;&#36873;&#25321;&#22320;&#32858;&#28966;&#20110;&#19968;&#20123;&#30456;&#20851;&#20107;&#29289;&#32780;&#24573;&#30053;&#20854;&#20182;&#20107;&#29289;&#30340;&#34892;&#20026;&#12290;&#26368;&#36817;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;Transformer&#26550;&#26500;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#25104;&#21151;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#20940;&#21464;&#20449;&#21495;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#26088;&#22312;&#39640;&#25928;&#20934;&#30830;&#22320;&#25429;&#25417;&#20940;&#21464;&#20449;&#21495;&#24182;&#23558;&#20854;&#19982;&#35823;&#21028;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#26469;&#20445;&#30041;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;CNN&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current space-based missions, such as the Transiting Exoplanet Survey Satellite (TESS), provide a large database of light curves that must be analysed efficiently and systematically. In recent years, deep learning (DL) methods, particularly convolutional neural networks (CNN), have been used to classify transit signals of candidate exoplanets automatically. However, CNNs have some drawbacks; for example, they require many layers to capture dependencies on sequential data, such as light curves, making the network so large that it eventually becomes impractical. The self-attention mechanism is a DL technique that attempts to mimic the action of selectively focusing on some relevant things while ignoring others. Models, such as the Transformer architecture, were recently proposed for sequential data with successful results. Based on these successful models, we present a new architecture for the automatic classification of transit signals. Our proposed architecture is designed to capture t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#20013;&#30340;&#20256;&#24863;&#21644;&#25968;&#25454;&#22788;&#29702;&#29616;&#29366;&#65292;&#21450;&#20854;&#23545;&#33021;&#28304;&#21644;&#29615;&#22659;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36827;&#19968;&#27493;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#22914;&#20309;&#24212;&#29992;&#21644;&#25972;&#21512;&#29616;&#26377;&#25216;&#26415;&#21019;&#26032;&#20197;&#23454;&#29616;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14271</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#33021;&#28304;&#39640;&#25928;&#36817;&#20284;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Approximate Edge AI for Energy Efficient Autonomous Driving Services. (arXiv:2304.14271v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14271
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#20013;&#30340;&#20256;&#24863;&#21644;&#25968;&#25454;&#22788;&#29702;&#29616;&#29366;&#65292;&#21450;&#20854;&#23545;&#33021;&#28304;&#21644;&#29615;&#22659;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36827;&#19968;&#27493;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#22914;&#20309;&#24212;&#29992;&#21644;&#25972;&#21512;&#29616;&#26377;&#25216;&#26415;&#21019;&#26032;&#20197;&#23454;&#29616;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#65292;&#22914;&#25668;&#20687;&#26426;&#12289;&#28608;&#20809;&#38647;&#36798;&#12289;&#38647;&#36798;&#21644;&#36890;&#20449;&#27169;&#22359;&#12290;&#22788;&#29702;&#20256;&#24863;&#25968;&#25454;&#30340;&#36890;&#24120;&#20570;&#27861;&#26159;&#22312;&#36710;&#36742;&#20869;&#37096;&#25918;&#32622;&#39640;&#24615;&#33021;&#35745;&#31639;&#21333;&#20803;&#65292;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#20316;&#20026;&#36710;&#36742;&#30340;&#22823;&#33041;&#25110;&#31649;&#29702;&#21592;&#12290;&#20174;&#24179;&#22343;&#34892;&#39542;&#26102;&#38388;&#29983;&#25104;&#30340;&#36710;&#36742;&#25968;&#25454;&#21487;&#20197;&#39640;&#36798;20TB&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36895;&#29575;&#21644;&#35268;&#26684;&#12290;&#37492;&#20110;&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#30340;&#35268;&#27169;&#21644;&#24555;&#36895;&#22686;&#38271;&#65292;&#23588;&#20854;&#26159;&#22312;&#21521;&#36710;&#36742;&#30005;&#27668;&#21270;&#65288;&#20363;&#22914;&#65292;&#30005;&#27744;&#39537;&#21160;&#65289;&#30340;&#36235;&#21183;&#20013;&#65292;&#25552;&#39640;&#24635;&#20307;&#33021;&#28304;&#21644;&#29615;&#22659;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290; &#34429;&#28982;&#36825;&#20123;&#39046;&#22495;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#12289;&#26080;&#32447;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;AI / ML&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22914;&#20309;&#24212;&#29992;&#21644;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#21019;&#26032;&#20197;&#23454;&#29616;&#33021;&#28304;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#36830;&#25509;&#30340;&#36710;&#36742;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving services rely heavily on sensors such as cameras, LiDAR, radar, and communication modules. A common practice of processing the sensed data is using a high-performance computing unit placed inside the vehicle, which deploys AI models and algorithms to act as the brain or administrator of the vehicle. The vehicular data generated from average hours of driving can be up to 20 Terabytes depending on the data rate and specification of the sensors. Given the scale and fast growth of services for autonomous driving, it is essential to improve the overall energy and environmental efficiency, especially in the trend towards vehicular electrification (e.g., battery-powered). Although the areas have seen significant advancements in sensor technologies, wireless communications, computing and AI/ML algorithms, the challenge still exists in how to apply and integrate those technology innovations to achieve energy efficiency. This survey reviews and compares the connected vehicular
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14251</link><description>&lt;p&gt;
&#31616;&#21270;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#20854;&#25512;&#23548;&#36807;&#31243;&#21487;&#33021;&#24456;&#32321;&#29712;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#23547;&#25214;&#20851;&#20110;&#24050;&#30693;&#20998;&#24067;&#26399;&#26395;&#30340;&#32447;&#24615;&#24615;&#65292;&#26469;&#30830;&#23450;&#21518;&#39564;&#20998;&#24067;&#24418;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#8220;&#35835;&#21462;&#8221;&#36825;&#20123;&#26399;&#26395;&#21069;&#30340;&#39033;&#65292;&#20889;&#20986;&#26356;&#26032;&#12290;&#36825;&#20010;&#26041;&#27861;&#20351;&#24471;&#25512;&#23548;&#26356;&#21152;&#31616;&#21333;&#65292;&#24555;&#36895;&#65292;&#31616;&#30701;&#21644;&#36890;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#8212;&#8212;&#31435;&#22330;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;SLTL&#65289;&#65292;&#23427;&#23558; LTL &#30340;&#26102;&#38388;&#29305;&#24615;&#19982; SL &#30340;&#22810;&#35270;&#35282;&#24314;&#27169;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20026;&#25193;&#23637;&#29616;&#26377;&#30340; LTL &#25512;&#29702;&#22120;&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.14243</link><description>&lt;p&gt;
&#31435;&#22330;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Standpoint Linear Temporal Logic. (arXiv:2304.14243v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#8212;&#8212;&#31435;&#22330;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;SLTL&#65289;&#65292;&#23427;&#23558; LTL &#30340;&#26102;&#38388;&#29305;&#24615;&#19982; SL &#30340;&#22810;&#35270;&#35282;&#24314;&#27169;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20026;&#25193;&#23637;&#29616;&#26377;&#30340; LTL &#25512;&#29702;&#22120;&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22797;&#26434;&#22330;&#26223;&#38656;&#35201;&#21327;&#35843;&#25317;&#26377;&#29420;&#29305;&#35266;&#28857;&#21644;&#19981;&#21516;&#35821;&#20041;&#25215;&#35834;&#30340;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31435;&#22330;&#36923;&#36753;&#65288;SL&#65289;&#26469;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#65292;&#36890;&#36807;&#32034;&#24341;&#27169;&#24577;&#21487;&#20197;&#25512;&#29702;&#20986;&#19981;&#21516;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#35266;&#28857;&#12290;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#22810;&#27169;&#24577;&#36923;&#36753;&#26159;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#23427;&#26159;&#34920;&#36798;&#31995;&#32479;&#21644;&#36807;&#31243;&#26102;&#38388;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#22312;&#24418;&#24335;&#21270;&#26041;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#36923;&#36753;&#8212;&#8212;&#31435;&#22330;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;SLTL&#65289;&#65292;&#23427;&#23558; LTL &#30340;&#26102;&#38388;&#29305;&#24615;&#19982; SL &#30340;&#22810;&#35270;&#35282;&#24314;&#27169;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102; SLTL &#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#65292;&#30830;&#23450;&#20102;&#20854;&#21487;&#20915;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32456;&#27490;&#30340;&#34920;&#28436;&#28436;&#31639;&#27861;&#26469;&#33258;&#21160;&#21270; SLTL &#25512;&#29702;&#12290;&#36825;&#20026;&#25193;&#23637;&#29616;&#26377;&#30340; LTL &#25512;&#29702;&#22120;&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#36335;&#24452;&#65292;&#24182;&#24102;&#26469;&#20102;&#23454;&#29992;&#25512;&#29702;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many complex scenarios require the coordination of agents possessing unique points of view and distinct semantic commitments. In response, standpoint logic (SL) was introduced in the context of knowledge integration, allowing one to reason with diverse and potentially conflicting viewpoints by means of indexed modalities. Another multi-modal logic of import is linear temporal logic (LTL) - a formalism used to express temporal properties of systems and processes, having prominence in formal methods and fields related to artificial intelligence. In this paper, we present standpoint linear temporal logic (SLTL), a new logic that combines the temporal features of LTL with the multi-perspective modelling capacity of SL. We define the logic SLTL, its syntax, and its semantics, establish its decidability and complexity, and provide a terminating tableau calculus to automate SLTL reasoning. Conveniently, this offers a clear path to extend existing LTL reasoners with practical reasoning support
&lt;/p&gt;</description></item><item><title>TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14226</link><description>&lt;p&gt;
TorchBench: &#29992;&#39640;API&#34920;&#38754;&#35206;&#30422;&#29575;&#35780;&#20272;PyTorch&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14226
&lt;/p&gt;
&lt;p&gt;
TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#26041;&#20415;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;PyTorch&#26159;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;PyTorch&#36719;&#20214;&#26632;&#30340;&#29983;&#24577;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#24182;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TorchBench&#65292;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#30740;&#31350;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19981;&#21516;&#65292;TorchBench&#21253;&#21547;&#20102;&#35768;&#22810;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;PyTorch API&#34920;&#38754;&#12290;TorchBench&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TorchBench&#30340;&#20004;&#20010;&#23454;&#38469;&#29992;&#20363;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#23545;TorchBench&#36827;&#34892;&#24615;&#33021;&#21078;&#26512;&#65292;&#20197;&#35782;&#21035;PyTorch&#30340;GPU&#24615;&#33021;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35768;&#22810;&#24615;&#33021;&#25925;&#38556;&#24182;&#21521;&#19978;&#28216;&#25552;&#20132;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
&lt;/p&gt;</description></item><item><title>LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.14211</link><description>&lt;p&gt;
LLT&#65306;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14211
&lt;/p&gt;
&lt;p&gt;
LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#36716;&#25442;(LLT )&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;LLT R&#21253;&#20197;&#28789;&#27963;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35813;&#31639;&#27861;&#12290;&#35813;&#21253;&#23558;&#23454;&#20363;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#65292;&#24182;&#21033;&#29992;&#26102;&#24310;&#23884;&#20837;&#21644;&#35889;&#20998;&#35299;&#25216;&#26415;&#65292;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;(&#21021;&#22987;&#29305;&#24449;)&#30340;&#25511;&#21046;&#27169;&#24335;(&#31216;&#20026;&#32447;&#24615;&#23450;&#24459;)&#12290;&#26368;&#21518;&#65292;&#23427;&#24212;&#29992;&#35757;&#32451;&#38598;&#30340;&#32447;&#24615;&#23450;&#24459;&#26469;&#36716;&#25442;&#27979;&#35797;&#38598;&#30340;&#21021;&#22987;&#29305;&#24449;&#12290;trainTest&#12289;trainLaw&#21644;testTrans&#19977;&#20010;&#21333;&#29420;&#30340;&#20989;&#25968;&#26469;&#25191;&#34892;&#36825;&#20123;&#27493;&#39588;&#65292;&#23427;&#20204;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#32467;&#26500;;&#28982;&#32780;&#65292;&#20026;&#20102;&#24555;&#36895;&#35745;&#31639;&#65292;&#23427;&#20204;&#21482;&#20351;&#29992;&#20869;&#32622;&#20989;&#25968;&#12290;LLT R&#21253;&#21644;&#36866;&#24403;&#25968;&#25454;&#32467;&#26500;&#30340;&#31034;&#20363;&#25968;&#25454;&#38598;&#22312;GitHub&#19978;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#30103;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;MOTOR&#65292;&#26088;&#22312;&#23454;&#29616;&#21307;&#30103;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65292;&#23558;&#36890;&#29992;&#21644;&#29305;&#23450;&#30693;&#35782;&#34701;&#21512;&#65292;&#20849;&#21516;&#20419;&#36827;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14204</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#23454;&#29616;&#21307;&#30103;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining. (arXiv:2304.14204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#30103;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;MOTOR&#65292;&#26088;&#22312;&#23454;&#29616;&#21307;&#30103;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65292;&#23558;&#36890;&#29992;&#21644;&#29305;&#23450;&#30693;&#35782;&#34701;&#21512;&#65292;&#20849;&#21516;&#20419;&#36827;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;MAGI&#65289;&#21487;&#20197;&#21033;&#29992;&#20849;&#20139;&#30340;&#21307;&#30103;&#30693;&#35782;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#21307;&#30103;&#20219;&#21153;&#65292;&#20943;&#23569;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#30103;&#25968;&#25454;&#26377;&#38480;&#19988;&#22797;&#26434;&#65292;&#35774;&#35745;&#20855;&#26377;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#21313;&#20998;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR)&#30340;&#26032;&#33539;&#20363;&#65292;&#20197;&#23454;&#29616;MAGI&#12290;&#22312;MOTOR&#20013;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#22522;&#26412;&#21307;&#30103;&#30693;&#35782;&#8212;&#8212;&#36890;&#29992;&#21644;&#29305;&#23450;&#30693;&#35782;&#34701;&#21512;&#65292;&#20849;&#21516;&#20419;&#36827;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#20854;&#20174;&#25918;&#23556;&#23398;&#25968;&#25454;&#20013;&#23398;&#20064;&#32039;&#20945;&#30340;&#34920;&#24449;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#12290;MOTOR&#32479;&#19968;&#20102;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#26159;&#23454;&#29616;&#21307;&#30103;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical artificial general intelligence (MAGI) enables one foundation model to solve different medical tasks, which is very practical in the medical domain. It can significantly reduce the requirement of large amounts of task-specific data by sufficiently sharing medical knowledge among different tasks. However, due to the challenges of designing strongly generalizable models with limited and complex medical data, most existing approaches tend to develop task-specific models. To take a step towards MAGI, we propose a new paradigm called Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR). In MOTOR, we combine two kinds of basic medical knowledge, i.e., general and specific knowledge, in a complementary manner to boost the general pretraining process. As a result, the foundation model with comprehensive basic knowledge can learn compact representations from pretraining radiographic data for better cross-modal alignment. MOTOR unifies the understanding and generation, which are two
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.14177</link><description>&lt;p&gt;
ChatGPT&#19982;&#29616;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22312;&#35780;&#20272;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#22810;&#23569;&#30740;&#31350;&#65292;&#36825;&#28041;&#21450;&#21040;&#20934;&#30830;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#30340;&#20449;&#24687;&#24615;&#30701;&#35821;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#23558;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#34920;&#29616;&#19982;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#23427;&#20316;&#20026;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#21644;&#38271;&#25991;&#26723;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#31185;&#23398;&#25991;&#31456;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#20845;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#22312;&#30701;&#25991;&#26723;&#21644;&#38271;&#25991;&#26723;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#65292;ChatGPT&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#29616;&#26377;&#27169;&#22411;&#65292;&#20135;&#29983;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#39640;&#36136;&#37327;&#20851;&#38190;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models, including ChatGPT, have demonstrated exceptional performance in various natural language generation tasks. However, there has been limited research evaluating ChatGPT's keyphrase generation ability, which involves identifying informative phrases that accurately reflect a document's content. This study seeks to address this gap by comparing ChatGPT's keyphrase generation performance with state-of-the-art models, while also testing its potential as a solution for two significant challenges in the field: domain adaptation and keyphrase generation from long documents. We conducted experiments on six publicly available datasets from scientific articles and news domains, analyzing performance on both short and long documents. Our results show that ChatGPT outperforms current state-of-the-art models in all tested datasets and environments, generating high-quality keyphrases that adapt well to diverse domains and document lengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#28857;&#20113;&#19978;&#36827;&#34892;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#22270;&#32467;&#26500;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21547;&#26377;&#20840;&#23616;&#29305;&#24449;&#27169;&#22359;&#21644;&#39034;&#24207;&#29305;&#24449;&#27169;&#22359;&#30340;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#31232;&#30095;&#21644;&#26102;&#38388;&#25299;&#25169;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14132</link><description>&lt;p&gt;
&#20351;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#28857;&#20113;&#36827;&#34892;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Human Semantic Segmentation using Millimeter-Wave Radar Sparse Point Clouds. (arXiv:2304.14132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#28857;&#20113;&#19978;&#36827;&#34892;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#22270;&#32467;&#26500;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21547;&#26377;&#20840;&#23616;&#29305;&#24449;&#27169;&#22359;&#21644;&#39034;&#24207;&#29305;&#24449;&#27169;&#22359;&#30340;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#31232;&#30095;&#21644;&#26102;&#38388;&#25299;&#25169;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#26102;&#24207;&#28857;&#20113;&#19978;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26694;&#26550;&#12290;&#30456;&#27604;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#20855;&#26377;&#19981;&#27844;&#38706;&#38544;&#31169;&#12289;&#24378;&#25239;&#24178;&#25200;&#33021;&#21147;&#21644;&#38271;&#26816;&#27979;&#36317;&#31163;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#27627;&#31859;&#27874;&#25968;&#25454;&#30340;&#31232;&#30095;&#21644;&#26102;&#38388;&#25299;&#25169;&#29305;&#24449;&#30340;&#25429;&#33719;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#20197;&#21069;&#30340;&#20808;&#36827;&#20998;&#21106;&#26041;&#27861; (&#22914;PointNet&#12289;PointCNN&#12289;Point Transformer) &#27809;&#26377;&#34987;&#20805;&#20998;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework for semantic segmentation on sparse sequential point clouds of millimeter-wave radar. Compared with cameras and lidars, millimeter-wave radars have the advantage of not revealing privacy, having a strong anti-interference ability, and having long detection distance. The sparsity and capturing temporal-topological features of mmWave data is still a problem. However, the issue of capturing the temporal-topological coupling features under the human semantic segmentation task prevents previous advanced segmentation methods (e.g PointNet, PointCNN, Point Transformer) from being well utilized in practical scenarios. To address the challenge caused by the sparsity and temporal-topological feature of the data, we (i) introduce graph structure and topological features to the point cloud, (ii) propose a semantic segmentation framework including a global feature-extracting module and a sequential feature-extracting module. In addition, we design an efficient and mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#27169;&#22411;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#12289;&#29702;&#35299;&#21147;&#21644;&#20449;&#20219;&#24230;&#20197;&#21450;&#33258;&#20449;&#24515;&#23545;&#20182;&#20204;&#30340;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.14130</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#19981;&#24182;&#23384;&#65311;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#21450;&#33258;&#20449;&#24515;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Why not both? Complementing explanations with uncertainty, and the role of self-confidence in Human-AI collaboration. (arXiv:2304.14130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#27169;&#22411;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#12289;&#29702;&#35299;&#21147;&#21644;&#20449;&#20219;&#24230;&#20197;&#21450;&#33258;&#20449;&#24515;&#23545;&#20182;&#20204;&#30340;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#21009;&#20107;&#21496;&#27861;&#31561;&#20851;&#38190;&#39046;&#22495;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23436;&#20840;&#33258;&#21160;&#21270;&#36825;&#20123;&#39640;&#39118;&#38505;&#24212;&#29992;&#21487;&#33021;&#24341;&#21457;&#36947;&#24503;&#25110;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#24212;&#35813;&#30001;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#21327;&#21161;&#20154;&#31867;&#65292;&#20351;&#24471;&#21452;&#26041;&#36890;&#36807;&#20132;&#27969;&#36798;&#25104;&#20849;&#21516;&#20915;&#31574;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26469;&#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#27169;&#22411;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#12289;&#29702;&#35299;&#21147;&#21644;&#20449;&#20219;&#24230;&#65292;&#24182;&#23547;&#27714;&#23558;&#20108;&#32773;&#32467;&#21512;&#36215;&#26469;&#24102;&#26469;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35797;&#22270;&#35780;&#20272;&#29992;&#25143;&#33258;&#20449;&#24515;&#23545;&#20182;&#20204;&#30340;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#35752;&#35770;&#21518;&#32773;&#22914;&#20309;&#25197;&#26354;&#22522;&#20110;&#21327;&#35758;&#21644;&#20999;&#25442;&#30334;&#20998;&#27604;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI and ML models have already found many applications in critical domains, such as healthcare and criminal justice. However, fully automating such high-stakes applications can raise ethical or fairness concerns. Instead, in such cases, humans should be assisted by automated systems so that the two parties reach a joint decision, stemming out of their interaction. In this work we conduct an empirical study to identify how uncertainty estimates and model explanations affect users' reliance, understanding, and trust towards a model, looking for potential benefits of bringing the two together. Moreover, we seek to assess how users' behaviour is affected by their own self-confidence in their abilities to perform a certain task, while we also discuss how the latter may distort the outcome of an analysis based on agreement and switching percentages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#20559;&#22909;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#37327;&#21270;&#20559;&#22909;&#26435;&#37325;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#30456;&#36739;&#20110;&#22522;&#32447;&#31639;&#27861;&#26377;&#30528;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25512;&#26029;&#23545;&#25163;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.14126</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#22522;&#20110;&#28436;&#31034;&#30340;&#20559;&#22909;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Preference Inference from Demonstration in Multi-objective Multi-agent Decision Making. (arXiv:2304.14126v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#20559;&#22909;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#37327;&#21270;&#20559;&#22909;&#26435;&#37325;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#30456;&#36739;&#20110;&#22522;&#32447;&#31639;&#27861;&#26377;&#30528;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25512;&#26029;&#23545;&#25163;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#37327;&#21270;&#19981;&#21516;&#30446;&#26631;&#30340;&#25968;&#20540;&#20559;&#22909;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#28436;&#31034;&#36890;&#24120;&#26159;&#21487;&#35775;&#38382;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#26368;&#20248;&#25110;&#36817;&#26368;&#20248;&#30340;&#28436;&#31034;&#20013;&#25512;&#26029;&#32447;&#24615;&#20559;&#22909;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#65292;&#24182;&#19982;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#25512;&#26029;&#30340;&#20559;&#22909;&#30340;&#26102;&#38388;&#35201;&#27714;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#65292;&#31639;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35745;&#21010;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35780;&#20272;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#30340;&#19968;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#22909;&#25512;&#26029;&#31639;&#27861;&#25512;&#26029;&#23545;&#25163;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to quantify numerical preferences for different objectives in a multi-objective decision-making problem. However, the demonstrations of a user are often accessible. We propose an algorithm to infer linear preference weights from either optimal or near-optimal demonstrations. The algorithm is evaluated in three environments with two baseline methods. Empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both time requirements and accuracy of the inferred preferences. In future work, we plan to evaluate the algorithm's effectiveness in a multi-agent system, where one of the agents is enabled to infer the preferences of an opponent using our preference inference algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26435;&#37325;&#30340;&#20559;&#22909;&#25512;&#29702;&#31639;&#27861;(DWPI)&#65292;&#36890;&#36807;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#36712;&#36857;&#65292;&#21487;&#20197;&#25512;&#26029;&#25191;&#34892;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DWPI&#22312;&#22810;&#20010;&#22810;&#30446;&#26631;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20004;&#31181;&#29616;&#26377;&#20559;&#22909;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14115</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#21160;&#24577;&#26435;&#37325;&#30340;&#28436;&#31034;&#20559;&#22909;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach. (arXiv:2304.14115v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26435;&#37325;&#30340;&#20559;&#22909;&#25512;&#29702;&#31639;&#27861;(DWPI)&#65292;&#36890;&#36807;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#36712;&#36857;&#65292;&#21487;&#20197;&#25512;&#26029;&#25191;&#34892;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DWPI&#22312;&#22810;&#20010;&#22810;&#30446;&#26631;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20004;&#31181;&#29616;&#26377;&#20559;&#22909;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20915;&#31574;&#38382;&#39064;&#37117;&#28041;&#21450;&#22810;&#20010;&#30446;&#26631;&#65292;&#32780;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19981;&#19968;&#23450;&#24635;&#26159;&#33021;&#30693;&#36947;&#20915;&#31574;&#32773;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#20915;&#31574;&#32773;&#30340;&#34892;&#20026;&#24448;&#24448;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#20013;&#65292;&#20559;&#22909;&#25512;&#29702;&#26159;&#25512;&#26029;&#20915;&#31574;&#32773;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#20559;&#22909;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26435;&#37325;&#30340;&#20559;&#22909;&#25512;&#29702;&#31639;&#27861;(DWPI)&#65292;&#36890;&#36807;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#36712;&#36857;&#65292;&#21487;&#20197;&#25512;&#26029;&#25191;&#34892;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Deep Sea Treasure&#12289;Traffic&#21644;Item Gathering)&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#20004;&#31181;&#29616;&#26377;&#20559;&#22909;&#25512;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;DWPI&#26041;&#27861;&#22312;&#22522;&#32447;&#31639;&#27861;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many decision-making problems feature multiple objectives. In such problems, it is not always possible to know the preferences of a decision-maker for different objectives. However, it is often possible to observe the behavior of decision-makers. In multi-objective decision-making, preference inference is the process of inferring the preferences of a decision-maker for different objectives. This research proposes a Dynamic Weight-based Preference Inference (DWPI) algorithm that can infer the preferences of agents acting in multi-objective decision-making problems, based on observed behavior trajectories in the environment. The proposed method is evaluated on three multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item Gathering. The performance of the proposed DWPI approach is compared to two existing preference inference methods from the literature, and empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both
&lt;/p&gt;</description></item><item><title>ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14106</link><description>&lt;p&gt;
ChatLog: &#35760;&#24405;&#21644;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14106
&lt;/p&gt;
&lt;p&gt;
ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;ChatGPT&#30340;&#30740;&#31350;&#65292;&#20294;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;ChatGPT&#30340;&#34892;&#20026;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#31895;&#21040;&#32454;&#30340;&#26102;&#38388;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ChatLog&#65292;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#27599;&#26376;&#21644;&#27599;&#22825;&#26356;&#26032;&#65306;ChatLog-Monthly&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#27599;&#20010;&#26376;&#25910;&#38598;&#30340;38,730&#20010;&#38382;&#39064;-&#22238;&#31572;&#23545;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;ChatLog-Daily&#21253;&#25324;ChatGPT&#27599;&#22825;&#23545;1000&#20010;&#30456;&#21516;&#38382;&#39064;&#30340;&#38271;&#31687;&#22238;&#31572;&#12290;&#25105;&#20204;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;ChatGPT&#36827;&#21270;&#27169;&#24335;&#23384;&#22312;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#21462;&#20854;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#20998;&#26512;&#20102;ChatGPT&#38543;&#26102;&#38388;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;RoBERTa&#30340;&#26816;&#27979;&#22120;&#22312;&#26032;&#29256;&#26412;&#30340;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#32487;&#32493;&#32500;&#25252;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.14102</link><description>&lt;p&gt;
SocNavGym&#65306;&#19968;&#20010;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#20223;&#30495;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#29615;&#22659;&#19979;&#65292;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#23548;&#33322;&#26102;&#38656;&#35201;&#36981;&#23432;&#31038;&#20132;&#35268;&#33539;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#36817;&#22312;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110;&#29983;&#25104;&#30340;&#31574;&#30053;&#19981;&#21463;&#20195;&#30721;&#22797;&#26434;&#24615;&#25110;&#22788;&#29702;&#30340;&#21464;&#37327;&#25968;&#37327;&#31561;&#20154;&#31867;&#38480;&#21046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;DRL&#31639;&#27861;&#32570;&#20047;&#23433;&#20840;&#20445;&#38556;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#65292;&#23548;&#33268;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#19981;&#22826;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#20223;&#30495;&#29615;&#22659;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#20808;&#36827;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;SocNavGym&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#21487;&#36731;&#26494;&#37197;&#32622;&#20197;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#37197;&#32622;&#20026;&#20351;&#29992;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#65292;&#24182;&#25903;&#25345;&#26080;&#38556;&#30861;&#29615;&#22659;&#30340;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14094</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#30068;&#22522;&#30784;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#24418;&#24335;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#22238;&#31572;&#19982;AI&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#35780;&#35770;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#25968;&#23398;&#22522;&#30784;&#26469;&#23450;&#20041;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#21363;&#20351;&#8220;&#35299;&#37322;&#8221;&#36825;&#20010;&#26415;&#35821;&#36824;&#32570;&#20047;&#31934;&#30830;&#23450;&#20041;&#12290;&#36825;&#20123;&#35780;&#35770;&#36824;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#20581;&#20840;&#32780;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;AI&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#19981;&#33391;&#25552;&#20986;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#27983;&#35272;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30693;&#35782;&#20307;&#31995;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#26159;&#22635;&#34917;&#35813;&#31354;&#30333;&#30340;&#39318;&#27425;&#23581;&#35797;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#21453;&#39304;&#21333;&#35843;&#33539;&#30068;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;AI&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36981;&#24490;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24341;&#20837;&#30340;&#29702;&#35770;&#26469;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#25152;&#26377;&#20027;&#35201;XAI&#31995;&#32479;&#31867;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;&#26694;&#26550;ClusterFlow&#65292;&#21487;&#20197;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#23494;&#38598;&#30340;&#22810;&#32500;&#31867;&#21035;&#21644;&#29305;&#24449;&#25968;&#25454;&#26469;&#26500;&#24314;&#36229;&#31354;&#38388;&#22320;&#22270;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#20855;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#30340;&#21151;&#33021;&#65292;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23454;&#29616;&#20851;&#31995;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14081</link><description>&lt;p&gt;
&#32858;&#31867;&#27969;&#65288;Cluster Flow&#65289;&#65306;&#22914;&#20309;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20855;&#40065;&#26834;&#24615;&#12289;&#26356;&#31526;&#21512;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#12289;&#26356;&#26131;&#20110;&#23454;&#29616;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning. (arXiv:2304.14081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;&#26694;&#26550;ClusterFlow&#65292;&#21487;&#20197;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#23494;&#38598;&#30340;&#22810;&#32500;&#31867;&#21035;&#21644;&#29305;&#24449;&#25968;&#25454;&#26469;&#26500;&#24314;&#36229;&#31354;&#38388;&#22320;&#22270;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#20855;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#30340;&#21151;&#33021;&#65292;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23454;&#29616;&#20851;&#31995;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65306;&#23427;&#20204;&#21487;&#20197;&#34987;&#26234;&#33021;&#25915;&#20987;&#65292;&#32780;&#20154;&#20204;&#26080;&#27861;&#34987;&#27450;&#39575;&#65292;&#20063;&#32570;&#20047;&#24120;&#35782;&#12290;&#24050;&#32463;&#26377;&#20154;&#35748;&#20026;&#65292;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#30784;&#26159;&#20154;&#31867;&#33021;&#22815;&#36827;&#34892;&#20851;&#31995;&#25512;&#29702;&#30340;&#33021;&#21147;&#65306;&#27604;&#36739;&#19981;&#21516;&#23545;&#35937;&#65292;&#27979;&#37327;&#30456;&#20284;&#24615;&#65292;&#25484;&#25569;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32858;&#31867;&#27969;&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#30340;&#23618;&#27425;&#32858;&#31867;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#39044; SoftMax &#23618;&#20013;&#21457;&#29616;&#30340;&#20016;&#23500;&#30340;&#22810;&#32500;&#31867;&#21035;&#21644;&#29305;&#24449;&#25968;&#25454;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340; NNs &#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#26500;&#24314;&#31867;/&#29305;&#24449;&#30340;&#36229;&#31354;&#38388;&#22320;&#22270;&#65292;&#24182;&#23558;&#36825;&#20123;&#21151;&#33021;&#28155;&#21152;&#21040;&#29616;&#20195;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26356;&#20855;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#30340;&#21151;&#33021;&#12290;&#20316;&#32773;&#36890;&#36807;3&#20010;&#20219;&#21153;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#21160;&#30011;&#30340;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;HOI&#30340;&#32452;&#21512;&#24335;&#21160;&#30011;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.14070</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;3D&#20154;&#29289;-&#29289;&#20307;&#31070;&#32463;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Compositional 3D Human-Object Neural Animation. (arXiv:2304.14070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#21160;&#30011;&#30340;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;HOI&#30340;&#32452;&#21512;&#24335;&#21160;&#30011;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#23545;&#20110;&#20154;&#31867;&#20013;&#24515;&#30340;&#22330;&#26223;&#29702;&#35299;&#24212;&#29992;&#65292;&#22914;&#20154;&#31867;&#20013;&#24515;&#30340;&#35270;&#35273;&#29983;&#25104;&#12289;AR/VR&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24335;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#21160;&#30011;&#30340;&#25361;&#25112;&#65292;&#21363;&#36890;&#36807;&#26032;&#30340;&#23039;&#21183;&#24207;&#21015;&#65292;&#21160;&#30011;&#21270;&#26032;&#30340;&#20154;&#29289;&#21644;/&#25110;&#26032;&#30340;&#29289;&#20307;&#30340;&#26032;&#30340;&#20132;&#20114;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#20154;&#29289;-&#29289;&#20307;&#21464;&#24418;&#26469;&#24314;&#27169;&#21644;&#28210;&#26579;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;HOI&#21160;&#24577;&#12290;&#20026;&#20102;&#20351;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#23039;&#21183;&#22312;&#19981;&#21516;&#30340;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#36716;&#31227;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#24335;&#26465;&#20214;&#31070;&#32463;&#36752;&#23556;&#22330;(CC-NeRF)&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28508;&#22312;&#30721;&#23558;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#35299;&#32806;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;HOI&#30340;&#32452;&#21512;&#24335;&#21160;&#30011;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#21508;&#31181;&#26032;&#30340;HOI&#21160;&#30011;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-object interactions (HOIs) are crucial for human-centric scene understanding applications such as human-centric visual generation, AR/VR, and robotics. Since existing methods mainly explore capturing HOIs, rendering HOI remains less investigated. In this paper, we address this challenge in HOI animation from a compositional perspective, i.e., animating novel HOIs including novel interaction, novel human and/or novel object driven by a novel pose sequence. Specifically, we adopt neural human-object deformation to model and render HOI dynamics based on implicit neural representations. To enable the interaction pose transferring among different persons and objects, we then devise a new compositional conditional neural radiance field (or CC-NeRF), which decomposes the interdependence between human and object using latent codes to enable compositionally animation control of novel HOIs. Experiments show that the proposed method can generalize well to various novel HOI animation setting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.14068</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#38459;&#27490;&#20102;&#23427;&#20204;&#33719;&#24471;&#23436;&#20840;&#30340;&#20154;&#31867;&#20449;&#20219;&#12290;&#27010;&#24565;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32500;&#27010;&#24565;&#23884;&#20837;&#34920;&#31034;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#27492;&#36136;&#30097;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Concept Reasoner(DCR)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#12290;&#22312;DCR&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#19981;&#30452;&#25509;&#36827;&#34892;&#20219;&#21153;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#27010;&#24565;&#23884;&#20837;&#24314;&#31435;&#35821;&#27861;&#35268;&#21017;&#32467;&#26500;&#12290;&#28982;&#21518;DCR&#22312;&#26377;&#24847;&#20041;&#30340;&#27010;&#24565;&#30495;&#20540;&#24230;&#19978;&#25191;&#34892;&#36825;&#20123;&#35268;&#21017;&#65292;&#20197;&#19981;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#25552;&#20379;&#26368;&#32456;&#30340;&#21487;&#35299;&#37322;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DCR&#65306;(i)&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;;(ii)&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;;(iii)&#24456;&#23481;&#26131;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14065</link><description>&lt;p&gt;
&#38754;&#21521;&#36965;&#24863;&#26102;&#24207;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14065
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#20256;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20250;&#30456;&#20851;&#24212;&#29992;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#30340;&#26631;&#31614;&#21487;&#33021;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#33719;&#24471;&#12290;&#36825;&#20010;&#25361;&#25112;&#24050;&#32463;&#25512;&#21160;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#36965;&#24863;&#25968;&#25454;&#35299;&#38145;&#22312;&#26631;&#35760;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#22320;&#29702;&#20301;&#32622;&#25110;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#36965;&#24863;&#25968;&#25454;&#35774;&#35745;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#23567;&#12289;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Remote Sensing Transformer&#65288;Presto&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#23545;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#21487;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;Presto&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#30830;&#23450;&#20102;&#20004;&#31181;&#22266;&#23450;&#21442;&#25968;&#21487;&#23398;&#20064;&#24615;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#33021;&#22815;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#20998;&#26512;&#39640;&#25928;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;PAC&#23398;&#20064;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.14058</link><description>&lt;p&gt;
PAC&#23398;&#20064;&#30340;&#21442;&#25968;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Parameterized Theory of PAC Learning. (arXiv:2304.14058v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#30830;&#23450;&#20102;&#20004;&#31181;&#22266;&#23450;&#21442;&#25968;&#21487;&#23398;&#20064;&#24615;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#33021;&#22815;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#20998;&#26512;&#39640;&#25928;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;PAC&#23398;&#20064;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#33021;&#20934;&#30830;(&#21363;PAC)&#23398;&#20064;&#26159;&#26679;&#26412;&#22797;&#26434;&#24230;&#29702;&#35770;&#30340;&#26680;&#24515;&#27010;&#24565;&#65292;&#39640;&#25928;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#32463;&#24120;&#34987;&#35270;&#20026;&#32463;&#20856;&#35745;&#31639;&#22797;&#26434;&#24230;P&#31867;&#30340;&#33258;&#28982;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#23835;&#36215;&#20351;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#32463;&#20856;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;P-NP&#8220;&#20108;&#20998;&#27861;&#8221;&#65292;&#24182;&#30830;&#23450;&#35768;&#22810;&#38382;&#39064;&#30340;&#21487;&#22788;&#29702;&#36793;&#30028;&#65292;&#20294;&#22312;&#26679;&#26412;&#22797;&#26434;&#24615;&#39046;&#22495;&#20013;&#27809;&#26377;&#31867;&#20284;&#30340;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#39640;&#25928;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20960;&#20010;&#26368;&#36817;&#21253;&#21547;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#20803;&#32032;&#30340;PAC&#23398;&#20064;&#32467;&#26524;&#36827;&#34892;&#26032;&#30340;&#35299;&#37322;&#12290;&#22312;&#35813;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#27490;&#19968;&#31181;&#22266;&#23450;&#21442;&#25968;&#21487;&#23398;&#20064;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#20004;&#31181;&#27010;&#24565;&#37117;&#26159;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#33539;&#24335;&#20013;&#26680;&#24515;&#27010;&#24565;FPT&#30340;&#29420;&#31435;&#23545;&#24212;&#29289;&#65292;&#24182;&#19988;&#20801;&#35768;&#25105;&#20204;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#20998;&#26512;&#39640;&#25928;&#21644;&#38590;&#20197;&#22788;&#29702;&#30340;PAC&#23398;&#20064;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct (i.e., PAC) learning is a core concept of sample complexity theory, and efficient PAC learnability is often seen as a natural counterpart to the class P in classical computational complexity. But while the nascent theory of parameterized complexity has allowed us to push beyond the P-NP ``dichotomy'' in classical computational complexity and identify the exact boundaries of tractability for numerous problems, there is no analogue in the domain of sample complexity that could push beyond efficient PAC learnability.  As our core contribution, we fill this gap by developing a theory of parameterized PAC learning which allows us to shed new light on several recent PAC learning results that incorporated elements of parameterized complexity. Within the theory, we identify not one but two notions of fixed-parameter learnability that both form distinct counterparts to the class FPT -- the core concept at the center of the parameterized complexity paradigm -- and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;IGANet&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#21333;&#35270;&#35282;&#22270;&#20687;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#24573;&#30053;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#22312;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14045</link><description>&lt;p&gt;
&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Interweaved Graph and Attention Network for 3D Human Pose Estimation. (arXiv:2304.14045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;IGANet&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#21333;&#35270;&#35282;&#22270;&#20687;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#24573;&#30053;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#22312;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21333;&#35270;&#35282;&#22270;&#20687;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20197;&#24448;&#30340;&#24037;&#20316;&#24456;&#23569;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#30456;&#20114;&#20851;&#31995;&#65292;&#23548;&#33268;&#23545;&#20154;&#20307;&#39592;&#39612;&#34920;&#31034;&#30340;&#23398;&#20064;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;IGANet&#65289;&#65292;&#23427;&#20801;&#35768;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGA&#27169;&#22359;&#65292;&#20854;&#20013;GCNs&#21521;&#27880;&#24847;&#21147;&#25552;&#20379;&#23616;&#37096;&#20449;&#24687;&#65292;&#27880;&#24847;&#21147;&#21521;GCNs&#27880;&#20837;&#20840;&#23616;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;uMLP&#65289;&#65292;&#21487;&#20197;&#25429;&#25417;&#36523;&#20307;&#20851;&#33410;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35797;&#39564;&#65288;&#21363;Human3. 6M&#21644;MPI-INF-3DHP&#65289;&#65292;&#20197;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;IGANet&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/xiu-cs/IGANet&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in 3D human pose estimation from a single-view image, prior works rarely explore global and local correlations, leading to insufficient learning of human skeleton representations. To address this issue, we propose a novel Interweaved Graph and Attention Network (IGANet) that allows bidirectional communications between graph convolutional networks (GCNs) and attentions. Specifically, we introduce an IGA module, where attentions are provided with local information from GCNs and GCNs are injected with global information from attentions. Additionally, we design a simple yet effective U-shaped multi-layer perceptron (uMLP), which can capture multi-granularity information for body joints. Extensive experiments on two popular benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate our proposed method.The results show that IGANet achieves state-of-the-art performance on both datasets. Code is available at https://github.com/xiu-cs/IGANet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#22871;&#20214;&#65292;&#20351;&#29992;&#22823;&#22411;&#20844;&#20849;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#38598;&#36827;&#34892;ICD-10&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#26631;&#20934;&#21270;&#25968;&#25454;&#39044;&#22788;&#29702;&#24182;&#24314;&#31435;&#20840;&#38754;&#30340;ICD&#32534;&#30721;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20419;&#36827;&#20102;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#22312;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13998</link><description>&lt;p&gt;
Mimic-IV-ICD&#65306;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification. (arXiv:2304.13998v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#22871;&#20214;&#65292;&#20351;&#29992;&#22823;&#22411;&#20844;&#20849;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#38598;&#36827;&#34892;ICD-10&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#26631;&#20934;&#21270;&#25968;&#25454;&#39044;&#22788;&#29702;&#24182;&#24314;&#31435;&#20840;&#38754;&#30340;ICD&#32534;&#30721;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20419;&#36827;&#20102;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#22312;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#31508;&#35760;&#34987;&#20998;&#37197;ICD&#20195;&#30721;&#65292;&#36825;&#26159;&#19968;&#32452;&#35786;&#26029;&#21644;&#25805;&#20316;&#20195;&#30721;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#26500;&#24314;&#20102;&#39044;&#27979;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;ICD&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#20844;&#20849;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#27169;&#22411;&#32570;&#20047;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#22871;&#20214;&#65292;&#20351;&#29992;&#20174;&#26368;&#36817;&#30340;&#20844;&#20849;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#38598;MIMIC-IV&#34893;&#29983;&#30340;&#22823;&#22411;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#38598;&#36827;&#34892;ICD-10&#32534;&#30721;&#12290;&#25105;&#20204;&#23454;&#26045;&#20102;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;ICD&#32534;&#30721;&#39044;&#27979;&#20219;&#21153;&#26041;&#27861;&#65292;&#20197;&#26631;&#20934;&#21270;&#25968;&#25454;&#39044;&#22788;&#29702;&#24182;&#24314;&#31435;&#20840;&#38754;&#30340;ICD&#32534;&#30721;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#21487;&#37325;&#22797;&#24615;&#21644;&#27169;&#22411;&#27604;&#36739;&#65292;&#21152;&#36895;&#20102;&#23558;&#33258;&#21160;&#21270;ICD&#32534;&#30721;&#24212;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;MIMIC-IV&#25968;&#25454;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;ICD-9&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#27604;MIMIC-III&#26356;&#22810;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#22810;&#30340;ICD&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#20195;&#30721;&#25552;&#20379;&#20102;&#26131;&#20110;&#35775;&#38382;&#30340;&#25968;&#25454;&#22788;&#29702;&#27493;&#39588;&#12289;&#22522;&#20934;&#21019;&#24314;&#21644;&#27169;&#22411;&#35780;&#20272;&#65292;&#25512;&#21160;&#20102;&#33258;&#21160;ICD&#32534;&#30721;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical notes are assigned ICD codes - sets of codes for diagnoses and procedures. In the recent years, predictive machine learning models have been built for automatic ICD coding. However, there is a lack of widely accepted benchmarks for automated ICD coding models based on large-scale public EHR data.  This paper proposes a public benchmark suite for ICD-10 coding using a large EHR dataset derived from MIMIC-IV, the most recent public EHR dataset. We implement and compare several popular methods for ICD coding prediction tasks to standardize data preprocessing and establish a comprehensive ICD coding benchmark dataset. This approach fosters reproducibility and model comparison, accelerating progress toward employing automated ICD coding in future studies. Furthermore, we create a new ICD-9 benchmark using MIMIC-IV data, providing more data points and a higher number of ICD codes than MIMIC-III. Our open-source code offers easy access to data processing steps, benchmark creation, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#36229;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#22270;&#20687;&#19978;&#23398;&#21040;&#20998;&#31163;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#21644;SCAN&#24456;&#22909;&#22320;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13995</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26059;&#36716;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Rotation and Translation Invariant Representation Learning with Implicit Neural Representations. (arXiv:2304.13995v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#36229;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#22270;&#20687;&#19978;&#23398;&#21040;&#20998;&#31163;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#21644;SCAN&#24456;&#22909;&#22320;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#22270;&#20687;&#26159;&#20197;&#20219;&#24847;&#25110;&#38543;&#26426;&#26059;&#36716;&#21644;&#24179;&#31227;&#30340;&#26041;&#24335;&#33719;&#21462;&#30340;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#33719;&#21462;&#19982;&#22270;&#20687;&#26041;&#21521;&#26080;&#20851;&#30340;&#35821;&#20041;&#34920;&#31034;&#26159;&#21487;&#21462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#21644;&#36229;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#23398;&#20064;&#65288;IRL-INR&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IRL-INR&#21487;&#20197;&#22312;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#26356;&#22797;&#26434;&#30340;&#22270;&#20687;&#19978;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#31163;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#34920;&#26126;&#36825;&#20123;&#35821;&#20041;&#34920;&#31034;&#21487;&#20197;&#19982;SCAN&#24456;&#22909;&#22320;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many computer vision applications, images are acquired with arbitrary or random rotations and translations, and in such setups, it is desirable to obtain semantic representations disentangled from the image orientation. Examples of such applications include semiconductor wafer defect inspection, plankton microscope images, and inference on single-particle cryo-electron microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant Representation Learning with Implicit Neural Representation (IRL-INR), which uses an implicit neural representation (INR) with a hypernetwork to obtain semantic representations disentangled from the orientation of the image. We show that IRL-INR can effectively learn disentangled semantic representations on more complex images compared to those considered in prior works and show that these semantic representations synergize well with SCAN to produce state-of-the-art unsupervised clustering results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20026;&#29992;&#25143;&#21644;&#39033;&#30446;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.13937</link><description>&lt;p&gt;
&#29992;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Collaborative Filtering with Taste Clusters Learning. (arXiv:2304.13937v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20026;&#29992;&#25143;&#21644;&#39033;&#30446;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#19988;&#26377;&#25928;&#30340;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28508;&#22312;&#23884;&#20837;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#12289;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#21644;LightGCN&#65289;&#24050;&#32463;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#32473;&#25512;&#33616;&#27169;&#22411;&#28155;&#21152;&#35299;&#37322;&#24615;&#65292;&#19981;&#20165;&#21487;&#20197;&#22686;&#21152;&#20154;&#20204;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#32780;&#19988;&#36824;&#26377;&#22810;&#20010;&#22909;&#22788;&#65292;&#22914;&#20026;&#39033;&#30446;&#25512;&#33616;&#25552;&#20379;&#26377;&#35828;&#26381;&#21147;&#30340;&#35299;&#37322;&#12289;&#20026;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#26126;&#30830;&#30340;&#25991;&#20214;&#12289;&#20026;&#39033;&#30446;&#21046;&#36896;&#21830;&#25552;&#20379;&#35774;&#35745;&#25913;&#36827;&#30340;&#21327;&#21161;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#26224;&#26377;&#25928;&#30340;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#23398;&#20064;&#26469;&#23454;&#29616;&#20004;&#20010;&#26368;&#33499;&#21051;&#30340;&#30446;&#26631;&#65306;&#65288;1&#65289;&#31934;&#30830;&#8212;&#8212;&#27169;&#22411;&#22312;&#36861;&#27714;&#21487;&#35299;&#37322;&#24615;&#26102;&#19981;&#24212;&#22949;&#21327;&#20934;&#30830;&#24615;&#65307;&#65288;2&#65289;&#33258;&#25105;&#35299;&#37322;&#8212;&#8212;&#27169;&#22411;&#30340;&#35299;&#37322;&#24212;&#26131;&#20110;&#20154;&#20204;&#29702;&#35299;&#12290;&#24341;&#20837;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#26469;&#26500;&#25104;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#30340;&#21516;&#26102;&#20445;&#35777;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements.  In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#23545;&#34920;&#29616;&#26356;&#22909;&#30340;&#23569;&#25968;&#26063;&#35028;&#36827;&#34892;&#36807;&#37319;&#26679;&#20250;&#31245;&#24494;&#20943;&#23569;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13933</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#34920;&#29616;&#26356;&#22909;&#30340;&#23569;&#25968;&#26063;&#32676;&#36827;&#34892;&#36807;&#37319;&#26679;&#20250;&#31245;&#24494;&#38477;&#20302;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy. (arXiv:2304.13933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#23545;&#34920;&#29616;&#26356;&#22909;&#30340;&#23569;&#25968;&#26063;&#35028;&#36827;&#34892;&#36807;&#37319;&#26679;&#20250;&#31245;&#24494;&#20943;&#23569;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#36827;&#34892;&#21592;&#24037;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#23545;&#23569;&#25968;&#26063;&#35028;&#65288;&#40657;&#20154;&#21644;&#35199;&#29677;&#29273;&#35028;&#65289;&#36827;&#34892;&#20102;&#27424;&#37319;&#26679;&#21644;&#36807;&#37319;&#26679;&#65292;&#20197;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#24433;&#21709;&#27604;&#29575;&#65292;&#24182;&#35843;&#26597;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#33391;&#24433;&#21709;&#27604;&#29575;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#33391;&#24433;&#21709;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#30003;&#35831;&#20154;&#30340;&#33258;&#25105;&#25253;&#21578;&#21644;&#38754;&#35797;&#35760;&#24405;&#65288;N = 2,501&#65289;&#35757;&#32451;&#20102;9,702&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#31579;&#36873;&#20915;&#31574;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#24433;&#21709;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#33391;&#24433;&#21709;&#21576;&#32447;&#24615;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#28040;&#38500;&#19981;&#33391;&#24433;&#21709;&#20165;&#31245;&#24494;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#32780;&#19988;&#20063;&#38477;&#20302;&#20102;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organizations are increasingly adopting machine learning (ML) for personnel assessment. However, concerns exist about fairness in designing and implementing ML assessments. Supervised ML models are trained to model patterns in data, meaning ML models tend to yield predictions that reflect subgroup differences in applicant attributes in the training data, regardless of the underlying cause of subgroup differences. In this study, we systematically under- and oversampled minority (Black and Hispanic) applicants to manipulate adverse impact ratios in training data and investigated how training data adverse impact ratios affect ML model adverse impact and accuracy. We used self-reports and interview transcripts from job applicants (N = 2,501) to train 9,702 ML models to predict screening decisions. We found that training data adverse impact related linearly to ML model adverse impact. However, removing adverse impact from training data only slightly reduced ML model adverse impact and tende
&lt;/p&gt;</description></item><item><title>NIMS-OS&#26159;&#19968;&#27454;Python&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#26448;&#26009;&#31185;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#23454;&#39564;&#20043;&#38388;&#38381;&#29615;&#30340;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#26080;&#38480;&#30446;&#26631;&#33258;&#30001;&#25506;&#32034;&#12289;&#30456;&#22270;&#26500;&#24314;&#21644;&#38543;&#26426;&#25506;&#32034;&#31561;AI&#25216;&#26415;&#65292;NIMS-OS&#33021;&#22815;&#22312;&#23454;&#39564;&#26465;&#20214;&#22914;&#21270;&#23398;&#25104;&#20998;&#12289;&#21453;&#24212;&#28201;&#24230;&#21644;&#21152;&#28909;&#26102;&#38388;&#25913;&#21464;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#20809;&#20652;&#21270;&#21058;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#20351;&#29992;NIMS-OS&#21487;&#20197;&#25214;&#21040;&#20855;&#26377;&#39640;&#20809;&#21270;&#23398;&#20135;&#27682;&#27963;&#24615;&#30340;&#21069;&#25152;&#26410;&#30693;&#30340;&#20809;&#20652;&#21270;&#21058;&#12290;</title><link>http://arxiv.org/abs/2304.13927</link><description>&lt;p&gt;
NIMS-OS&#65306;&#29992;&#20110;&#23454;&#29616;&#26448;&#26009;&#31185;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#23454;&#39564;&#20043;&#38388;&#38381;&#29615;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
NIMS-OS: An automation software to implement a closed loop between artificial intelligence and robotic experiments in materials science. (arXiv:2304.13927v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13927
&lt;/p&gt;
&lt;p&gt;
NIMS-OS&#26159;&#19968;&#27454;Python&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#26448;&#26009;&#31185;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#23454;&#39564;&#20043;&#38388;&#38381;&#29615;&#30340;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#26080;&#38480;&#30446;&#26631;&#33258;&#30001;&#25506;&#32034;&#12289;&#30456;&#22270;&#26500;&#24314;&#21644;&#38543;&#26426;&#25506;&#32034;&#31561;AI&#25216;&#26415;&#65292;NIMS-OS&#33021;&#22815;&#22312;&#23454;&#39564;&#26465;&#20214;&#22914;&#21270;&#23398;&#25104;&#20998;&#12289;&#21453;&#24212;&#28201;&#24230;&#21644;&#21152;&#28909;&#26102;&#38388;&#25913;&#21464;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#20809;&#20652;&#21270;&#21058;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#20351;&#29992;NIMS-OS&#21487;&#20197;&#25214;&#21040;&#20855;&#26377;&#39640;&#20809;&#21270;&#23398;&#20135;&#27682;&#27963;&#24615;&#30340;&#21069;&#25152;&#26410;&#30693;&#30340;&#20809;&#20652;&#21270;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NIMS-OS&#65288;NIMS Orchestration System&#65289;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#26448;&#26009;&#25506;&#32034;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#23454;&#39564;&#30340;&#38381;&#29615;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23427;&#20351;&#29992;&#21508;&#31181;&#32452;&#21512;&#30340;&#27169;&#22359;&#36827;&#34892;&#33258;&#20027;&#25805;&#20316;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#20805;&#24403;&#26448;&#26009;&#25506;&#32034;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#20154;&#23454;&#39564;&#30340;&#25511;&#21046;&#22120;&#12290;&#21487;&#20197;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;PHYSBO&#65289;&#65292;&#26080;&#38480;&#30446;&#26631;&#33258;&#30001;&#25506;&#32034;&#65288;BLOX&#65289;&#65292;&#30456;&#22270;&#26500;&#24314;&#65288;PDC&#65289;&#21644;&#38543;&#26426;&#25506;&#32034;&#65288;RE&#65289;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31216;&#20026;NIMS&#33258;&#21160;&#26426;&#22120;&#20154;&#30005;&#21270;&#23398;&#23454;&#39564;&#65288;NAREE&#65289;&#30340;&#31995;&#32479;&#21487;&#29992;&#20316;&#19968;&#32452;&#26426;&#22120;&#20154;&#23454;&#39564;&#35774;&#22791;&#12290;&#32467;&#26524;&#21487;&#35270;&#21270;&#24037;&#20855;&#20063;&#21253;&#25324;&#22312;&#20869;&#65292;&#29992;&#25143;&#21487;&#20197;&#23454;&#26102;&#26816;&#26597;&#20248;&#21270;&#32467;&#26524;&#12290;&#21487;&#20197;&#36731;&#26494;&#28155;&#21152;&#26032;&#30340;AI&#21644;&#26426;&#22120;&#20154;&#23454;&#39564;&#27169;&#22359;&#20197;&#25193;&#23637;&#31995;&#32479;&#30340;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;GUI&#24212;&#29992;&#31243;&#24207;&#26469;&#25511;&#21046;NIMS-OS&#12290;
&lt;/p&gt;
&lt;p&gt;
NIMS-OS (NIMS Orchestration System) is a Python library created to realize a closed loop of robotic experiments and artificial intelligence (AI) without human intervention for automated materials exploration. It uses various combinations of modules to operate autonomously. Each module acts as an AI for materials exploration or a controller for a robotic experiments. As AI techniques, Bayesian optimization (PHYSBO), boundless objective-free exploration (BLOX), phase diagram construction (PDC), and random exploration (RE) methods can be used. Moreover, a system called NIMS automated robotic electrochemical experiments (NAREE) is available as a set of robotic experimental equipment. Visualization tools for the results are also included, which allows users to check the optimization results in real time. Newly created modules for AI and robotic experiments can be added easily to extend the functionality of the system. In addition, we developed a GUI application to control NIMS-OS.To demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#29983;&#25104;&#36866;&#24212;&#29609;&#23478;&#30340;&#21160;&#24577;&#20851;&#21345;&#36827;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13922</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20851;&#21345;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Level Assembly as a Markov Decision Process. (arXiv:2304.13922v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#29983;&#25104;&#36866;&#24212;&#29609;&#23478;&#30340;&#21160;&#24577;&#20851;&#21345;&#36827;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28216;&#25103;&#37117;&#37319;&#29992;&#19981;&#36866;&#24212;&#29609;&#23478;&#30340;&#20851;&#21345;&#36827;&#24230;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#29609;&#23478;&#22312;&#36827;&#24230;&#36807;&#20110;&#22256;&#38590;&#26102;&#21345;&#20303;&#65292;&#32780;&#22312;&#36827;&#24230;&#36807;&#24930;&#26102;&#20250;&#24863;&#21040;&#26080;&#32842;&#65292;&#23548;&#33268;&#19981;&#24895;&#24847;&#31561;&#24453;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20851;&#21345;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20026;&#29609;&#23478;&#29983;&#25104;&#20851;&#21345;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#21160;&#24577;&#35268;&#21010;&#65288;ADP&#65289;&#26469;&#35299;&#20915;MDP&#65292;&#28982;&#21518;&#20877;&#32452;&#35013;&#20986;&#20851;&#21345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;ADP&#20248;&#20110;&#20004;&#20010;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29609;&#23478;&#20195;&#29702;&#30340;&#23454;&#39564;&#65292;&#24182;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#20999;&#25442;&#20102;&#23427;&#20204;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#36816;&#34892;ADP&#20043;&#21069;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;ADP&#65292;&#25105;&#20204;&#21487;&#20197;&#25628;&#32034;&#25972;&#20010;MDP&#65292;&#24182;&#20135;&#29983;&#36866;&#24212;&#29609;&#23478;&#30340;&#21160;&#24577;&#20851;&#21345;&#36827;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many games feature a progression of levels that doesn't adapt to the player. This can be problematic because some players may get stuck if the progression is too difficult, while others may find it boring if the progression is too slow to get to more challenging levels. This can be addressed by building levels based on the player's performance and preferences. In this work, we formulate the problem of generating levels for a player as a Markov Decision Process (MDP) and use adaptive dynamic programming (ADP) to solve the MDP before assembling a level. We tested with two case studies and found that using an ADP outperforms two baselines. Furthermore, we experimented with player proxies and switched them in the middle of play, and we show that a simple modification prior to running ADP results in quick adaptation. By using ADP, which searches the entire MDP, we produce a dynamic progression of levels that adapts to the player.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Fed-SP-SC&#21644;Fed-DP-CoT&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#37030;&#25552;&#31034;&#21644;&#38142;&#24335;&#25512;&#29702;&#25913;&#36827;&#20998;&#24067;&#24335;&#21516;&#20041;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#30340;&#31934;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#20805;&#20998;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.13911</link><description>&lt;p&gt;
&#25552;&#39640;LLM&#31572;&#26696;&#20934;&#30830;&#24230;&#30340;&#32852;&#37030;&#25552;&#31034;&#21644;&#38142;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering. (arXiv:2304.13911v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Fed-SP-SC&#21644;Fed-DP-CoT&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#37030;&#25552;&#31034;&#21644;&#38142;&#24335;&#25512;&#29702;&#25913;&#36827;&#20998;&#24067;&#24335;&#21516;&#20041;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#30340;&#31934;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#20805;&#20998;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20998;&#24067;&#24335;&#29992;&#25143;&#25552;&#20986;&#30340;&#24120;&#35265;&#38382;&#39064;&#30340;&#22238;&#31572;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#20856;&#22411;&#24773;&#20917;&#65292;&#21363;&#29992;&#25143;&#35810;&#38382;&#28041;&#21450;&#30456;&#21516;&#30340;&#25968;&#23398;&#25512;&#29702;&#27493;&#39588;&#21644;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#30456;&#20284;&#26597;&#35810;&#12290;&#30001;&#20110;LLMs&#29420;&#31435;&#38382;&#39064;&#30340;&#38646;-shot&#25552;&#31034;&#30340;&#20934;&#30830;&#24615;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#27965;&#24615;&#65288;SC&#65289;&#21644;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25216;&#26415;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;&#21516;&#20041;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20247;&#21253;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#21516;&#20041;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#32852;&#37030;&#38382;&#39064;&#27744;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#20855;&#26377;&#30456;&#21516;&#25110;&#19981;&#21516;&#21442;&#25968;&#30340;&#32852;&#37030;&#21516;&#20041;&#38382;&#39064;&#20026;SP&#38382;&#39064;&#25110;DP&#38382;&#39064;&#65292;&#20998;&#21035;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;Fed-SP-SC&#21644;Fed-DP-CoT&#65292;&#23427;&#20204;&#21487;&#20197;&#20026;&#25152;&#26377;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#27169;&#22411;&#35843;&#25972;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#36827;&#34892;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#38450;&#23433;&#20840;&#23041;&#32961;&#21644;&#26816;&#27979;&#28431;&#27934;&#20026;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.13905</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LSTM based IoT Device Identification. (arXiv:2304.13905v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#36827;&#34892;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#38450;&#23433;&#20840;&#23041;&#32961;&#21644;&#26816;&#27979;&#28431;&#27934;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#22823;&#37327;&#35774;&#22791;&#36896;&#25104;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#19979;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#25104;&#20026;&#37325;&#35201;&#30340;&#39044;&#38450;&#24615;&#23433;&#20840;&#25514;&#26045;&#65292;&#21487;&#20197;&#35782;&#21035;&#36825;&#20123;&#35774;&#22791;&#24182;&#26816;&#27979;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;Aalto&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the use of the Internet of Things is becoming more and more popular, many security vulnerabilities are emerging with the large number of devices being introduced to the market. In this environment, IoT device identification methods provide a preventive security measure as an important factor in identifying these devices and detecting the vulnerabilities they suffer from. In this study, we present a method that identifies devices in the Aalto dataset using Long short-term memory (LSTM)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BAET&#30340;&#26032;&#22411;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#20197;&#21452;&#20998;&#27861;&#29305;&#23450;&#20107;&#20214;&#26641;&#20026;&#22522;&#30784;&#30340;&#23618;&#27425;&#34920;&#31034;&#27861;&#65292;&#22312;&#21033;&#29992;&#29616;&#26377;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13895</link><description>&lt;p&gt;
&#20197;&#21452;&#20998;&#27861;&#29305;&#23450;&#20107;&#20214;&#26641;&#20026;&#22522;&#30784;&#30340;&#23618;&#27425;&#34920;&#31034;&#27861;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rumor Detection with Hierarchical Representation on Bipartite Adhoc Event Trees. (arXiv:2304.13895v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BAET&#30340;&#26032;&#22411;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#20197;&#21452;&#20998;&#27861;&#29305;&#23450;&#20107;&#20214;&#26641;&#20026;&#22522;&#30784;&#30340;&#23618;&#27425;&#34920;&#31034;&#27861;&#65292;&#22312;&#21033;&#29992;&#29616;&#26377;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#22686;&#38271;&#23545;&#20449;&#24687;&#20256;&#25773;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#35875;&#35328;&#26816;&#27979;&#30340;&#26816;&#27979;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#35875;&#35328;&#20505;&#36873;&#30340;&#36716;&#21457;&#20256;&#25773;&#36827;&#34892;&#26816;&#27979;&#65292;&#23558;&#25152;&#26377;&#36716;&#21457;&#21040;&#35875;&#35328;&#20505;&#36873;&#30340;&#36716;&#21457;&#35270;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#36716;&#21457;&#24207;&#21015;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20174;&#20256;&#25773;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#36716;&#21457;&#20316;&#32773;&#30340;&#24433;&#21709;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#25903;&#25345;&#26469;&#25581;&#31359;&#35875;&#35328;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#36890;&#24120;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27969;&#36890;&#30340;&#22768;&#31216;&#24086;&#23376;&#32452;&#32455;&#20026;&#19968;&#20010;adhoc&#20107;&#20214;&#26641;&#65292;&#25552;&#21462;&#20107;&#20214;&#20803;&#32032;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#22522;&#20110;&#24086;&#23376;&#21644;&#20316;&#32773;&#30340;&#21452;&#20998;&#27861;adhoc&#20107;&#20214;&#26641;&#65292;&#21363;&#20316;&#32773;&#26641;&#21644;&#24086;&#23376;&#26641;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#20998;&#27861;&#29305;&#23450;&#20107;&#20214;&#26641;&#30340;&#23618;&#27425;&#34920;&#31034;&#30340;&#26032;&#22411;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;BAET&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21333;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23398;&#20064;&#27599;&#20010;&#20107;&#20214;&#20803;&#32032;&#30340;&#35821;&#20041;&#65292;&#24182;&#32452;&#25104;&#26356;&#39640;&#23618;&#27425;&#30340;&#20107;&#20214;&#34920;&#31034;&#12290;&#20511;&#21161;&#20110;&#29616;&#26377;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#21147;&#65292;BAET&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20256;&#25773;&#26641;&#32467;&#26500;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BAET&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of social media has caused tremendous effects on information propagation, raising extreme challenges in detecting rumors. Existing rumor detection methods typically exploit the reposting propagation of a rumor candidate for detection by regarding all reposts to a rumor candidate as a temporal sequence and learning semantics representations of the repost sequence. However, extracting informative support from the topological structure of propagation and the influence of reposting authors for debunking rumors is crucial, which generally has not been well addressed by existing methods. In this paper, we organize a claim post in circulation as an adhoc event tree, extract event elements, and convert it to bipartite adhoc event trees in terms of both posts and authors, i.e., author tree and post tree. Accordingly, we propose a novel rumor detection model with hierarchical representation on the bipartite adhoc event trees called BAET. Specifically, we introduce word embedding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#23545;&#35774;&#22791;&#36827;&#34892;&#35782;&#21035;&#24182;&#26816;&#27979;&#20854;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2304.13894</link><description>&lt;p&gt;
&#22522;&#20110;CNN&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CNN based IoT Device Identification. (arXiv:2304.13894v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#23545;&#35774;&#22791;&#36827;&#34892;&#35782;&#21035;&#24182;&#26816;&#27979;&#20854;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#24066;&#22330;&#19978;&#24341;&#20837;&#20102;&#22823;&#37327;&#35774;&#22791;&#65292;&#23548;&#33268;&#35768;&#22810;&#23433;&#20840;&#28431;&#27934;&#30340;&#20135;&#29983;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#20316;&#20026;&#35782;&#21035;&#36825;&#20123;&#35774;&#22791;&#24182;&#26816;&#27979;&#23427;&#20204;&#25152;&#21463;&#28431;&#27934;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#19968;&#39033;&#39044;&#38450;&#24615;&#30340;&#23433;&#20840;&#25514;&#26045;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35782;&#21035;Aalto&#25968;&#25454;&#38598;&#20013;&#35774;&#22791;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the use of the Internet of Things is becoming more and more popular, many security vulnerabilities are emerging with the large number of devices being introduced to the market. In this environment, IoT device identification methods provide a preventive security measure as an important factor in identifying these devices and detecting the vulnerabilities they suffer from. In this study, we present a method that identifies devices in the Aalto dataset using the convolutional neural network (CNN).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.13892</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23637;&#29616;&#20102;&#20174;&#39640;&#32500;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#26159;&#25163;&#24037;&#36741;&#21161;&#20219;&#21153;&#21644;&#20266;&#22870;&#21169;&#12290;&#33258;&#21160;&#21270;&#22320;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#65292;&#20197;&#26399;&#23454;&#29616;&#25511;&#21046;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35797;&#22270;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#34987;&#21457;&#29616;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#23545;&#20219;&#21153;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenStreetMap&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#24494;&#21306;&#22495;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#22320;&#22270;&#20845;&#36793;&#24418;&#22312;&#21253;&#21547;&#30340;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13865</link><description>&lt;p&gt;
highway2vec&#8212;&#8212;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#23558;OpenStreetMap&#24494;&#21306;&#22495;&#34920;&#31034;&#20986;&#26469;
&lt;/p&gt;
&lt;p&gt;
highway2vec -- representing OpenStreetMap microregions with respect to their road network characteristics. (arXiv:2304.13865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenStreetMap&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#24494;&#21306;&#22495;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#22320;&#22270;&#20845;&#36793;&#24418;&#22312;&#21253;&#21547;&#30340;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#23436;&#25104;&#19968;&#20123;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#38656;&#35201;&#32771;&#34385;&#31354;&#38388;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22320;&#22270;&#21306;&#22495;&#34920;&#31034;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#21019;&#24314;&#29305;&#24449;&#34920;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#30340;&#22320;&#22270;&#21306;&#22495;&#34920;&#31034;&#26041;&#27861;&#38750;&#24120;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenStreetMap&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#24494;&#21306;&#22495;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#21033;&#29992;H3&#31354;&#38388;&#32034;&#24341;&#23454;&#29616;&#21487;&#37325;&#22797;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#23398;&#20064;&#24182;&#33719;&#24471;&#30690;&#37327;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#22320;&#22270;&#20845;&#36793;&#24418;&#22312;&#21253;&#21547;&#30340;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years brought advancements in using neural networks for representation learning of various language or visual phenomena. New methods freed data scientists from hand-crafting features for common tasks. Similarly, problems that require considering the spatial variable can benefit from pretrained map region representations instead of manually creating feature tables that one needs to prepare to solve a task. However, very few methods for map area representation exist, especially with respect to road network characteristics. In this paper, we propose a method for generating microregions' embeddings with respect to their road infrastructure characteristics. We base our representations on OpenStreetMap road networks in a selection of cities and use the H3 spatial index to allow reproducible and scalable representation learning. We obtained vector representations that detect how similar map hexagons are in the road networks they contain. Additionally, we observe that embeddings yield a
&lt;/p&gt;</description></item><item><title>Ensoul&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;enerstatic&#32593;&#32476;&#21644;&#24320;&#25918;&#36827;&#21270;&#25216;&#26415;&#32467;&#21512;&#65292;&#21019;&#36896;&#20102;&#33021;&#22815;&#29420;&#31435;&#20110;&#20854;&#23884;&#20837;&#30340;&#22522;&#36136;&#30340;&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;(SOULS)&#12290;</title><link>http://arxiv.org/abs/2304.13863</link><description>&lt;p&gt;
Ensoul: &#36890;&#36807;&#36827;&#21270;&#30340;enerstatic&#32593;&#32476;&#21019;&#24314;&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;(SOULS)&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ensoul: A framework for the creation of self organizing intelligent ultra low power systems (SOULS) through evolutionary enerstatic networks. (arXiv:2304.13863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13863
&lt;/p&gt;
&lt;p&gt;
Ensoul&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;enerstatic&#32593;&#32476;&#21644;&#24320;&#25918;&#36827;&#21270;&#25216;&#26415;&#32467;&#21512;&#65292;&#21019;&#36896;&#20102;&#33021;&#22815;&#29420;&#31435;&#20110;&#20854;&#23884;&#20837;&#30340;&#22522;&#36136;&#30340;&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;(SOULS)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Ensoul&#26159;&#19968;&#20010;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33021;&#37327;&#31283;&#24577;(enerstatic)&#22238;&#36335;&#21644;&#24320;&#25918;&#24335;&#36827;&#21270;&#25216;&#26415;&#30340;&#32593;&#32476;&#21644;&#23884;&#22871;&#32467;&#26500;&#30340;&#32467;&#21512;&#65292;&#21019;&#24314;&#20986;&#26356;&#22810;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#24320;&#21457;&#30340;&#29983;&#25104;&#25216;&#26415;&#26082;&#26159;&#28909;&#21147;&#23398;&#39537;&#21160;&#22797;&#26434;&#31995;&#32479;&#30340;&#31616;&#21333;&#32780;&#26377;&#27934;&#35265;&#30340;&#27169;&#22411;&#65292;&#20063;&#26159;&#21019;&#26032;&#25216;&#26415;&#30340;&#24378;&#22823;&#28304;&#27849;&#12290; "&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;"&#65288;SOULS&#65289;&#26159;&#19968;&#20010;&#33021;&#22815;&#25551;&#36848;&#27492;&#31867;&#29983;&#25104;&#25216;&#26415;&#21450;&#20854;&#20135;&#29983;&#30340;&#25216;&#26415;&#30340;&#26415;&#35821;&#12290;&#35813;&#26415;&#35821;&#26088;&#22312;&#25429;&#25417;&#36825;&#20123;&#25216;&#26415;&#30340;&#25277;&#35937;&#26412;&#36136;&#65292;&#21363;&#23427;&#20204;&#29420;&#31435;&#20110;&#20854;&#23884;&#20837;&#30340;&#22522;&#36136;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;SOULS&#21487;&#20197;&#26159;&#29983;&#29289;&#12289;&#20154;&#24037;&#25110;&#28151;&#21512;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensoul is a framework proposed for the purpose of creating technologies that create more technologies through the combined use of networks, and nests, of energy homeostatic (enerstatic) loops and open-ended evolutionary techniques. Generative technologies developed by such an approach serve as both simple, yet insightful models of thermodynamically driven complex systems and as powerful sources of novel technologies. "Self Organizing intelligent Ultra Low power Systems" (SOULS) is a term that well describes the technologies produced by such a generative technology, as well as the generative technology itself. The term is meant to capture the abstract nature of such technologies as being independent of the substrate in which they are embedded. In other words, SOULS can be biological, artificial or hybrid in form.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;(MCAS)&#65292;&#29992;&#20110;&#34913;&#37327;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#19982;&#21333;&#27169;&#24577;&#23567;&#22411;&#21333;&#38454;&#27573;&#27169;&#22411;&#30340;&#24230;&#37327;&#19982;&#37327;&#21270;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#27979;&#37327;&#22823;&#22411;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.13855</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;&#65306;&#34913;&#37327;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models. (arXiv:2304.13855v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;(MCAS)&#65292;&#29992;&#20110;&#34913;&#37327;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#19982;&#21333;&#27169;&#24577;&#23567;&#22411;&#21333;&#38454;&#27573;&#27169;&#22411;&#30340;&#24230;&#37327;&#19982;&#37327;&#21270;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#27979;&#37327;&#22823;&#22411;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#12290;&#20687;DALL-E&#21644;Stable Diffusion&#36825;&#26679;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25104;&#21151;&#22320;&#20174;&#25991;&#26412;&#20013;&#21019;&#24314;&#22270;&#20687;&#65292;&#32463;&#24120;&#32467;&#21512;&#25277;&#35937;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#20687;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20063;&#21453;&#26144;&#20102;&#20174;&#20114;&#32852;&#32593;&#20013;&#29228;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#25163;&#21160;&#23457;&#26680;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#19988;&#30001;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#25509;&#21463;&#30340;&#36755;&#20837;&#30340;&#26080;&#38480;&#21046;&#21644;&#19981;&#21463;&#32422;&#26463;&#30340;&#24615;&#36136;&#20351;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#12290; &#23545;&#20110;&#20559;&#35265;&#30340;&#24230;&#37327;&#19982;&#37327;&#21270;&#30340;&#30740;&#31350;&#36890;&#24120;&#20391;&#37325;&#20110;&#21333;&#27169;&#24577;&#30340;&#23567;&#22411;&#21333;&#38454;&#27573;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22810;&#38454;&#27573;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20986;&#29616;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;&#65288;MCAS&#65289;&#20316;&#20026;&#34913;&#37327;&#22810;&#27169;&#24577;&#29983;&#25104;&#24335;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#24046;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;MCAS&#35780;&#20272;DALL-E 2&#21644;&#31283;&#23450;&#25193;&#25955;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#27979;&#24182;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#20013;&#24615;&#21035;&#20559;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DALL-E 2&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#26368;&#23567;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32780;&#31283;&#23450;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;MCAS&#25351;&#26631;&#20801;&#35768;&#23545;&#22823;&#22411;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative multimodal models based on diffusion models have seen tremendous growth and advances in recent years. Models such as DALL-E and Stable Diffusion have become increasingly popular and successful at creating images from texts, often combining abstract ideas. However, like other deep learning models, they also reflect social biases they inherit from their training data, which is often crawled from the internet. Manually auditing models for biases can be very time and resource consuming and is further complicated by the unbounded and unconstrained nature of inputs these models can take. Research into bias measurement and quantification has generally focused on small single-stage models working on a single modality. Thus the emergence of multistage multimodal models requires a different approach. In this paper, we propose Multimodal Composite Association Score (MCAS) as a new method of measuring gender bias in multimodal generative models. Evaluating both DALL-E 2 and Stable Diffu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KIEST&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#39537;&#21160;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26174;&#24335;&#22320;&#26816;&#32034;&#30456;&#20851;&#23454;&#20307;&#21644;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#30693;&#35782;&#31890;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#33258;&#22238;&#24402;&#29983;&#25104;&#25152;&#26377;&#23454;&#20307;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.13854</link><description>&lt;p&gt;
&#29702;&#35299;&#21160;&#24577;&#19990;&#30028;&#65306;&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#23454;&#20307;&#29366;&#24577;&#36319;&#36394;&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking. (arXiv:2304.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KIEST&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#39537;&#21160;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26174;&#24335;&#22320;&#26816;&#32034;&#30456;&#20851;&#23454;&#20307;&#21644;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#30693;&#35782;&#31890;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#33258;&#22238;&#24402;&#29983;&#25104;&#25152;&#26377;&#23454;&#20307;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#23454;&#20307;&#29366;&#24577;&#36319;&#36394;&#26088;&#22312;&#39044;&#27979;&#23454;&#20307;&#30340;&#21512;&#29702;&#29366;&#24577;&#21464;&#21270;&#65288;&#21363;[&#23454;&#20307;]&#30340;[&#23646;&#24615;]&#22312;[&#20043;&#21069;&#29366;&#24577;]&#21644;[&#20043;&#21518;&#29366;&#24577;]&#20043;&#38388;&#21457;&#29983;&#20102;&#21464;&#21270;&#65289;&#65292;&#32473;&#23450;&#21160;&#20316;&#25551;&#36848;&#65292;&#36825;&#23545;&#35768;&#22810;&#25512;&#29702;&#20219;&#21153;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#25903;&#25345;&#20154;&#31867;&#26085;&#24120;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#23454;&#20307;&#37117;&#19982;&#21160;&#20316;&#21450;&#20854;&#23646;&#24615;&#38544;&#21547;&#30456;&#20851;&#65292;&#24182;&#19988;&#26469;&#33258;&#20110;&#24320;&#25918;&#35789;&#27719;&#65292;&#22240;&#27492;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#23454;&#20307;&#29366;&#24577;&#36319;&#36394;&#30340;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#39537;&#21160;&#26694;&#26550;&#65292;&#21363;KIEST&#65292;&#23427;&#26126;&#30830;&#22320;&#20174;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#65288;&#21363;ConceptNet&#65289;&#26816;&#32034;&#30456;&#20851;&#23454;&#20307;&#21644;&#23646;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26032;&#39062;&#21160;&#24577;&#30693;&#35782;&#31890;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#19968;&#36215;&#29992;&#20110;&#33258;&#22238;&#24402;&#29983;&#25104;&#25152;&#26377;&#23454;&#20307;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open domain entity state tracking aims to predict reasonable state changes of entities (i.e., [attribute] of [entity] was [before_state] and [after_state] afterwards) given the action descriptions. It's important to many reasoning tasks to support human everyday activities. However, it's challenging as the model needs to predict an arbitrary number of entity state changes caused by the action while most of the entities are implicitly relevant to the actions and their attributes as well as states are from open vocabularies. To tackle these challenges, we propose a novel end-to-end Knowledge Informed framework for open domain Entity State Tracking, namely KIEST, which explicitly retrieves the relevant entities and attributes from external knowledge graph (i.e., ConceptNet) and incorporates them to autoregressively generate all the entity state changes with a novel dynamic knowledge grained encoder-decoder framework. To enforce the logical coherence among the predicted entities, attribute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#23545;&#30005;&#21160;&#21644;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#30340;&#29983;&#20135;&#19982;&#21457;&#23637;&#30340;&#24433;&#21709;&#65292;&#20026;&#26356;&#21152;&#29615;&#20445;&#30340;&#20132;&#36890;&#38656;&#27714;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.13841</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#26469;&#20445;&#38556;&#30005;&#21160;/&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#26410;&#26469;&#21457;&#23637;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
AI-based Predictive Analytic Approaches for safeguarding the Future of Electric/Hybrid Vehicles. (arXiv:2304.13841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#23545;&#30005;&#21160;&#21644;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#30340;&#29983;&#20135;&#19982;&#21457;&#23637;&#30340;&#24433;&#21709;&#65292;&#20026;&#26356;&#21152;&#29615;&#20445;&#30340;&#20132;&#36890;&#38656;&#27714;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#33021;&#28304;&#30340;&#38656;&#27714;&#65292;&#32511;&#33394;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#32511;&#33394;&#22522;&#30784;&#35774;&#26045;&#33021;&#22815;&#36731;&#26131;&#25972;&#21512;&#36827;&#20837;&#20840;&#29699;&#33021;&#28304;&#31995;&#32479;&#20043;&#21069;&#65292;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#21319;&#32423;&#12290;&#36890;&#36807;&#25913;&#36827;&#33021;&#28304;&#22522;&#30784;&#35774;&#26045;&#21644;&#20915;&#31574;&#21046;&#23450;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#30001;&#20110;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#21644;&#23545;&#26356;&#21152;&#29615;&#20445;&#30340;&#20132;&#36890;&#38656;&#27714;&#65292;&#28151;&#21160;&#30005;&#21160;&#27773;&#36710;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28151;&#21160;&#30005;&#21160;&#27773;&#36710;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20687;AI&#36825;&#26679;&#30340;&#23574;&#31471;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#38477;&#20302;&#20102;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#24182;&#20419;&#36827;&#20102;&#21487;&#25345;&#32493;&#20986;&#34892;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;EV&#30340;&#29983;&#20135;&#20250;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#21644;&#26448;&#26009;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#33258;&#28982;&#29615;&#22659;&#36896;&#25104;&#25439;&#23475;&#12290;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#39044;&#27979;&#20998;&#26512;&#31561;&#32511;&#33394;&#25216;&#26415;&#26469;&#25913;&#36827;EV&#30340;&#29983;&#20135;&#12290;&#30005;&#21160;&#21644;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#65288;EHVs&#65289;&#21487;&#20197;&#24110;&#21161;&#28385;&#36275;&#23545;&#26356;&#21152;&#29615;&#20445;&#30340;&#20132;&#36890;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the global need for sustainable energy, green technology may help fight climate change. Before green infrastructure to be easily integrated into the world's energy system, it needs upgrading. By improving energy infrastructure and decision-making, artificial intelligence (AI) may help solve this challenge. EHVs have grown in popularity because to concerns about global warming and the need for more ecologically friendly transportation. EHVs may work better with cutting-edge technologies like AI. Electric vehicles (EVs) reduce greenhouse gas emissions and promote sustainable mobility. Electric automobiles (EVs) are growing in popularity due to their benefits for climate change mitigation and sustainable mobility. Unfortunately, EV production consumes a lot of energy and materials, which may harm nature. EV production is being improved using green technologies like artificial intelligence and predictive analysis. Electric and hybrid vehicles (EHVs) may help meet the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;ProgramPort&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#25351;&#20196;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#35821;&#20041;&#35299;&#26512;&#22120;&#24674;&#22797;&#19968;&#20010;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#30001;&#36328;&#19981;&#21516;&#27169;&#24577;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#34892;&#21160;&#30340;&#21151;&#33021;&#27169;&#22359;&#32452;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.13826</link><description>&lt;p&gt;
&#21487;&#32534;&#31243;&#25509;&#22320;&#65292;&#32452;&#21512;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. (arXiv:2304.13826v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;ProgramPort&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#25351;&#20196;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#35821;&#20041;&#35299;&#26512;&#22120;&#24674;&#22797;&#19968;&#20010;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#30001;&#36328;&#19981;&#21516;&#27169;&#24577;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#34892;&#21160;&#30340;&#21151;&#33021;&#27169;&#22359;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#38656;&#35201;&#20016;&#23500;&#30340;&#25805;&#20316;&#25216;&#33021;&#20197;&#21450;&#22312;&#20309;&#26102;&#24212;&#29992;&#36825;&#20123;&#25216;&#33021;&#26041;&#38754;&#20855;&#26377;&#35821;&#20041;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#30340;&#35821;&#20041;&#34920;&#31034;&#38598;&#25104;&#21040;&#25805;&#20316;&#27169;&#22411;&#20013;&#65292;&#36171;&#20104;&#20854;&#26356;&#36890;&#29992;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#20110;&#25972;&#21512;&#27492;&#31867;&#34920;&#31034;&#30340;&#20256;&#32479;&#39044;&#35757;&#32451;&#24494;&#35843;&#27969;&#31243;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#34892;&#21160;&#20449;&#24687;&#21644;&#39046;&#22495;&#36890;&#29992;&#30340;&#35270;&#35273;&#20449;&#24687;&#32416;&#32544;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#19988;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20219;&#21153;&#24456;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ProgramPort&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451; VL &#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#25351;&#20196;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#35821;&#20041;&#35299;&#26512;&#22120;&#24674;&#22797;&#19968;&#20010;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#30001;&#36328;&#19981;&#21516;&#27169;&#24577;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#34892;&#21160;&#30340;&#21151;&#33021;&#27169;&#22359;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional pretraining-finetuning pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose ProgramPort, a modular approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#31435;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20248;&#21270;&#26041;&#27861;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#26469;&#35745;&#31639;&#20445;&#35777;&#30340;&#37327;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20445;&#35777;&#36755;&#20986;&#35823;&#24046;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13812</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#30340;&#37327;&#21270;&#35823;&#24046;&#35745;&#31639;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Quantization Error Computation for Neural Network Model Compression. (arXiv:2304.13812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#31435;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20248;&#21270;&#26041;&#27861;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#26469;&#35745;&#31639;&#20445;&#35777;&#30340;&#37327;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20445;&#35777;&#36755;&#20986;&#35823;&#24046;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#24037;&#19994;&#31995;&#32479;&#20013;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#38382;&#39064;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#24102;&#26377;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20445;&#35777;&#36755;&#20986;&#35823;&#24046;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#37327;&#21270;&#29256;&#26412;&#21512;&#24182;&#65292;&#20197;&#20135;&#29983;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#31934;&#30830;&#36755;&#20986;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#21040;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35745;&#31639;&#20445;&#35777;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#20363;&#23376;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21019;&#24314;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13765</link><description>&lt;p&gt;
&#36208;&#21521;&#20262;&#29702;&#22810;&#27169;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards ethical multimodal systems. (arXiv:2304.13765v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21019;&#24314;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#27491;&#22312;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#20363;&#22914;&#65292;ChatGPT&#27491;&#22312;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#27835;&#30103;&#24212;&#29992;&#30340;&#27979;&#35797;&#65292;&#22914;Koko&#65292;Stable Diffusion&#29983;&#25104;&#30340;&#33402;&#26415;&#20316;&#21697;&#19982;&#20154;&#31867;&#33402;&#26415;&#23478;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#34892;&#20026;&#21644;&#24212;&#29992;&#30340;&#20262;&#29702;&#38382;&#39064;&#36817;&#24180;&#26469;&#19981;&#26029;&#22686;&#21152;&#65292;AI&#23545;&#40784;&#39046;&#22495;&#8212;&#8212;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#34892;&#20026;&#24341;&#23548;&#21521;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#26041;&#21521;&#8212;&#8212;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#23376;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21516;&#26102;&#20197;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23436;&#25104;&#20316;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#25110;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20998;&#20004;&#27493;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65306;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#30340;&#21019;&#24314;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of artificial intelligence systems on our society is increasing at an unprecedented speed. For instance, ChatGPT is being tested in mental health treatment applications such as Koko, Stable Diffusion generates pieces of art competitive with (or outperforming) human artists, and so on. Ethical concerns regarding the behavior and applications of generative AI systems have been increasing over the past years, and the field of AI alignment - steering the behavior of AI systems towards being aligned with human values - is a rapidly growing subfield of modern AI. In this paper, we address the challenges involved in ethical evaluation of a multimodal artificial intelligence system. The multimodal systems we focus on take both text and an image as input and output text, completing the sentence or answering the question asked as input. We perform the evaluation of these models in two steps: we first discus the creation of a multimodal ethical database and then use this database to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TR0N&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#39640;&#24230;&#20219;&#24847;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;TR0N&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;MS-COCO&#19978;&#23454;&#29616;&#38646;-shot FID 10.9&#65292;&#24182;&#22312;&#37319;&#26679;&#36895;&#24230;&#19978;&#20248;&#20110;&#31454;&#21697;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.13742</link><description>&lt;p&gt;
TR0N&#65306;0-Shot&#21363;&#25554;&#21363;&#29992;&#26465;&#20214;&#29983;&#25104;&#30340;&#32763;&#35793;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation. (arXiv:2304.13742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TR0N&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#39640;&#24230;&#20219;&#24847;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;TR0N&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;MS-COCO&#19978;&#23454;&#29616;&#38646;-shot FID 10.9&#65292;&#24182;&#22312;&#37319;&#26679;&#36895;&#24230;&#19978;&#20248;&#20110;&#31454;&#21697;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TR0N&#65292;&#19968;&#20010;&#39640;&#24230;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GAN&#21644;VAE&#65292;&#36716;&#25442;&#20026;&#26465;&#20214;&#27169;&#22411;&#12290;&#26465;&#20214;&#21487;&#20197;&#26159;&#39640;&#24230;&#20219;&#24847;&#30340;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20998;&#31867;&#22120;&#23558;&#26080;&#26465;&#20214;&#27169;&#22411;&#36716;&#21270;&#20026;&#31867;&#21035;&#26465;&#20214;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;CLIP&#23558;&#20854;&#36716;&#21270;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;TR0N&#23398;&#20064;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#38543;&#26426;&#26144;&#23556;&#65292;&#35813;&#26144;&#23556;&#22312;&#26465;&#20214;&#31354;&#38388;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#8220;&#32763;&#35793;&#8221;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#24212;&#20110;&#28385;&#36275;&#25152;&#38656;&#26465;&#20214;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;Langevin&#21160;&#24577;&#36827;&#19968;&#27493;&#25913;&#36827;&#32763;&#35793;&#21518;&#30340;&#28508;&#22312;&#26679;&#26412;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;TR0N&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#24494;&#35843;&#65292;&#20294;&#21487;&#20197;&#22312;MS-COCO&#19978;&#23454;&#29616;&#38646;-shot FID 10.9&#65292;&#19981;&#20165;&#22312;&#36825;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#31454;&#21697;&#65292;&#32780;&#19988;&#22312;&#37319;&#26679;&#36895;&#24230;&#19978;&#20063;&#19982;&#20854;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TR0N, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models. The conditioning can be highly arbitrary, and requires only a pre-trained auxiliary model. For example, we show how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP. TR0N learns a lightweight stochastic mapping which "translates" between the space of conditions and the latent space of the generative model, in such a way that the generated latent corresponds to a data sample satisfying the desired condition. The translated latent samples are then further improved upon through Langevin dynamics, enabling us to obtain higher-quality data samples. TR0N requires no training data nor fine-tuning, yet can achieve a zero-shot FID of 10.9 on MS-COCO, outperforming competing alternatives not only on this metric, but also in sampling speed -- all while retaining 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#27010;&#36848;&#24120;&#29992;&#30340;AI&#26694;&#26550;&#21644;&#20113;&#26381;&#21153;&#12289;&#25506;&#35752;&#22522;&#20110;&#20113;&#30340;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#35752;&#35770;&#20113;&#20013;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;AI&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13738</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#65306;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency. (arXiv:2304.13738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#27010;&#36848;&#24120;&#29992;&#30340;AI&#26694;&#26550;&#21644;&#20113;&#26381;&#21153;&#12289;&#25506;&#35752;&#22522;&#20110;&#20113;&#30340;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#35752;&#35770;&#20113;&#20013;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;AI&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#20113;&#35745;&#31639;&#30456;&#32467;&#21512;&#24050;&#25104;&#20026;&#35299;&#20915;AI&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#24120;&#29992;&#30340;AI&#26694;&#26550;&#21644;&#20113;&#26381;&#21153;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#20113;&#30340;AI&#31995;&#32479;&#20013;&#25968;&#25454;&#23384;&#20648;&#21644;&#31649;&#29702;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#27169;&#22411;&#30340;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#20998;&#21306;&#12289;&#36890;&#20449;&#31574;&#30053;&#21644;&#22522;&#20110;&#20113;&#30340;&#35757;&#32451;&#26550;&#26500;&#12290;&#22312;&#38543;&#21518;&#30340;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20113;&#20013;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#36127;&#36733;&#24179;&#34913;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#33258;&#21160;&#32553;&#25918;&#21644;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of artificial intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. We first provide an overview of popular AI frameworks and cloud services, highlighting their respective strengths and weaknesses. Next, we delve into the critical aspects of data storage and management in cloud-based AI systems, discussing data preprocessing, feature engineering, privacy, and security. We then explore parallel and distributed training techniques for AI models, focusing on model partitioning, communication strategies, and cloud-based training architectures.  In subsequent chapters, we discuss optimization strategies for AI workloads in the cloud, covering load balancing, resource allocation, auto-scaling, and performance benc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13734</link><description>&lt;p&gt;
&#19968;&#20010;LLM&#30693;&#36947;&#33258;&#24049;&#22312;&#25746;&#35854;&#30340;&#20869;&#37096;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#65288;&#21487;&#33021;&#65289;&#26368;&#20026;&#31361;&#20986;&#30340;&#32570;&#28857;&#26159;&#20197;&#33258;&#20449;&#30340;&#35821;&#27668;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#25581;&#31034;&#19968;&#20010;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;LLM&#25152;&#29983;&#25104;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35821;&#21477;&#12290;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#26681;&#25454;LLM&#30340;&#28608;&#27963;&#20540;&#26469;&#26816;&#27979;&#21738;&#20010;&#35821;&#21477;&#26159;&#30495;&#23454;&#30340;&#25110;&#34394;&#20551;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#22120;&#25509;&#25910;LLM&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#21477;&#29983;&#25104;&#30340;&#28608;&#27963;&#20540;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#26816;&#27979;&#35821;&#21477;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#29978;&#33267;&#27604;&#23569;&#37327;&#25552;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;</title><link>http://arxiv.org/abs/2304.13731</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24040;&#22823;&#35268;&#27169;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#27604;&#22914;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#24605;&#36335;&#38142;&#30340;&#24494;&#35843;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#38646;&#27425;&#21644;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#19968;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#29983;&#25104;&#20219;&#21153;&#8212;&#8212;&#30446;&#26631;&#26159;&#26681;&#25454;&#20854;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#39057;&#12290;&#20043;&#21069;&#20851;&#20110;TTA&#30340;&#24037;&#20316;&#35201;&#20040;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#32852;&#21512;&#30340;&#25991;&#26412;-&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#20010;&#38750;&#25351;&#20196;&#35843;&#35856;&#30340;&#27169;&#22411;&#65292;&#22914;T5&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;AudioLDM&#26356;&#22909;&#30340;&#22823;&#22810;&#25968;&#25351;&#26631;&#65292;&#24182;&#22312;&#20854;&#20313;&#25351;&#26631;&#19978;&#25345;&#24179;&#65292;&#23613;&#31649;&#25105;&#20204;&#20351;&#29992;&#20102;63&#20493;&#23567;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;LDM&#65292;&#24182;&#20445;&#25345;&#25991;&#26412;&#32534;&#30721;&#22120;&#19981;&#21464;&#12290;&#36825;&#31181;&#25913;&#36827;&#21487;&#33021;&#36824;&#24402;&#22240;&#20110;&#37319;&#29992;&#22522;&#20110;&#38899;&#39057;&#21387;&#21147;&#32423;&#30340;&#28151;&#38899;&#35757;&#32451;&#38598;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#38598;&#25104;CNN&#30340;&#26041;&#27861;&#23545;&#20083;&#33146;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;5%&#20197;&#19978;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.13727</link><description>&lt;p&gt;
&#38598;&#25104;CNN&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Ensemble CNNs for Breast Tumor Classification. (arXiv:2304.13727v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#38598;&#25104;CNN&#30340;&#26041;&#27861;&#23545;&#20083;&#33146;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;5%&#20197;&#19978;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#26426;&#36741;&#21161;&#20083;&#33146;&#32959;&#22359;&#20998;&#31867;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26426;&#21046;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65292;&#28982;&#21518;&#20998;&#21035;&#35757;&#32451;&#19977;&#20010;&#27169;&#22411;&#65292;&#21363;XceptionNet&#12289;DenseNet&#21644;EfficientNet&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#32593;&#32476;&#36755;&#20986;&#30340;&#27010;&#29575;&#30456;&#21152;&#26469;&#38598;&#25104;&#26426;&#21046;&#65292;&#20174;&#32780;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102;5%&#12290;&#35813;&#26041;&#26696;&#24050;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#65292;&#24182;&#23454;&#29616;&#20102;88%&#30340;&#20934;&#30830;&#29575;&#12289;85%&#30340;&#31934;&#24230;&#21644;76%&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the recognition ability of computer-aided breast mass classification among mammographic images, in this work we explore the state-of-the-art classification networks to develop an ensemble mechanism. First, the regions of interest (ROIs) are obtained from the original dataset, and then three models, i.e., XceptionNet, DenseNet, and EfficientNet, are trained individually. After training, we ensemble the mechanism by summing the probabilities outputted from each network which enhances the performance up to 5%. The scheme has been validated on a public dataset and we achieved accuracy, precision, and recall 88%, 85%, and 76% respectively.
&lt;/p&gt;</description></item><item><title>SamurAI&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#29289;&#32852;&#32593;&#33410;&#28857;&#65292;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21796;&#37266;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#33455;&#29255;&#20869;&#23376;&#31995;&#32479;&#26469;&#24357;&#21512;&#22788;&#29702;&#21644;&#33021;&#37327;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20026;&#29289;&#32852;&#32593;&#24212;&#29992;&#25552;&#20379;&#20102;&#35782;&#21035;&#21644;&#33258;&#36866;&#24212;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13726</link><description>&lt;p&gt;
SamurAI: &#19968;&#31181;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21796;&#37266;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#22810;&#21151;&#33021;&#29289;&#32852;&#32593;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
SamurAI: A Versatile IoT Node With Event-Driven Wake-Up and Embedded ML Acceleration. (arXiv:2304.13726v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13726
&lt;/p&gt;
&lt;p&gt;
SamurAI&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#29289;&#32852;&#32593;&#33410;&#28857;&#65292;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21796;&#37266;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#33455;&#29255;&#20869;&#23376;&#31995;&#32479;&#26469;&#24357;&#21512;&#22788;&#29702;&#21644;&#33021;&#37327;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20026;&#29289;&#32852;&#32593;&#24212;&#29992;&#25552;&#20379;&#20102;&#35782;&#21035;&#21644;&#33258;&#36866;&#24212;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#24212;&#29992;&#29616;&#22312;&#38656;&#35201;&#20855;&#22791;&#35782;&#21035;&#21644;&#33258;&#36866;&#24212;&#31561;&#21151;&#33021;&#12290;&#34429;&#28982;&#29289;&#32852;&#32593;&#33410;&#28857;&#30340;&#21151;&#32791;&#26159;&#36825;&#20123;&#24212;&#29992;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#20294;&#30001;&#20110;&#26080;&#32447;&#32593;&#32476;&#19978;&#36830;&#32493;&#20256;&#36755;&#20256;&#24863;&#22120;&#25110;&#22270;&#20687;&#25968;&#25454;&#65292;&#22522;&#20110;&#20113;&#30340;&#22788;&#29702;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#22240;&#27492;&#65292;&#24212;&#22312;&#29289;&#32852;&#32593;&#33410;&#28857;&#20013;&#38598;&#25104;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#21644;&#25968;&#25454;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#29289;&#32852;&#32593;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#38388;&#27463;&#24615;&#25968;&#25454;&#35760;&#24405;&#21644;&#33021;&#32791;&#39640;&#30340;&#25968;&#25454;&#22788;&#29702;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#12290;&#22240;&#27492;&#65292;&#33410;&#28857;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#35299;&#20915;&#22788;&#29702;&#21644;&#33021;&#28304;&#30340;&#24191;&#27867;&#38656;&#27714;&#26041;&#38754;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SamurAI&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#29289;&#32852;&#32593;&#33410;&#28857;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#33455;&#29255;&#20869;&#23376;&#31995;&#32479;&#26469;&#24357;&#21512;&#22788;&#29702;&#21644;&#33021;&#37327;&#26041;&#38754;&#30340;&#24046;&#36317;&#65306;&#19968;&#20010;&#20302;&#21151;&#29575;&#12289;&#26080;&#26102;&#38047;&#12289;&#20107;&#20214;&#39537;&#21160;&#30340;Always-Responsive (AR) &#37096;&#20998;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;On-Demand (OD) &#37096;&#20998;&#12290; AR&#21253;&#21547;&#19968;&#20010;1.7MOPS&#20107;&#20214;&#39537;&#21160;&#30340;&#24322;&#27493; Wake-up &#25511;&#21046;&#22120;&#65288;WuC&#65289;&#65292;207ns&#21796;&#37266;&#26102;&#38388;&#30340;&#20248;&#21270;&#36866;&#29992;&#20110;&#38388;&#27463;&#24615;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased capabilities such as recognition and self-adaptability are now required from IoT applications. While IoT node power consumption is a major concern for these applications, cloud-based processing is becoming unsustainable due to continuous sensor or image data transmission over the wireless network. Thus optimized ML capabilities and data transfers should be integrated in the IoT node. Moreover, IoT applications are torn between sporadic data-logging and energy-hungry data processing (e.g. image classification). Thus, the versatility of the node is key in addressing this wide diversity of energy and processing needs. This paper presents SamurAI, a versatile IoT node bridging this gap in processing and in energy by leveraging two on-chip sub-systems: a low power, clock-less, event-driven Always-Responsive (AR) part and an energy-efficient On-Demand (OD) part. AR contains a 1.7MOPS event-driven, asynchronous Wake-up Controller (WuC) with a 207ns wake-up time optimized for sporadi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#12290;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#24182;&#39044;&#27979;&#22797;&#21457;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2304.13725</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prediction of brain tumor recurrence location based on multi-modal fusion and nonlinear correlation learning. (arXiv:2304.13725v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#12290;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#24182;&#39044;&#27979;&#22797;&#21457;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#23548;&#33268;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#39640;&#32423;&#21035;&#30340;&#33041;&#32959;&#30244;&#29978;&#33267;&#22312;&#26631;&#20934;&#27835;&#30103;&#21518;&#23481;&#26131;&#22797;&#21457;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#31181;&#39044;&#27979;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#35268;&#21010;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#26377;&#21487;&#33021;&#24310;&#38271;&#24739;&#32773;&#30340;&#29983;&#23384;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#39044;&#27979;&#32593;&#32476;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#39044;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;BraTS 2021&#19978;&#35757;&#32451;&#19968;&#20010;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36716;&#31227;&#21040;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#65292;&#20197;&#25552;&#21462;&#20016;&#23500;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#22810;&#36890;&#36947;&#29305;&#24449;&#34701;&#21512;&#27169;&#22411;&#21644;&#19968;&#20010;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#27169;&#22359;&#26469;&#23398;&#20064;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#22810;&#36890;&#36947;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30001;&#19968;&#20010;&#38750;&#32447;&#24615;&#20851;&#31995;&#27169;&#22411;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumor is one of the leading causes of cancer death. The high-grade brain tumors are easier to recurrent even after standard treatment. Therefore, developing a method to predict brain tumor recurrence location plays an important role in the treatment planning and it can potentially prolong patient's survival time. There is still little work to deal with this issue. In this paper, we present a deep learning-based brain tumor recurrence location prediction network. Since the dataset is usually small, we propose to use transfer learning to improve the prediction. We first train a multi-modal brain tumor segmentation network on the public dataset BraTS 2021. Then, the pre-trained encoder is transferred to our private dataset for extracting the rich semantic features. Following that, a multi-scale multi-channel feature fusion model and a nonlinear correlation learning module are developed to learn the effective features. The correlation between multi-channel features is modeled by a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13712</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#22312;&#23454;&#36341;&#20013;&#30340;&#21147;&#37327;&#65306;ChatGPT&#21450;&#20854;&#24212;&#29992;&#30340;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20174;&#20107;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Large Language Models&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#20351;&#29992;&#35752;&#35770;&#21644;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;GPT&#21644;BERT&#26679;&#24335;&#30340;LLMs&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20363;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12289;&#32039;&#24613;&#33021;&#21147;&#20197;&#21450;&#29305;&#23450;&#20219;&#21153;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21508;&#31181;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#35828;&#26126;LLMs&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;&#25968;&#25454;&#23545;&#20110;LLMs&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#65292;&#25361;&#25112;&#22312;&#20110;&#24314;&#31435;&#22810;&#26041;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#21270;&#20102;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#19994;&#21153;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.13688</link><description>&lt;p&gt;
&#21512;&#20316;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#37322;&#25918;&#65306;&#20851;&#20110;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning. (arXiv:2304.13688v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13688
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#65292;&#25361;&#25112;&#22312;&#20110;&#24314;&#31435;&#22810;&#26041;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#21270;&#20102;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#19994;&#21153;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#39072;&#35206;&#24615;&#28508;&#21147;&#28304;&#20110;&#22823;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#20294;&#26159;&#24456;&#22823;&#19968;&#37096;&#20998;&#25968;&#25454;&#20998;&#25955;&#22312;&#25968;&#25454;&#23396;&#23707;&#20013;&#65292;&#20854;&#28508;&#21147;&#26410;&#33021;&#24471;&#21040;&#37322;&#25918;&#12290;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#20998;&#25955;&#30340;&#12289;&#28508;&#22312;&#30340;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#22312;&#25216;&#26415;&#19978;&#21487;&#20197;&#25171;&#24320;&#25968;&#25454;&#23396;&#23707;&#65292;&#20174;&#32780;&#37322;&#25918;&#32463;&#27982;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22810;&#20010;&#25317;&#26377;&#25968;&#25454;&#23396;&#23707;&#30340;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#24314;&#31435;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#26159;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24403;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#25104;&#21151;&#23454;&#29616;&#21512;&#20316;AI&#39033;&#30446;&#25152;&#24517;&#39035;&#32771;&#34385;&#30340;&#25351;&#21335;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#22238;&#39038;&#12289;&#28966;&#28857;&#23567;&#32452;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#30340;&#25361;&#25112;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#25193;&#23637;&#30340;&#19994;&#21153;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disruptive potential of AI systems roots in the emergence of big data. Yet, a significant portion is scattered and locked in data silos, leaving its potential untapped. Federated Machine Learning is a novel AI paradigm enabling the creation of AI models from decentralized, potentially siloed data. Hence, Federated Machine Learning could technically open data silos and therefore unlock economic potential. However, this requires collaboration between multiple parties owning data silos. Setting up collaborative business models is complex and often a reason for failure. Current literature lacks guidelines on which aspects must be considered to successfully realize collaborative AI projects. This research investigates the challenges of prevailing collaborative business models and distinct aspects of Federated Machine Learning. Through a systematic literature review, focus group, and expert interviews, we provide a systemized collection of socio-technical challenges and an extended Busin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.13671</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#30340;&#22810;&#30446;&#26631;&#29289;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#23558;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#38134;&#34892;&#36816;&#33829;&#30340;&#21508;&#20010;&#26041;&#38754;&#21487;&#20197;&#25913;&#21892;&#27969;&#31243;&#33258;&#21160;&#21270;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#26381;&#21153;&#27700;&#24179;&#25552;&#21319;&#12290;&#34429;&#28982;ATM&#29616;&#37329;&#29289;&#27969;&#26159;&#24433;&#21709;&#36816;&#33829;&#25104;&#26412;&#21644;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#21364;&#24456;&#23569;&#26377;&#21162;&#21147;&#26469;&#21152;&#20197;&#25913;&#36827;&#12290;&#29305;&#21035;&#26159;&#22312;&#36234;&#21335;&#65292;&#25317;&#26377;&#36229;&#36807;2&#19975;&#21488;ATM&#30340;&#24066;&#22330;&#19978;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21644;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ATM&#29616;&#37329;&#34917;&#20805;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#36827;&#34892;&#20102;&#27010;&#25324;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#21644;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#20248;&#21270;&#25200;&#21160;&#19979;&#30340;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#36798;&#38477;&#20302;10.9%&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#21644;&#26368;&#39640;&#36798;&#25552;&#39640;47.9%&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#65292;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13443</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#22312;&#19981;&#30830;&#23450;&#25200;&#21160;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning. (arXiv:2304.13443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#21644;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#20248;&#21270;&#25200;&#21160;&#19979;&#30340;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#36798;&#38477;&#20302;10.9%&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#21644;&#26368;&#39640;&#36798;&#25552;&#39640;47.9%&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#65292;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20132;&#36890;&#39046;&#22495;&#65292;&#22320;&#38081;&#31995;&#32479;&#26159;&#20851;&#38190;&#30340;&#21487;&#25345;&#32493;&#20844;&#20849;&#20132;&#36890;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#33021;&#28304;&#28040;&#32791;&#23545;&#21487;&#25345;&#32493;&#24615;&#30446;&#26631;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#24310;&#35823;&#21644;&#20056;&#23458;&#27969;&#21464;&#21270;&#31561;&#25200;&#21160;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23545;&#22320;&#38081;&#31995;&#32479;&#30340;&#33021;&#28304;&#25928;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#65292;&#24182;&#20248;&#21270;&#21463;&#25200;&#21160;&#24433;&#21709;&#30340;&#22320;&#38081;&#31995;&#32479;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;10.9&#65285;&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#21644;&#26368;&#39640;&#36798;47.9&#65285;&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hint-Aug&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#39044;&#35757;&#32451;&#30340;FViTs&#23398;&#21040;&#30340;&#39640;&#24230;&#20195;&#34920;&#24615;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;FViTs&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#8220;&#39269;&#39295;&#8221;&#29305;&#24615;&#65292;&#24182;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#35843;&#21442;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12520</link><description>&lt;p&gt;
Hint-Aug: &#20174;&#22522;&#30784;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#33719;&#21462;&#25552;&#31034;&#65292;&#23454;&#29616;&#22686;&#24378;&#30340;&#23569;&#26679;&#26412;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning. (arXiv:2304.12520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12520
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hint-Aug&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#39044;&#35757;&#32451;&#30340;FViTs&#23398;&#21040;&#30340;&#39640;&#24230;&#20195;&#34920;&#24615;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;FViTs&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#8220;&#39269;&#39295;&#8221;&#29305;&#24615;&#65292;&#24182;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#35843;&#21442;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#38656;&#35201;&#35843;&#20248;&#22522;&#30784;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;FViT&#65289;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#23569;&#26679;&#26412;&#35843;&#20248;&#65289;&#65292;&#20805;&#20998;&#21457;&#25381;FViTs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;FViTs&#30340;&#25968;&#25454;&#29305;&#24615;&#26159;&#39269;&#39295;&#30340;&#12290;&#30001;&#20110;&#23569;&#31034;&#20363;&#35843;&#21442;&#25968;&#25454;&#21253;&#21547;&#26377;&#38480;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#27492;&#24773;&#20917;&#19979;&#26080;&#27861;&#21457;&#25381;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;FViTs&#22312;&#23569;&#26679;&#26412;&#35843;&#20248;&#26041;&#38754;&#30340;&#26426;&#20250;&#65306;&#39044;&#20808;&#35757;&#32451;&#30340;FViTs&#24050;&#32463;&#20174;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#20102;&#39640;&#24230;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#36807;&#31243;&#20013;&#23436;&#20840;&#20445;&#30041;&#12290;&#25105;&#20204;&#22240;&#27492;&#20551;&#35774;&#21033;&#29992;&#36825;&#20123;&#24050;&#23398;&#20064;&#30340;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;FViT&#35843;&#20248;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Hint-based Data Augmentation (Hint-Aug) &#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35843;&#25972;&#26679;&#26412;&#30340;&#36807;&#24230;&#25311;&#21512;&#37096;&#20998;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;FViTs&#30340;&#23398;&#20064;&#29305;&#24449;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;FViT&#30340;&#23569;&#26679;&#26412;&#35843;&#20248;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Hint-Aug&#26174;&#30528;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#21442;&#65292;&#20351;FViTs&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12395</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#30340;&#26497;&#38480;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#26377;&#29992;&#27493;&#39588;&#12290; SMART&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#21069;k&#20010;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#31867;&#22411;&#12290;&#30001;&#20110;KG&#20013;&#23384;&#22312;&#22823;&#37327;&#31867;&#22411;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20855;&#20307;&#22320;&#25913;&#21892;&#20102;XBERT&#27969;&#31243;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;KG&#20013;&#27966;&#29983;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;SMART&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
&lt;/p&gt;</description></item><item><title>DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.11015</link><description>&lt;p&gt;
DIN-SQL: &#33258;&#32416;&#27491;&#30340;&#25991;&#26412;&#21040;SQL&#20998;&#35299;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11015
&lt;/p&gt;
&lt;p&gt;
DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#20998;&#35299;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;SQL&#26597;&#35810;&#20855;&#26377;&#22768;&#26126;&#24335;&#32467;&#26500;&#65292;&#20294;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#22823;&#32422;&#25552;&#39640;&#20102;10&#65285;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25512;&#21521;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#22312;Holdout Spider&#25968;&#25454;&#38598;&#19978;&#29978;&#33267;&#36229;&#36807;&#20102;&#32463;&#36807;&#31934;&#35843;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.07788</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#27010;&#29575;&#20915;&#31574;&#26641;&#30340;&#20020;&#24202;&#23454;&#36341;&#36741;&#21161;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assisting clinical practice with fuzzy probabilistic decision trees. (arXiv:2304.07788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07788
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24847;&#35782;&#21040;&#38656;&#35201;&#23436;&#20840;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#26102;&#65292;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36741;&#21161;&#25935;&#24863;&#39046;&#22495;&#20915;&#31574;&#30340;&#36235;&#21183;&#23558;&#20250;&#22686;&#38271;&#65292;&#24182;&#19988;&#21363;&#23558;&#20986;&#21488;&#30340;&#27861;&#35268;&#23558;&#20250;&#21152;&#24378;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20542;&#26012;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20999;&#20837;&#28857;&#20043;&#19968;&#26159;&#21307;&#23398;&#23454;&#36341;&#65292;&#23427;&#21487;&#20197;&#21463;&#30410;&#20110;&#31934;&#30830;&#30340;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20250;&#20135;&#29983;&#20449;&#20219;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;MedFP&#65292;&#23427;&#32467;&#21512;&#20102;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#26469;&#36741;&#21161;&#20020;&#24202;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65307;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#22330;&#26223;&#20013;&#65306;&#32959;&#30244;&#20998;&#31867;&#21644;&#31958;&#23615;&#30149;&#31867;&#22411;2&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for fully human-understandable models is increasingly being recognised as a central theme in AI research. The acceptance of AI models to assist in decision making in sensitive domains will grow when these models are interpretable, and this trend towards interpretable models will be amplified by upcoming regulations. One of the killer applications of interpretable AI is medical practice, which can benefit from accurate decision support methodologies that inherently generate trust. In this work, we propose FPT, (MedFP), a novel method that combines probabilistic trees and fuzzy logic to assist clinical practice. This approach is fully interpretable as it allows clinicians to generate, control and verify the entire diagnosis procedure; one of the methodology's strength is the capability to decrease the frequency of misdiagnoses by providing an estimate of uncertainties and counterfactuals. Our approach is applied as a proof-of-concept to two real medical scenarios: classifying ma
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05350</link><description>&lt;p&gt;
Astroformer&#65306;&#20998;&#31867;&#24182;&#19981;&#24635;&#26159;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#25110;&#37096;&#32626;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26143;&#31995;&#24418;&#24577;&#23545;&#20110;&#29702;&#35299;&#26143;&#31995;&#30340;&#24418;&#25104;&#21644;&#28436;&#21270;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#38656;&#35201;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#31867;&#26143;&#31995;&#24418;&#24577;&#65292;&#24182;&#20174;&#29616;&#20195;&#22825;&#25991;&#23398;&#35843;&#26597;&#20013;&#25552;&#21462;&#29289;&#29702;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#65292;&#20174;CoAtNet&#21644;MaxViT&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26032;&#22534;&#26632;&#35774;&#35745;&#21644;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#30340;Transformer - &#21367;&#31215;&#28151;&#21512;&#12290;&#24182;&#23558;&#20854;&#19982;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#37197;&#23545;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20180;&#32454;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#20026;&#35748;&#35782;&#20154;&#31867;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#35745;&#31639;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.12259</link><description>&lt;p&gt;
&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#30340;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired bodily self-perception model that replicates the rubber hand illusion. (arXiv:2303.12259v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#20026;&#35748;&#35782;&#20154;&#31867;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#35745;&#31639;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#26680;&#24515;&#26159;&#23545;&#33258;&#24049;&#36523;&#20307;&#25317;&#26377;&#26435;&#30340;&#24863;&#30693;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22823;&#33041;&#23545;&#33258;&#36523;&#36523;&#20307;&#32534;&#30721;&#30340;&#26426;&#21046;&#65292;&#20154;&#20204;&#20570;&#20986;&#20102;&#21508;&#31181;&#23581;&#35797;&#65292;&#21457;&#23637;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#30456;&#20851;&#30340;&#34892;&#20026;&#21644;&#31070;&#32463;&#29983;&#29702;&#29616;&#35937;&#12290;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#35299;&#37322;&#27233;&#33014;&#25163;&#24187;&#35273;&#36825;&#26679;&#30340;&#36523;&#20307;&#38169;&#35273;&#23454;&#38469;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#26377;&#20851;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#26426;&#21046;&#21644;&#21487;&#33021;&#30456;&#20851;&#30340;&#33041;&#21306;&#30340;&#27010;&#24565;&#24615;&#25551;&#36848;&#65292;&#20294;&#29616;&#26377;&#30340;&#29702;&#35770;&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#35299;&#37322;&#22823;&#33041;&#22914;&#20309;&#32534;&#30721;&#23545;&#33258;&#24049;&#36523;&#20307;&#30340;&#24863;&#30693;&#21644;&#22914;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25105;&#20204;&#20027;&#35266;&#24863;&#30693;&#30340;&#36523;&#20307;&#38169;&#35273;&#30340;&#35745;&#31639;&#26426;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25972;&#21512;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#29983;&#29289;&#23398;&#21457;&#29616;&#65292;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#20351;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#21050;&#28608;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#26500;&#24314;&#12290;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#27169;&#25311;&#22797;&#21046;&#20102;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#31070;&#32463;&#26426;&#21046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the core of bodily self-consciousness is the perception of the ownership of one's body. Recent efforts to gain a deeper understanding of the mechanisms behind the brain's encoding of the self-body have led to various attempts to develop a unified theoretical framework to explain related behavioral and neurophysiological phenomena. A central question to be explained is how body illusions such as the rubber hand illusion actually occur. Despite the conceptual descriptions of the mechanisms of bodily self-consciousness and the possible relevant brain areas, the existing theoretical models still lack an explanation of the computational mechanisms by which the brain encodes the perception of one's body and how our subjectively perceived body illusions can be generated by neural networks. Here we integrate the biological findings of bodily self-consciousness to propose a Brain-inspired bodily self-perception model, by which perceptions of bodily self can be autonomously constructed withou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;HealthSyn&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#22312;&#20840;&#29699;&#21355;&#29983;&#39046;&#22495;&#20013;&#21457;&#23637;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2303.01954</link><description>&lt;p&gt;
&#22312;&#20840;&#29699;&#21355;&#29983;&#39046;&#22495;&#20013;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generator for Adaptive Interventions in Global Health. (arXiv:2303.01954v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01954
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;HealthSyn&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#22312;&#20840;&#29699;&#21355;&#29983;&#39046;&#22495;&#20013;&#21457;&#23637;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#23383;&#20581;&#24247;&#26377;&#26395;&#25913;&#21464;&#20840;&#29699;&#21355;&#29983;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#36827;&#34892;&#31639;&#27861;&#27979;&#35797;&#21644;&#39564;&#35777;&#30340;&#20851;&#38190;&#26159;&#33021;&#22815;&#35775;&#38382;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;HealthSyn&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#27979;&#35797;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#20013;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#65288;&#20363;&#22914;&#25552;&#37266;&#12289;&#25512;&#33616;&#21644;&#28608;&#21169;&#65289;&#12290;&#29983;&#25104;&#22120;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#20855;&#26377;&#20010;&#20307;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#20010;&#24615;&#21270;&#24178;&#39044;&#32780;&#25913;&#21464;&#12290;&#36825;&#20123;&#34892;&#20026;&#36716;&#21270;&#20026;&#23454;&#38469;&#26085;&#24535;&#65292;&#20351;&#29992;ML&#19987;&#29992;&#30340;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#23450;&#20110;HealthKit&#19982;&#24320;&#28304;SDK&#20013;&#21253;&#21547;&#30340;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#21151;&#33021;&#12290;&#36825;&#20123;&#26085;&#24535;&#21487;&#20197;&#25552;&#20379;&#29992;&#25143;&#25351;&#26631;&#12290;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#34892;&#20026;&#21644;&#27169;&#25311;&#25216;&#26415;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#21516;&#26102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and digital health have the potential to transform global health. However, having access to representative data to test and validate algorithms in realistic production environments is essential. We introduce HealthSyn, an open-source synthetic data generator of user behavior for testing reinforcement learning algorithms in the context of mobile health interventions. The generator utilizes Markov processes to generate diverse user actions, with individual user behavioral patterns that can change in reaction to personalized interventions (i.e., reminders, recommendations, and incentives). These actions are translated into actual logs using an ML-purposed data schema specific to the mobile health application functionality included with HealthKit, and open-source SDK. The logs can be fed to pipelines to obtain user metrics. The generated data, which is based on real-world behaviors and simulation techniques, can be used to develop, test, and evaluate, both ML algori
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.06675</link><description>&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#30340;&#31526;&#21495;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21457;&#29616;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#25928;&#25628;&#32034;&#25216;&#26415;&#26469;&#25506;&#32034;&#26080;&#38480;&#21644;&#31232;&#30095;&#30340;&#31243;&#24207;&#31354;&#38388;&#12290;&#20026;&#20102;&#22635;&#34917;&#20195;&#29702;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#24040;&#22823;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31243;&#24207;&#36873;&#25321;&#21644;&#31616;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;$ \textbf {Lion} $&#65288;$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $&#65289;&#12290;&#23427;&#30340;&#35760;&#24518;&#25928;&#29575;&#27604;Adam&#26356;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#36319;&#36394;&#21160;&#37327;&#12290;&#19982;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#19981;&#21516;&#65292;&#36890;&#36807;&#31526;&#21495;&#36816;&#31639;&#35745;&#31639;&#30340;&#27599;&#20010;&#21442;&#25968;&#30340;&#26356;&#26032;&#20855;&#26377;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#23558;Lion&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#21644;Adafactor&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;Lion&#23558;&#22312;ImageNet&#19978;ViT&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;2&#65285;&#65292;&#24182;&#33410;&#30465;&#20102;&#22810;&#36798;5&#20493;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#21270;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#32500;&#24230;&#20010;&#24615;&#21270;&#36793;&#32536;&#27169;&#22411;&#26469;&#28040;&#38500;&#32852;&#37030;&#23398;&#20064;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.04464</link><description>&lt;p&gt;
&#22810;&#32500;&#20010;&#24615;&#21270;&#36793;&#32536;&#27169;&#22411;&#22312;&#23454;&#29616;&#26356;&#20844;&#24179;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models. (arXiv:2302.04464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#21270;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#32500;&#24230;&#20010;&#24615;&#21270;&#36793;&#32536;&#27169;&#22411;&#26469;&#28040;&#38500;&#32852;&#37030;&#23398;&#20064;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#25252;&#38544;&#31169;&#35757;&#32451;&#22823;&#35268;&#27169;&#22320;&#29702;&#20998;&#24067;&#24335;&#36793;&#32536;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36793;&#32536;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#22312;&#20844;&#24179;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#36890;&#24120;&#20250;&#23548;&#33268;&#26368;&#36817;&#19968;&#27969;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65288;CFL&#65289;&#26469;&#28040;&#38500;&#22810;&#32500;&#24230;&#32852;&#37030;&#23398;&#20064;&#30340;&#24322;&#36136;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;CFL &#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#37327;&#36523;&#23450;&#21046;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#22312;&#32447;&#35757;&#32451;&#30340;&#27169;&#22411;&#25628;&#32034;&#21161;&#25163;&#21644;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#20849;&#21516;&#24341;&#23548;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CFL &#22312;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20840;&#26632;&#20248;&#21183;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#65288;&#22312;&#38750;&#24322;&#36136;&#29615;&#22659;&#19979;&#39640;&#36798; 7.2%&#65292;&#22312;&#24322;&#36136;&#29615;&#22659;&#19979;&#39640;&#36798; 21.8%&#65289;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging technique that trains massive and geographically distributed edge data while maintaining privacy. However, FL has inherent challenges in terms of fairness and computational efficiency due to the rising heterogeneity of edges, and thus usually results in sub-optimal performance in recent state-of-the-art (SOTA) solutions. In this paper, we propose a Customized Federated Learning (CFL) system to eliminate FL heterogeneity from multiple dimensions. Specifically, CFL tailors personalized models from the specially designed global model for each client jointly guided by an online trained model-search helper and a novel aggregation algorithm. Extensive experiments demonstrate that CFL has full-stack advantages for both FL training and edge reasoning and significantly improves the SOTA performance w.r.t. model accuracy (up to 7.2% in the non-heterogeneous environment and up to 21.8% in the heterogeneous environment), efficiency, and FL fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20445;&#25252;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#25928;&#29992;&#25200;&#21160;&#19981;&#22826;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UPGD&#26377;&#21161;&#20110;&#20943;&#23569;&#36951;&#24536;&#21644;&#20445;&#25345;&#21487;&#22609;&#24615;&#65292;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#22823;&#26377;&#29992;&#22788;&#12290;</title><link>http://arxiv.org/abs/2302.03281</link><description>&lt;p&gt;
&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65306;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20445;&#25252;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#25928;&#29992;&#25200;&#21160;&#19981;&#22826;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UPGD&#26377;&#21161;&#20110;&#20943;&#23569;&#36951;&#24536;&#21644;&#20445;&#25345;&#21487;&#22609;&#24615;&#65292;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#22823;&#26377;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#38754;&#23545;&#38750;&#31283;&#24577;&#38382;&#39064;&#26102;&#24448;&#24448;&#38590;&#20197;&#24555;&#36895;&#36866;&#24212;&#65292;&#22240;&#20026;&#23427;&#20204;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#34928;&#20943;&#30340;&#21487;&#22609;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#23398;&#20064;&#32773;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#22240;&#20026;&#20182;&#20204;&#21487;&#33021;&#20250;&#36951;&#24536;&#26377;&#29992;&#30340;&#29305;&#24449;&#25110;&#38590;&#20197;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21464;&#24471;&#26080;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36825;&#31181;&#31639;&#27861;&#38750;&#24120;&#36866;&#21512;&#36830;&#32493;&#23398;&#20064;&#20195;&#29702;&#12290;UPGD&#20445;&#25252;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#19981;&#34987;&#36951;&#24536;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#25928;&#29992;&#25200;&#21160;&#19981;&#22826;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;UPGD&#26377;&#21161;&#20110;&#20943;&#23569;&#36951;&#24536;&#21644;&#20445;&#25345;&#21487;&#22609;&#24615;&#65292;&#20351;&#29616;&#20195;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#26377;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861; $\textit{MPC-RRL}$&#65292;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#24471;&#21040;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.13313</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#32435;&#20837;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#65292;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recurrent Reinforcement Learning into Model Predictive Control for Adaptive Control in Autonomous Driving. (arXiv:2301.13313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861; $\textit{MPC-RRL}$&#65292;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#24471;&#21040;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25511;&#21046;&#25216;&#26415;&#65292;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290; MPC&#25511;&#21046;&#22120;&#30340;&#25104;&#21151;&#24378;&#28872;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#20869;&#37096;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290; &#28982;&#32780;&#65292;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#30340;&#38745;&#24577;&#21442;&#25968;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#20869;&#37096;&#21644;&#22806;&#37096;&#24178;&#25200;&#12290; &#26412;&#25991;&#39318;&#20808;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#21560;&#25910;&#21040;&#35266;&#23519;&#20013;&#24182;&#23558;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#32500;&#25252;&#21040;&#38544;&#34255;&#29366;&#24577;&#20013;; &#20854;&#27425;&#65292;&#36890;&#36807;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65288;RRL&#65289;&#23398;&#20064;&#36882;&#24402;&#31574;&#30053;&#65292;&#25345;&#32493;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#21442;&#25968;&#20197;&#23454;&#29616;&#26368;&#20248;&#21644;&#33258;&#36866;&#24212;&#25511;&#21046;; &#26368;&#21518;&#65292;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#23545;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65288;&#31216;&#20026; $\textit{MPC-RRL}$&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25200;&#21160;&#33539;&#22260;&#20869;&#23454;&#29616;&#40065;&#26834;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control (MPC) is attracting tremendous attention in the autonomous driving task as a powerful control technique. The success of an MPC controller strongly depends on an accurate internal dynamics model. However, the static parameters, usually learned by system identification, often fail to adapt to both internal and external perturbations in real-world scenarios. In this paper, we firstly (1) reformulate the problem as a Partially Observed Markov Decision Process (POMDP) that absorbs the uncertainties into observations and maintains Markov property into hidden states; and (2) learn a recurrent policy continually adapting the parameters of the dynamics model via Recurrent Reinforcement Learning (RRL) for optimal and adaptive control; and (3) finally evaluate the proposed algorithm (referred as $\textit{MPC-RRL}$) in CARLA simulator and leading to robust behaviours under a wide range of perturbations.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#24605;&#32771;&#26102;&#24102;&#26377;&#21453;&#24605;&#29305;&#36136;&#65292;&#21487;&#20197;&#24212;&#23545;&#19990;&#30028;&#30340;&#27169;&#31946;&#24615;&#12289;&#26032;&#20852;&#30693;&#35782;&#21644;&#31038;&#20250;&#32972;&#26223;&#12290;&#24403;&#21069;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.10823</link><description>&lt;p&gt;
&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Reflective Artificial Intelligence. (arXiv:2301.10823v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10823
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24605;&#32771;&#26102;&#24102;&#26377;&#21453;&#24605;&#29305;&#36136;&#65292;&#21487;&#20197;&#24212;&#23545;&#19990;&#30028;&#30340;&#27169;&#31946;&#24615;&#12289;&#26032;&#20852;&#30693;&#35782;&#21644;&#31038;&#20250;&#32972;&#26223;&#12290;&#24403;&#21069;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#26159;&#35753;&#35745;&#31639;&#26426;&#20570;&#20154;&#31867;&#21487;&#20197;&#20570;&#30340;&#20107;&#24773;&#65292;&#38543;&#30528;&#25105;&#20204;&#21521;&#36825;&#20010;&#30446;&#26631;&#36808;&#36827;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#20154;&#31867;&#20219;&#21153;&#20132;&#32473;&#26426;&#22120;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36890;&#24120;&#22312;&#27934;&#23519;&#21147;&#21644;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#23450;&#31243;&#24230;&#30340;&#19981;&#24179;&#34913;&#65306;&#26032;&#30340;&#28145;&#20837;&#27934;&#23519;&#34429;&#28982;&#23384;&#22312;&#65292;&#20294;&#35768;&#22810;&#20154;&#31867;&#24605;&#32771;&#26102;&#24102;&#26469;&#30340;&#37325;&#35201;&#21697;&#36136;&#21364;&#23436;&#20840;&#32570;&#22833;&#12290;&#22240;&#27492;&#65292;&#20107;&#20851;&#20309;&#31181;&#24515;&#26234;&#29305;&#24449;&#24050;&#34987;&#22797;&#21046;&#65292;&#20309;&#31181;&#29305;&#24449;&#23384;&#22312;&#32570;&#22833;&#20197;&#21450;&#20854;&#37325;&#35201;&#24615;&#21313;&#20998;&#20851;&#38190;&#12290;&#22312;&#22788;&#29702;&#19990;&#30028;&#24102;&#26469;&#30340;&#27169;&#31946;&#24615;&#12289;&#26032;&#20852;&#30693;&#35782;&#21644;&#31038;&#20250;&#32972;&#26223;&#26102;&#65292;&#20154;&#31867;&#23558;&#21453;&#24605;&#20316;&#20026;&#26680;&#24515;&#29305;&#24449;&#24102;&#20837;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#24403;&#21069;&#20027;&#27969;&#30340;&#20154;&#24037;&#26234;&#33021;&#21364;&#23436;&#20840;&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20160;&#20040;&#26159;&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#31995;&#32479;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#26234;&#33021;&#20307;&#20013;&#30340;&#21453;&#24605;&#27010;&#24565;&#65292;&#27010;&#36848;&#20102;&#19968;&#31181;&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is about making computers that do the sorts of things that minds can do, and as we progress towards this goal, we tend to increasingly delegate human tasks to machines. However, AI systems usually do these tasks with an unusual imbalance of insight and understanding: new, deeper insights are present, yet many important qualities that a human mind would have previously brought to the activity are utterly absent. Therefore, it is crucial to ask which features of minds have we replicated, which are missing, and if that matters. One core feature that humans bring to tasks, when dealing with the ambiguity, emergent knowledge, and social context presented by the world, is reflection. Yet this capability is utterly missing from current mainstream AI. In this paper we ask what reflective AI might look like. Then, drawing on notions of reflection in complex systems, cognitive science, and agents, we sketch an architecture for reflective AI agents, and highlight ways
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#24403;&#21069;&#37197;&#32622;&#30340;&#21333;&#20010;RGB&#22270;&#20687;&#23601;&#21487;&#20197;&#24674;&#22797;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#20851;&#33410;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#24182;&#35757;&#32451;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#22312;&#32570;&#23569;&#26412;&#20307;&#24863;&#30693;&#26102;&#24674;&#22797;&#31995;&#32479;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.02051</link><description>&lt;p&gt;
&#20174;RGB&#22270;&#20687;&#20013;&#24674;&#22797;&#26426;&#22120;&#20154;&#20851;&#33410;&#35282;&#24230;&#30340;&#36317;&#31163;&#20960;&#20309;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image. (arXiv:2301.02051v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#24403;&#21069;&#37197;&#32622;&#30340;&#21333;&#20010;RGB&#22270;&#20687;&#23601;&#21487;&#20197;&#24674;&#22797;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#20851;&#33410;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#24182;&#35757;&#32451;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#22312;&#32570;&#23569;&#26412;&#20307;&#24863;&#30693;&#26102;&#24674;&#22797;&#31995;&#32479;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#24178;&#39044;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#27700;&#19979;&#65292;&#22826;&#31354;&#25110;&#21361;&#38505;&#29615;&#22659;&#65289;&#25805;&#20316;&#30340;&#33258;&#20027;&#25805;&#32437;&#31995;&#32479;&#38656;&#35201;&#39640;&#24230;&#24378;&#20581;&#30340;&#24863;&#30693;&#21644;&#36890;&#20449;&#25925;&#38556;&#12290;&#20851;&#38190;&#26159;&#65292;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31639;&#27861;&#38656;&#35201;&#25552;&#20379;&#20851;&#33410;&#32534;&#30721;&#22120;&#25552;&#20379;&#30340;&#20934;&#30830;&#20851;&#33410;&#35282;&#24230;&#25968;&#25454;&#27969;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#23548;&#33268;&#21151;&#33021;&#20007;&#22833;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#24403;&#21069;&#37197;&#32622;&#30340;&#21333;&#20010;RGB&#22270;&#20687;&#23601;&#21487;&#20197;&#26816;&#32034;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#20851;&#33410;&#35282;&#24230;&#65292;&#20026;&#24674;&#22797;&#31995;&#32479;&#21151;&#33021;&#24320;&#36767;&#20102;&#19968;&#26465;&#36884;&#24452;&#65292;&#36825;&#26102;&#24120;&#29992;&#30340;&#26412;&#20307;&#24863;&#30693;&#26080;&#27861;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#37197;&#32622;&#31354;&#38388;&#30340;&#36317;&#31163;&#20960;&#20309;&#34920;&#31034;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25191;&#34892;&#19982;&#26816;&#27979;&#21040;&#30340;&#32467;&#26500;&#20851;&#38190;&#28857;&#30456;&#20851;&#30340;&#36317;&#31163;&#30340;2D&#21040;3D&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous manipulation systems operating in domains where human intervention is difficult or impossible (e.g., underwater, extraterrestrial or hazardous environments) require a high degree of robustness to sensing and communication failures. Crucially, motion planning and control algorithms require a stream of accurate joint angle data provided by joint encoders, the failure of which may result in an unrecoverable loss of functionality. In this paper, we present a novel method for retrieving the joint angles of a robot manipulator using only a single RGB image of its current configuration, opening up an avenue for recovering system functionality when conventional proprioceptive sensing is unavailable. Our approach, based on a distance-geometric representation of the configuration space, exploits the knowledge of a robot's kinematic model with the goal of training a shallow neural network that performs a 2D-to-3D regression of distances associated with detected structural keypoints. It
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21521;&#20154;&#31867;&#21327;&#20316;&#32773;&#26597;&#35810;&#26576;&#20123;&#27010;&#24565;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.07430</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interactive Concept Bottleneck Models. (arXiv:2212.07430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21521;&#20154;&#31867;&#21327;&#20316;&#32773;&#26597;&#35810;&#26576;&#20123;&#27010;&#24565;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#26159;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#39318;&#20808;&#38024;&#23545;&#19982;&#39044;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#39044;&#27979;&#26631;&#31614;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#39044;&#27979;&#26368;&#32456;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;CBMs&#25193;&#23637;&#21040;&#20132;&#20114;&#24335;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21521;&#20154;&#31867;&#21327;&#20316;&#32773;&#26597;&#35810;&#26576;&#20123;&#27010;&#24565;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20132;&#20114;&#31574;&#30053;&#65292;&#22312;&#39044;&#27979;&#26102;&#36873;&#25321;&#35201;&#35831;&#27714;&#26631;&#31614;&#30340;&#27010;&#24565;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#26368;&#32456;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#27010;&#24565;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27010;&#24565;&#23545;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#22312;Caltech-UCSD Birds&#12289;CheXpert&#21644;OAI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#38745;&#24577;&#26041;&#27861;&#21644;&#20027;&#21160;&#29305;&#24449;&#37319;&#38598;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#20114;&#24335;CBM&#21482;&#38656;&#36827;&#34892;5&#27425;&#20132;&#20114;&#65292;&#23601;&#33021;&#22312;&#31454;&#20105;&#22522;&#20934;&#19978;&#23454;&#29616;5-10%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models (CBMs) are interpretable neural networks that first predict labels for human-interpretable concepts relevant to the prediction task, and then predict the final label based on the concept label predictions. We extend CBMs to interactive prediction settings where the model can query a human collaborator for the label to some concepts. We develop an interaction policy that, at prediction time, chooses which concepts to request a label for so as to maximally improve the final prediction. We demonstrate that a simple policy combining concept prediction uncertainty and influence of the concept on the final prediction achieves strong performance and outperforms static approaches as well as active feature acquisition methods proposed in the literature. We show that the interactive CBM can achieve accuracy gains of 5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD Birds, CheXpert and OAI datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.06027</link><description>&lt;p&gt;
&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#20195;&#29702;&#21830;&#19982;&#22810;&#20010;&#23545;&#31435;&#20195;&#29702;&#21830;&#36827;&#34892;&#25112;&#30053;&#20114;&#21160;&#65292;&#23545;&#25163;&#21487;&#33021;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#24773;&#22659;&#65292;&#35774;&#35745;&#20195;&#29702;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#35745;&#31639;&#25110;&#36924;&#36817;&#30456;&#20851;&#30340;&#21338;&#24328;&#29702;&#35770;&#35299;&#65292;&#22914;&#32435;&#20160;&#22343;&#34913;&#65292;&#28982;&#21518;&#36981;&#24490;&#35268;&#23450;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31574;&#30053;&#24573;&#30053;&#20102;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#20219;&#20309;&#35266;&#23519;&#65292;&#36825;&#20123;&#35266;&#23519;&#21487;&#33021;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#25910;&#38598;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#23545;&#19977;&#20154; Kuhn &#25169;&#20811;&#23637;&#24320;&#20102;&#23545;&#35768;&#22810;&#30495;&#23454;&#23545;&#25163;&#21644;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65288;VQT&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;Vision Transformers&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;VQT&#22312;&#35757;&#32451;&#20013;&#20855;&#26377;&#20869;&#23384;&#25928;&#29575;&#65292;&#30456;&#27604;&#20110;&#35768;&#22810;&#20854;&#20182; fine-tuning &#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.03220</link><description>&lt;p&gt;
&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65306;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#21442;&#25968;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning. (arXiv:2212.03220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65288;VQT&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;Vision Transformers&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;VQT&#22312;&#35757;&#32451;&#20013;&#20855;&#26377;&#20869;&#23384;&#25928;&#29575;&#65292;&#30456;&#27604;&#20110;&#35768;&#22810;&#20854;&#20182; fine-tuning &#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#35757;&#32451;&#27169;&#22411;&#30340;&#20013;&#38388;&#29305;&#24449;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#38750;&#24120;&#26377;&#29992;&#65292;&#21363;&#20351;&#27169;&#22411;&#39592;&#24178;&#20445;&#25345;&#20923;&#32467;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20013;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;-&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65288;VQT&#65289;&#65292;&#29992;&#20110;&#32858;&#21512;Vision Transformers&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#23618;&#24341;&#20837;&#23569;&#37327;&#21487;&#23398;&#20064;&#30340;&#8220;&#26597;&#35810;&#8221;&#20196;&#29260;&#65292;VQT&#21033;&#29992;Transformer&#30340;&#20869;&#37096;&#36816;&#34892;&#26426;&#21046;&#26469;&#8220;&#24635;&#32467;&#8221;&#27599;&#20010;&#23618;&#30340;&#20016;&#23500;&#20013;&#38388;&#29305;&#24449;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#22836;&#12290;&#30001;&#20110;VQT&#20445;&#25345;&#20102;&#20013;&#38388;&#29305;&#24449;&#30340;&#23436;&#25972;&#24615;&#24182;&#20165;&#23398;&#20064;&#20102;&#22914;&#20309;&#32452;&#21512;&#23427;&#20204;&#65292;&#22240;&#27492;&#19982;&#35768;&#22810;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#27604;&#65292;VQT&#22312;&#35757;&#32451;&#20013;&#20855;&#26377;&#20869;&#23384;&#25928;&#29575;&#65292;&#21518;&#32773;&#38656;&#35201;&#23398;&#20064;&#22914;&#20309;&#36866;&#24212;&#29305;&#24449;&#24182;&#38656;&#35201;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#36825;&#20063;&#34920;&#26126;&#20102;VQT&#19982;&#36825;&#20123;&#26041;&#27861;&#22312;&#32763;&#35793;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;VQT&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#23545;&#35937;&#26816;&#27979;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intermediate features of a pre-trained model have been shown informative for making accurate predictions on downstream tasks, even if the model backbone is kept frozen. The key challenge is how to utilize these intermediate features given their gigantic amount. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. Through introducing a handful of learnable ``query'' tokens to each layer, VQT leverages the inner workings of Transformers to ``summarize'' rich intermediate features of each layer, which can then be used to train the prediction heads of downstream tasks. As VQT keeps the intermediate features intact and only learns to combine them, it enjoys memory efficiency in training, compared to many other parameter-efficient fine-tuning approaches that learn to adapt features and need back-propagation through the entire backbone. This also suggests the complementary role between VQT and those approaches in tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crown-CAM&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#30417;&#30563;&#36873;&#25321;&#28608;&#27963;&#26144;&#23556;&#12289;&#35745;&#31639;&#23616;&#37096;&#24471;&#20998;&#26144;&#23556;&#21644;&#38750;&#19978;&#19979;&#25991;&#32972;&#26223;&#25233;&#21046;&#31561;&#27493;&#39588;&#65292;&#26082;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#26641;&#20896;&#30340;&#31934;&#32454;&#23450;&#20301;&#65292;&#21448;&#21487;&#20197;&#37327;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13126</link><description>&lt;p&gt;
Crown-CAM&#65306;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images. (arXiv:2211.13126v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crown-CAM&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#30417;&#30563;&#36873;&#25321;&#28608;&#27963;&#26144;&#23556;&#12289;&#35745;&#31639;&#23616;&#37096;&#24471;&#20998;&#26144;&#23556;&#21644;&#38750;&#19978;&#19979;&#25991;&#32972;&#26223;&#25233;&#21046;&#31561;&#27493;&#39588;&#65292;&#26082;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#26641;&#20896;&#30340;&#31934;&#32454;&#23450;&#20301;&#65292;&#21448;&#21487;&#20197;&#37327;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#30340;&#21487;&#35270;&#35299;&#37322;&#20801;&#35768;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#20197;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26641;&#20896;&#26816;&#27979;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#65288;Crown-CAM&#65289;&#65292;&#23427;&#20811;&#26381;&#20102;&#20197;&#24448;&#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#30340;&#23450;&#20301;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21516;&#26102;&#20026;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21160;&#24577;&#30340;&#38382;&#39064;&#25552;&#20379;&#21487;&#38752;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#12290;&#23427;&#21253;&#25324;&#26080;&#30417;&#30563;&#36873;&#25321;&#28608;&#27963;&#26144;&#23556;&#65292;&#35745;&#31639;&#23616;&#37096;&#24471;&#20998;&#26144;&#23556;&#20197;&#21450;&#38750;&#19978;&#19979;&#25991;&#32972;&#26223;&#25233;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#26641;&#20896;&#30340;&#31934;&#32454;&#23450;&#20301;&#65292;&#36866;&#29992;&#20110;&#23494;&#26519;&#25110;&#32773;&#27809;&#26377;&#26641;&#20896;&#30340;&#22330;&#26223;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20110;IoU&#30340;&#25351;&#26631;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37327;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#22270;&#20687;&#20013;&#26641;&#20896;&#25110;&#26080;&#26641;&#20896;&#21306;&#22495;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual explanation of ``black-box'' models allows researchers in explainable artificial intelligence (XAI) to interpret the model's decisions in a human-understandable manner. In this paper, we propose interpretable class activation mapping for tree crown detection (Crown-CAM) that overcomes inaccurate localization &amp; computational complexity of previous methods while generating reliable visual explanations for the challenging and dynamic problem of tree crown detection in aerial images. It consists of an unsupervised selection of activation maps, computation of local score maps, and non-contextual background suppression to efficiently provide fine-grain localization of tree crowns in scenarios with dense forest trees or scenes without tree crowns. Additionally, two Intersection over Union (IoU)-based metrics are introduced to effectively quantify both the accuracy and inaccuracy of generated explanations with respect to regions with or even without tree crowns in the image. Empirical e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#35299;&#20915;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.10858</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25913;&#21892;&#30382;&#32932;&#30142;&#30149;&#19981;&#21516;&#35786;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An interpretable imbalanced semi-supervised deep learning framework for improving differential diagnosis of skin diseases. (arXiv:2211.10858v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#20266;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#35299;&#20915;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#30142;&#30149;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#30142;&#30149;&#20043;&#19968;&#12290;&#26412;&#25991;&#21033;&#29992;58,457&#24352;&#30382;&#32932;&#22270;&#20687;&#21644;10,857&#20010;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#23545;&#22810;&#31867;&#26234;&#33021;&#30382;&#32932;&#35786;&#26029;&#26694;&#26550;&#65288;ISDL&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#19981;&#24179;&#34913;&#24615;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#26679;&#26412;&#19978;&#33258;&#25105;&#35757;&#32451;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20351;&#23569;&#25968;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#26679;&#26412;&#20855;&#26377;&#26356;&#39640;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#21033;&#29992;&#65292;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;ISDL&#22312;&#22810;&#26631;&#31614;&#30382;&#32932;&#30149;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20934;&#30830;&#29575;&#20026;0.979&#65292;&#28789;&#25935;&#24230;&#20026;0.975&#65292;&#29305;&#24322;&#24230;&#20026;0.973&#65292;&#23439;F1&#20998;&#25968;&#20026;0.974&#65292;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.999&#12290;&#25105;&#20204;&#36824;&#23558;Shapley Additive explanation (SHAP)&#26041;&#27861;&#19982;ISDL&#32467;&#21512;&#65292;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24335;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20020;&#24202;&#35786;&#26029;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#20998;&#24067;&#26368;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dermatological diseases are among the most common disorders worldwide. This paper presents the first study of the interpretability and imbalanced semi-supervised learning of the multiclass intelligent skin diagnosis framework (ISDL) using 58,457 skin images with 10,857 unlabeled samples. Pseudo-labelled samples from minority classes have a higher probability at each iteration of class-rebalancing self-training, thereby promoting the utilization of unlabeled samples to solve the class imbalance problem. Our ISDL achieved a promising performance with an accuracy of 0.979, sensitivity of 0.975, specificity of 0.973, macro-F1 score of 0.974 and area under the receiver operating characteristic curve (AUC) of 0.999 for multi-label skin disease classification. The Shapley Additive explanation (SHAP) method is combined with our ISDL to explain how the deep learning model makes predictions. This finding is consistent with the clinical diagnosis. We also proposed a sampling distribution optimisa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#23436;&#22791;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; GCPNet&#65292;&#29992;&#20110;3D&#20998;&#23376;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20960;&#20309;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20854;&#20013;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#24471;&#21040;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#39640;&#20986;5%&#20197;&#19978;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.02504</link><description>&lt;p&gt;
&#29992;&#20110;&#19977;&#32500;&#20998;&#23376;&#22270;&#30340;&#20960;&#20309;&#23436;&#22791;&#24863;&#30693;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Complete Perceptron Networks for 3D Molecular Graphs. (arXiv:2211.02504v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#23436;&#22791;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; GCPNet&#65292;&#29992;&#20110;3D&#20998;&#23376;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20960;&#20309;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20854;&#20013;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#24471;&#21040;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#39640;&#20986;5%&#20197;&#19978;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#21019;&#26032;&#21644;&#24378;&#22823;&#30340;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21457;&#23637;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#31561;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#23398;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#20174;&#32780;&#22312;&#31185;&#23398;&#39046;&#22495;&#22914;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#21644;&#35774;&#35745;&#20013;&#23454;&#29616;&#20102;&#31361;&#30772;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GCPNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#23436;&#22791;&#12289;SE(3)-&#31561;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;3D&#20998;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22235;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20219;&#21153;&#30340;&#20005;&#23494;&#23454;&#39564;&#35777;&#26126;&#20102;GCPNet&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#30456;&#20851;&#31995;&#25968;&#20026;0.608&#65292;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#39640;&#20986;5%&#20197;&#19978;&#65307;&#65288;2&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#25490;&#21517;&#22312;&#30446;&#26631;&#26412;&#22320;&#21644;&#25968;&#25454;&#38598;&#20840;&#23616;&#20043;&#38388;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#65292;&#20998;&#21035;&#20026;0.616&#21644;0.871&#65307;&#65288;3&#65289;Newtownian&#22810;&#20307;&#31995;&#32479;&#30340;&#24314;&#27169;&#24179;&#22343;&#25104;&#32489;&#36798;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
The field of geometric deep learning has had a profound impact on the development of innovative and powerful graph neural network architectures. Disciplines such as computer vision and computational biology have benefited significantly from such methodological advances, which has led to breakthroughs in scientific domains such as protein structure prediction and design. In this work, we introduce GCPNet, a new geometry-complete, SE(3)-equivariant graph neural network designed for 3D molecular graph representation learning. Rigorous experiments across four distinct geometric tasks demonstrate that GCPNet's predictions (1) for protein-ligand binding affinity achieve a statistically significant correlation of 0.608, more than 5% greater than current state-of-the-art methods; (2) for protein structure ranking achieve statistically significant target-local and dataset-global correlations of 0.616 and 0.871, respectively; (3) for Newtownian many-body systems modeling achieve a task-averaged 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#39046;&#22495;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20986;&#20102;&#20174;&#22836;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#23637;&#21644;&#20960;&#20010;&#20851;&#38190;&#35282;&#24230;&#65292;&#21253;&#25324;&#20998;&#23376;&#25551;&#36848;&#31526;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.16484</link><description>&lt;p&gt;
&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey of Chemical Pre-trained Models. (arXiv:2210.16484v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#39046;&#22495;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20986;&#20102;&#20174;&#22836;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#23637;&#21644;&#20960;&#20010;&#20851;&#38190;&#35282;&#24230;&#65292;&#21253;&#25324;&#20998;&#23376;&#25551;&#36848;&#31526;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#23545;&#20110;&#21508;&#31181;&#29983;&#21270;&#24212;&#29992;&#65292;&#20174;&#23646;&#24615;&#39044;&#27979;&#21040;&#33647;&#29289;&#35774;&#35745;&#65292;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20998;&#23376;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33719;&#21462;&#36825;&#20123;&#26631;&#35760;&#36890;&#24120;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#20184;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#20998;&#23376;&#25968;&#25454;&#24211;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#23613;&#31649;&#29289;&#26377;&#25152;&#20540;&#65292;&#20294;&#36825;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#39046;&#22495;&#32570;&#20047;&#31995;&#32479;&#30340;&#22238;&#39038;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24635;&#32467;CPMs&#36827;&#23637;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20174;&#22836;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#25512;&#21160;CPM&#30340;&#30740;&#31350;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#35282;&#24230;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20998;&#23376;&#25551;&#36848;&#31526;&#65292;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Molecular Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder arc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#32452;&#25439;&#22833;&#65292;&#24182;&#34920;&#26126;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#21644;NLP&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#20998;&#32452;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2210.16315</link><description>&lt;p&gt;
&#36229;&#36234;&#26657;&#20934;&#65306;&#20272;&#35745;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#32452;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Beyond calibration: estimating the grouping loss of modern neural networks. (arXiv:2210.16315v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#32452;&#25439;&#22833;&#65292;&#24182;&#34920;&#26126;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#21644;NLP&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#20998;&#32452;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20998;&#31867;&#22120;&#32473;&#20986;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26159;&#30830;&#20445;&#30693;&#24773;&#20915;&#31574;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35823;&#26657;&#20934;&#19978;&#65292;&#21363;&#27169;&#22411;&#20998;&#25968;&#30340;&#36807;&#24230;&#25110;&#19981;&#36275;&#32622;&#20449;&#12290;&#28982;&#32780;&#65292;&#26657;&#20934;&#36824;&#19981;&#22815;&#65306;&#21363;&#20351;&#20934;&#30830;&#29575;&#26368;&#39640;&#30340;&#23436;&#32654;&#26657;&#20934;&#20998;&#31867;&#22120;&#20063;&#21487;&#33021;&#20855;&#26377;&#19982;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#30456;&#21435;&#29978;&#36828;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#36825;&#26159;&#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#25152;&#36896;&#25104;&#30340;&#65292;&#21363;&#20197;&#30456;&#21516;&#32622;&#20449;&#24230;&#24471;&#20998;&#20294;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#19981;&#21516;&#30340;&#26679;&#26412;&#12290;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#29702;&#35770;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#26657;&#20934;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#24449;&#21333;&#20010;&#38169;&#35823;&#30340;&#32570;&#22833;&#37096;&#20998;&#26159;&#20998;&#32452;&#25439;&#22833;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#26657;&#20934;&#25439;&#22833;&#30340;&#20272;&#35745;&#22120;&#65292;&#20294;&#22312;&#26631;&#20934;&#35774;&#32622;&#20013;&#19981;&#23384;&#22312;&#20998;&#32452;&#25439;&#22833;&#30340;&#20272;&#35745;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#20998;&#32452;&#25439;&#22833;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#22312;&#35270;&#35273;&#21644;NLP&#20013;&#34920;&#29616;&#20986;&#20998;&#32452;&#25439;&#22833;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#20559;&#31227;&#35774;&#32622;&#20013;&#65292;&#36825;&#31361;&#26174;&#20102;&#23427;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#20998;&#35299;&#36335;&#24452;&#26469;&#25171;&#30772;betaTCVAE&#20013;&#30340;&#20840;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;VAE&#33021;&#22815;&#26356;&#21152;&#28789;&#27963;&#22320;&#21010;&#20998;&#25968;&#25454;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#23481;&#37327;&#21644;&#28508;&#21464;&#37327;&#20998;&#32452;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.08794</link><description>&lt;p&gt;
&#25171;&#30772;betaTCVAE&#20013;&#20840;&#30456;&#20851;&#24615;&#30340;&#39764;&#21650;
&lt;/p&gt;
&lt;p&gt;
Break The Spell Of Total Correlation In betaTCVAE. (arXiv:2210.08794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#20998;&#35299;&#36335;&#24452;&#26469;&#25171;&#30772;betaTCVAE&#20013;&#30340;&#20840;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;VAE&#33021;&#22815;&#26356;&#21152;&#28789;&#27963;&#22320;&#21010;&#20998;&#25968;&#25454;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#23481;&#37327;&#21644;&#28508;&#21464;&#37327;&#20998;&#32452;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#20154;&#24037;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#20013;&#30340;&#29420;&#31435;&#29305;&#24449;&#21644;&#20381;&#36182;&#29305;&#24449;&#26159;&#28151;&#20081;&#30340;&#12290;&#22914;&#20309;&#26500;&#24314;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#20197;&#28789;&#27963;&#21010;&#20998;&#24182;&#26377;&#25928;&#22320;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#29305;&#24449;&#65292;&#26159;&#26080;&#30417;&#30563;&#30340;&#20998;&#31163;&#34920;&#24449;&#23398;&#20064;&#30340;&#20027;&#35201;&#28966;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24635;&#30456;&#20851;&#24615;&#30340;&#36845;&#20195;&#20998;&#35299;&#36335;&#24452;&#65292;&#24182;&#20174;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;VAE&#30340;&#20998;&#31163;&#34920;&#24449;&#33021;&#21147;&#12290;&#26032;&#24320;&#21457;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#28508;&#21464;&#37327;&#32500;&#24230;&#32452;&#21512;&#25104;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#20943;&#36731;&#32452;&#21512;&#20013;&#36793;&#32536;&#20998;&#24067;&#30340;&#29420;&#31435;&#24615;&#32422;&#26463;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#26356;&#26131;&#25805;&#20316;&#20808;&#39564;&#20998;&#24067;&#30340;&#28508;&#21464;&#37327;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20351;&#24471;VAE&#33021;&#22815;&#35843;&#25972;&#21442;&#25968;&#23481;&#37327;&#65292;&#20197;&#28789;&#27963;&#21010;&#20998;&#30456;&#20851;&#21644;&#29420;&#31435;&#30340;&#25968;&#25454;&#29305;&#24449;&#12290;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#27169;&#22411;&#23481;&#37327;&#19982;&#28508;&#21464;&#37327;&#20998;&#32452;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the absence of artificial labels, the independent and dependent features in the data are cluttered. How to construct the inductive biases of the model to flexibly divide and effectively contain features with different complexity is the main focal point of unsupervised disentangled representation learning. This paper proposes a new iterative decomposition path of total correlation and explains the disentangled representation ability of VAE from the perspective of model capacity allocation. The newly developed objective function combines latent variable dimensions into joint distribution while relieving the independence constraints of marginal distributions in combination, leading to latent variables with a more manipulable prior distribution. The novel model enables VAE to adjust the parameter capacity to divide dependent and independent data features flexibly. Experimental results on various datasets show an interesting relevance between model capacity and the latent variable groupi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38381;&#24335;&#38382;&#31572;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#29983;&#25104;&#19978;&#19979;&#25991;&#26469;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#24320;&#25918;&#24335;&#26041;&#27861;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.06349</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#29983;&#25104;&#25552;&#39640;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38381;&#24335;&#38382;&#31572;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#29983;&#25104;&#19978;&#19979;&#25991;&#26469;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#24320;&#25918;&#24335;&#26041;&#27861;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23553;&#38381;&#24335;&#38382;&#31572;&#38656;&#35201;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22238;&#31572;&#24320;&#25918;&#24335;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#24037;&#20316;&#35201;&#20040;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30452;&#25509;&#24494;&#35843;&#65292;&#35201;&#20040;&#36890;&#36807;&#25552;&#31034;&#20449;&#24687;&#26469;&#21033;&#29992;&#23384;&#20648;&#30340;&#30693;&#35782;&#12290;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38381;&#24335;&#38382;&#31572;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#31895;&#30053;&#21040;&#31934;&#32454;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#21644;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#39044;&#20808;&#35757;&#32451;&#30340;LM&#29983;&#25104;&#38024;&#23545;&#32473;&#23450;&#38382;&#39064;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#20351;&#29992;&#36825;&#20010;LM&#36890;&#36807;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#25552;&#31034;&#31572;&#26696;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28040;&#38500;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#36824;&#23545;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#20102;&#36793;&#38469;&#21270;&#22788;&#29702;&#12290;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#23553;&#38381;&#24335;&#38382;&#31572;&#26041;&#27861;&#65288;&#22914;&#31934;&#30830;&#21305;&#37197; 68.6% &#23545; 55.3%&#65289;&#65292;&#19988;&#19982;&#24320;&#25918;&#24335;&#26041;&#27861;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Closed-book question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge. Prior work on closed-book QA either directly finetunes or prompts a pretrained language model (LM) to leverage the stored knowledge. However, they do not fully exploit the parameterized knowledge. To address this issue, we propose a two-stage, closed-book QA framework which employs a coarse-to-fine approach to extract relevant knowledge and answer a question. Our approach first generates a related context for a given question by prompting a pretrained LM. We then prompt the same LM for answer prediction using the generated context and the question. Additionally, to eliminate failure caused by context uncertainty, we marginalize over generated contexts. Experimental results on three QA benchmarks show that our method significantly outperforms previous closed-book QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book methods th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32454;&#32990;Latent Go-Explore&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#65292;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#27867;&#21270;&#21040;&#20219;&#20309;&#29615;&#22659;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2208.14928</link><description>&lt;p&gt;
&#26080;&#32454;&#32990;Latent Go-Explore
&lt;/p&gt;
&lt;p&gt;
Cell-Free Latent Go-Explore. (arXiv:2208.14928v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32454;&#32990;Latent Go-Explore&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#65292;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#27867;&#21270;&#21040;&#20219;&#20309;&#29615;&#22659;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Latent Go-Explore (LGE)&#30340;&#31616;&#21333;&#36890;&#29992;&#26041;&#27861;&#65292;&#22522;&#20110;Go-Explore&#33539;&#24335;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;Go-Explore&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;&#27809;&#26377;&#39046;&#22495;&#30693;&#35782;&#21644;&#21333;&#20803;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#21040;&#20219;&#20309;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LGE&#21487;&#20197;&#28789;&#27963;&#22320;&#19982;&#20219;&#20309;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#30340;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LGE&#27604;Go-Explore&#26356;&#21152;&#40065;&#26834;&#65292;&#22312;&#22810;&#20010;&#38590;&#20197;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#65288;&#21253;&#25324;Montezuma&#30340;&#22797;&#20167;&#65289;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25506;&#32034;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning (RL). Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. We argue that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. Thus, we show that LGE can be flexibly combined with any strategy for learning a latent representation. Our results indicate that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Reven
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2206.04530</link><description>&lt;p&gt;
DORA&#65306;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#20540;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#23398;&#20064;&#22797;&#26434;&#25277;&#35937;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#24847;&#22806;&#22320;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#26816;&#26597;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24847;&#22806;&#30340;&#27010;&#24565;&#24448;&#24448;&#34920;&#29616;&#20026;&#19982;&#25152;&#38656;&#30340;&#20219;&#21153;&#19981;&#31526;&#30340;&#24322;&#24120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DORA&#65288;Data-agnOstic Representation Analysis&#65289;&#65306;&#29992;&#20110;&#20998;&#26512;DNN&#34920;&#31034;&#31354;&#38388;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#25152;&#25552;&#20986;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#26497;&#31471;&#28608;&#27963;&#65288;EA&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#32593;&#32476;&#20869;&#33258;&#35828;&#26126;&#33021;&#21147;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#24230;&#37327;&#30340;&#27491;&#30830;&#24615;&#21644;&#19982;&#20154;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#36317;&#31163;&#30340;&#19968;&#33268;&#24615;&#12290;EA&#36317;&#31163;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#34920;&#24449;&#65292;&#20854;&#22522;&#26412;&#27010;&#24565;&#34987;&#35748;&#20026;&#26159;&#19981;&#33258;&#28982;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38656;&#35201;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2205.14410</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2205.14410v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38656;&#35201;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#25484;&#25569;&#32473;&#23450;&#20219;&#21153;&#26102;&#38656;&#35201;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#27425;&#25968;&#12290;&#36801;&#31227;&#23398;&#20064;&#25552;&#20986;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21738;&#20010;&#28304;&#20219;&#21153;&#26377;&#36164;&#26684;&#29992;&#20110;&#30693;&#35782;&#25552;&#21462;&#65292;&#20197;&#21450;&#36873;&#25321;&#21738;&#20123;&#31639;&#27861;&#32452;&#20214;&#36827;&#34892;&#36801;&#31227;&#65292;&#26159;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#20005;&#37325;&#38556;&#30861;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#27169;&#22359;&#21270;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#32780;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#22870;&#21169;&#20989;&#25968;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#25511;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#22495;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.
&lt;/p&gt;</description></item><item><title>SIBILA&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.06234</link><description>&lt;p&gt;
SIBILA: &#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#22312;&#21307;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SIBILA: A novel interpretable ensemble of general-purpose machine learning models applied to medical contexts. (arXiv:2205.06234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06234
&lt;/p&gt;
&lt;p&gt;
SIBILA&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#30103;&#20173;&#28982;&#26159;&#31185;&#23398;&#23478;&#20204;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;&#39044;&#27979;&#20010;&#20307;&#24739;&#32773;&#26368;&#21512;&#36866;&#27835;&#30103;&#26041;&#27861;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#24320;&#21457;&#23450;&#21046;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#32570;&#20047;&#23545;&#32467;&#26524;&#30340;&#35299;&#37322;&#20197;&#21450;&#39640;&#35745;&#31639;&#35201;&#27714;&#20351;&#35768;&#22810;&#20154;&#19981;&#24895;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#33410;&#30465;&#26102;&#38388;&#24182;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;SIBILA&#24212;&#36816;&#32780;&#29983;&#12290;SIBILA&#26159;&#19968;&#32452;&#24212;&#29992;&#20102;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#20197;&#35782;&#21035;&#26368;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#30001;&#20110;&#35299;&#37322;&#24615;&#31639;&#27861;&#21487;&#33021;&#19981;&#24444;&#27492;&#19968;&#33268;&#65292;&#22240;&#27492;&#23454;&#26045;&#20102;&#20849;&#35782;&#38454;&#27573;&#26469;&#20272;&#35745;&#27599;&#20010;&#21464;&#37327;&#23545;&#39044;&#27979;&#30340;&#20840;&#23616;&#36129;&#29486;&#12290;SIBILA&#34987;&#35013;&#31665;&#20197;&#22312;&#20219;&#20309;&#39640;&#24615;&#33021;&#35745;&#31639;&#24179;&#21488;&#19978;&#36816;&#34892;&#12290;&#23613;&#31649;&#26368;&#21021;&#30446;&#30340;&#26159;&#20316;&#20026;&#21629;&#20196;&#34892;&#24037;&#20855;&#65292;&#20294;&#23427;&#20063;&#21487;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;Web&#30028;&#38754;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized medicine remains a major challenge for scientists. The rapid growth of Machine learning and Deep learning has made them a feasible alternative for predicting the most appropriate therapy for individual patients. However, the need to develop a custom model for every dataset, the lack of interpretation of their results and high computational requirements make many reluctant to use these methods. Aiming to save time and bring light to the way models work internally, SIBILA has been developed. SIBILA is an ensemble of machine learning and deep learning models that applies a range of interpretability algorithms to identify the most relevant input features. Since the interpretability algo- rithms may not be in line with each other, a consensus stage has been imple- mented to estimate the global attribution of each variable to the predictions. SIBILA is containerized to be run on any high-performance computing plat- form. Although conceived as a command-line tool, it is also av
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#21367;&#65292;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25506;&#32034;&#20250;&#35758;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2205.02370</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#21367;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#30340;PREME
&lt;/p&gt;
&lt;p&gt;
PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#21367;&#65292;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25506;&#32034;&#20250;&#35758;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20250;&#35758;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#31649;&#29702;&#21644;&#32452;&#32455;&#26448;&#26009;&#65292;&#29305;&#21035;&#26159;&#24403;&#21442;&#19982;&#32773;&#38169;&#36807;&#35752;&#35770;&#24182;&#38656;&#35201;&#24110;&#21161;&#24555;&#36895;&#25506;&#32034;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#20559;&#22909;&#30340;&#20250;&#35758;&#25506;&#32034;&#30340;&#20132;&#20114;&#24335;&#38382;&#21367;&#12290;&#32467;&#26524;&#65292;&#29992;&#25143;&#20250;&#33719;&#24471;&#19968;&#20010;&#25512;&#33616;&#38382;&#39064;&#21015;&#34920;&#65292;&#20197;&#21453;&#26144;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#30001;&#20110;&#36825;&#39033;&#20219;&#21153;&#26159;&#26032;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#31574;&#30053;&#12290;&#21363;&#65292;&#23427;&#36890;&#36807;&#24230;&#37327;&#36890;&#36807;&#38382;&#21367;&#29983;&#25104;&#30340;&#38382;&#39064;&#26377;&#22810;&#23569;&#21487;&#22238;&#31572;&#26469;&#30830;&#20445;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#20026;&#25506;&#32034;&#28304;&#20250;&#35758;&#30340;&#28145;&#24230;&#25552;&#20379;&#20102;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in the volume of online meetings necessitates automated tools for managing and organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy. Namely, it measures how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration.
&lt;/p&gt;</description></item><item><title>GrIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.07281</link><description>&lt;p&gt;
GrIPS&#65306;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25351;&#20196;&#25628;&#32034;&#65292;&#29992;&#20110;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07281
&lt;/p&gt;
&lt;p&gt;
GrIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#25552;&#31034;&#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#29992;&#26032;&#33539;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#36890;&#36807;&#25163;&#21160;&#37325;&#20889;&#25110;&#26799;&#24230;&#35843;&#25972;&#26469;&#25552;&#39640;&#36825;&#20123;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#32791;&#26102;&#19988;&#38656;&#35201;&#20027;&#35266;&#35299;&#37322;&#65292;&#32780;&#22522;&#20110;&#26799;&#24230;&#30340;&#35843;&#25972;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#65292;&#23545;&#20110;&#22522;&#20110;API&#30340;&#27169;&#22411;&#26469;&#35828;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gradient-free Instructional Prompt Search (GrIPS)&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#12290;GrIPS&#25509;&#21463;&#38754;&#21521;&#20154;&#31867;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#24182;&#33258;&#21160;&#36820;&#22238;&#23436;&#21892;&#30340;&#32534;&#36753;&#25552;&#31034;&#65292;&#21516;&#26102;&#20801;&#35768;&#22522;&#20110;API&#30340;&#35843;&#25972;&#12290;&#20351;&#29992;InstructGPT&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;GrIPS&#23558;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;4.30&#20010;&#30334;&#20998;&#28857;&#65288;OPT&#65292;BLOOM&#31561;&#20219;&#21153;&#20063;&#26377;&#31867;&#20284;&#30340;&#25913;&#36827;&#65289;
&lt;/p&gt;
&lt;p&gt;
Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PETRAW&#25361;&#25112;&#65292;&#25506;&#35752;&#22522;&#20110;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#21644;&#20998;&#21106;&#25968;&#25454;&#36827;&#34892;&#25163;&#26415;&#24037;&#20316;&#27969;&#31243;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.05821</link><description>&lt;p&gt;
PEg TRAnsfer Workflow recognition challenge&#25253;&#21578;&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#26159;&#21542;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?. (arXiv:2202.05821v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PETRAW&#25361;&#25112;&#65292;&#25506;&#35752;&#22522;&#20110;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#21644;&#20998;&#21106;&#25968;&#25454;&#36827;&#34892;&#25163;&#26415;&#24037;&#20316;&#27969;&#31243;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;PEg TRAnsfert Workflow recognition&#8221;&#65288;PETRAW&#65289;&#25361;&#25112;&#30340;&#35774;&#35745;&#19982;&#32467;&#26524;&#65292;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#21644;&#20998;&#21106;&#31561;&#21333;&#19968;&#25110;&#22810;&#31181;&#27169;&#24577;&#24320;&#21457;&#22806;&#31185;&#25163;&#26415;&#24037;&#20316;&#27969;&#31243;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;&#23427;&#20204;&#30340;&#38468;&#21152;&#20540;&#12290;PETRAW&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;&#34394;&#25311;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#30340;&#25554;&#31649;&#36716;&#31227;&#24207;&#21015;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#24037;&#20316;&#27969;&#31243;&#27880;&#37322;&#32452;&#25104;&#65292;&#25152;&#36848;&#27880;&#37322;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#65288;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#27963;&#21160;&#65289;&#19978;&#25551;&#36848;&#20102;&#24207;&#21015;&#12290;&#21442;&#36187;&#32773;&#34987;&#25552;&#20986;&#20102;&#20116;&#39033;&#20219;&#21153;&#65306;&#20854;&#20013;&#19977;&#39033;&#20219;&#21153;&#19982;&#25152;&#26377;&#31890;&#24230;&#30340;&#35782;&#21035;&#19982;&#19968;&#31181;&#21487;&#29992;&#30340;&#27169;&#24577;&#30456;&#20851;&#65292;&#32780;&#20854;&#20182;&#20219;&#21153;&#21017;&#36890;&#36807;&#22810;&#31181;&#27169;&#24577;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#35782;&#21035;&#38382;&#39064;&#12290;&#24179;&#22343;&#24212;&#29992;&#30456;&#20851;&#24179;&#34913;&#20934;&#30830;&#24615;&#65288;AD-Accuracy&#65289;&#34987;&#29992;&#20316;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#32771;&#34385;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#22240;&#20026;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20855;&#26377;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the design and results of the "PEg TRAnsfert Workflow recognition" (PETRAW) challenge whose objective was to develop surgical workflow recognition methods based on one or several modalities, among video, kinematic, and segmentation data, in order to study their added value. The PETRAW challenge provided a data set of 150 peg transfer sequences performed on a virtual simulator. This data set was composed of videos, kinematics, semantic segmentation, and workflow annotations which described the sequences at three different granularity levels: phase, step, and activity. Five tasks were proposed to the participants: three of them were related to the recognition of all granularities with one of the available modalities, while the others addressed the recognition with a combination of modalities. Average application-dependent balanced accuracy (AD-Accuracy) was used as evaluation metric to take unbalanced classes into account and because it is more clinically relevant tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#26500;&#24314;&#23436;&#25972;&#30340;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#31995;&#32479;&#65292;&#20197;&#25552;&#21319;&#20854;&#33258;&#20027;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2110.00273</link><description>&lt;p&gt;
&#20174;SLAM&#21040;&#24773;&#22659;&#24863;&#30693;&#65306;&#25361;&#25112;&#19982;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#26500;&#24314;&#23436;&#25972;&#30340;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#31995;&#32479;&#65292;&#20197;&#25552;&#21319;&#20854;&#33258;&#20027;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#12289;&#23433;&#20840;&#22320;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20854;&#23545;&#29615;&#22659;&#65288;&#21363;&#24773;&#20917;&#65289;&#30340;&#20102;&#35299;&#12290;&#20808;&#36827;&#30340;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25191;&#34892;&#25216;&#33021;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#33258;&#20027;&#34892;&#21160;&#12290;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#26159;&#20154;&#31867;&#30340;&#19968;&#31181;&#22522;&#26412;&#33021;&#21147;&#65292;&#24050;&#22312;&#24515;&#29702;&#23398;&#12289;&#20891;&#20107;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#28145;&#20837;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#23578;&#26410;&#32771;&#34385;&#24773;&#22659;&#24863;&#30693;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#27010;&#24565;&#65292;&#22914;&#24863;&#30693;&#12289;&#31354;&#38388;&#24863;&#30693;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#12289;&#29366;&#24577;&#20272;&#35745;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#26144;&#23556;&#65288;SLAM&#65289;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#33258;&#20027;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23436;&#25972;&#30340;SA&#31995;&#32479;&#38138;&#24179;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26500;&#24314;&#26426;&#22120;&#20154;SA&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#25152;&#28041;&#21450;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;
&lt;/p&gt;
&lt;p&gt;
The capability of a mobile robot to efficiently and safely perform complex missions is limited by its knowledge of the environment, namely the situation. Advanced reasoning, decision-making, and execution skills enable an intelligent agent to act autonomously in unknown environments. Situational Awareness (SA) is a fundamental capability of humans that has been deeply studied in various fields, such as psychology, military, aerospace, and education. Nevertheless, it has yet to be considered in robotics, which has focused on single compartmentalized concepts such as sensing, spatial perception, sensor fusion, state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, the present research aims to connect the broad multidisciplinary existing knowledge to pave the way for a complete SA system for mobile robotics that we deem paramount for autonomy. To this aim, we define the principal components to structure a robotic SA and their area of competence. Accordingly, this paper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21830;&#19994;&#26694;&#26550;&#20013;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21518;&#38376;&#23433;&#20840;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#25915;&#20987;&#23454;&#29616;&#21518;&#38376;&#35302;&#21457;&#24182;&#36867;&#36991;&#26816;&#27979;&#65292;&#20174;&#32780;&#21361;&#21450;&#24050;&#37096;&#32626;&#30340;&#27169;&#22411;&#23433;&#20840;&#65292;&#21487;&#33021;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2108.09187</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21830;&#19994;&#26694;&#26550;&#30340;&#37327;&#21270;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Quantization Backdoors to Deep Learning Commercial Frameworks. (arXiv:2108.09187v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21830;&#19994;&#26694;&#26550;&#20013;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21518;&#38376;&#23433;&#20840;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#25915;&#20987;&#23454;&#29616;&#21518;&#38376;&#35302;&#21457;&#24182;&#36867;&#36991;&#26816;&#27979;&#65292;&#20174;&#32780;&#21361;&#21450;&#24050;&#37096;&#32626;&#30340;&#27169;&#22411;&#23433;&#20840;&#65292;&#21487;&#33021;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#30001;&#20110;&#20302;&#24310;&#36831;&#21644;&#39640;&#38544;&#31169;&#20445;&#25252;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#37096;&#32626;&#22312;&#26080;&#22788;&#19981;&#22312;&#30340;&#36793;&#32536;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, there is a burgeoning demand for deploying deep learning (DL) models on ubiquitous edge Internet of Things (IoT) devices attributed to their low latency and high privacy preservation. However, DL models are often large in size and require large-scale computation, which prevents them from being placed directly onto IoT devices, where resources are constrained and 32-bit floating-point (float-32) operations are unavailable. Commercial framework (i.e., a set of toolkits) empowered model quantization is a pragmatic solution that enables DL deployment on mobile devices and embedded systems by effortlessly post-quantizing a large high-precision model (e.g., float-32) into a small low-precision model (e.g., int-8) while retaining the model inference accuracy. However, their usability might be threatened by security vulnerabilities.  This work reveals that the standard quantization toolkits can be abused to activate a backdoor. We demonstrate that a full-precision backdoored model w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;SQN&#65292;&#36890;&#36807;&#21033;&#29992;&#28857;&#20113;&#23616;&#37096;&#37051;&#22495;&#20869;&#24378;&#28872;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#38544;&#24335;&#22686;&#24378;&#39640;&#24230;&#31232;&#30095;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20351;&#24471;&#20165;&#38656;&#35201;0.1&#65285;&#30340;&#38543;&#26426;&#27880;&#37322;&#28857;&#36827;&#34892;&#35757;&#32451;&#21363;&#21487;&#22312;&#19971;&#20010;&#22823;&#35268;&#27169;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#23436;&#25972;&#28857;&#20113;&#27880;&#37322;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2104.04891</link><description>&lt;p&gt;
SQN: &#22823;&#35268;&#27169;&#19977;&#32500;&#28857;&#20113;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds. (arXiv:2104.04891v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;SQN&#65292;&#36890;&#36807;&#21033;&#29992;&#28857;&#20113;&#23616;&#37096;&#37051;&#22495;&#20869;&#24378;&#28872;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#38544;&#24335;&#22686;&#24378;&#39640;&#24230;&#31232;&#30095;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20351;&#24471;&#20165;&#38656;&#35201;0.1&#65285;&#30340;&#38543;&#26426;&#27880;&#37322;&#28857;&#36827;&#34892;&#35757;&#32451;&#21363;&#21487;&#22312;&#19971;&#20010;&#22823;&#35268;&#27169;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#23436;&#25972;&#28857;&#20113;&#27880;&#37322;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#23436;&#20840;&#26631;&#27880;&#32791;&#26102;&#26114;&#36149;&#12290;&#38543;&#30528;&#25317;&#26377;&#25968;&#21313;&#20159;&#28857;&#30340;&#22823;&#22411;&#28857;&#20113;&#25968;&#25454;&#38598;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#25105;&#20204;&#26159;&#21542;&#26377;&#24517;&#35201;&#36827;&#34892;&#23436;&#25972;&#27880;&#37322;&#65311;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#38754;&#23545;1&#65285;&#38543;&#26426;&#28857;&#27880;&#37322;&#65292;&#22522;&#20110;&#23436;&#20840;&#27880;&#37322;&#20551;&#35774;&#35774;&#35745;&#30340;&#29616;&#26377;&#22522;&#32447;&#20063;&#21482;&#20250;&#30053;&#24494;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#36229;&#36807;&#36825;&#20010;&#28857;&#65292;&#20363;&#22914;&#22312;0.1&#65285;&#27880;&#37322;&#22788;&#65292;&#20998;&#21106;&#20934;&#30830;&#24615;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#30001;&#20110;&#28857;&#20113;&#26159;3D&#19990;&#30028;&#30340;&#26679;&#26412;&#65292;&#23616;&#37096;&#37051;&#22495;&#20869;&#28857;&#30340;&#20998;&#24067;&#30456;&#23545;&#22343;&#21248;&#65292;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#26469;&#38544;&#24335;&#22686;&#24378;&#39640;&#24230;&#31232;&#30095;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;Semantic Query Network&#65288;SQN&#65289;&#22312;&#24369;&#30417;&#30563;&#26041;&#26696;&#19979;&#22312;&#19971;&#20010;&#22823;&#35268;&#27169;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20165;&#38656;&#35201;0.1&#65285;&#30340;&#38543;&#26426;&#27880;&#37322;&#28857;&#36827;&#34892;&#35757;&#32451;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#23436;&#25972;&#28857;&#20113;&#27880;&#37322;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labelling point clouds fully is highly time-consuming and costly. As larger point cloud datasets with billions of points become more common, we ask whether the full annotation is even necessary, demonstrating that existing baselines designed under a fully annotated assumption only degrade slightly even when faced with 1% random point annotations. However, beyond this point, e.g., at 0.1% annotations, segmentation accuracy is unacceptably low. We observe that, as point clouds are samples of the 3D world, the distribution of points in a local neighborhood is relatively homogeneous, exhibiting strong semantic similarity. Motivated by this, we propose a new weak supervision method to implicitly augment highly sparse supervision signals. Extensive experiments demonstrate the proposed Semantic Query Network (SQN) achieves promising performance on seven large-scale open datasets under weak supervision schemes, while requiring only 0.1% randomly annotated points for training, greatly reducing 
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#24212;&#29992;&#20110;UAV&#36965;&#24863;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2101.10861</link><description>&lt;p&gt;
&#12298;UAV&#36965;&#24863;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#24212;&#29992;&#20110;UAV&#36965;&#24863;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20855;&#26377;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#24449;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#26102;&#38388;&#24207;&#21015;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#26377;&#20851;DNN&#31639;&#27861;&#24212;&#29992;&#30340;&#35843;&#26597;&#21644;&#25991;&#29486;&#32508;&#36848;&#65292;&#35797;&#22270;&#24635;&#32467;&#20854;&#23376;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#24212;&#29992;&#22312;&#33322;&#31354;&#24863;&#30693;&#30740;&#31350;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#36827;&#34892;&#23558;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#21644;&#8220;UAV&#36965;&#24863;&#8221;&#20004;&#20010;&#20027;&#39064;&#32467;&#21512;&#36215;&#26469;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21160;&#26426;&#26159;&#23545;&#24212;&#29992;&#20110;&#22522;&#20110;UAV&#25104;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#22522;&#26412;&#21407;&#29702;&#36827;&#34892;&#32508;&#21512;&#35780;&#36848;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#25551;&#36848;&#29992;&#20110;&#26368;&#36817;UAV&#33719;&#21462;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#20849;&#31579;&#36873;&#20102;232&#31687;&#21457;&#34920;&#22312;&#22269;&#38469;&#31185;&#23398;&#26399;&#21002;&#36164;&#26009;&#20013;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) learn representation from data with an impressive capability, and brought important breakthroughs for processing images, time-series, natural language, audio, video, and many others. In the remote sensing field, surveys and literature revisions specifically involving DNNs algorithms' applications have been conducted in an attempt to summarize the amount of information produced in its subfields. Recently, Unmanned Aerial Vehicles (UAV) based applications have dominated aerial sensing research. However, a literature revision that combines both "deep learning" and "UAV remote sensing" thematics has not yet been conducted. The motivation for our work was to present a comprehensive review of the fundamentals of Deep Learning (DL) applied in UAV-based imagery. We focused mainly on describing classification and regression techniques used in recent applications with UAV-acquired data. For that, a total of 232 papers published in international scientific journal data
&lt;/p&gt;</description></item></channel></rss>