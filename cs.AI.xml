<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.10172</link><description>&lt;p&gt;
DialogStudio&#65306;&#38754;&#21521;&#20250;&#35805; AI &#30340;&#26368;&#20016;&#23500;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#32479;&#19968;&#25968;&#25454;&#38598;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10172
&lt;/p&gt;
&lt;p&gt;
DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20250;&#35805; AI &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; DialogStudio&#65306;&#26368;&#22823;&#12289;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20197;&#19968;&#33268;&#30340;&#26684;&#24335;&#32479;&#19968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#38750;&#24120;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378; DialogStudio &#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30830;&#23450;&#20102;&#35768;&#21487;&#35777;&#65292;&#24182;&#20026;&#36873;&#23450;&#23545;&#35805;&#35774;&#35745;&#20102;&#39046;&#22495;&#24863;&#30693;&#25552;&#31034;&#65292;&#20197;&#20415;&#20419;&#36827;&#25351;&#23548;&#24863;&#30693;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#38598;&#21512;&#24320;&#21457;&#20102;&#20250;&#35805; AI &#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#25688;&#35201;&#29983;&#25104;&#21644;&#20998;&#24067;&#24335;&#25991;&#23383;&#22522;&#20934;&#23545;&#35805;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
&lt;/p&gt;</description></item><item><title>LightPath&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#21644;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10171</link><description>&lt;p&gt;
LightPath: &#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LightPath: Lightweight and Scalable Path Representation Learning. (arXiv:2307.10171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10171
&lt;/p&gt;
&lt;p&gt;
LightPath&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#21644;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36335;&#24452;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#20132;&#36890;&#21644;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#26381;&#21153;&#36825;&#20123;&#24212;&#29992;&#65292;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#25552;&#20379;&#36335;&#24452;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#36335;&#24452;&#25490;&#24207;&#21644;&#26053;&#34892;&#25104;&#26412;&#20272;&#35745;&#65289;&#20013;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#25805;&#20316;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#21644;&#32511;&#33394;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#23545;&#36164;&#28304;&#28040;&#32791;&#21644;&#21487;&#20280;&#32553;&#24615;&#27425;&#35201;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;LightPath&#65292;&#26088;&#22312;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#65292;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.  We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good sca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24635;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24635;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35752;&#35770;&#20013;&#20174;&#19981;&#23384;&#22312;&#21040;&#26080;&#22788;&#19981;&#22312;&#21482;&#29992;&#20102;&#20960;&#24180;&#30340;&#26102;&#38388;&#12290;&#30001;&#20110;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24456;&#38590;&#30830;&#23450;&#21097;&#20313;&#30340;&#25361;&#25112;&#21644;&#24050;&#32463;&#21462;&#24471;&#30340;&#24212;&#29992;&#25104;&#21151;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31995;&#32479;&#30340;&#19968;&#32452;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#20197;&#20415;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#24555;&#22320;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#20803;&#20803;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#31038;&#20132;&#36710;&#36742;&#22810;&#26679;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10160</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;&#39550;&#39542;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Driving Policy Learning with Guided Meta Reinforcement Learning. (arXiv:2307.10160v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#20803;&#20803;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#31038;&#20132;&#36710;&#36742;&#22810;&#26679;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#20132;&#20114;&#24335;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#25104;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#22266;&#23450;&#30340;&#34892;&#20026;&#31574;&#30053;&#26469;&#25511;&#21046;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#31038;&#20132;&#36710;&#36742;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#21040;&#30340;&#39550;&#39542;&#31574;&#30053;&#36807;&#25311;&#21512;&#29615;&#22659;&#65292;&#20351;&#20854;&#38590;&#20197;&#19982;&#20855;&#26377;&#19981;&#21516;&#12289;&#26410;&#35265;&#36807;&#34892;&#20026;&#30340;&#36710;&#36742;&#33391;&#22909;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#26679;&#30340;&#39550;&#39542;&#31574;&#30053;&#20316;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#20803;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38543;&#26426;&#21270;&#31038;&#20132;&#36710;&#36742;&#30340;&#22522;&#20110;&#20132;&#20114;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#30340;&#24341;&#23548;&#31574;&#30053;&#26377;&#25928;&#22320;&#35757;&#32451;&#20803;&#20803;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#31038;&#20132;&#36710;&#36742;&#30001;&#23398;&#20064;&#21040;&#30340;&#20803;&#20803;&#31574;&#30053;&#25511;&#21046;&#30340;&#29615;&#22659;&#65292;&#26469;&#22686;&#24378;&#33258;&#20027;&#36710;&#36742;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#19968;&#31181;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#30340;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unsee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#20351;&#29992;&#26631;&#20934;&#24418;&#24335;&#30340;&#22870;&#21169;&#22609;&#36896;&#21644;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#39640;&#32500;&#31995;&#32479;&#20013;&#65292;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#65288;PBRS&#65289;&#23545;&#20110;&#25910;&#25947;&#36895;&#24230;&#30340;&#25552;&#21319;&#25928;&#26524;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.10142</link><description>&lt;p&gt;
&#23545;&#20110;&#23398;&#20064;&#20154;&#24418;&#26426;&#26800;&#34892;&#36208;&#30340;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#20351;&#29992;&#26631;&#20934;&#24418;&#24335;&#30340;&#22870;&#21169;&#22609;&#36896;&#21644;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#39640;&#32500;&#31995;&#32479;&#20013;&#65292;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#65288;PBRS&#65289;&#23545;&#20110;&#25910;&#25947;&#36895;&#24230;&#30340;&#25552;&#21319;&#25928;&#26524;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;(RL)&#27969;&#31243;&#20013;&#65292;&#20027;&#35201;&#25361;&#25112;&#24448;&#24448;&#26159;&#35774;&#35745;&#21644;&#35843;&#25972;&#22870;&#21169;&#20989;&#25968;&#12290;&#33391;&#22909;&#35774;&#35745;&#30340;&#22609;&#24418;&#22870;&#21169;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#21046;&#23450;&#22870;&#21169;&#21487;&#33021;&#19982;&#26399;&#26395;&#30340;&#34892;&#20026;&#30456;&#20914;&#31361;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#35843;&#25972;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#29978;&#33267;&#19981;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#24418;(PBRS)&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26368;&#20248;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#24418;&#26469;&#21152;&#24555;&#23398;&#20064;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#23616;&#38480;&#20110;&#32593;&#26684;&#19990;&#30028;&#21644;&#20302;&#32500;&#31995;&#32479;&#65292;&#32780;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#26631;&#20934;&#24418;&#24335;&#30340;&#22870;&#21169;&#22609;&#36896;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;PBRS&#30340;&#26631;&#20934;&#24418;&#24335;&#21644;&#22609;&#24418;&#36827;&#34892;&#20102;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#39640;&#32500;&#31995;&#32479;&#20013;&#65292;PBRS&#30340;&#25910;&#25947;&#36895;&#24230;&#21482;&#26377;&#24494;&#23567;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;PBRS&#22870;&#21169;&#39033;&#20855;&#26377;&#37325;&#22823;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10112</link><description>&lt;p&gt;
&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#24739;&#32773;&#38431;&#21015;&#37325;&#32452;&#20026;&#25152;&#35859;&#30340;&#20154;&#21475;&#22270;&#26102;&#65292;&#26368;&#21021;&#29420;&#31435;&#30340;&#25968;&#25454;&#28857;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#22270;&#32467;&#26500;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;&#36825;&#31181;&#20154;&#21475;&#22270;&#36827;&#34892;&#21307;&#23398;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36866;&#21512;&#30340;&#22270;&#32467;&#26500;&#30340;&#26500;&#24314;&#26159;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27493;&#39588;&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#22270;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#20165;&#36866;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#21644;&#31163;&#25955;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#21482;&#35206;&#30422;&#20102;&#19968;&#23567;&#37096;&#20998;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#30340;&#25193;&#23637;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#20855;&#20307;&#30340;GAMs&#65306;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#25105;&#20204;&#23558;GAMs&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20010;&#36339;&#36291;&#65292;&#24182;&#20026;&#22238;&#24402;&#20219;&#21153;&#23450;&#20041;&#20102;&#21516;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#21644;&#21382;&#21490;&#25968;&#25454;&#65292;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#31185;&#23398;&#20915;&#31574;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#35299;&#20915;&#36947;&#36335;&#32500;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10085</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36947;&#36335;&#27573;&#25512;&#33616;&#32500;&#25252;&#30340;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A decision making framework for recommended maintenance of road segments. (arXiv:2307.10085v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10085
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#21644;&#21382;&#21490;&#25968;&#25454;&#65292;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#31185;&#23398;&#20915;&#31574;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#35299;&#20915;&#36947;&#36335;&#32500;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#36947;&#36335;&#20132;&#36890;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#22269;&#24050;&#23436;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#24314;&#35774;&#12290;&#28982;&#32780;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#25361;&#25112;&#22312;&#20110;&#29616;&#26377;&#36947;&#36335;&#30340;&#32500;&#25252;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#21508;&#22269;&#22312;&#36947;&#36335;&#32500;&#25252;&#39033;&#30446;&#19978;&#30340;&#39044;&#31639;&#26377;&#38480;&#65292;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#22312;&#36827;&#34892;&#31185;&#23398;&#20915;&#31574;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#23558;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#19982;&#21382;&#21490;&#32500;&#25252;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#36866;&#24212;&#36947;&#36335;&#32500;&#25252;&#31185;&#23398;&#20915;&#31574;&#30340;&#32972;&#26223;&#65292;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#26088;&#22312;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#26356;&#31185;&#23398;&#30340;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#36827;&#34892;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26694;&#26550;&#20027;&#35201;&#35299;&#20915;&#20197;&#19979;&#22235;&#20010;&#38382;&#39064;&#65306;1&#65289;&#39044;&#27979;&#21508;&#36335;&#32447;&#30340;&#36335;&#38754;&#24615;&#33021;&#65292;2&#65289;&#30830;&#23450;&#32500;&#25252;&#36335;&#32447;&#30340;&#20248;&#20808;&#32423;&#65292;3&#65289;&#22522;&#20110;&#35780;&#20272;&#26631;&#20934;&#21046;&#23450;&#32500;&#25252;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of global road transportation, countries worldwide have completed the construction of road networks. However, the ensuing challenge lies in the maintenance of existing roads. It is well-known that countries allocate limited budgets to road maintenance projects, and road management departments face difficulties in making scientifically informed maintenance decisions. Therefore, integrating various artificial intelligence decision-making techniques to thoroughly explore historical maintenance data and adapt them to the context of road maintenance scientific decision-making has become an urgent issue. This integration aims to provide road management departments with more scientific tools and evidence for decision-making. The framework proposed in this paper primarily addresses the following four issues: 1) predicting the pavement performance of various routes, 2) determining the prioritization of maintenance routes, 3) making maintenance decisions based on the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24182;&#19982;&#20998;&#26512;&#27169;&#22411;&#30456;&#27604;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#34701;&#20837;&#20102;&#22810;&#20010;&#19981;&#21464;&#24615;&#65292;&#21478;&#19968;&#20010;&#21482;&#34701;&#20837;&#20102;&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10060</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accurate deep learning sub-grid scale models for large eddy simulations. (arXiv:2307.10060v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24182;&#19982;&#20998;&#26512;&#27169;&#22411;&#30456;&#27604;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#34701;&#20837;&#20102;&#22810;&#20010;&#19981;&#21464;&#24615;&#65292;&#21478;&#19968;&#20010;&#21482;&#34701;&#20837;&#20102;&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#28237;&#27969;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#20204;&#30340;&#24320;&#21457;&#38656;&#35201;&#21046;&#23450;&#32463;&#36807;&#29289;&#29702;&#39564;&#35777;&#30340;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20998;&#26512;&#24314;&#27169;&#25216;&#26415;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#20135;&#29983;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#39640;&#38454;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#36890;&#36807;&#20174;&#20004;&#20010;&#25705;&#25830;&#38647;&#35834;&#25968;&#32422;&#20026;395&#21644;590&#30340;&#20856;&#22411;&#36890;&#36947;&#27969;&#30340;&#30452;&#25509;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#26174;&#24335;&#28388;&#27874;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#20934;&#30830;&#25968;&#25454;&#12290;&#36825;&#20004;&#32452;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20854;&#20013;&#19968;&#31181;&#26550;&#26500;&#20351;&#29992;&#24352;&#37327;&#22522;&#31070;&#32463;&#32593;&#32476;&#65288;TBNN&#65289;&#65292;&#23884;&#20837;&#20102;&#31616;&#21270;&#30340;&#20998;&#26512;&#27169;&#22411;&#24418;&#24335;&#30340;&#19968;&#33324;&#26377;&#25928;&#31896;&#24615;&#20551;&#35774;&#65292;&#20174;&#32780;&#34701;&#20837;&#20102;&#20285;&#21033;&#30053;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#19981;&#21464;&#24615;&#12290;&#32780;&#21478;&#19968;&#31181;&#26550;&#26500;&#26159;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;&#23427;&#21482;&#33021;&#34701;&#20837;&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;100&#21517;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#30340;&#35748;&#35782;&#21644;&#32435;&#20837;&#20262;&#29702;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26377;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#20027;&#35201;&#24402;&#22240;&#20110;&#24037;&#20316;&#22330;&#25152;&#35268;&#21017;&#21644;&#25919;&#31574;&#65292;&#24182;&#19988;&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#26159;&#20182;&#20204;&#29087;&#24713;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#25945;&#32946;&#21644;&#22521;&#35757;&#34987;&#35748;&#20026;&#22312;&#24110;&#21161;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#32435;&#20837;&#20262;&#29702;&#26041;&#38754;&#26377;&#25152;&#24110;&#21161;&#12290;&#25361;&#25112;&#21253;&#25324;&#32570;&#20047;&#20849;&#35782;&#12289;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#21644;&#22256;&#24785;&#30340;&#39046;&#23548;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10057</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#20262;&#29702;&#23398;&#65306;&#23545;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#24847;&#35782;&#21644;&#25361;&#25112;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ethics in the Age of AI: An Analysis of AI Practitioners' Awareness and Challenges. (arXiv:2307.10057v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;100&#21517;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#30340;&#35748;&#35782;&#21644;&#32435;&#20837;&#20262;&#29702;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26377;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#20027;&#35201;&#24402;&#22240;&#20110;&#24037;&#20316;&#22330;&#25152;&#35268;&#21017;&#21644;&#25919;&#31574;&#65292;&#24182;&#19988;&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#26159;&#20182;&#20204;&#29087;&#24713;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#25945;&#32946;&#21644;&#22521;&#35757;&#34987;&#35748;&#20026;&#22312;&#24110;&#21161;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#32435;&#20837;&#20262;&#29702;&#26041;&#38754;&#26377;&#25152;&#24110;&#21161;&#12290;&#25361;&#25112;&#21253;&#25324;&#32570;&#20047;&#20849;&#35782;&#12289;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#21644;&#22256;&#24785;&#30340;&#39046;&#23548;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#24050;&#25104;&#20026;&#20844;&#20247;&#21644;&#19987;&#23478;&#35752;&#35770;&#30340;&#28909;&#35758;&#35805;&#39064;&#12290;&#20294;&#26159;&#65292;&#37027;&#20123;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154; - &#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20182;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#30340;&#29702;&#35299;&#20197;&#21450;&#23558;&#20854;&#32435;&#20837;&#20182;&#20204;&#24320;&#21457;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#26377;&#20309;&#35828;&#27861;&#21602;&#65311;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#30340;&#30475;&#27861;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#26159;&#26368;&#25509;&#36817;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#33021;&#22815;&#24102;&#26469;&#21464;&#38761;&#21644;&#25913;&#36827;&#30340;&#20154;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#26088;&#22312;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#30340;&#35748;&#35782;&#21644;&#32435;&#20837;&#20262;&#29702;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26681;&#25454;100&#21517;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#30340;&#22238;&#31572;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26377;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#24037;&#20316;&#22330;&#25152;&#30340;&#35268;&#21017;&#21644;&#25919;&#31574;&#12290;&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#26159;&#20182;&#20204;&#20013;&#22823;&#22810;&#25968;&#20154;&#29087;&#24713;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#27491;&#35268;&#25945;&#32946;/&#22521;&#35757;&#22312;&#24110;&#21161;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#32435;&#20837;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#26377;&#25152;&#24110;&#21161;&#30340;&#12290;&#25361;&#25112;&#26041;&#38754;&#65292;&#20182;&#20204;&#35748;&#20026;&#32570;&#20047;&#20849;&#35782;&#12289;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#21644;&#22256;&#24785;&#30340;&#39046;&#23548;&#21147;&#26159;&#26368;&#20027;&#35201;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating it in the AI-based systems they develop? Understanding AI practitioners' views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners' awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners' responses, our findings indicate that majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#23558;MiniZinc&#31243;&#24207;&#36716;&#25442;&#20026;QUBO&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#20248;&#21270;&#21644;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#20174;&#20248;&#21270;&#38382;&#39064;&#21040;QUBO&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.10032</link><description>&lt;p&gt;
MiniZinc&#31243;&#24207;&#33258;&#21160;&#36716;&#25442;&#20026;QUBO
&lt;/p&gt;
&lt;p&gt;
Automatic Conversion of MiniZinc Programs to QUBO. (arXiv:2307.10032v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#23558;MiniZinc&#31243;&#24207;&#36716;&#25442;&#20026;QUBO&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#20248;&#21270;&#21644;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#20174;&#20248;&#21270;&#38382;&#39064;&#21040;QUBO&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#29992;&#20110;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#30340;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#29289;&#29702;&#37327;&#23376;&#35745;&#31639;&#26426;&#65288;&#22914;DWave&#36864;&#28779;&#22120;&#65289;&#19978;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#22914;&#20170;&#26159;&#19968;&#20010;&#20887;&#38271;&#32780;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#23558;&#25152;&#26377;&#38382;&#39064;&#21464;&#37327;&#37325;&#26032;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#24182;&#23558;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#21387;&#32553;&#20026;&#19968;&#20010;&#20108;&#27425;&#22810;&#39033;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25253;&#21578;&#20102;&#25105;&#20204;&#20174;MiniZinc&#21040;QUBO&#30340;&#33258;&#21160;&#36716;&#25442;&#22120;&#30340;&#22522;&#30784;&#65292;&#35813;&#36716;&#25442;&#22120;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#21644;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#31561;&#25928;&#30340;QUBOs&#65292;&#26377;&#25928;&#22320;&#20248;&#21270;&#25972;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining Quadratic Unconstrained Binary Optimisation models for various optimisation problems, in order to solve those on physical quantum computers (such as the the DWave annealers) is nowadays a lengthy and tedious process that requires one to remodel all problem variables as binary variables and squeeze the target function and the constraints into a single quadratic polynomial into these new variables.  We report here on the basis of our automatic converter from MiniZinc to QUBO, which is able to process a large set of constraint optimisation and constraint satisfaction problems and turn them into equivalent QUBOs, effectively optimising the whole process.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10025</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#29983;&#32946;&#25919;&#31574;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#32946;&#38382;&#39064;&#19982;&#20154;&#21475;&#23433;&#20840;&#23494;&#20999;&#30456;&#20851;&#65292;&#20013;&#22269;60&#24180;&#26469;&#39318;&#27425;&#20986;&#29616;&#20154;&#21475;&#36127;&#22686;&#38271;&#36235;&#21183;&#65292;&#29983;&#32946;&#25919;&#31574;&#30340;&#21464;&#21270;&#24341;&#36215;&#20102;&#31038;&#20250;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#37319;&#29992;&#20849;&#29616;&#35821;&#20041;&#20998;&#26512;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#22810;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#21457;&#29616;&#20851;&#20110;&#8220;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#8221;&#30340;&#25552;&#26696;&#35752;&#35770;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
&lt;/p&gt;</description></item><item><title>Rob\^oCIn&#22242;&#38431;&#26159;RoboCup&#23567;&#35268;&#27169;&#32852;&#36187;&#30340;&#24378;&#38431;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20182;&#20204;&#20026;&#20445;&#21355;B&#32452;&#20896;&#20891;&#25152;&#20570;&#30340;&#25913;&#36827;&#24037;&#20316;&#65292;&#21253;&#25324;&#26032;&#26550;&#26500;&#23454;&#26045;&#12289;&#36719;&#20214;&#21644;&#20154;&#24037;&#26234;&#33021;&#37325;&#26500;&#20197;&#21450;&#26426;&#26800;&#31995;&#32479;&#25972;&#21512;&#31561;&#12290;&#20182;&#20204;&#36824;&#20998;&#20139;&#20102;&#22242;&#38431;&#22312;&#30456;&#20851;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#20004;&#31687;&#25991;&#31456;&#20197;&#21450;&#20182;&#20204;&#20026;&#27604;&#36187;&#20570;&#30340;&#20934;&#22791;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.10018</link><description>&lt;p&gt;
Rob\^oCIn&#22823;&#35268;&#27169;&#32852;&#36187;&#25193;&#23637;&#22242;&#38431;&#35770;&#25991;&#25551;&#36848;&#8212;&#8212;RoboCup 2023 &#65288;arXiv:2307.10018v1 [cs.RO]&#65289;
&lt;/p&gt;
&lt;p&gt;
Rob\^oCIn Small Size League Extended Team Description Paper for RoboCup 2023. (arXiv:2307.10018v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10018
&lt;/p&gt;
&lt;p&gt;
Rob\^oCIn&#22242;&#38431;&#26159;RoboCup&#23567;&#35268;&#27169;&#32852;&#36187;&#30340;&#24378;&#38431;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20182;&#20204;&#20026;&#20445;&#21355;B&#32452;&#20896;&#20891;&#25152;&#20570;&#30340;&#25913;&#36827;&#24037;&#20316;&#65292;&#21253;&#25324;&#26032;&#26550;&#26500;&#23454;&#26045;&#12289;&#36719;&#20214;&#21644;&#20154;&#24037;&#26234;&#33021;&#37325;&#26500;&#20197;&#21450;&#26426;&#26800;&#31995;&#32479;&#25972;&#21512;&#31561;&#12290;&#20182;&#20204;&#36824;&#20998;&#20139;&#20102;&#22242;&#38431;&#22312;&#30456;&#20851;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#20004;&#31687;&#25991;&#31456;&#20197;&#21450;&#20182;&#20204;&#20026;&#27604;&#36187;&#20570;&#30340;&#20934;&#22791;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Rob\^oCIn&#33258;2019&#24180;&#36215;&#21442;&#21152;RoboCup&#23567;&#35268;&#27169;&#32852;&#36187;&#65292;&#22312;2022&#24180;&#33719;&#24471;&#20102;&#39318;&#20010;&#19990;&#30028;&#20896;&#20891;&#65288;B&#32452;&#65289;&#65292;&#24182;&#36830;&#32493;&#19977;&#27425;&#25104;&#20026;&#25289;&#19969;&#32654;&#27954;&#20896;&#20891;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#20026;&#20445;&#21355;RoboCup 2023&#23567;&#35268;&#27169;&#32852;&#36187;&#65288;SSL&#65289;B&#32452;&#20896;&#20891;&#25152;&#20570;&#30340;&#25913;&#36827;&#24037;&#20316;&#65292;&#27604;&#36187;&#23558;&#22312;&#27861;&#22269;&#27874;&#23572;&#22810;&#20030;&#34892;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#20139;&#25105;&#20204;&#22242;&#38431;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#24320;&#23637;&#30340;&#19968;&#20123;&#23398;&#26415;&#30740;&#31350;&#12290;&#25105;&#20204;&#22242;&#38431;&#24050;&#25104;&#21151;&#22312;&#20004;&#20010;&#39640;&#24433;&#21709;&#21147;&#20250;&#35758;&#19978;&#21457;&#34920;&#20102;&#19982;SSL&#30456;&#20851;&#30340;&#20004;&#31687;&#25991;&#31456;&#65306;&#31532;25&#23626;RoboCup&#22269;&#38469;&#30740;&#35752;&#20250;&#21644;&#31532;19&#23626;IEEE&#25289;&#19969;&#32654;&#27954;&#26426;&#22120;&#20154;&#30740;&#35752;&#20250;&#65288;LARS 2022&#65289;&#12290;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#25105;&#20204;&#19968;&#30452;&#22312;&#23558;&#36807;&#21435;&#30340;&#20195;&#30721;&#24211;&#36801;&#31227;&#21040;Unification&#19978;&#12290;&#25105;&#20204;&#23558;&#25551;&#36848;&#26032;&#23454;&#26045;&#30340;&#26550;&#26500;&#20197;&#21450;&#36719;&#20214;&#21644;&#20154;&#24037;&#26234;&#33021;&#37325;&#26500;&#30340;&#19968;&#20123;&#35201;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#26426;&#26800;&#32452;&#20214;&#25972;&#21512;&#21040;&#26426;&#26800;&#31995;&#32479;&#20013;&#30340;&#36807;&#31243;&#65292;&#20197;&#21450;&#21435;&#24180;&#21442;&#21152;&#35270;&#21147;&#36974;&#25377;&#25361;&#25112;&#36187;&#30340;&#24320;&#21457;&#24773;&#20917;&#20197;&#21450;&#25105;&#20204;&#27491;&#22312;&#20026;&#20043;&#20934;&#22791;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rob\^oCIn has participated in RoboCup Small Size League since 2019, won its first world title in 2022 (Division B), and is currently a three-times Latin-American champion. This paper presents our improvements to defend the Small Size League (SSL) division B title in RoboCup 2023 in Bordeaux, France. This paper aims to share some of the academic research that our team developed over the past year. Our team has successfully published 2 articles related to SSL at two high-impact conferences: the 25th RoboCup International Symposium and the 19th IEEE Latin American Robotics Symposium (LARS 2022). Over the last year, we have been continuously migrating from our past codebase to Unification. We will describe the new architecture implemented and some points of software and AI refactoring. In addition, we discuss the process of integrating machined components into the mechanical system, our development for participating in the vision blackout challenge last year and what we are preparing for t
&lt;/p&gt;</description></item><item><title>6G&#32593;&#32476;&#19994;&#21153;&#25903;&#25345;&#31995;&#32479;&#26159;&#19979;&#19968;&#20195;&#26234;&#33021;&#21644;&#38598;&#25104;&#30340;&#25968;&#23383;&#20449;&#24687;&#22522;&#30784;&#35774;&#26045;&#65292;&#23558;&#24341;&#39046;&#32463;&#27982;&#21644;&#31038;&#20250;&#30340;&#25968;&#23383;&#21270;&#12289;&#26234;&#33021;&#21270;&#21644;&#32511;&#33394;&#36716;&#22411;&#65292;&#24182;&#36890;&#36807;&#21152;&#24378;&#38598;&#25104;&#65292;&#25552;&#39640;&#23458;&#25143;&#30340;&#36816;&#33829;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.10004</link><description>&lt;p&gt;
6G&#32593;&#32476;&#19994;&#21153;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
6G Network Business Support System. (arXiv:2307.10004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10004
&lt;/p&gt;
&lt;p&gt;
6G&#32593;&#32476;&#19994;&#21153;&#25903;&#25345;&#31995;&#32479;&#26159;&#19979;&#19968;&#20195;&#26234;&#33021;&#21644;&#38598;&#25104;&#30340;&#25968;&#23383;&#20449;&#24687;&#22522;&#30784;&#35774;&#26045;&#65292;&#23558;&#24341;&#39046;&#32463;&#27982;&#21644;&#31038;&#20250;&#30340;&#25968;&#23383;&#21270;&#12289;&#26234;&#33021;&#21270;&#21644;&#32511;&#33394;&#36716;&#22411;&#65292;&#24182;&#36890;&#36807;&#21152;&#24378;&#38598;&#25104;&#65292;&#25552;&#39640;&#23458;&#25143;&#30340;&#36816;&#33829;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
6G&#26159;&#19979;&#19968;&#20195;&#26234;&#33021;&#21644;&#38598;&#25104;&#30340;&#25968;&#23383;&#20449;&#24687;&#22522;&#30784;&#35774;&#26045;&#65292;&#20855;&#26377;&#26222;&#36941;&#20114;&#32852;&#12289;&#26412;&#22320;&#26234;&#33021;&#12289;&#22810;&#32500;&#24863;&#30693;&#12289;&#20840;&#29699;&#35206;&#30422;&#12289;&#32511;&#33394;&#20302;&#30899;&#12289;&#26412;&#22320;&#32593;&#32476;&#23433;&#20840;&#31561;&#29305;&#28857;&#12290;6G&#23558;&#23454;&#29616;&#20174;&#20026;&#20154;&#21644;&#20154;&#29289;&#36890;&#20449;&#26381;&#21153;&#36716;&#21521;&#25903;&#25345;&#26234;&#33021;&#20195;&#29702;&#30340;&#39640;&#25928;&#36830;&#25509;&#65292;&#24182;&#20840;&#38754;&#24341;&#39046;&#32463;&#27982;&#21644;&#31038;&#20250;&#30340;&#25968;&#23383;&#21270;&#12289;&#26234;&#33021;&#21270;&#21644;&#32511;&#33394;&#36716;&#22411;&#12290;&#20316;&#20026;&#31227;&#21160;&#36890;&#20449;&#32593;&#32476;&#30340;&#26680;&#24515;&#25903;&#25345;&#31995;&#32479;&#65292;6G BSS&#38656;&#35201;&#19982;&#19979;&#19968;&#20195;&#20114;&#32852;&#32593;&#21644;&#20449;&#24687;&#25216;&#26415;&#21457;&#23637;&#24102;&#26469;&#30340;&#26032;&#19994;&#21153;&#27169;&#24335;&#38598;&#25104;&#65292;&#20174;&#8220;&#32593;&#32476;&#20026;&#20013;&#24515;&#8221;&#21319;&#32423;&#20026;&#8220;&#20197;&#19994;&#21153;&#21644;&#26381;&#21153;&#20026;&#20013;&#24515;&#8221;&#21644;&#8220;&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#8221;&#12290;6G OSS&#21644;BSS&#31995;&#32479;&#38656;&#35201;&#21152;&#24378;&#38598;&#25104;&#65292;&#36890;&#36807;&#36830;&#25509;&#20379;&#38656;&#20004;&#20391;&#30340;&#25968;&#23383;&#26234;&#33021;&#25903;&#25345;&#33021;&#21147;&#65292;&#25552;&#39640;&#23458;&#25143;&#30340;&#36816;&#33829;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
6G is the next-generation intelligent and integrated digital information infrastructure, characterized by ubiquitous interconnection, native intelligence, multi-dimensional perception, global coverage, green and low-carbon, native network security, etc. 6G will realize the transition from serving people and people-things communication to supporting the efficient connection of intelligent agents, and comprehensively leading the digital, intelligent and green transformation of the economy and the society. As the core support system for mobile communication network, 6 6G BSS need to integrate with new business models brought about by the development of the next-generation Internet and IT, upgrade from "network-centric" to "business and service centric" and "customer-centric". 6G OSS and BSS systems need to strengthen their integration to improve the operational efficiency and benefits of customers by connecting the digital intelligence support capabilities on both sides of supply and dema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10003</link><description>&lt;p&gt;
TbExplain: &#19968;&#31181;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#32479;&#35745;&#39044;&#27979;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#39046;&#22495;&#26088;&#22312;&#25552;&#39640;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#24314;&#31435;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#28909;&#22270;&#26159;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39044;&#27979;&#30340;&#22522;&#26412;&#26041;&#27861;&#20043;&#19968;&#12290;&#28909;&#22270;&#22312;&#20154;&#31867;&#20013;&#20960;&#20046;&#21487;&#20197;&#29702;&#35299;&#65292;&#20294;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#38750;&#19987;&#19994;&#29992;&#25143;&#21487;&#33021;&#19981;&#23436;&#20840;&#29702;&#35299;&#28909;&#22270;&#30340;&#36923;&#36753;&#65288;&#21363;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#25110;&#39068;&#33394;&#31361;&#20986;&#26174;&#31034;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#20687;&#32032;&#30340;&#36923;&#36753;&#65289;&#12290;&#27492;&#22806;&#65292;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#36890;&#24120;&#26080;&#27861;&#23436;&#20840;&#36890;&#36807;&#28909;&#22270;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#20197;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;TbExplain&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09985</link><description>&lt;p&gt;
&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;MovieLens&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65306;&#36825;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#20856;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#22312;&#26576;&#19968;&#26102;&#38388;&#27573;&#20869;&#22312;&#24179;&#21488;&#19978;&#29983;&#25104;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#20132;&#20114;&#29983;&#25104;&#26426;&#21046;&#37096;&#20998;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29992;&#25143;&#19982;&#29289;&#21697;&#36827;&#34892;&#20132;&#20114;&#65288;&#22914;&#21916;&#27426;&#12289;&#36141;&#20080;&#12289;&#35780;&#20998;&#65289;&#20197;&#21450;&#29305;&#23450;&#20132;&#20114;&#21457;&#29983;&#30340;&#32972;&#26223;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;MovieLens&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#24182;&#35299;&#37322;&#20102;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#26102;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19968;&#20123;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#29992;&#25143;&#19982;MovieLens&#24179;&#21488;&#20132;&#20114;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26089;&#26399;&#20132;&#20114;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23450;&#20041;&#20102;&#29992;&#25143;&#30011;&#20687;&#65292;&#24433;&#21709;&#20102;&#21518;&#32493;&#30340;&#20132;&#20114;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#20869;&#37096;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24456;&#22823;&#24433;&#21709;&#12290;&#21024;&#38500;&#38752;&#36817;&#26368;&#21518;&#20960;&#27425;&#20132;&#20114;&#30340;&#20132;&#20114;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.09955</link><description>&lt;p&gt;
XSkill&#65306;&#36328;&#20307;&#29616;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31034;&#33539;&#35270;&#39057;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24191;&#27867;&#25968;&#25454;&#28304;&#65292;&#24182;&#19988;&#26159;&#34920;&#36798;&#25152;&#38656;&#34892;&#20026;&#30340;&#30452;&#35266;&#29992;&#25143;&#30028;&#38754;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#35270;&#39057;&#20013;&#25552;&#21462;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#25216;&#33021;&#38754;&#20020;&#30528;&#20307;&#29616;&#24046;&#24322;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#20307;&#29616;&#24046;&#36317;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;XSkill&#65292;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#21517;&#20026;&#25216;&#33021;&#21407;&#22411;&#30340;&#36328;&#20307;&#29616;&#34920;&#31034;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#25216;&#33021;&#34920;&#31034;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#20154;&#31867;&#25552;&#31034;&#35270;&#39057;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#26469;&#23436;&#25104;&#26410;&#35265;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#20419;&#36827;&#20102;&#26410;&#35265;&#20219;&#21153;&#30340;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22411;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65288;U-CE&#65289;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#21152;&#26435;&#23558;&#21160;&#24577;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33879;&#21517;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;U-CE&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#22312;&#24615;&#33021;&#21644;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.09947</link><description>&lt;p&gt;
U-CE: &#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#20041;&#20998;&#21106;&#20132;&#21449;&#29109;
&lt;/p&gt;
&lt;p&gt;
U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation. (arXiv:2307.09947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22411;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65288;U-CE&#65289;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#21152;&#26435;&#23558;&#21160;&#24577;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33879;&#21517;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;U-CE&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#22312;&#24615;&#33021;&#21644;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#20542;&#21521;&#20026;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#37096;&#32626;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#27169;&#22411;&#39044;&#27979;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22411;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65288;U-CE&#65289;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#21152;&#26435;&#23558;&#21160;&#24577;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33879;&#21517;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65288;CE&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;U-CE&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Cityscapes&#21644;ACDC&#19978;&#19982;&#24120;&#35265;&#30340;ResNet-18&#21644;ResNet-101&#20004;&#31181;&#20027;&#24178;&#26550;&#26500;&#30456;&#27604;&#65292;&#20855;&#26377;&#20248;&#21183;&#12290;&#36890;&#36807;&#20351;&#29992;U-CE&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#20986;&#19981;&#20165;&#22312;&#20998;&#21106;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#21319;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#21518;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#20041;&#20998;&#21106;&#20132;&#21449;&#29109;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown exceptional performance in various tasks, but their lack of robustness, reliability, and tendency to be overconfident pose challenges for their deployment in safety-critical applications like autonomous driving. In this regard, quantifying the uncertainty inherent to a model's prediction is a promising endeavour to address these shortcomings. In this work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that incorporates dynamic predictive uncertainties into the training process by pixel-wise weighting of the well-known cross-entropy loss (CE). Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training. Consequently, we contribute to the devel
&lt;/p&gt;</description></item><item><title>TREEMENT&#26159;&#19968;&#31181;&#37319;&#29992;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#27169;&#22411;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#20020;&#24202;&#26412;&#20307;&#30693;&#35782;&#21644;&#21512;&#26684;&#26631;&#20934;&#23884;&#20837;&#23398;&#20064;&#65292;&#25552;&#20379;&#20934;&#30830;&#32780;&#26377;&#35299;&#37322;&#24615;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.09942</link><description>&lt;p&gt;
TREEMENT: &#21487;&#35299;&#37322;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#27169;&#22411;&#36890;&#36807;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network. (arXiv:2307.09942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09942
&lt;/p&gt;
&lt;p&gt;
TREEMENT&#26159;&#19968;&#31181;&#37319;&#29992;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#27169;&#22411;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#20020;&#24202;&#26412;&#20307;&#30693;&#35782;&#21644;&#21512;&#26684;&#26631;&#20934;&#23884;&#20837;&#23398;&#20064;&#65292;&#25552;&#20379;&#20934;&#30830;&#32780;&#26377;&#35299;&#37322;&#24615;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#23545;&#20110;&#33647;&#29289;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#26114;&#36149;&#32780;&#20302;&#25928;&#30340;&#24739;&#32773;&#25307;&#21215;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#32437;&#21521;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#21512;&#26684;&#26631;&#20934;&#33258;&#21160;&#21305;&#37197;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#65292;&#20197;&#21152;&#36895;&#24739;&#32773;&#25307;&#21215;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35201;&#20040;&#20381;&#36182;&#20110;&#26080;&#27861;&#25193;&#23637;&#21040;&#20854;&#20182;&#35797;&#39564;&#30340;&#19987;&#23478;&#35268;&#21017;&#65292;&#35201;&#20040;&#20197;&#40657;&#31665;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#36890;&#29992;&#30340;&#21305;&#37197;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#32467;&#26524;&#38590;&#20197;&#37319;&#29992;&#12290;&#20026;&#20102;&#25552;&#20379;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TREEMENT&#30340;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#23618;&#27425;&#21270;&#20020;&#24202;&#26412;&#20307;&#30693;&#35782;&#25193;&#23637;&#20102;&#20174;&#24207;&#21015;EHR&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20010;&#24615;&#21270;&#24739;&#32773;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#21512;&#26684;&#26631;&#20934;&#23884;&#20837;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#26463;&#25628;&#32034;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.  To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09933</link><description>&lt;p&gt;
Spuriosity&#24182;&#27809;&#26377;&#23548;&#33268;&#20998;&#31867;&#22120;&#22833;&#36133;&#65306;&#21033;&#29992;&#19981;&#21464;&#30340;&#39044;&#27979;&#26469;&#21033;&#29992;&#34394;&#20551;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#25552;&#21462;&#20855;&#26377;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#31283;&#23450;&#25110;&#19981;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#33293;&#24323;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#20851;&#31995;&#21464;&#21270;&#30340;"&#34394;&#20551;"&#25110;&#19981;&#31283;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#19981;&#31283;&#23450;&#29305;&#24449;&#24120;&#24120;&#25658;&#24102;&#20851;&#20110;&#26631;&#31614;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#22495;&#20013;&#27491;&#30830;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22914;&#20309;&#22312;&#27979;&#35797;&#22495;&#20013;&#20351;&#29992;&#36825;&#20123;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#21487;&#33021;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#31283;&#23450;&#29305;&#24449;&#30340;&#20266;&#26631;&#31614;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25351;&#23548;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#21069;&#25552;&#26159;&#22312;&#32473;&#23450;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#65292;&#31283;&#23450;&#29305;&#24449;&#21644;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#29305;&#24449;&#22686;&#24378;&#65288;SFB&#65289;&#31639;&#27861;&#65306;(i)&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20998;&#31163;&#31283;&#23450;&#29305;&#24449;&#21644;&#26465;&#20214;&#29420;&#31435;&#19981;&#31283;&#23450;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#65307;(ii)&#20351;&#29992;&#31283;&#23450;&#29305;&#24449;&#39044;&#27979;&#26469;&#36866;&#24212;&#27979;&#35797;&#22495;
&lt;/p&gt;
&lt;p&gt;
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09913</link><description>&lt;p&gt;
&#25506;&#32034;&#20855;&#26377;&#25551;&#36848;&#36923;&#36753;&#29305;&#24449;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#38750;&#27491;&#21017;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09913
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#23545;&#35937;&#26159;ALCreg&#21644;ALCvpl&#65292;&#20998;&#21035;&#26159;&#20351;&#29992;&#27491;&#21017;&#21644;&#21487;&#35265;&#25512;&#19979;&#35821;&#35328;&#30340;&#36335;&#24452;&#34920;&#36798;&#24335;&#30340;&#25193;&#23637;&#12290;&#31532;&#19968;&#20010;ALCreg&#26159;Fischer&#21644;Ladner&#25152;&#29087;&#30693;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#19968;&#31181;&#21464;&#31181;&#12290;&#31532;&#20108;&#20010;ALCvpl&#26159;&#30001;Loding&#21644;Serre&#22312;2007&#24180;&#24341;&#20837;&#21644;&#30740;&#31350;&#30340;&#12290;ALCvpl&#36923;&#36753;&#24191;&#20041;&#19978;&#25512;&#24191;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;&#21487;&#20915;&#23450;&#24615;&#38750;&#27491;&#21017;&#25193;&#23637;&#30340;ALCreg&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#30475;&#20284;&#26080;&#23475;&#30340;Self&#25805;&#20316;&#31526;&#21518;&#65292;&#23545;&#20110;ALCvpl&#20013;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#21487;&#20915;&#23450;&#24615;&#20007;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#22312;ALCvpl&#20013;&#28155;&#21152;&#20010;&#20307;&#35789;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#35777;&#26126;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;&#38750;&#27491;&#21017;&#65288;&#21487;&#35265;&#25512;&#19979;&#65289;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.  We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) langu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#22686;&#24378;&#23545;&#35805;&#20195;&#29702;&#22312;&#36807;&#31243;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.09909</link><description>&lt;p&gt;
Chit-Chat&#25110;&#32773;Deep Talk&#65306;&#29992;&#20110;&#36807;&#31243;&#25366;&#25496;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Chit-Chat or Deep Talk: Prompt Engineering for Process Mining. (arXiv:2307.09909v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#22686;&#24378;&#23545;&#35805;&#20195;&#29702;&#22312;&#36807;&#31243;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#36807;&#31243;&#25366;&#25496;&#20013;&#22686;&#24378;&#23545;&#35805;&#20195;&#29702;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24212;&#23545;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#35201;&#27714;&#12290;&#34429;&#28982;LLM&#30340;&#36827;&#23637;&#20026;&#23545;&#35805;&#24335;&#36807;&#31243;&#25366;&#25496;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#20294;&#29983;&#25104;&#39640;&#25928;&#30340;&#36755;&#20986;&#20173;&#28982;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20026;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#30340;&#20808;&#21069;&#30740;&#31350;&#30340;&#20102;&#35299;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#21033;&#29992;LLM&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25913;&#21892;&#20102;&#21487;&#35775;&#38382;&#24615;&#21644;&#20195;&#29702;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#20844;&#20849;&#38382;&#39064;&#21644;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#36827;&#19968;&#27493;&#25506;&#32034;LLM&#22312;&#36807;&#31243;&#25366;&#25496;&#20013;&#30340;&#20316;&#29992;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;LLM&#35760;&#24518;&#12289;&#23454;&#26045;&#23454;&#26102;&#29992;&#25143;&#27979;&#35797;&#20197;&#21450;&#26816;&#26597;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research investigates the application of Large Language Models (LLMs) to augment conversational agents in process mining, aiming to tackle its inherent complexity and diverse skill requirements. While LLM advancements present novel opportunities for conversational process mining, generating efficient outputs is still a hurdle. We propose an innovative approach that amend many issues in existing solutions, informed by prior research on Natural Language Processing (NLP) for conversational agents. Leveraging LLMs, our framework improves both accessibility and agent performance, as demonstrated by experiments on public question and data sets. Our research sets the stage for future explorations into LLMs' role in process mining and concludes with propositions for enhancing LLM memory, implementing real-time user testing, and examining diverse data sets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.09906</link><description>&lt;p&gt;
&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#20174;&#30446;&#26631;&#39537;&#21160;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#21160;&#24577;&#23039;&#21183;&#21644;&#34920;&#24773;&#26469;&#32473;&#38745;&#24577;&#22270;&#20687;&#20013;&#30340;&#20154;&#33080;&#28155;&#21152;&#21160;&#30011;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#20013;&#30340;&#20010;&#20154;&#36523;&#20221;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#35270;&#39057;&#20013;&#25103;&#21095;&#24615;&#21644;&#22797;&#26434;&#30340;&#36816;&#21160;&#20250;&#23548;&#33268;&#29983;&#25104;&#27169;&#31946;&#19981;&#28165;&#65292;&#22240;&#20026;&#38745;&#24577;&#28304;&#22270;&#20687;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#22806;&#35266;&#20449;&#24687;&#26469;&#22788;&#29702;&#34987;&#36974;&#25377;&#21306;&#22495;&#25110;&#24494;&#22937;&#30340;&#34920;&#24773;&#21464;&#21270;&#65292;&#36825;&#20250;&#20135;&#29983;&#20005;&#37325;&#30340;&#20266;&#24433;&#24182;&#20005;&#37325;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20840;&#23616;&#20154;&#33080;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#31216;&#20026;MCNet&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image. However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation.~Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features 
&lt;/p&gt;</description></item><item><title>PyTAG&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26700;&#38754;&#28216;&#25103;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;Python API&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;Tabletop Games framework (TAG)&#36827;&#34892;&#20132;&#20114;&#30340;&#21151;&#33021;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#29616;&#20195;&#26700;&#38754;&#28216;&#25103;&#20013;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#25216;&#26415;&#21644;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09905</link><description>&lt;p&gt;
PyTAG&#65306;&#24378;&#21270;&#23398;&#20064;&#22312;&#26700;&#38754;&#28216;&#25103;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
PyTAG: Challenges and Opportunities for Reinforcement Learning in Tabletop Games. (arXiv:2307.09905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09905
&lt;/p&gt;
&lt;p&gt;
PyTAG&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26700;&#38754;&#28216;&#25103;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;Python API&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;Tabletop Games framework (TAG)&#36827;&#34892;&#20132;&#20114;&#30340;&#21151;&#33021;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#29616;&#20195;&#26700;&#38754;&#28216;&#25103;&#20013;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#25216;&#26415;&#21644;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30456;&#27604;&#35270;&#39057;&#28216;&#25103;&#65292;&#29616;&#20195;&#26700;&#38754;&#28216;&#25103;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#40092;&#26377;&#20851;&#27880;&#65292;&#23613;&#31649;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19982;&#20247;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyTAG&#65292;&#35813;Python API&#29992;&#20110;&#19982;Tabletop Games framework (TAG)&#36827;&#34892;&#20132;&#20114;&#12290;TAG&#21253;&#21547;&#20102;&#36229;&#36807;20&#20010;&#29616;&#20195;&#26700;&#38754;&#28216;&#25103;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;API&#20379;AI&#20195;&#29702;&#20351;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;&#37096;&#20998;&#28216;&#25103;&#19978;&#20351;&#29992;Proximal Policy Optimisation&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#32467;&#26524;&#30340;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22797;&#26434;&#29616;&#20195;&#26700;&#38754;&#28216;&#25103;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#36890;&#36807;PyTAG&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Game AI research has made important breakthroughs using Reinforcement Learning (RL). Despite this, RL for modern tabletop games has gained little to no attention, even when they offer a range of unique challenges compared to video games. To bridge this gap, we introduce PyTAG, a Python API for interacting with the Tabletop Games framework (TAG). TAG contains a growing set of more than 20 modern tabletop games, with a common API for AI agents. We present techniques for training RL agents in these games and introduce baseline results after training Proximal Policy Optimisation algorithms on a subset of games. Finally, we discuss the unique challenges complex modern tabletop games provide, now open to RL research through PyTAG.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#33258;&#22238;&#24402;TTS&#20013;&#36873;&#25321;&#19981;&#21516;&#35828;&#35805;&#32773;&#23884;&#20837;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#23884;&#20837;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#65292;&#32593;&#32476;&#37117;&#21487;&#20197;&#21516;&#26679;&#33391;&#22909;&#22320;&#22788;&#29702;&#21508;&#31181;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#24182;&#19988;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#35828;&#35805;&#32773;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2307.09898</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;TTS&#20013;&#35828;&#35805;&#32773;&#23884;&#20837;&#36873;&#25321;&#30340;&#25928;&#26524;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An analysis on the effects of speaker embedding choice in non auto-regressive TTS. (arXiv:2307.09898v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#33258;&#22238;&#24402;TTS&#20013;&#36873;&#25321;&#19981;&#21516;&#35828;&#35805;&#32773;&#23884;&#20837;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#23884;&#20837;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#65292;&#32593;&#32476;&#37117;&#21487;&#20197;&#21516;&#26679;&#33391;&#22909;&#22320;&#22788;&#29702;&#21508;&#31181;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#24182;&#19988;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#35828;&#35805;&#32773;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#29702;&#35299;&#38750;&#33258;&#22238;&#24402;&#30340;&#20998;&#35299;&#22810;&#35828;&#35805;&#32773;&#35821;&#38899;&#21512;&#25104;&#26550;&#26500;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#35828;&#35805;&#32773;&#23884;&#20837;&#38598;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32852;&#21512;&#23398;&#20064;&#34920;&#31034;&#21644;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#21021;&#22987;&#21270;&#23427;&#20204;&#26159;&#21542;&#33021;&#25552;&#39640;&#30446;&#26631;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#36136;&#37327;&#12290;&#22312;&#21478;&#19968;&#20010;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#23884;&#20837;&#38598;&#23545;&#32593;&#32476;&#30340;&#26680;&#24515;&#35821;&#38899;&#25277;&#35937;&#65288;&#21363;&#38646;&#26465;&#20214;&#65289;&#22312;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#23884;&#20837;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#65292;&#32593;&#32476;&#37117;&#21487;&#20197;&#21516;&#26679;&#33391;&#22909;&#22320;&#22788;&#29702;&#21508;&#31181;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#35821;&#38899;&#36755;&#20986;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#36804;&#20170;&#20026;&#27490;&#37319;&#29992;&#30340;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21512;&#25104;&#31995;&#32479;&#30340;&#26680;&#24515;&#32467;&#26500;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#35828;&#35805;&#32773;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce a first attempt on understanding how a non-autoregressive factorised multi-speaker speech synthesis architecture exploits the information present in different speaker embedding sets. We analyse if jointly learning the representations, and initialising them from pretrained models determine any quality improvements for target speaker identities. In a separate analysis, we investigate how the different sets of embeddings impact the network's core speech abstraction (i.e. zero conditioned) in terms of speaker identity and representation learning. We show that, regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#29289;&#21697;&#21453;&#24212;&#29702;&#35770;&#20013;&#25552;&#20986;&#20102;&#25674;&#38144;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#36873;&#25321;&#23545;&#20110;&#23398;&#29983;&#20998;&#24067;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#27979;&#35797;&#39033;&#30446;&#65292;&#24182;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#36827;&#34892;&#25674;&#38144;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2307.09891</link><description>&lt;p&gt;
&#29289;&#21697;&#21453;&#24212;&#29702;&#35770;&#30340;&#25674;&#38144;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Amortised Design Optimization for Item Response Theory. (arXiv:2307.09891v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29289;&#21697;&#21453;&#24212;&#29702;&#35770;&#20013;&#25552;&#20986;&#20102;&#25674;&#38144;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#36873;&#25321;&#23545;&#20110;&#23398;&#29983;&#20998;&#24067;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#27979;&#35797;&#39033;&#30446;&#65292;&#24182;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#36827;&#34892;&#25674;&#38144;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#21697;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25945;&#32946;&#21644;&#24515;&#29702;&#23398;&#20013;&#20154;&#31867;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;IRT&#34987;&#29992;&#26469;&#25512;&#26029;&#23398;&#29983;&#33021;&#21147;&#21644;&#27979;&#35797;&#39033;&#30446;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#23398;&#29983;&#30340;&#22238;&#31572;&#12290;&#19982;&#23398;&#29983;&#30340;&#20114;&#21160;&#26159;&#26114;&#36149;&#30340;&#65292;&#38656;&#35201;&#39640;&#25928;&#22320;&#25910;&#38598;&#25512;&#26029;&#23398;&#29983;&#33021;&#21147;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#39640;&#26114;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25674;&#38144;&#23454;&#39564;&#35774;&#35745;&#32435;&#20837;IRT&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#35745;&#31639;&#25104;&#26412;&#34987;&#36716;&#31227;&#21040;&#39044;&#35745;&#31639;&#38454;&#27573;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#12290;&#35813;&#20195;&#29702;&#34987;&#35757;&#32451;&#20026;&#36873;&#25321;&#23545;&#20110;&#23398;&#29983;&#20998;&#24067;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#27979;&#35797;&#39033;&#30446;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#25674;&#38144;&#25512;&#26029;&#12290;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#20195;&#29702;&#26681;&#25454;&#25968;&#25454;&#20272;&#35745;&#21442;&#25968;&#65292;&#24182;&#20026;&#23398;&#29983;&#24314;&#35758;&#19979;&#19968;&#20010;&#27979;&#35797;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item Response Theory (IRT) is a well known method for assessing responses from humans in education and psychology. In education, IRT is used to infer student abilities and characteristics of test items from student responses. Interactions with students are expensive, calling for methods that efficiently gather information for inferring student abilities. Methods based on Optimal Experimental Design (OED) are computationally costly, making them inapplicable for interactive applications. In response, we propose incorporating amortised experimental design into IRT. Here, the computational cost is shifted to a precomputing phase by training a Deep Reinforcement Learning (DRL) agent with synthetic data. The agent is trained to select optimally informative test items for the distribution of students, and to conduct amortised inference conditioned on the experiment outcomes. During deployment the agent estimates parameters from data, and suggests the next test item for the student, in close t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;VQA&#31639;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#26356;&#36866;&#24403;&#30340;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#31639;&#27861;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.09886</link><description>&lt;p&gt;
VQA&#39564;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;&#24212;&#29992;&#20110;&#31958;&#23615;&#30149;&#40644;&#26001;&#27700;&#32959;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
A reinforcement learning approach for VQA validation: an application to diabetic macular edema grading. (arXiv:2307.09886v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;VQA&#31639;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#26356;&#36866;&#24403;&#30340;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#31639;&#27861;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#21151;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38544;&#34255;&#30340;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#25972;&#21512;&#12290;&#21487;&#35299;&#37322;&#24615;&#21644;&#20449;&#20219;&#34987;&#35270;&#20026;&#29616;&#20195;&#26041;&#27861;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#20197;&#20415;&#22312;&#20020;&#24202;&#31038;&#21306;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22312;&#26377;&#38480;&#30340;&#26041;&#24335;&#19979;&#36827;&#34892;&#39564;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20026;&#21151;&#33021;&#24378;&#22823;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31639;&#27861;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#26356;&#36866;&#24403;&#30340;&#39564;&#35777;&#26041;&#27861;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#36825;&#39033;&#24037;&#20316;&#20851;&#27880;&#20110;&#33258;&#21160;&#35270;&#35273;&#22270;&#28789;&#27979;&#35797;&#65288;VTT&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38382;&#31572;&#26041;&#27861;&#65292;&#26088;&#22312;&#26292;&#38706;VQA&#31639;&#27861;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning models have greatly increased the performance of automated methods in medical image analysis. However, the internal functioning of such models is largely hidden, which hinders their integration in clinical practice. Explainability and trust are viewed as important aspects of modern methods, for the latter's widespread use in clinical communities. As such, validation of machine learning models represents an important aspect and yet, most methods are only validated in a limited way. In this work, we focus on providing a richer and more appropriate validation approach for highly powerful Visual Question Answering (VQA) algorithms. To better understand the performance of these methods, which answer arbitrary questions related to images, this work focuses on an automatic visual Turing test (VTT). That is, we propose an automatic adaptive questioning method, that aims to expose the reasoning behavior of a VQA algorithm. Specifically, we introduce a reinfor
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#20102;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#21644;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65292;&#30830;&#20445;&#31038;&#21306;&#31119;&#31049;&#21644;&#27979;&#35797;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09885</link><description>&lt;p&gt;
&#32771;&#35797;&#32773;&#26377;&#35805;&#35828;&#65306;&#29702;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Test-takers have a say: understanding the implications of the use of AI in language tests. (arXiv:2307.09885v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09885
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#21644;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65292;&#30830;&#20445;&#31038;&#21306;&#31119;&#31049;&#21644;&#27979;&#35797;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27979;&#35797;&#34913;&#37327;&#19968;&#20010;&#20154;&#22312;&#21548;&#12289;&#35828;&#12289;&#35835;&#12289;&#20889;&#26041;&#38754;&#20351;&#29992;&#26576;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#36825;&#31867;&#27979;&#35797;&#22312;&#23398;&#26415;&#12289;&#32844;&#19994;&#21644;&#31227;&#27665;&#39046;&#22495;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#25945;&#32946;&#26426;&#26500;&#12289;&#19987;&#19994;&#35748;&#35777;&#26426;&#26500;&#21644;&#25919;&#24220;&#31561;&#23454;&#20307;&#26426;&#26500;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;&#20505;&#36873;&#20154;&#30340;&#35821;&#35328;&#29087;&#32451;&#31243;&#24230;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#35821;&#35328;&#27979;&#35797;&#25552;&#20379;&#32773;&#25506;&#32034;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#35821;&#35328;&#27979;&#35797;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#22260;&#32469;&#35821;&#35328;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#21464;&#38761;&#24615;&#27963;&#21160;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#24615;&#23384;&#22312;&#25285;&#24551;&#30340;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20102;&#35299;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#35821;&#35328;&#27979;&#35797;&#20013;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#30693;&#35782;&#23558;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#20445;&#25252;&#31038;&#21306;&#30340;&#31119;&#31049;&#21644;&#27979;&#35797;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#20999;&#21644;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language tests measure a person's ability to use a language in terms of listening, speaking, reading, or writing. Such tests play an integral role in academic, professional, and immigration domains, with entities such as educational institutions, professional accreditation bodies, and governments using them to assess candidate language proficiency. Recent advances in Artificial Intelligence (AI) and the discipline of Natural Language Processing have prompted language test providers to explore AI's potential applicability within language testing, leading to transformative activity patterns surrounding language instruction and learning. However, with concerns over AI's trustworthiness, it is imperative to understand the implications of integrating AI into language testing. This knowledge will enable stakeholders to make well-informed decisions, thus safeguarding community well-being and testing integrity. To understand the concerns and effects of AI usage in language tests, we conducted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.09882</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#26679;&#26412;&#21608;&#22260;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#27880;&#24847;&#21040;&#22312;&#33021;&#37327;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#23548;&#33268;&#21028;&#21035;&#22120;&#25552;&#20379;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65288;&#36890;&#24120;&#31216;&#20026;&#33021;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;1&#65289;Wasserstein GAN&#23545;&#20998;&#21306;&#20989;&#25968;&#36827;&#34892;&#20102;&#26377;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#65307;2&#65289;&#22312;&#26368;&#20248;&#21270;&#20284;&#28982;&#26102;&#65292;&#24517;&#39035;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#12290;&#36825;&#34987;&#20551;&#35774;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#24335;&#35206;&#30422;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#12290;&#36825;&#26159;&#35774;&#35745;&#26080;&#20559;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#29983;&#25104;&#22120;&#29109;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29983;&#25104;&#23494;&#24230;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#32593;&#32476;&#26469;&#33719;&#24471;&#30340;&#65292;&#31216;&#20026;&#21333;&#21521;&#27969;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#36873;&#25321;&#23454;&#39564;&#35774;&#35745;&#65292;&#29992;&#27169;&#25311;&#25968;&#25454;&#20195;&#26367;&#22823;&#37327;&#20154;&#31867;&#25968;&#25454;&#26469;&#20998;&#25674;&#23454;&#39564;&#35774;&#35745;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#22312;&#20132;&#20114;&#35774;&#35745;&#20013;&#20272;&#35745;&#29992;&#25143;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25351;&#21521;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.09878</link><description>&lt;p&gt;
&#12298;&#29992;&#25143;&#25351;&#21521;&#30340;&#25674;&#38144;&#23454;&#39564;&#35774;&#35745;&#21644;&#21442;&#25968;&#20272;&#35745;&#12299;
&lt;/p&gt;
&lt;p&gt;
Amortised Experimental Design and Parameter Estimation for User Models of Pointing. (arXiv:2307.09878v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#36873;&#25321;&#23454;&#39564;&#35774;&#35745;&#65292;&#29992;&#27169;&#25311;&#25968;&#25454;&#20195;&#26367;&#22823;&#37327;&#20154;&#31867;&#25968;&#25454;&#26469;&#20998;&#25674;&#23454;&#39564;&#35774;&#35745;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#22312;&#20132;&#20114;&#35774;&#35745;&#20013;&#20272;&#35745;&#29992;&#25143;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25351;&#21521;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#27169;&#22411;&#22312;&#20132;&#20114;&#35774;&#35745;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25903;&#25345;&#33258;&#21160;&#21270;&#20132;&#20114;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24517;&#39035;&#20174;&#29992;&#25143;&#25968;&#25454;&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;&#34429;&#28982;&#26377;&#26102;&#38656;&#35201;&#22823;&#37327;&#30340;&#29992;&#25143;&#25968;&#25454;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#35774;&#35745;&#23454;&#39564;&#26469;&#23613;&#21487;&#33021;&#39640;&#25928;&#22320;&#25910;&#38598;&#25968;&#25454;&#24182;&#25512;&#26029;&#21442;&#25968;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#25968;&#25454;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#36873;&#25321;&#23454;&#39564;&#35774;&#35745;&#30340;&#31574;&#30053;&#26469;&#20998;&#25674;&#23454;&#39564;&#35774;&#35745;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#29992;&#27169;&#25311;&#21442;&#19982;&#32773;&#26469;&#25910;&#38598;&#23454;&#39564;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#19982;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;&#27169;&#25311;&#20195;&#29702;&#30456;&#20114;&#20316;&#29992;&#26469;&#23398;&#20064;&#21738;&#20123;&#23454;&#39564;&#23545;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#26368;&#26377;&#29992;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#32780;&#19981;&#26159;&#22823;&#37327;&#30340;&#20154;&#31867;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#26085;&#30410;&#22797;&#26434;&#30340;&#25351;&#21521;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
User models play an important role in interaction design, supporting automation of interaction design choices. In order to do so, model parameters must be estimated from user data. While very large amounts of user data are sometimes required, recent research has shown how experiments can be designed so as to gather data and infer parameters as efficiently as possible, thereby minimising the data requirement. In the current article, we investigate a variant of these methods that amortises the computational cost of designing experiments by training a policy for choosing experimental designs with simulated participants. Our solution learns which experiments provide the most useful data for parameter estimation by interacting with in-silico agents sampled from the model space thereby using synthetic data rather than vast amounts of human data. The approach is demonstrated for three progressively complex models of pointing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.09866</link><description>&lt;p&gt;
&#26816;&#27979;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#25551;&#36848;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30340;&#33030;&#24369;&#24615;&#23545;&#25105;&#20204;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#26159;&#22478;&#24066;&#27491;&#24120;&#36816;&#34892;&#25152;&#24517;&#38656;&#30340;&#24037;&#31243;&#35774;&#26045;&#65292;&#20197;&#32593;&#32476;&#30340;&#24418;&#24335;&#33258;&#28982;&#23384;&#22312;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#21253;&#25324;&#20445;&#25252;&#33030;&#24369;&#35774;&#26045;&#21644;&#35774;&#35745;&#31283;&#20581;&#30340;&#25299;&#25169;&#32467;&#26500;&#31561;&#12290;&#30001;&#20110;&#19981;&#21516;&#25299;&#25169;&#29305;&#24615;&#21644;&#22522;&#30784;&#35774;&#26045;&#33030;&#24369;&#24615;&#20197;&#21450;&#20854;&#22797;&#26434;&#30340;&#28436;&#21270;&#26426;&#21046;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#65292;&#19968;&#20123;&#21551;&#21457;&#24335;&#20998;&#26512;&#21644;&#26426;&#22120;&#36741;&#21161;&#20998;&#26512;&#22312;&#35299;&#20915;&#36825;&#31181;&#22330;&#26223;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20934;&#30830;&#22320;&#25551;&#36848;&#22478;&#24066;&#31995;&#32479;&#30340;&#33030;&#24369;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#24322;&#26500;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#32423;&#32852;&#22833;&#36133;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#32676;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#32467;&#26500;&#21160;&#21147;&#23398;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#29289;&#29702;&#22522;&#30784;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30456;&#20284;&#29289;&#29702;&#29616;&#35937;&#30340;&#20154;&#32676;&#20013;&#23398;&#20064;&#20851;&#31995;&#65292;&#26500;&#24314;&#21487;&#36716;&#31227;&#12289;&#21487;&#35299;&#37322;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09862</link><description>&lt;p&gt;
&#36808;&#21521;&#22522;&#20110;&#20154;&#32676;&#20449;&#24687;&#30340;&#32467;&#26500;&#21160;&#21147;&#23398;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#23450;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a population-informed approach to the definition of data-driven models for structural dynamics. (arXiv:2307.09862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#32676;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#32467;&#26500;&#21160;&#21147;&#23398;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#29289;&#29702;&#22522;&#30784;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30456;&#20284;&#29289;&#29702;&#29616;&#35937;&#30340;&#20154;&#32676;&#20013;&#23398;&#20064;&#20851;&#31995;&#65292;&#26500;&#24314;&#21487;&#36716;&#31227;&#12289;&#21487;&#35299;&#37322;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#24433;&#21709;&#20102;&#35768;&#22810;&#39046;&#22495;&#20013;&#22810;&#31181;&#29616;&#35937;&#30340;&#24314;&#27169;&#26041;&#24335;&#65292;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;&#32467;&#26500;&#21160;&#21147;&#23398;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#38382;&#39064;&#29305;&#23450;&#30340;&#65292;&#20182;&#20204;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20063;&#38656;&#35201;&#20998;&#26512;&#32773;&#23545;&#38382;&#39064;&#30340;&#29289;&#29702;&#22522;&#30784;&#26377;&#25152;&#20102;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#20351;&#29992;&#20174;&#30456;&#20284;&#29289;&#29702;&#29616;&#35937;&#30340;&#20154;&#32676;&#20013;&#23398;&#20064;&#36825;&#31181;&#20851;&#31995;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#21463;&#21040;&#29289;&#29702;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#21487;&#36716;&#31227;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#65292;&#32780;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#20123;&#23646;&#24615;&#24182;&#19981;&#26159;&#36731;&#26494;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#26377;&#31867;&#21035;&#20998;&#26512;&#20013;&#30340;&#19981;&#27491;&#30830;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#21035;&#26102;&#36807;&#20110;&#33258;&#20449;&#19988;&#19981;&#22815;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.09858</link><description>&lt;p&gt;
&#36890;&#36807;&#20010;&#20307;&#26657;&#20934;&#23454;&#29616;&#22270;&#19978;&#21487;&#38752;&#30340;&#31232;&#26377;&#31867;&#21035;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Rare Category Analysis on Graphs via Individual Calibration. (arXiv:2307.09858v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#26377;&#31867;&#21035;&#20998;&#26512;&#20013;&#30340;&#19981;&#27491;&#30830;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#21035;&#26102;&#36807;&#20110;&#33258;&#20449;&#19988;&#19981;&#22815;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#26377;&#31867;&#21035;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#22312;&#21508;&#31181;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#32597;&#35265;&#30142;&#30149;&#35786;&#26029;&#12290;&#31232;&#26377;&#31867;&#21035;&#20998;&#26512;(RCA)&#26159;&#25351;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#26816;&#27979;&#12289;&#34920;&#24449;&#21644;&#29702;&#35299;&#23569;&#25968;&#31867;&#21035;&#34892;&#20026;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;RCA&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#19978;&#65292;&#20294;&#23569;&#25968;&#20960;&#20010;&#22522;&#30784;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#36804;&#20170;&#20026;&#27490;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#21644;&#36739;&#23569;&#25506;&#32034;&#65306;&#22312;&#31232;&#26377;&#31867;&#21035;&#20998;&#26512;&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#26377;&#22810;&#33258;&#20449;&#25110;&#19981;&#30830;&#23450;&#65311;&#25105;&#20204;&#22914;&#20309;&#37327;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#31232;&#26377;&#31867;&#21035;&#20998;&#26512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#29616;&#26377;RCA&#26041;&#27861;&#20013;&#30340;&#19981;&#27491;&#30830;&#26657;&#20934;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;RCA&#26041;&#27861;&#20027;&#35201;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#21035;&#26102;&#36807;&#20110;&#33258;&#20449;&#19988;&#19981;&#22815;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rare categories abound in a number of real-world networks and play a pivotal role in a variety of high-stakes applications, including financial fraud detection, network intrusion detection, and rare disease diagnosis. Rare category analysis (RCA) refers to the task of detecting, characterizing, and comprehending the behaviors of minority classes in a highly-imbalanced data distribution. While the vast majority of existing work on RCA has focused on improving the prediction performance, a few fundamental research questions heretofore have received little attention and are less explored: How confident or uncertain is a prediction model in rare category analysis? How can we quantify the uncertainty in the learning process and enable reliable rare category analysis?  To answer these questions, we start by investigating miscalibration in existing RCA methods. Empirical results reveal that state-of-the-art RCA methods are mainly over-confident in predicting minority classes and under-confide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#22320;&#22270;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#36816;&#29992;&#22810;&#31181;&#25216;&#26415;&#65292;&#23398;&#20064;&#21040;&#25152;&#26377;&#20195;&#29702;&#30340;&#21160;&#24577;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.09831</link><description>&lt;p&gt;
&#22312;&#20132;&#36890;&#20013;&#30340;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#24555;&#36895;&#21644;&#26080;&#22320;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Fast and Map-Free Model for Trajectory Prediction in Traffics. (arXiv:2307.09831v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#22320;&#22270;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#36816;&#29992;&#22810;&#31181;&#25216;&#26415;&#65292;&#23398;&#20064;&#21040;&#25152;&#26377;&#20195;&#29702;&#30340;&#21160;&#24577;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#20004;&#20010;&#32570;&#28857;&#65292;&#65288;i&#65289;&#20960;&#20046;&#25152;&#26377;&#27169;&#22411;&#37117;&#20381;&#36182;&#20110;&#39640;&#28165;&#22320;&#22270;&#65292;&#28982;&#32780;&#22320;&#22270;&#20449;&#24687;&#22312;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#32780;&#19988;&#24314;&#31435;&#39640;&#28165;&#22320;&#22270;&#20063;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#65307;&#65288;ii&#65289;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#27880;&#37325;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#28982;&#32780;&#25928;&#29575;&#23545;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#20132;&#36890;&#22320;&#22270;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#23545;&#21333;&#20010;&#20195;&#29702;&#30340;&#26102;&#31354;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#25506;&#32034;&#22810;&#20010;&#20195;&#29702;&#30340;&#26102;&#31354;&#20132;&#20114;&#12290;&#36890;&#36807;&#32508;&#21512;&#36816;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;LSTM&#12289;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26102;&#24207;&#36716;&#25442;&#22120;&#31561;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#25152;&#26377;&#20195;&#29702;&#30340;&#20016;&#23500;&#21160;&#24577;&#21644;&#20132;&#20114;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#26080;&#22320;&#22270;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the two shortcomings of existing methods, (i)nearly all models rely on high-definition (HD) maps, yet the map information is not always available in real traffic scenes and HD map-building is expensive and time-consuming and (ii) existing models usually focus on improving prediction accuracy at the expense of reducing computing efficiency, yet the efficiency is crucial for various real applications, this paper proposes an efficient trajectory prediction model that is not dependent on traffic maps. The core idea of our model is encoding single-agent's spatial-temporal information in the first stage and exploring multi-agents' spatial-temporal interactions in the second stage. By comprehensively utilizing attention mechanism, LSTM, graph convolution network and temporal transformer in the two stages, our model is able to learn rich dynamic and interaction information of all agents. Our model achieves the highest performance when comparing with existing map-free methods and also
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RobOCLe&#26041;&#27861;&#65292;&#38024;&#23545;&#23460;&#20869;&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#20016;&#23500;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#35745;&#31639;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09827</link><description>&lt;p&gt;
&#22312;&#23460;&#20869;&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning for Robust Indoor Object Recognition. (arXiv:2307.09827v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RobOCLe&#26041;&#27861;&#65292;&#38024;&#23545;&#23460;&#20869;&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#20016;&#23500;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#35745;&#31639;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23478;&#24237;&#26426;&#22120;&#20154;&#19978;&#30340;&#35270;&#35273;&#31995;&#32479;&#38656;&#35201;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#19982;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#36827;&#34892;&#20132;&#20114;&#12290;&#26426;&#22120;&#20154;&#20855;&#26377;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#26631;&#35760;&#25968;&#25454;&#21644;&#23384;&#20648;&#33021;&#21147;&#12290;&#36825;&#20123;&#35201;&#27714;&#24102;&#26469;&#20102;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#24212;&#35813;&#20197;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#36866;&#24212;&#24182;&#19981;&#20250;&#24536;&#35760;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#23450;&#20041;&#20026;&#23569;&#26679;&#26412;&#65288;FS&#65289;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#20195;&#29702;&#20174;&#19968;&#31995;&#21015;&#38750;&#37325;&#22797;&#30340;&#23569;&#26679;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21482;&#26356;&#26032;&#23569;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#20250;&#36935;&#21040;&#19981;&#21516;&#30340;&#26465;&#20214;&#65292;&#29289;&#20307;&#21487;&#33021;&#20197;&#19981;&#21516;&#30340;&#23039;&#21183;&#65288;&#20363;&#22914;&#65292;&#27700;&#24179;&#25110;&#22402;&#30452;&#65289;&#21644;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#30333;&#22825;&#25110;&#40657;&#22812;&#65289;&#20986;&#29616;&#65292;&#20026;&#20102;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RobOCLe&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#35745;&#31639;&#26679;&#26412;&#30340;&#23884;&#20837;&#29305;&#24449;&#30340;&#39640;&#38454;&#32479;&#35745;&#37327;&#26469;&#26500;&#24314;&#20016;&#23500;&#30340;&#29305;&#24449;&#31354;&#38388;&#65307;2&#65289;&#35745;&#31639;&#20016;&#23500;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#30340;&#39640;&#38454;&#32479;&#35745;&#37327;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#39044;&#27979;&#26679;&#26412;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision systems mounted on home robots need to interact with unseen classes in changing environments. Robots have limited computational resources, labelled data and storage capability. These requirements pose some unique challenges: models should adapt without forgetting past knowledge in a data- and parameter-efficient way. We characterize the problem as few-shot (FS) online continual learning (OCL), where robotic agents learn from a non-repeated stream of few-shot data updating only a few model parameters. Additionally, such models experience variable conditions at test time, where objects may appear in different poses (e.g., horizontal or vertical) and environments (e.g., day or night). To improve robustness of CL agents, we propose RobOCLe, which; 1) constructs an enriched feature space computing high order statistical moments from the embedded features of samples; and 2) computes similarity between high order statistics of the samples on the enriched feature space, and predicts the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.09782</link><description>&lt;p&gt;
ZeroQuant-FP: &#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#30340;&#19968;&#39033;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09782
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;&#28014;&#28857;&#65288;FP&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;FP8&#21644;FP4&#65292;&#20197;&#24212;&#23545;&#22343;&#21248;&#37327;&#21270;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#24182;&#21463;&#21040;NVIDIA H100&#30828;&#20214;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;LLMs&#20013;&#65292;FP8&#28608;&#27963;&#22987;&#32456;&#20248;&#20110;&#20854;&#25972;&#25968;&#65288;INT8&#65289;&#31561;&#25928;&#65292;&#24615;&#33021;&#20248;&#21183;&#22312;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#23545;&#20110;&#26435;&#37325;&#37327;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FP4&#30340;&#24615;&#33021;&#19982;INT4&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#20248;&#65292;&#31616;&#21270;&#20102;&#22312;&#20687;H100&#36825;&#26679;&#25903;&#25345;FP&#30340;&#30828;&#20214;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20943;&#23569;&#30001;&#26435;&#37325;&#21644;&#28608;&#27963;&#20043;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#31934;&#24230;&#23545;&#40784;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32553;&#25918;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#23618;&#36974;&#32617;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.09781</link><description>&lt;p&gt;
Text2Layer: &#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#23618;&#36974;&#32617;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23618;&#21512;&#25104;&#26159;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20316;&#27969;&#20043;&#19968;&#12290;&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#30340;&#35282;&#24230;&#25506;&#32034;&#22270;&#23618;&#21512;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#29983;&#25104;&#32972;&#26223;&#12289;&#21069;&#26223;&#12289;&#22270;&#23618;&#36974;&#32617;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#19968;&#24133;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#37325;&#24314;&#20998;&#23618;&#22270;&#20687;&#65292;&#24182;&#22312;&#28508;&#22312;&#34920;&#31034;&#19978;&#35757;&#32451;&#20102;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#22909;&#22788;&#26159;&#22312;&#39640;&#36136;&#37327;&#22270;&#20687;&#36755;&#20986;&#20043;&#22806;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#12290;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#29983;&#25104;&#27604;&#36890;&#36807;&#22270;&#20687;&#20998;&#21106;&#30340;&#29420;&#31435;&#27493;&#39588;&#20135;&#29983;&#30340;&#22270;&#23618;&#36974;&#32617;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#23618;&#36974;&#32617;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23618;&#22270;&#20687;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#22880;&#23450;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICECREAM&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#21270;&#34913;&#37327;&#20102;&#21464;&#37327;&#32852;&#21512;&#23545;&#30446;&#26631;&#21464;&#37327;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#33021;&#22815;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#32467;&#26524;&#25152;&#38656;&#30340;&#20851;&#38190;&#22240;&#32032;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.09779</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#19968;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#26032;&#26041;&#27861;ICECREAM
&lt;/p&gt;
&lt;p&gt;
Beyond Single-Feature Importance with ICECREAM. (arXiv:2307.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09779
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICECREAM&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#21270;&#34913;&#37327;&#20102;&#21464;&#37327;&#32852;&#21512;&#23545;&#30446;&#26631;&#21464;&#37327;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#33021;&#22815;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#32467;&#26524;&#25152;&#38656;&#30340;&#20851;&#38190;&#22240;&#32032;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#30830;&#23450;&#29305;&#24449;&#38598;&#21512;&#26159;&#21738;&#20123;&#65311;&#20113;&#35745;&#31639;&#24212;&#29992;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#21738;&#20123;&#32452;&#20214;&#65311;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;ICECREAM&#65292;&#35299;&#31572;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#21464;&#37327;&#32852;&#21512;&#23545;&#30446;&#26631;&#21464;&#37327;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#21644;&#22240;&#26524;&#36129;&#29486;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#32467;&#26524;&#25152;&#38656;&#30340;&#20851;&#38190;&#22240;&#32032;&#38598;&#21512;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23545;&#20010;&#20307;&#22240;&#32032;&#36827;&#34892;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ICECREAM&#22312;&#35299;&#37322;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;Minecraft&#20013;&#29983;&#25104;&#22478;&#24066;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31639;&#27861;&#26469;&#29983;&#25104;&#24314;&#31569;&#24067;&#23616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#31639;&#27861;&#23545;&#20110;&#24179;&#22374;&#22320;&#22270;&#26356;&#24555;&#25214;&#21040;&#21487;&#25509;&#21463;&#30340;&#24067;&#23616;&#65292;&#32780;&#28436;&#21270;&#24067;&#23616;&#31639;&#27861;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.09777</link><description>&lt;p&gt;
&#22312;Minecraft&#20013;&#29983;&#25104;&#31867;&#20284;&#32418;&#30707;&#39118;&#26684;&#30340;&#22478;&#24066;
&lt;/p&gt;
&lt;p&gt;
Generating Redstone Style Cities in Minecraft. (arXiv:2307.09777v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;Minecraft&#20013;&#29983;&#25104;&#22478;&#24066;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31639;&#27861;&#26469;&#29983;&#25104;&#24314;&#31569;&#24067;&#23616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#31639;&#27861;&#23545;&#20110;&#24179;&#22374;&#22320;&#22270;&#26356;&#24555;&#25214;&#21040;&#21487;&#25509;&#21463;&#30340;&#24067;&#23616;&#65292;&#32780;&#28436;&#21270;&#24067;&#23616;&#31639;&#27861;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Minecraft&#20013;&#31243;&#24207;&#21270;&#29983;&#25104;&#22478;&#24066;&#20026;&#29609;&#23478;&#25552;&#20379;&#20102;&#26356;&#22810;&#22810;&#26679;&#21270;&#22330;&#26223;&#30340;&#21487;&#33021;&#65292;&#20063;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#25913;&#36827;&#20854;&#20182;&#25968;&#23383;&#19990;&#30028;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22478;&#24066;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22478;&#24066;&#29983;&#25104;&#22120;&#65292;&#20316;&#20026;&#21442;&#21152;Minecraft&#23450;&#23621;&#29983;&#25104;&#27604;&#36187;&#30340;&#21442;&#36187;&#20316;&#21697;&#12290;&#29983;&#25104;&#36807;&#31243;&#21253;&#25324;&#26893;&#34987;&#28165;&#29702;&#12289;&#22320;&#24418;&#25972;&#24418;&#12289;&#24314;&#31569;&#24067;&#23616;&#29983;&#25104;&#12289;&#36335;&#24452;&#35268;&#21010;&#12289;&#36335;&#28783;&#24067;&#32622;&#21644;&#22260;&#22681;&#24314;&#35774;&#31561;&#20845;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#20351;&#29992;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#31639;&#27861;&#12289;&#28436;&#21270;&#24067;&#23616;&#31639;&#27861;&#21644;&#38543;&#26426;&#31639;&#27861;&#26469;&#29983;&#25104;&#24314;&#31569;&#24067;&#23616;&#65292;&#24182;&#36890;&#36807;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#22312;&#38543;&#26426;&#22320;&#22270;&#19978;&#29983;&#25104;&#22478;&#24066;&#36827;&#34892;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;&#24179;&#22374;&#22320;&#22270;&#65292;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#31639;&#27861;&#33021;&#26356;&#24555;&#22320;&#25214;&#21040;&#21487;&#25509;&#21463;&#30340;&#24314;&#31569;&#24067;&#23616;&#65292;&#32780;&#28436;&#21270;&#24067;&#23616;&#31639;&#27861;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedurally generating cities in Minecraft provides players more diverse scenarios and could help understand and improve the design of cities in other digital worlds and the real world. This paper presents a city generator that was submitted as an entry to the 2023 Edition of Minecraft Settlement Generation Competition for Minecraft. The generation procedure is composed of six main steps, namely vegetation clearing, terrain reshaping, building layout generation, route planning, streetlight placement, and wall construction. Three algorithms, including a heuristic-based algorithm, an evolving layout algorithm, and a random one are applied to generate the building layout, thus determining where to place different redstone style buildings, and tested by generating cities on random maps in limited time. Experimental results show that the heuristic-based algorithm is capable of finding an acceptable building layout faster for flat maps, while the evolving layout algorithm performs better in
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25200;&#21160;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#33041;&#20869;&#26377;&#25928;&#36830;&#25509;&#24615;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#20449;&#21495;&#26469;&#35745;&#31639;&#25200;&#21160;&#20043;&#21518;&#23545;&#20854;&#20182;&#33041;&#21306;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#33041;&#21306;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#27979;&#35797;&#32467;&#26524;&#22312;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09770</link><description>&lt;p&gt;
&#36890;&#36807;&#25200;&#21160;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#26377;&#25928;&#36830;&#25509;&#24615;&#65306;&#26469;&#33258;&#21512;&#25104;&#33041;&#30005;&#25968;&#25454;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Perturbing a Neural Network to Infer Effective Connectivity: Evidence from Synthetic EEG Data. (arXiv:2307.09770v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25200;&#21160;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#33041;&#20869;&#26377;&#25928;&#36830;&#25509;&#24615;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#20449;&#21495;&#26469;&#35745;&#31639;&#25200;&#21160;&#20043;&#21518;&#23545;&#20854;&#20182;&#33041;&#21306;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#33041;&#21306;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#27979;&#35797;&#32467;&#26524;&#22312;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22823;&#33041;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21363;&#26377;&#25928;&#36830;&#25509;&#24615;&#65292;&#23545;&#20110;&#20102;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#21644;&#35748;&#30693;&#21151;&#33021;&#20855;&#26377;&#37325;&#35201;&#27934;&#35265;&#12290;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#26174;&#31034;&#20986;&#33041;&#20869;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#21306;&#22495;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#34920;&#24449;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#26041;&#27861;&#20173;&#30456;&#23545;&#19981;&#22815;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25200;&#21160;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#26029;&#26377;&#25928;&#36830;&#25509;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;CNN&#12289;vanilla RNN&#12289;GRU&#12289;LSTM&#21644;Transformer&#65289;&#65292;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#30340;EEG&#20449;&#21495;&#65292;&#28982;&#21518;&#25200;&#21160;&#32593;&#32476;&#30340;&#36755;&#20837;&#20197;&#33719;&#21462;&#25200;&#21160;&#30340;EEG&#36890;&#36947;&#19982;&#20854;&#20182;&#36890;&#36947;&#20043;&#38388;&#30340;&#26377;&#25928;&#36830;&#25509;&#24615;&#65288;EC&#65289;&#12290;EC&#21453;&#26144;&#20102;&#25200;&#21160;&#19968;&#20010;&#33410;&#28857;&#23545;&#20854;&#20182;&#33410;&#28857;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#24615;&#33021;&#27979;&#35797;&#26159;&#22312;&#30001;&#29983;&#29289;&#21512;&#29702;&#30340;Jansen-Rit&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;EEG&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying causal relationships among distinct brain areas, known as effective connectivity, holds key insights into the brain's information processing and cognitive functions. Electroencephalogram (EEG) signals exhibit intricate dynamics and inter-areal interactions within the brain. However, methods for characterizing nonlinear causal interactions among multiple brain regions remain relatively underdeveloped. In this study, we proposed a data-driven framework to infer effective connectivity by perturbing the trained neural networks. Specifically, we trained neural networks (i.e., CNN, vanilla RNN, GRU, LSTM, and Transformer) to predict future EEG signals according to historical data and perturbed the networks' input to obtain effective connectivity (EC) between the perturbed EEG channel and the rest of the channels. The EC reflects the causal impact of perturbing one node on others. The performance was tested on the synthetic EEG generated by a biological-plausible Jansen-Rit model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32447;&#24615;&#21464;&#25442;&#21644;&#31614;&#21517;&#21464;&#25442;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#65292;&#26082;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#24615;&#65292;&#21448;&#24341;&#20837;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20984;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09767</link><description>&lt;p&gt;
Sig-Splines&#65306;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#36890;&#29992;&#36924;&#36817;&#21644;&#20984;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Sig-Splines: universal approximation and convex calibration of time series generative models. (arXiv:2307.09767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32447;&#24615;&#21464;&#25442;&#21644;&#31614;&#21517;&#21464;&#25442;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#65292;&#26082;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#24615;&#65292;&#21448;&#24341;&#20837;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20984;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#12290;&#21463;&#31070;&#32463;&#26679;&#26465;&#27969;&#26500;&#36896;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#32447;&#24615;&#21464;&#25442;&#21644;&#31614;&#21517;&#21464;&#25442;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#26367;&#20195;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#22266;&#26377;&#30340;&#36890;&#29992;&#24615;&#65292;&#36824;&#24341;&#20837;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20984;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#20559;&#22909;&#25511;&#21046;&#27169;&#22359;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#37197;&#32622;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#23398;&#20064;&#20013;&#23545;&#39057;&#29575;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.09763</link><description>&lt;p&gt;
&#29992;&#39057;&#29575;&#20559;&#24046;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Building More Robust Models with Frequency Bias. (arXiv:2307.09763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#20559;&#22909;&#25511;&#21046;&#27169;&#22359;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#37197;&#32622;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#23398;&#20064;&#20013;&#23545;&#39057;&#29575;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#26159;&#23427;&#20204;&#24212;&#29992;&#24191;&#27867;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#24378;&#35843;&#20302;&#39057;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#21033;&#29992;&#36825;&#31181;&#39057;&#29575;&#29305;&#24615;&#65292;&#20294;&#30452;&#25509;&#23558;&#20302;&#36890;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#20250;&#23548;&#33268;&#19981;&#21487;&#36870;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#23545;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39057;&#29575;&#20559;&#22909;&#25511;&#21046;&#27169;&#22359;&#30340;&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#37197;&#32622;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;&#39057;&#29575;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability of deep neural networks to adversarial samples has been a major impediment to their broad applications, despite their success in various fields. Recently, some works suggested that adversarially-trained models emphasize the importance of low-frequency information to achieve higher robustness. While several attempts have been made to leverage this frequency characteristic, they have all faced the issue that applying low-pass filters directly to input images leads to irreversible loss of discriminative information and poor generalizability to datasets with distinct frequency features. This paper presents a plug-and-play module called the Frequency Preference Control Module that adaptively reconfigures the low- and high-frequency components of intermediate feature representations, providing better utilization of frequency in robust learning. Empirical studies show that our proposed module can be easily incorporated into any adversarial training framework, further improvi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28388;&#27874;&#21644;&#27169;&#24335;&#35782;&#21035;&#24378;&#21270;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#34987;&#29992;&#20110;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#32500;&#24230;&#20351;&#24471;&#20854;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;POD&#31561;&#38477;&#32500;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODEs)&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#19982;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23545;&#27604;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;logit&#31354;&#38388;&#20013;&#30340;&#30417;&#30563;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#31354;&#38388;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;logit&#31354;&#38388;&#21644;&#34920;&#31034;&#31354;&#38388;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.09755</link><description>&lt;p&gt;
Space Engage: &#22522;&#20110;&#23545;&#27604;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#21327;&#21516;&#31354;&#38388;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation. (arXiv:2307.09755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09755
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;logit&#31354;&#38388;&#20013;&#30340;&#30417;&#30563;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#31354;&#38388;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;logit&#31354;&#38388;&#21644;&#34920;&#31034;&#31354;&#38388;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26088;&#22312;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#22270;&#20687;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#22270;&#20687;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#24378;&#22823;&#30340;&#26041;&#27861;&#22312;&#28508;&#22312;&#31354;&#38388;&#65288;&#21363;&#34920;&#31034;&#31354;&#38388;&#65289;&#20013;&#24341;&#20837;&#20102;&#20687;&#32032;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20840;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#34920;&#31034;&#32858;&#21512;&#21040;&#20854;&#21407;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22522;&#20110;&#23545;&#27604;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#26410;&#26631;&#35760;&#35757;&#32451;&#20013;&#27169;&#22411;&#36755;&#20986;&#65288;logits&#65289;&#22312;logit&#31354;&#38388;&#20013;&#30340;&#30417;&#30563;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;logit&#31354;&#38388;&#21644;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36755;&#20986;&#26469;&#21327;&#21516;&#33719;&#24471;&#30417;&#30563;&#12290;&#20004;&#20010;&#31354;&#38388;&#30340;&#30417;&#30563;&#21457;&#25381;&#20004;&#20010;&#20316;&#29992;&#65306;1&#65289;&#20511;&#21161;&#34920;&#31034;&#20943;&#23569;&#22312;logits&#20013;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;2&#65289;&#22686;&#24378;&#20004;&#20010;&#31354;&#38388;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#12290;&#27492;&#22806;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#34920;&#31034;&#19982;&#21407;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#23545;&#27604;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model with limited labeled images and a substantial volume of unlabeled images. To improve the robustness of representations, powerful methods introduce a pixel-wise contrastive learning approach in latent space (i.e., representation space) that aggregates the representations to their prototypes in a fully supervised manner. However, previous contrastive-based S4 methods merely rely on the supervision from the model's output (logits) in logit space during unlabeled training. In contrast, we utilize the outputs in both logit space and representation space to obtain supervision in a collaborative way. The supervision from two spaces plays two roles: 1) reduces the risk of over-fitting to incorrect semantic information in logits with the help of representations; 2) enhances the knowledge exchange between the two spaces. Furthermore, unlike previous approaches, we use the similarity between representations and prototyp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20013;&#22269;&#20449;&#24687;&#26816;&#32034;&#30028;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#25112;&#30053;&#25253;&#21578;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#30693;&#35782;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#33021;&#21147;&#65292;&#20026;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;IR&#27169;&#22411;&#12289;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#20449;&#24687;&#23547;&#27714;&#25216;&#26415;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#20173;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#12289;&#21487;&#20449;&#24230;&#12289;&#39046;&#22495;&#29305;&#23450;&#38480;&#21046;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09751</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20013;&#22269;&#20449;&#24687;&#26816;&#32034;&#30028;&#30340;&#25112;&#30053;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. (arXiv:2307.09751v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20013;&#22269;&#20449;&#24687;&#26816;&#32034;&#30028;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#25112;&#30053;&#25253;&#21578;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#30693;&#35782;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#33021;&#21147;&#65292;&#20026;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;IR&#27169;&#22411;&#12289;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#20449;&#24687;&#23547;&#27714;&#25216;&#26415;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#20173;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#12289;&#21487;&#20449;&#24230;&#12289;&#39046;&#22495;&#29305;&#23450;&#38480;&#21046;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#25628;&#32034;&#65292;&#20197;&#28385;&#36275;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#30693;&#35782;&#25512;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20026;IR&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#22865;&#26426;&#12290;LLM&#19981;&#20165;&#33021;&#22815;&#20419;&#36827;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#36824;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#29992;&#25143;&#29702;&#35299;&#12289;&#27169;&#22411;&#35780;&#20272;&#21644;&#29992;&#25143;&#31995;&#32479;&#20132;&#20114;&#26041;&#26696;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;IR&#27169;&#22411;&#12289;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#26500;&#25104;&#20102;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#20449;&#24687;&#23547;&#27714;&#25216;&#26415;&#33539;&#24335;&#12290;IR&#27169;&#22411;&#25552;&#20379;&#23454;&#26102;&#21644;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;LLM&#36129;&#29486;&#20869;&#37096;&#30693;&#35782;&#65292;&#32780;&#20154;&#31867;&#22312;&#20449;&#24687;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#38656;&#27714;&#32773;&#21644;&#35780;&#20272;&#32773;&#30340;&#20013;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#30528;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;&#21487;&#20449;&#24230;&#38382;&#39064;&#12289;&#39046;&#22495;&#29305;&#23450;&#38480;&#21046;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;GPT4&#36827;&#34892;ASR&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;GPT4&#32416;&#27491;&#30340;&#36716;&#24405;&#23548;&#33268;&#26356;&#39640;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#23613;&#31649;WER&#26377;&#25152;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.09744</link><description>&lt;p&gt;
&#25552;&#39640;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#35805;&#36136;&#37327;&#65306;&#23545;GPT4&#22312;ASR&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction. (arXiv:2307.09744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;GPT4&#36827;&#34892;ASR&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;GPT4&#32416;&#27491;&#30340;&#36716;&#24405;&#23548;&#33268;&#26356;&#39640;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#23613;&#31649;WER&#26377;&#25152;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65288;NLP&#65289;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#30340;&#25972;&#21512;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#23398;&#20064;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#21475;&#35821;&#24320;&#25918;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#29992;&#20316;&#21475;&#35821;&#20249;&#20276;&#65292;&#24110;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#25552;&#21319;&#35821;&#35328;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#22312;&#35782;&#21035;&#38750;&#27597;&#35821;/&#38750;&#27969;&#21033;&#35821;&#38899;&#26102;&#30340;&#39640;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#36825;&#20250;&#25171;&#26029;&#23545;&#35805;&#27969;&#31243;&#24182;&#20196;&#23398;&#20064;&#32773;&#24863;&#21040;&#22833;&#26395;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#20351;&#29992;GPT4&#36827;&#34892;ASR&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;WER&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#21644;&#19979;&#19968;&#20010;&#22238;&#22797;&#30340;&#21512;&#29702;&#24615;&#65288;NRS&#65289;&#25351;&#26631;&#26469;&#35780;&#20272;&#38169;&#35823;&#32416;&#27491;&#27169;&#22411;&#23545;&#23545;&#35805;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;GPT4&#32416;&#27491;&#30340;&#36716;&#24405;&#23548;&#33268;&#26356;&#39640;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#23613;&#31649;WER&#26377;&#25152;&#22686;&#21152;&#12290;GPT4&#36824;&#20248;&#20110;&#26631;&#20934;&#30340;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39046;&#22495;&#19987;&#23646;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of natural language processing (NLP) technologies into educational applications has shown promising results, particularly in the language learning domain. Recently, many spoken open-domain chatbots have been used as speaking partners, helping language learners improve their language skills. However, one of the significant challenges is the high word-error-rate (WER) when recognizing non-native/non-fluent speech, which interrupts conversation flow and leads to disappointment for learners. This paper explores the use of GPT4 for ASR error correction in conversational settings. In addition to WER, we propose to use semantic textual similarity (STS) and next response sensibility (NRS) metrics to evaluate the impact of error correction models on the quality of the conversation. We find that transcriptions corrected by GPT4 lead to higher conversation quality, despite an increase in WER. GPT4 also outperforms standard error correction methods without the need for in-domain tr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MIMIC&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#20132;&#20114;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#20114;&#34917;&#19981;&#20805;&#20998;&#21644;&#22122;&#22768;&#25968;&#25454;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09721</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#20132;&#20114;&#32593;&#32476;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Multi-Grained Multimodal Interaction Network for Entity Linking. (arXiv:2307.09721v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MIMIC&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#20132;&#20114;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#20114;&#34917;&#19981;&#20805;&#20998;&#21644;&#22122;&#22768;&#25968;&#25454;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#65288;MEL&#65289;&#20219;&#21153;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#21547;&#31946;&#25552;&#21450;&#26041;&#38754;&#21560;&#24341;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#25506;&#32034;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#30340;&#20114;&#34917;&#25928;&#24212;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26410;&#33021;&#20805;&#20998;&#21560;&#25910;&#31616;&#20889;&#25991;&#26412;&#19978;&#19979;&#25991;&#21644;&#38544;&#21547;&#35270;&#35273;&#25351;&#31034;&#30340;&#32508;&#21512;&#34920;&#36798;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#19981;&#36275;&#65292;&#20005;&#37325;&#38477;&#20302;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#20132;&#20114;&#32593;&#32476;(MIMIC)&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;MEL&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#23558;&#25552;&#21450;&#21644;&#23454;&#20307;&#30340;&#32479;&#19968;&#36755;&#20837;&#30001;&#25991;&#26412;/&#35270;&#35273;&#32534;&#30721;&#22120;&#20998;&#21035;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#25552;&#21462;&#20840;&#23616;&#25551;&#36848;&#29305;&#24449;&#21644;&#23616;&#37096;&#35814;&#32454;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#20026;&#27599;&#20010;&#25552;&#21450;-&#23454;&#20307;&#23545;&#27966;&#29983;&#30456;&#20284;&#24615;&#21305;&#37197;&#20998;&#25968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#20132;&#20114;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$ framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#33258;&#20027;&#31227;&#21160;&#25511;&#21046;&#21644;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#30340;&#20004;&#20010;&#26041;&#27861;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#12290;&#20854;&#20013;&#65292;&#36890;&#20449;&#32593;&#32476;&#26159;&#26368;&#27969;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19968;&#65292;&#23558;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#35757;&#32451;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#20445;&#35777;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#35802;&#20449;&#24230;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#30410;&#12290;&#25972;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#23545;&#20110;&#23454;&#29616;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#33258;&#20027;&#31227;&#21160;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.09711</link><description>&lt;p&gt;
&#20004;&#31181;&#25925;&#20107;&#20171;&#32461;&#20102;&#29992;&#20110;&#33258;&#20027;&#31227;&#21160;&#25511;&#21046;&#30340;&#36830;&#38431;&#24773;&#25253;:&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
Two Tales of Platoon Intelligence for Autonomous Mobility Control: Enabling Deep Learning Recipes. (arXiv:2307.09711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#33258;&#20027;&#31227;&#21160;&#25511;&#21046;&#21644;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#30340;&#20004;&#20010;&#26041;&#27861;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#12290;&#20854;&#20013;&#65292;&#36890;&#20449;&#32593;&#32476;&#26159;&#26368;&#27969;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19968;&#65292;&#23558;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#35757;&#32451;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#20445;&#35777;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#35802;&#20449;&#24230;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#30410;&#12290;&#25972;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#23545;&#20110;&#23454;&#29616;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#33258;&#20027;&#31227;&#21160;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#20197;&#35299;&#20915;&#33258;&#20027;&#31227;&#21160;&#25511;&#21046;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#26080;&#20154;&#26426;&#30340;&#39640;&#25928;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#21363;(i)&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#21644;(ii)&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#12290;&#20316;&#20026;&#20195;&#34920;&#65292;&#24341;&#20837;&#20102;&#36890;&#20449;&#32593;&#32476;&#65288;CommNet&#65289;&#65292;&#36825;&#26159;&#26368;&#27969;&#34892;&#30340;MARL&#31639;&#27861;&#20043;&#19968;&#65292;&#36890;&#36807;&#35757;&#32451;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#37319;&#21462;&#34892;&#21160;&#26469;&#23454;&#29616;&#20849;&#20139;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#20445;&#35777;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#35802;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#24230;&#21160;&#24577;&#31995;&#32479;&#30340;&#26368;&#20248;&#25910;&#30410;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;MARL&#21644;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#30340;&#33258;&#20027;&#31227;&#21160;&#25511;&#21046;&#30340;&#26368;&#36817;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;MARL&#21644;&#31070;&#32463;&#36808;&#23572;&#36874;&#25293;&#21334;&#30340;&#25972;&#21512;&#23545;&#20110;&#23454;&#29616;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#33258;&#20027;&#31227;&#21160;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the deep learning-based recent achievements to resolve the problem of autonomous mobility control and efficient resource management of autonomous vehicles and UAVs, i.e., (i) multi-agent reinforcement learning (MARL), and (ii) neural Myerson auction. Representatively, communication network (CommNet), which is one of the most popular MARL algorithms, is introduced to enable multiple agents to take actions in a distributed manner for their shared goals by training all agents' states and actions in a single neural network. Moreover, the neural Myerson auction guarantees trustfulness among multiple agents as well as achieves the optimal revenue of highly dynamic systems. Therefore, we survey the recent studies on autonomous mobility control based on MARL and neural Myerson auction. Furthermore, we emphasize that integration of MARL and neural Myerson auction is expected to be critical for efficient and trustful autonomous mobility services.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;STRAPPER&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#22686;&#24378;&#21644;&#21516;&#20276;&#27491;&#21017;&#21270;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#20316;&#32773;&#21457;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30456;&#20284;&#24615;&#38519;&#38449;&#29616;&#35937;&#65292;&#21363;&#30456;&#20284;&#30340;&#29255;&#27573;&#23545;&#21487;&#33021;&#20250;&#23384;&#22312;&#25130;&#28982;&#30456;&#21453;&#30340;&#20559;&#22909;&#65292;&#23545;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#36896;&#25104;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09692</link><description>&lt;p&gt;
STRAPPER&#65306;&#36890;&#36807;&#33258;&#35757;&#32451;&#22686;&#24378;&#21644;&#21516;&#20276;&#27491;&#21017;&#21270;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;STRAPPER&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#22686;&#24378;&#21644;&#21516;&#20276;&#27491;&#21017;&#21270;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#20316;&#32773;&#21457;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30456;&#20284;&#24615;&#38519;&#38449;&#29616;&#35937;&#65292;&#21363;&#30456;&#20284;&#30340;&#29255;&#27573;&#23545;&#21487;&#33021;&#20250;&#23384;&#22312;&#25130;&#28982;&#30456;&#21453;&#30340;&#20559;&#22909;&#65292;&#23545;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#36896;&#25104;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#25215;&#35834;&#36890;&#36807;&#20108;&#36827;&#21046;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#22797;&#26434;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20154;&#31867;&#21442;&#19982;&#30340;&#24418;&#24335;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#26469;&#20026;&#29255;&#27573;&#23545;&#20998;&#37197;&#20559;&#22909;&#26631;&#31614;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#37325;&#22797;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#29255;&#27573;&#65292;&#38544;&#21547;&#22320;&#38416;&#26126;&#20102;&#29255;&#27573;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20154;&#20204;&#30340;&#21162;&#21147;&#12290;&#24182;&#19988;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#19982;&#26222;&#36890;&#30340;&#20998;&#31867;&#20219;&#21153;&#19981;&#21516;&#65292;PbRL&#20013;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23450;&#20041;&#20026;&#30456;&#20284;&#24615;&#38519;&#38449;&#30340;&#29420;&#29305;&#29616;&#35937;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#20154;&#31867;&#23545;&#20110;&#30456;&#20284;&#30340;&#29255;&#27573;&#23545;&#21487;&#33021;&#20250;&#23384;&#22312;&#25130;&#28982;&#30456;&#21453;&#30340;&#20559;&#22909;&#65292;&#20294;&#36825;&#31181;&#30456;&#20284;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22312;PbRL&#20013;&#22833;&#36133;&#12290;&#30001;&#20110;&#30456;&#20284;&#24615;&#38519;&#38449;&#30340;&#23384;&#22312;&#65292;&#36825;&#26679;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#19981;&#36866;&#24403;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's pr
&lt;/p&gt;</description></item><item><title>Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09688</link><description>&lt;p&gt;
Amazon-M2: &#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09688
&lt;/p&gt;
&lt;p&gt;
Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#26469;&#35828;&#65292;&#24314;&#27169;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#29702;&#35299;&#23458;&#25143;&#30340;&#20559;&#22909;&#23545;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25216;&#26415;&#21033;&#29992;&#23458;&#25143;&#20250;&#35805;&#25968;&#25454;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#20114;&#21160;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20250;&#35805;&#25968;&#25454;&#38598;&#22312;&#39033;&#30446;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#20840;&#38754;&#22320;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20559;&#22909;&#30340;&#35889;&#31995;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Amazon Multilingual Multi-locale Shopping Session Dataset&#65292;&#21363;Amazon-M2&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#30001;&#26469;&#33258;&#20845;&#20010;&#19981;&#21516;&#21306;&#22495;&#30340;&#25968;&#30334;&#19975;&#29992;&#25143;&#20250;&#35805;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20135;&#21697;&#30340;&#20027;&#35201;&#35821;&#35328;&#26159;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#26085;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09683</link><description>&lt;p&gt;
PubMed&#21450;&#20854;&#20182;&#65306;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search. (arXiv:2307.09683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20854;&#20013;&#24456;&#22810;&#21482;&#33021;&#36890;&#36807;&#25991;&#29486;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#26816;&#32034;&#26159;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#22312;&#20808;&#21069;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23558;&#21151;&#33021;&#25193;&#23637;&#21040;&#20102;&#36229;&#36234;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25628;&#32034;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#21487;&#33021;&#23545;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#36824;&#27604;&#36739;&#38476;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#29305;&#23450;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#22320;&#28385;&#36275;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;PubMed&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#25913;&#36827;&#21644;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20116;&#31181;&#29305;&#23450;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65306;1.&#20026;&#24490;&#35777;&#21307;&#23398;&#23547;&#25214;&#39640;&#36136;&#37327;&#20020;&#24202;&#30740;&#31350;&#12290;2.&#20026;&#31934;&#20934;&#21307;&#23398;&#21644;&#22522;&#22240;&#32452;&#23398;&#26816;&#32034;&#22522;&#22240;&#30456;&#20851;&#20449;&#24687;&#12290;3.&#26681;&#25454;&#24847;&#20041;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, inc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#35843;&#26597;&#20102;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24182;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#36825;&#19968;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.09673</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#21487;&#35299;&#37322;&#27169;&#22411;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
What's meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09673
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#35843;&#26597;&#20102;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24182;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#36825;&#19968;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#22312;&#25551;&#36848;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24212;&#29992;&#30340;&#35770;&#25991;&#26631;&#39064;&#20013;&#30475;&#21040;&#21487;&#35299;&#37322;&#36825;&#20010;&#26415;&#35821;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;XAI&#20013;&#30340;&#35299;&#37322;&#26159;&#29305;&#23450;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#65292;&#22240;&#27492;&#22312;&#29992;&#20110;&#35299;&#37322;&#29305;&#23450;&#24212;&#29992;&#38382;&#39064;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#25581;&#31034;&#20102;&#20107;&#21518;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#65292;&#26263;&#31034;&#23427;&#20204;&#24182;&#19981;&#33021;&#25104;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;XAI&#26041;&#27861;&#26102;&#65292;&#24212;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#20449;&#24687;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#22240;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#26469;&#30740;&#31350;&#24212;&#29992;AI&#27169;&#22411;&#21644;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#27169;&#22411;&#31216;&#20026;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#23884;&#20837;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26680;&#24515;&#25512;&#29702;&#24037;&#20855;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#25968;&#25454;&#22797;&#29992;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22797;&#29992;&#23398;&#21040;&#30340;&#25216;&#33021;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09668</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#22522;&#30784;&#27169;&#22411;&#30340;&#32479;&#19968;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified Agent with Foundation Models. (arXiv:2307.09668v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#23884;&#20837;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26680;&#24515;&#25512;&#29702;&#24037;&#20855;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#25968;&#25454;&#22797;&#29992;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22797;&#29992;&#23398;&#21040;&#30340;&#25216;&#33021;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12289;&#25512;&#29702;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#35268;&#21010;&#34892;&#20026;&#31561;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#33021;&#21147;&#23884;&#20837;&#21644;&#21033;&#29992;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#20316;&#20026;&#26680;&#24515;&#25512;&#29702;&#24037;&#20855;&#30340;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#36825;&#22914;&#20309;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#24212;&#23545;&#19968;&#31995;&#21015;&#22522;&#30784;RL&#25361;&#25112;&#65292;&#22914;&#39640;&#25928;&#25506;&#32034;&#12289;&#22797;&#29992;&#32463;&#39564;&#25968;&#25454;&#12289;&#35843;&#24230;&#25216;&#33021;&#21644;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#65292;&#36825;&#20123;&#20256;&#32479;&#19978;&#38656;&#35201;&#21333;&#29420;&#35774;&#35745;&#30340;&#22402;&#30452;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#22534;&#21472;&#19968;&#32452;&#29289;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#22797;&#29992;&#25968;&#25454;&#26041;&#38754;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22797;&#29992;&#23398;&#21040;&#30340;&#25216;&#33021;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve no
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#39044;&#27979;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#23433;&#20840;&#21644;&#26680;&#19981;&#25193;&#25955;&#31561;&#39046;&#22495;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09665</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#39044;&#27979;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers. (arXiv:2307.09665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#39044;&#27979;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#23433;&#20840;&#21644;&#26680;&#19981;&#25193;&#25955;&#31561;&#39046;&#22495;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#39044;&#27979;&#20840;&#29699;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#36235;&#21183;&#23545;&#22269;&#23478;&#21644;&#20840;&#29699;&#23433;&#20840;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#26680;&#19981;&#25193;&#25955;&#65288;NN&#65289;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31561;&#24555;&#36895;&#20852;&#36215;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#21327;&#20316;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65289;&#65292;&#24182;&#21046;&#23450;&#20102;&#20351;&#29992;&#21160;&#24577;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#39044;&#27979;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#33021;&#21147;&#26469;&#39044;&#27979;&#19981;&#21516;&#31890;&#24230;&#65288;&#20363;&#22914;&#31185;&#23398;&#23478;&#21644;&#26426;&#26500;&#32423;&#21035;&#65289;&#30340;&#21327;&#20316;&#27169;&#24335;&#65292;&#20316;&#32773;&#34892;&#20026;&#21644;&#25216;&#26415;&#33021;&#21147;&#28436;&#36827;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#65288;DGT&#65289;&#31070;&#32463;&#26550;&#26500;&#65292;&#36890;&#36807;&#65288;a&#65289;&#39044;&#27979;&#24322;&#26500;&#65288;&#32780;&#19981;&#26159;&#21516;&#26500;&#65289;&#33410;&#28857;&#21644;&#36793;&#32536;&#65292;&#24182;&#65288;b&#65289;&#20381;&#36182;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#29305;&#24449;&#65292;&#25512;&#21160;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- 
&lt;/p&gt;</description></item><item><title>HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.09653</link><description>&lt;p&gt;
HAT-CL: &#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;
&lt;/p&gt;
&lt;p&gt;
HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09653
&lt;/p&gt;
&lt;p&gt;
HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20007;&#22833;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30828;&#27880;&#24847;&#21147;&#20219;&#21153;(HAT)&#26426;&#21046;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#21463;&#21040;&#20102;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HAT-CL&#65292;&#36825;&#26159;HAT&#26426;&#21046;&#30340;&#29992;&#25143;&#21451;&#22909;&#12289;&#19982;PyTorch&#20860;&#23481;&#30340;&#37325;&#26032;&#35774;&#35745;&#12290;HAT-CL&#19981;&#20165;&#33258;&#21160;&#21270;&#20102;&#26799;&#24230;&#25805;&#20316;&#65292;&#36824;&#31616;&#21270;&#20102;PyTorch&#27169;&#22359;&#36716;&#21270;&#20026;HAT&#27169;&#22359;&#30340;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#25552;&#20379;&#19968;&#22871;&#20840;&#38754;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#25552;&#20379;&#20102;&#19982;TIMM&#24211;&#24179;&#28369;&#38598;&#25104;&#30340;&#21487;&#29992;&#30340;HAT&#32593;&#32476;&#12290;&#38500;&#20102;&#23545;HAT&#30340;&#37325;&#26032;&#35774;&#35745;&#21644;&#37325;&#26032;&#23454;&#29616;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29992;&#20110;HAT&#30340;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VISER&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#29992;&#20110;&#22788;&#29702;&#20449;&#24687;&#19981;&#23545;&#31216;&#21338;&#24328;&#12290;VISER&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#39044;&#27979;&#21338;&#24328;&#30340;&#32467;&#26524;&#65292;&#24182;&#20801;&#35768;&#21463;&#23475;&#32773;&#26356;&#22909;&#22320;&#33258;&#21355;&#20197;&#21450;&#30830;&#23450;&#25915;&#20987;&#32773;&#21487;&#20351;&#29992;&#30340;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#65292;&#27599;&#20010;&#29609;&#23478;&#30340;VISER&#31574;&#30053;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#12290;&#35770;&#25991;&#36824;&#23558;VISER&#25193;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#23545;&#24212;&#35268;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#27714;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09652</link><description>&lt;p&gt;
VISER: &#19968;&#31181;&#22788;&#29702;&#20449;&#24687;&#19981;&#23545;&#31216;&#21338;&#24328;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
VISER: A Tractable Solution Concept for Games with Information Asymmetry. (arXiv:2307.09652v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VISER&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#29992;&#20110;&#22788;&#29702;&#20449;&#24687;&#19981;&#23545;&#31216;&#21338;&#24328;&#12290;VISER&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#39044;&#27979;&#21338;&#24328;&#30340;&#32467;&#26524;&#65292;&#24182;&#20801;&#35768;&#21463;&#23475;&#32773;&#26356;&#22909;&#22320;&#33258;&#21355;&#20197;&#21450;&#30830;&#23450;&#25915;&#20987;&#32773;&#21487;&#20351;&#29992;&#30340;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#65292;&#27599;&#20010;&#29609;&#23478;&#30340;VISER&#31574;&#30053;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#12290;&#35770;&#25991;&#36824;&#23558;VISER&#25193;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#23545;&#24212;&#35268;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#27714;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#21338;&#24328;&#23384;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65306;&#19968;&#20010;&#29609;&#23478;&#21482;&#30693;&#36947;&#33258;&#24049;&#30340;&#25910;&#30410;&#65292;&#32780;&#21478;&#19968;&#20010;&#29609;&#23478;&#25317;&#26377;&#23436;&#25972;&#30340;&#28216;&#25103;&#20449;&#24687;&#12290;&#20363;&#22914;&#23433;&#20840;&#21338;&#24328;&#21644;&#23545;&#25239;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#20449;&#24687;&#19981;&#23545;&#31216;&#20351;&#24471;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#22914;&#24378;&#21183;&#26031;&#22612;&#20811;&#36125;&#26684;&#22343;&#34913;&#65288;SSE&#65289;&#21644;&#40065;&#26834;&#20248;&#21270;&#22343;&#34913;&#65288;ROE&#65289;&#22833;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#31216;&#20026;VISER&#65288;&#21463;&#23475;&#32773;&#23433;&#20840;&#65292;&#21093;&#21066;&#32773;&#26368;&#20339;&#22238;&#24212;&#65289;&#12290;VISER&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#39044;&#27979;&#36825;&#31867;&#21338;&#24328;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;VISER&#20801;&#35768;&#21463;&#23475;&#32773;&#26356;&#22909;&#22320;&#33258;&#21355;&#65292;&#24182;&#30830;&#23450;&#25915;&#20987;&#32773;&#21487;&#20351;&#29992;&#30340;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27599;&#20010;&#29609;&#23478;&#30340;VISER&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#29420;&#31435;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#23558;VISER&#25193;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#23545;&#24212;&#35268;&#21017;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#35745;&#31639;&#21487;&#20197;&#39640;&#25928;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world games suffer from information asymmetry: one player is only aware of their own payoffs while the other player has the full game information. Examples include the critical domain of security games and adversarial multi-agent reinforcement learning. Information asymmetry renders traditional solution concepts such as Strong Stackelberg Equilibrium (SSE) and Robust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution concept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables an external observer to predict the outcome of such games. In particular, for security applications, VISER allows the victim to better defend itself while characterizing the most damaging attacks available to the attacker. We show that each player's VISER strategy can be computed independently in polynomial time using linear programming (LP). We also extend VISER to its Markov-perfect counterpart for Markov games, which can be solved efficiently using a series of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Reddit&#30340;r/place&#22312;&#32447;&#31038;&#21306;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#39044;&#27979;&#22312;&#32447;&#31038;&#21306;&#22312;&#22823;&#35268;&#27169;&#21327;&#20316;&#27963;&#21160;&#20013;&#30340;&#25104;&#21151;&#27700;&#24179;&#65292;&#20197;&#21450;&#20998;&#26512;&#31038;&#21306;&#25104;&#21592;&#20043;&#38388;&#30340;&#24213;&#23618;&#21160;&#24577;&#23545;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09650</link><description>&lt;p&gt;
&#20197;&#39556;&#20154;&#25112;&#32489;&#65306;&#39044;&#27979;&#22823;&#35268;&#27169;&#21327;&#20316;&#27963;&#21160;&#20013;&#31038;&#21306;&#30340;&#25104;&#21151;
&lt;/p&gt;
&lt;p&gt;
With Flying Colors: Predicting Community Success in Large-scale Collaborative Campaigns. (arXiv:2307.09650v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Reddit&#30340;r/place&#22312;&#32447;&#31038;&#21306;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#39044;&#27979;&#22312;&#32447;&#31038;&#21306;&#22312;&#22823;&#35268;&#27169;&#21327;&#20316;&#27963;&#21160;&#20013;&#30340;&#25104;&#21151;&#27700;&#24179;&#65292;&#20197;&#21450;&#20998;&#26512;&#31038;&#21306;&#25104;&#21592;&#20043;&#38388;&#30340;&#24213;&#23618;&#21160;&#24577;&#23545;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#31038;&#21306;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24314;&#31435;&#31038;&#20250;&#35268;&#33539;&#65292;&#24182;&#22312;&#20854;&#25104;&#21592;&#20043;&#38388;&#23637;&#29616;&#30528;&#19981;&#21516;&#30340;&#21160;&#24577;&#12290;&#22312;&#32447;&#31038;&#21306;&#30340;&#27963;&#21160;&#24448;&#24448;&#23548;&#33268;&#20855;&#20307;&#30340;&#8220;&#31163;&#32447;&#8221;&#34892;&#21160;&#65292;&#23545;&#31038;&#20250;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#65288;&#20363;&#22914;&#25919;&#27835;&#34903;&#22836;&#25239;&#35758;&#21644;&#19982;&#24615;&#34892;&#20026;&#19981;&#24403;&#26377;&#20851;&#30340;&#35268;&#33539;&#65289;&#12290;&#23613;&#31649;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#23545;&#31038;&#21306;&#21160;&#24577;&#12289;&#20449;&#24687;&#20256;&#25773;&#21644;&#22312;&#32447;&#21327;&#20316;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#34913;&#37327;&#22312;&#32447;&#31038;&#21306;&#22312;&#25512;&#21160;&#20854;&#35758;&#31243;&#26041;&#38754;&#26377;&#25928;&#24615;&#30340;&#23450;&#37327;&#30740;&#31350;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31038;&#21306;&#30340;&#25928;&#26524;&#65288;&#36890;&#36807;&#20854;&#22312;&#31454;&#20105;&#24615;&#22312;&#32447;&#27963;&#21160;&#20013;&#30340;&#25104;&#21151;&#27700;&#24179;&#26469;&#34913;&#37327;&#65289;&#21644;&#20854;&#25104;&#21592;&#20043;&#38388;&#30340;&#24213;&#23618;&#21160;&#24577;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#39044;&#27979;Reddit&#30340;r/place&#22312;&#32447;&#31038;&#21306;&#30340;&#25104;&#21151;&#27700;&#24179;--&#36825;&#26159;&#19968;&#20010;&#38656;&#35201;&#31038;&#21306;&#25104;&#21592;&#20043;&#38388;&#21327;&#20316;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23454;&#39564;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#25104;&#21151;&#27700;&#24179;&#30340;&#23450;&#20041;&#65307;&#27599;&#20010;&#23450;&#20041;&#34920;&#31034;&#31038;&#21306;&#22312;&#27963;&#21160;&#20013;&#30340;&#19981;&#21516;&#23618;&#38754;&#33719;&#21462;&#25104;&#21151;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online communities develop unique characteristics, establish social norms, and exhibit distinct dynamics among their members. Activity in online communities often results in concrete ``off-line'' actions with a broad societal impact (e.g., political street protests and norms related to sexual misconduct). While community dynamics, information diffusion, and online collaborations have been widely studied in the past two decades, quantitative studies that measure the effectiveness of online communities in promoting their agenda are scarce. In this work, we study the correspondence between the effectiveness of a community, measured by its success level in a competitive online campaign, and the underlying dynamics between its members. To this end, we define a novel task: predicting the success level of online communities in Reddit's r/place - a large-scale distributed experiment that required collaboration between community members. We consider an array of definitions for success level; ea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09638</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#38454;&#27573;&#20419;&#36827;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#65292;&#29305;&#21035;&#26159;Adam&#65292;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#23545;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#26356;&#21152;&#40065;&#26834;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#27867;&#21270;&#25928;&#26524;&#26356;&#24046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#24402;&#22240;&#20110;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#65306;&#33258;&#36866;&#24212;&#26041;&#27861;&#20542;&#21521;&#20110;&#22312;&#25439;&#22833;&#20989;&#25968;&#26354;&#38754;&#20013;&#26356;&#23574;&#38160;&#30340;&#30406;&#22320;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#20351;&#24471;&#20248;&#21270;&#22120;&#22914;&#26524;&#30406;&#22320;&#30340;&#21560;&#24341;&#33539;&#22260;&#19981;&#22815;&#23485;&#65292;&#23601;&#20250;&#36229;&#20986;&#20854;&#33539;&#22260;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#21160;&#23383;&#24149;&#30340;&#20132;&#36890;&#39046;&#22495;&#35270;&#39057;&#38382;&#31572;(TRIVIA)&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20132;&#36890;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#21040;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#23545;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.09636</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#21160;&#23383;&#24149;&#30340;&#20132;&#36890;&#39046;&#22495;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Traffic-Domain Video Question Answering with Automatic Captioning. (arXiv:2307.09636v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#21160;&#23383;&#24149;&#30340;&#20132;&#36890;&#39046;&#22495;&#35270;&#39057;&#38382;&#31572;(TRIVIA)&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20132;&#36890;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#21040;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#23545;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;(VidQA)&#22312;&#26234;&#33021;&#20132;&#36890;&#30417;&#25511;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#26377;&#30528;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20419;&#36827;&#20808;&#36827;&#30340;&#26426;&#22120;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#23558;&#22478;&#24066;&#20132;&#36890;&#22330;&#26223;&#30693;&#35782;&#25972;&#21512;&#21040;VidQA&#31995;&#32479;&#20013;&#30340;&#27880;&#24847;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#33258;&#21160;&#23383;&#24149;&#30340;&#20132;&#36890;&#39046;&#22495;&#35270;&#39057;&#38382;&#31572;(TRIVIA)&#65292;&#23427;&#20316;&#20026;&#19968;&#31181;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#23558;&#20132;&#36890;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20174;SUTD-TrafficQA&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#23454;&#35777;&#32467;&#26524;&#20984;&#26174;&#20102;TRIVIA&#25152;&#21462;&#24471;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;&#20195;&#34920;&#24615;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;6.5&#20010;&#28857;(19.88%)&#65292;&#30456;&#27604;&#20110;&#22522;&#32447;&#35774;&#32622;&#12290;&#36825;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#23545;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#20805;&#28385;&#20102;&#28508;&#21147;&#65292;&#28608;&#21169;&#30528;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20204;&#21457;&#25496;&#20854;&#20840;&#37096;&#28508;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VidQA) exhibits remarkable potential in facilitating advanced machine reasoning capabilities within the domains of Intelligent Traffic Monitoring and Intelligent Transportation Systems. Nevertheless, the integration of urban traffic scene knowledge into VidQA systems has received limited attention in previous research endeavors. In this work, we present a novel approach termed Traffic-domain Video Question Answering with Automatic Captioning (TRIVIA), which serves as a weak-supervision technique for infusing traffic-domain knowledge into large video-language models. Empirical findings obtained from the SUTD-TrafficQA task highlight the substantial enhancements achieved by TRIVIA, elevating the accuracy of representative video-language models by a remarkable 6.5 points (19.88%) compared to baseline settings. This pioneering methodology holds great promise for driving advancements in the field, inspiring researchers and practitioners alike to unlock the full pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26131;&#35299;&#37322;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#30452;&#35273;&#65292;&#36824;&#38656;&#25506;&#32034;&#22914;&#20309;&#39564;&#35777;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09615</link><description>&lt;p&gt;
&#28145;&#20837;&#25506;&#31350;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26131;&#35299;&#37322;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#30452;&#35273;&#65292;&#36824;&#38656;&#25506;&#32034;&#22914;&#20309;&#39564;&#35777;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#20943;&#36731;&#20102;&#23545;&#38169;&#35823;&#26131;&#21457;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#30340;&#25285;&#24551;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#26368;&#36817;&#30340;DL&#22522;&#20110;&#31070;&#32463;&#24433;&#20687;&#23398;&#30340;&#30740;&#31350;&#20063;&#22312;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20027;&#35201;&#29992;&#20110;&#25581;&#31034;&#27169;&#22411;&#22914;&#20309;&#20316;&#20986;&#20915;&#31574;&#30340;&#30452;&#35273;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25191;&#27861;&#26426;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#35299;&#37322;&#24615;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#20173;&#19981;&#28165;&#26970;&#21518;&#26399;&#26041;&#27861;&#25581;&#31034;&#20102;&#27169;&#22411;&#23398;&#20064;&#30340;&#21738;&#20010;&#26041;&#38754;&#20197;&#21450;&#22914;&#20309;&#39564;&#35777;&#20854;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20934;&#30830;&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#24182;&#33021;&#22815;&#21457;&#29616;&#21512;&#29702;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.09607</link><description>&lt;p&gt;
&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Sequential Monte Carlo Learning for Time Series Structure Discovery. (arXiv:2307.09607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20934;&#30830;&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#24182;&#33021;&#22815;&#21457;&#29616;&#21512;&#29702;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20934;&#30830;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#39640;&#26031;&#36807;&#31243;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31526;&#21495;&#31354;&#38388;&#19978;&#24037;&#20316;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#20808;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#21644;&#26059;&#25442;MCMC&#30340;&#26032;&#22411;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#8220;&#22312;&#32447;&#8221;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#26032;&#25968;&#25454;&#39034;&#24207;&#22320;&#21512;&#24182;&#22312;&#26102;&#38388;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#8220;&#31163;&#32447;&#8221;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#30340;&#23884;&#22871;&#23376;&#38598;&#23545;&#21518;&#39564;&#36827;&#34892;&#36864;&#28779;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#30340;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20043;&#21069;&#38024;&#23545;&#30456;&#21516;&#27169;&#22411;&#26063;&#30340;MCMC&#21644;&#36138;&#24515;&#25628;&#32034;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;10&#20493;&#33267;100&#20493;&#30340;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;1,428&#20010;&#35745;&#37327;&#32463;&#27982;&#25968;&#25454;&#38598;&#30340;&#30693;&#21517;&#22522;&#20934;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#39640;&#26031;&#36807;&#31243;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#23398;&#20064;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#21512;&#29702;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new approach to automatically discovering accurate models of complex time series data. Working within a Bayesian nonparametric prior over a symbolic space of Gaussian process time series models, we present a novel structure learning algorithm that integrates sequential Monte Carlo (SMC) and involutive MCMC for highly effective posterior inference. Our method can be used both in "online" settings, where new data is incorporated sequentially in time, and in "offline" settings, by using nested subsets of historical data to anneal the posterior. Empirical measurements on real-world time series show that our method can deliver 10x--100x runtime speedups over previous MCMC and greedy-search structure learning algorithms targeting the same model family. We use our method to perform the first large-scale evaluation of Gaussian process time series structure learning on a prominent benchmark of 1,428 econometric datasets. The results show that our method discovers sensible 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#26679;&#26465;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#20984;&#22810;&#36793;&#24418;&#21644;&#20998;&#27573;&#32447;&#24615;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#25191;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36924;&#36817;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#29305;&#24449;&#22270;&#30340;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09602</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20985;&#34920;&#31034;&#30340;Legendre&#21464;&#25442;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;max-affine&#26679;&#26465;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation. (arXiv:2307.09602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09602
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#26679;&#26465;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#20984;&#22810;&#36793;&#24418;&#21644;&#20998;&#27573;&#32447;&#24615;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#25191;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36924;&#36817;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#29305;&#24449;&#22270;&#30340;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#26679;&#26465;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#38656;&#35201;&#20984;&#22810;&#36793;&#24418;&#21644;&#20998;&#27573;&#32447;&#24615;&#32593;&#32476;&#25805;&#20316;&#31526;&#26469;&#21019;&#24314;max-affine&#26679;&#26465;&#24418;&#24335;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#25918;&#23485;&#20102;&#36825;&#20010;&#32422;&#26463;&#12290;&#21807;&#19968;&#30340;&#32422;&#26463;&#26159;&#20989;&#25968;&#24212;&#35813;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#20108;&#38454;&#23548;&#25968;&#65292;&#23613;&#31649;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#24182;&#19981;&#26159;&#20005;&#26684;&#24517;&#38656;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#25191;&#34892;&#65292;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#23618;&#19978;&#29420;&#31435;&#25191;&#34892;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19968;&#26679;&#65292;&#36825;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36924;&#36817;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#20063;&#23454;&#29616;&#20102;&#32593;&#32476;&#29305;&#24449;&#22270;&#30340;&#21487;&#35270;&#21270;&#12290;&#36890;&#36807;&#20174;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#25552;&#21462;&#36924;&#36817;&#35823;&#24046;&#21644;&#29305;&#24449;&#22270;&#36827;&#34892;&#25968;&#23398;&#35777;&#26126;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel algorithm for transforming a neural network into a spline representation. Unlike previous work that required convex and piecewise-affine network operators to create a max-affine spline alternate form, this work relaxes this constraint. The only constraint is that the function be bounded and possess a well-define second derivative, although this was shown experimentally to not be strictly necessary. It can also be performed over the whole network rather than on each layer independently. As in previous work, this bridges the gap between neural networks and approximation theory but also enables the visualisation of network feature maps. Mathematical proof and experimental investigation of the technique is performed with approximation error and feature maps being extracted from a range of architectures, including convolutional neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30828;&#26408;&#31181;&#31867;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20154;&#31867;&#19987;&#23478;&#31867;&#20284;&#65292;&#26410;&#26469;&#23558;&#26377;&#21161;&#20110;&#20445;&#25252;&#26862;&#26519;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.09588</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#21270;&#36827;&#34892;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#26408;&#26448;&#31181;&#31867;&#26816;&#27979;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning. (arXiv:2307.09588v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30828;&#26408;&#31181;&#31867;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20154;&#31867;&#19987;&#23478;&#31867;&#20284;&#65292;&#26410;&#26469;&#23558;&#26377;&#21161;&#20110;&#20445;&#25252;&#26862;&#26519;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#29983;&#25104;&#22823;&#37327;&#30340;&#30772;&#35299;&#26408;&#26448;&#21442;&#32771;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#29983;&#25104;&#20102;&#20061;&#20010;&#30828;&#26408;&#31181;&#23646;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65292;&#39318;&#27425;&#33258;&#21160;&#21270;&#35782;&#21035;&#32420;&#32500;&#26448;&#26009;&#26174;&#24494;&#22270;&#20687;&#20013;&#30828;&#26408;&#31181;&#31867;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#28789;&#27963;&#30340;&#31649;&#36947;&#65292;&#20415;&#20110;&#23545;&#23548;&#31649;&#20803;&#32032;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#12290;&#23558;&#26469;&#65292;&#36825;&#23558;&#25913;&#21892;&#23545;&#20840;&#29699;&#26408;&#36136;&#32420;&#32500;&#20135;&#21697;&#27969;&#30340;&#25511;&#21046;&#65292;&#20197;&#20445;&#25252;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have developed a methodology for the systematic generation of a large image dataset of macerated wood references, which we used to generate image data for nine hardwood genera. This is the basis for a substantial approach to automate, for the first time, the identification of hardwood species in microscopic images of fibrous materials by deep learning. Our methodology includes a flexible pipeline for easy annotation of vessel elements. We compare the performance of different neural network architectures and hyperparameters. Our proposed method performs similarly well to human experts. In the future, this will improve controls on global wood fiber product flows to protect forests.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#22810;&#36718;&#26377;&#23475;&#34892;&#20026;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#24037;&#20855;&#26080;&#27861;&#26816;&#27979;&#20986;82%&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#30340;&#21333;&#21477;&#37117;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;\toxicbot&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#35302;&#21457;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.09579</link><description>&lt;p&gt;
&#29702;&#35299;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#22810;&#36718;&#26377;&#23475;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots. (arXiv:2307.09579v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#22810;&#36718;&#26377;&#23475;&#34892;&#20026;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#24037;&#20855;&#26080;&#27861;&#26816;&#27979;&#20986;82%&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#30340;&#21333;&#21477;&#37117;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;\toxicbot&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#35302;&#21457;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22914;ChatGPT&#21487;&#20197;&#19982;&#20154;&#31867;&#29992;&#25143;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#26377;&#23475;&#30340;&#22810;&#36718;&#23545;&#35805;&#20013;&#29983;&#25104;&#26377;&#23475;&#25110;&#26377;&#30861;&#30340;&#22238;&#24212;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#21333;&#21477;&#27979;&#35797;&#65292;&#32780;&#25105;&#20204;&#21457;&#29616;82&#65285;&#22240;&#20026;&#21333;&#19968;&#21477;&#23376;&#32780;&#22312;&#23545;&#35805;&#20013;&#35825;&#21457;&#26377;&#23475;&#34892;&#20026;&#30340;&#21477;&#23376;&#34987;&#29616;&#26377;&#24037;&#20855;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;\toxicbot&#65292;&#36890;&#36807;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24494;&#35843;&#19982;&#30446;&#26631;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#24494;&#35843;&#20026;&#21463;&#25511;&#30340;&#20250;&#35805;&#24207;&#21015;&#12290;&#29305;&#21035;&#26159;&#65292;&#27599;&#20010;&#23545;&#35805;&#30340;&#36215;&#22987;&#37117;&#26469;&#33258;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21477;&#23376;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#35302;&#21457;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing and machine learning have led to the development of chatbot models, such as ChatGPT, that can engage in conversational dialogue with human users. However, the ability of these models to generate toxic or harmful responses during a non-toxic multi-turn conversation remains an open research question. Existing research focuses on single-turn sentence testing, while we find that 82\% of the individual non-toxic sentences that elicit toxic behaviors in a conversation are considered safe by existing tools. In this paper, we design a new attack, \toxicbot, by fine-tuning a chatbot to engage in conversation with a target open-domain chatbot. The chatbot is fine-tuned with a collection of crafted conversation sequences. Particularly, each conversation begins with a sentence from a crafted prompt sentences dataset. Our extensive evaluation shows that open-domain chatbot models can be triggered to generate toxic responses in a multi-turn conversation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#25628;&#32034;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09564</link><description>&lt;p&gt;
&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Syntax-Guided Synthesis. (arXiv:2307.09564v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#25628;&#32034;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#26159;&#26681;&#25454;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#20219;&#21153;&#12290;&#22312;&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#65288;SyGuS&#65289;&#20013;&#65292;&#35268;&#33539;&#26159;&#19968;&#20010;&#35821;&#27861;&#27169;&#26495;&#21644;&#19968;&#20010;&#36923;&#36753;&#20844;&#24335;&#30340;&#32452;&#21512;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#34987;&#35777;&#26126;&#28385;&#36275;&#35268;&#33539;&#12290;&#20687;SyGuS&#36825;&#26679;&#30340;&#25216;&#26415;&#23545;&#20110;&#30830;&#20445;&#27491;&#30830;&#30340;&#32508;&#21512;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#31243;&#24207;&#32508;&#21512;&#20013;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;SyGuS&#20013;&#30446;&#21069;&#30340;&#25216;&#26415;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#33258;&#21160;&#25512;&#29702;&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#26522;&#20030;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#25628;&#32034;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#29992;&#25968;&#25454;&#38598;&#30456;&#23545;&#36739;&#23567;&#30340;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36890;&#29992;SyGuS&#38382;&#39064;&#26500;&#24314;&#20026;&#26641;&#25628;&#32034;&#65292;&#24182;&#22522;&#20110;Monte-Carlo Tree Search (MCTS)&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#30340;SyGuS&#32508;&#21512;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#23398;&#20064;&#30340;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#19982;&#29992;&#20110;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26641;&#30340;&#19978;&#38480;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis(SyGuS) this specification is a combination of a syntactic template and a logical formula, and any generated code is proven to satisfy both. Techniques like SyGuS are critical to guaranteeing correct synthesis results. Despite the proliferation of machine learning in other types of program synthesis, state-of-the-art techniques in SyGuS are still driven by automated reasoning tools and simple enumeration. We hypothesize this is for two reasons: first the complexity of the search problem, and second the relatively small data sets available. In this work, we tackle these challenges by framing general SyGuS problems as a tree-search, and present a reinforcement learning guided synthesis algorithm for SyGuS based on Monte-Carlo Tree Search (MCTS). Our algorithm incorporates learned policy and value functions combined with the upper confidence bound for trees to balance explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21361;&#38505;&#29615;&#22659;&#20013;&#20010;&#20307;&#34892;&#20026;&#30340;&#30095;&#25955;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#25105;&#20204;&#23545;&#30095;&#25955;&#36807;&#31243;&#30340;&#29702;&#35299;&#24182;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#30095;&#25955;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09485</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21644;&#20154;&#24037;&#26234;&#33021;&#25552;&#21319;&#30095;&#25955;&#35268;&#21010;&#65306;&#29702;&#35299;&#21361;&#38505;&#29615;&#22659;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Enhancing Evacuation Planning through Multi-Agent Simulation and Artificial Intelligence: Understanding Human Behavior in Hazardous Environments. (arXiv:2307.09485v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21361;&#38505;&#29615;&#22659;&#20013;&#20010;&#20307;&#34892;&#20026;&#30340;&#30095;&#25955;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#25105;&#20204;&#23545;&#30095;&#25955;&#36807;&#31243;&#30340;&#29702;&#35299;&#24182;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#30095;&#25955;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#35299;&#20915;&#21361;&#38505;&#22330;&#25152;&#30095;&#25955;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23545;&#21327;&#35843;&#21592;&#12289;&#27963;&#21160;&#20027;&#21150;&#26041;&#21644;&#31649;&#29702;&#26426;&#26500;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#20419;&#36827;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#65292;&#26412;&#25991;&#36816;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30095;&#25955;&#27169;&#25311;&#27169;&#22411;&#12290;&#30001;&#20110;NetLogo&#33021;&#22815;&#20840;&#38754;&#20102;&#35299;&#21361;&#38505;&#29615;&#22659;&#20013;&#21387;&#36843;&#24773;&#20917;&#19979;&#20154;&#31867;&#34892;&#20026;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#39318;&#36873;&#27169;&#25311;&#24037;&#20855;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22686;&#36827;&#25105;&#20204;&#23545;&#20010;&#20307;&#22312;&#36825;&#31181;&#21387;&#36843;&#24773;&#20917;&#19979;&#30340;&#21453;&#24212;&#21644;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#27169;&#25311;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#30095;&#25955;&#24773;&#26223;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#20351;&#20915;&#31574;&#32773;&#21644;&#24212;&#24613;&#35268;&#21010;&#32773;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65292;&#23454;&#26045;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#30340;&#30095;&#25955;&#31574;&#30053;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20419;&#36827;&#30095;&#25955;&#35268;&#21010;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the crucial task of addressing the evacuation of hazardous places, which holds great importance for coordinators, event hosts, and authorities. To facilitate the development of effective solutions, the paper employs Artificial Intelligence (AI) techniques, specifically Multi-Agent Systems (MAS), to construct a simulation model for evacuation. NetLogo is selected as the simulation tool of choice due to its ability to provide a comprehensive understanding of human behaviour in distressing situations within hazardous environments. The primary objective of this paper is to enhance our comprehension of how individuals react and respond during such distressing situations. By leveraging AI and MAS, the simulation model aims to capture the complex dynamics of evacuation scenarios, enabling policymakers and emergency planners to make informed decisions and implement more efficient and effective evacuation strategies. This paper endeavours to contribute to the advancement o
&lt;/p&gt;</description></item><item><title>Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.09288</link><description>&lt;p&gt;
Llama 2: &#24320;&#25918;&#22522;&#30784;&#21644;&#20248;&#21270;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09288
&lt;/p&gt;
&lt;p&gt;
Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#21457;&#24067;&#20102;Llama 2&#65292;&#19968;&#20010;&#21253;&#21547;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20854;&#35268;&#27169;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;LLM&#65292;&#31216;&#20026;Llama 2-Chat&#65292;&#22312;&#23545;&#35805;&#20351;&#29992;&#26696;&#20363;&#20013;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#65292;&#23427;&#20204;&#21487;&#33021;&#26159;&#38381;&#28304;&#27169;&#22411;&#30340;&#21512;&#36866;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;Llama 2-Chat&#30340;&#20248;&#21270;&#21644;&#23433;&#20840;&#24615;&#25913;&#36827;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#35753;&#31038;&#21306;&#33021;&#22815;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#26500;&#24314;&#24182;&#20026;LLM&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
&lt;/p&gt;</description></item><item><title>M-FLAG&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#24341;&#20837;&#27491;&#20132;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#28508;&#31354;&#38388;&#20960;&#20309;&#20851;&#31995;&#12290;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#65292;M-FLAG&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;78%&#30340;&#21442;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.08347</link><description>&lt;p&gt;
M-FLAG&#65306;&#20351;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#28508;&#31354;&#38388;&#20960;&#20309;&#20248;&#21270;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08347
&lt;/p&gt;
&lt;p&gt;
M-FLAG&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#24341;&#20837;&#27491;&#20132;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#28508;&#31354;&#38388;&#20960;&#20309;&#20851;&#31995;&#12290;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#65292;M-FLAG&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;78%&#30340;&#21442;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#21307;&#23398;&#24433;&#20687;&#21644;&#20020;&#24202;&#25991;&#26412;&#30340;&#29305;&#24449;&#20849;&#23398;&#20064;&#21644;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#36215;&#26469;&#24182;&#19981;&#23481;&#26131;&#65292;&#24182;&#19988;&#28508;&#31354;&#38388;&#34920;&#31034;&#21487;&#20197;&#38750;&#24120;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21629;&#21517;&#20026;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#19982;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#28508;&#31354;&#38388;&#20960;&#20309;&#20248;&#21270;&#65288;M-FLAG&#65289;&#65292;&#21033;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#26469;&#31283;&#23450;&#21644;&#39640;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#20132;&#25439;&#22833;&#20989;&#25968;&#26469;&#21327;&#35843;&#28508;&#31354;&#38388;&#20960;&#20309;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#21147;&#65306;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;M-FLAG&#22312;&#20943;&#23569;78&#65285;&#30340;&#21442;&#25968;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;M-FLAG&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\%. Notably, M-FLAG achieves outstanding performance o
&lt;/p&gt;</description></item><item><title>IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06698</link><description>&lt;p&gt;
IntelliGraphs: &#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06698
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25991;&#29486;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#19981;&#20165;&#20165;&#26159;&#38142;&#25509;&#30340;&#38598;&#21512;&#65292;&#36824;&#20855;&#26377;&#20854;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#12290;&#35821;&#20041;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26597;&#35810;&#22238;&#31572;&#25110;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#22270;&#25512;&#26029;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#24517;&#39035;&#29983;&#25104;&#21487;&#33021;&#30340;&#24182;&#19988;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGraphs&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#12290;IntelliGraphs&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#20256;&#32479;KGE&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#35821;&#20041;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#22522;&#20934;&#23558;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.06608</link><description>&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#24341;&#20837;&#65306;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#20102;&#26368;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#30340;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#32570;&#20047;&#35748;&#35782;&#12290;&#21463;&#21040;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;1&#65289;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#20197;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65307;2&#65289;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#21019;&#26032;&#24605;&#24819;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36873;&#25321;&#20195;&#29702;&#27169;&#22411;&#30340;&#20004;&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#30784;&#27169;&#22411;&#26159;&#36825;&#19968;&#35282;&#33394;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30683;&#30462;&#22320;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#36825;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25105;&#20204;&#24402;&#22240;&#20110;&#32570;&#20047;&#19978;&#36848;&#25351;&#23548;&#21407;&#21017;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.06333</link><description>&lt;p&gt;
&#35786;&#26029;&#12289;&#21453;&#39304;&#12289;&#36866;&#24212;&#24615;: &#29992;&#20110;&#27979;&#35797;&#26102;&#25919;&#31574;&#35843;&#25972;&#30340;&#20154;-&#26426;&#29615;&#36335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#24120;&#24120;&#30001;&#20110;&#20998;&#24067;&#20559;&#31227;&#32780;&#22833;&#25928;&#8212;&#8212;&#21363;&#24403;&#25919;&#31574;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#65292;&#29366;&#24577;&#21644;&#22870;&#21169;&#21457;&#29983;&#21464;&#21270;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#20351;&#27169;&#22411;&#23545;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#26469;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#32773;&#22312;&#20107;&#20808;&#24448;&#24448;&#19981;&#30693;&#36947;&#21738;&#20123;&#27010;&#24565;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#23545;&#20219;&#21153;&#25191;&#34892;&#26041;&#24335;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#29983;&#25104;&#21453;&#20107;&#23454;&#28436;&#31034;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#30830;&#23450;&#21487;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#21644;&#26080;&#20851;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#21033;&#29992;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#30340;&#30693;&#35782;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#36866;&#24212;&#20110;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#20351;&#29992;&#25143;&#33021;&#22815;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24314;&#31435;&#35821;&#20041;&#26377;&#24847;&#20041;&#12289;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20449;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#26469;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01158</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#24515;&#29702;&#25512;&#29702;&#20316;&#20026;&#20869;&#22312;&#21160;&#26426;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning. (arXiv:2307.01158v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24314;&#31435;&#35821;&#20041;&#26377;&#24847;&#20041;&#12289;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20449;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#26469;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20182;&#20154;&#20869;&#24515;&#29366;&#24577;&#23545;&#20110;&#20154;&#31867;&#30340;&#31038;&#20250;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20063;&#21487;&#20197;&#25552;&#20379;&#31867;&#20284;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#26469;&#24314;&#31435;&#35821;&#20041;&#26377;&#24847;&#20041;&#12289;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20449;&#24565;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#38454;&#20449;&#24565;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#30340;&#33021;&#21147;&#21487;&#20197;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#28151;&#21512;&#30340;&#21512;&#20316;&#31454;&#20105;&#29615;&#22659;&#20013;&#21576;&#29616;&#20102;&#21021;&#27493;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15079</link><description>&lt;p&gt;
&#20174;$O(\sqrt{n})$&#21040;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25968;&#20540;&#20248;&#21270;&#29702;&#35770;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#22256;&#25200;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#20840;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#21644;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20197;&#26377;&#30028;&#30418;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65288;Box-QP&#65289;&#20026;&#36215;&#28857;&#65292;&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#36716;&#21270;&#20026;Box-QP&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;QP&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#20854;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#30452;&#25509;&#8221;&#26041;&#27861;&#65306;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#31934;&#30830;&#20540;&#20026;$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#22312;&#24403;&#20170;&#30340;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.13197</link><description>&lt;p&gt;
Gradient-based Attribution Methods&#20013;Pre&#25110;Post-Softmax Scores&#65292;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13197
&lt;/p&gt;
&lt;p&gt;
&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24037;&#20316;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#20351;&#29992;&#32593;&#32476;&#20998;&#25968;&#30340;&#26799;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#21644;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#23454;&#38469;&#24046;&#24322;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient based attribution methods for neural networks working as classifiers use gradients of network scores. Here we discuss the practical differences between using gradients of pre-softmax scores versus post-softmax scores, and their respective advantages and disadvantages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21512;&#20316;&#28216;&#25103;&#20013;&#26368;&#20339;&#21327;&#35843;&#25152;&#38656;&#30340;&#26368;&#23567;&#30693;&#35782;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20851;&#20110;&#25112;&#30053;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#20108;&#20998;&#27861;&#20197;&#21450;&#36890;&#36807;Bellman&#22791;&#20221;&#36816;&#31639;&#31526;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#25112;&#30053;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20182;&#20204;&#24212;&#29992;&#35813;&#31639;&#27861;&#20998;&#26512;&#20102;&#26631;&#20934;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;Overcooked&#29615;&#22659;&#20013;&#20219;&#21153;&#30340;&#25112;&#30053;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#36739;&#20110;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09309</link><description>&lt;p&gt;
&#35841;&#38656;&#35201;&#30693;&#36947;&#65311;&#26368;&#23567;&#30693;&#35782;&#29992;&#20110;&#26368;&#20339;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Who Needs to Know? Minimal Knowledge for Optimal Coordination. (arXiv:2306.09309v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21512;&#20316;&#28216;&#25103;&#20013;&#26368;&#20339;&#21327;&#35843;&#25152;&#38656;&#30340;&#26368;&#23567;&#30693;&#35782;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20851;&#20110;&#25112;&#30053;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#20108;&#20998;&#27861;&#20197;&#21450;&#36890;&#36807;Bellman&#22791;&#20221;&#36816;&#31639;&#31526;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#25112;&#30053;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20182;&#20204;&#24212;&#29992;&#35813;&#31639;&#27861;&#20998;&#26512;&#20102;&#26631;&#20934;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;Overcooked&#29615;&#22659;&#20013;&#20219;&#21153;&#30340;&#25112;&#30053;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#36739;&#20110;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#28216;&#25103;&#20013;&#19982;&#20182;&#20154;&#36827;&#34892;&#26368;&#20339;&#21327;&#35843;&#26102;&#65292;&#20102;&#35299;&#21512;&#20316;&#20249;&#20276;&#30340;&#20449;&#24687;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65306;&#25104;&#21151;&#39550;&#39542;&#35201;&#27714;&#20102;&#35299;&#22312;&#21738;&#19968;&#20391;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#21512;&#20316;&#20249;&#20276;&#30340;&#27599;&#20010;&#29305;&#24449;&#22312;&#25112;&#30053;&#19978;&#37117;&#26159;&#30456;&#20851;&#30340;&#65306;&#22312;&#20445;&#25345;&#26368;&#20339;&#21327;&#35843;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#24573;&#30053;&#39550;&#39542;&#32773;&#30340;&#31934;&#32454;&#21152;&#36895;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25112;&#30053;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#23450;&#20041;&#30340;&#20108;&#20998;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#65292;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#65292;&#36825;&#31181;&#20108;&#20998;&#27861;&#20855;&#26377;&#21487;&#36890;&#36807;Bellman&#22791;&#20221;&#36816;&#31639;&#31526;&#39640;&#25928;&#35745;&#31639;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#27492;&#31639;&#27861;&#24212;&#29992;&#20110;&#20998;&#26512;&#26631;&#20934;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;Overcooked&#29615;&#22659;&#20013;&#20219;&#21153;&#30340;&#25112;&#30053;&#30456;&#20851;&#20449;&#24687;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;&#22522;&#32447;&#26041;&#27861;&#26174;&#33879;&#26356;&#39640;&#25928;&#12290;&#35270;&#39057;&#21487;&#22312;https://minknowledge.github.io&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
To optimally coordinate with others in cooperative games, it is often crucial to have information about one's collaborators: successful driving requires understanding which side of the road to drive on. However, not every feature of collaborators is strategically relevant: the fine-grained acceleration of drivers may be ignored while maintaining optimal coordination. We show that there is a well-defined dichotomy between strategically relevant and irrelevant information. Moreover, we show that, in dynamic games, this dichotomy has a compact representation that can be efficiently computed via a Bellman backup operator. We apply this algorithm to analyze the strategically relevant information for tasks in both a standard and a partially observable version of the Overcooked environment. Theoretical and empirical results show that our algorithms are significantly more efficient than baselines. Videos are available at https://minknowledge.github.io.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#22768;&#35270;&#39057;&#20013;&#23454;&#26102;&#36827;&#34892;&#30149;&#21464;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#20043;&#21069;&#24103;&#20013;&#30340;&#36127;&#38754;&#26102;&#38388;&#32972;&#26223;&#26469;&#25233;&#21046;&#20551;&#38451;&#24615;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18060</link><description>&lt;p&gt;
&#22312;&#23454;&#26102;&#36229;&#22768;&#30149;&#21464;&#26816;&#27979;&#20013;&#25366;&#25496;&#36127;&#38754;&#26102;&#38388;&#32972;&#26223;&#20197;&#25233;&#21046;&#20551;&#38451;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mining Negative Temporal Contexts For False Positive Suppression In Real-Time Ultrasound Lesion Detection. (arXiv:2305.18060v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#22768;&#35270;&#39057;&#20013;&#23454;&#26102;&#36827;&#34892;&#30149;&#21464;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#20043;&#21069;&#24103;&#20013;&#30340;&#36127;&#38754;&#26102;&#38388;&#32972;&#26223;&#26469;&#25233;&#21046;&#20551;&#38451;&#24615;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#22768;&#25195;&#25551;&#36807;&#31243;&#20013;&#65292;&#23454;&#26102;&#30149;&#21464;&#26816;&#27979;&#21487;&#20197;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#20934;&#30830;&#35786;&#26029;&#30284;&#30151;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#24212;&#29992;&#20110;&#36229;&#22768;&#35270;&#39057;&#26102;&#65292;&#36890;&#29992;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#21487;&#33021;&#38169;&#35823;&#22320;&#25253;&#21578;&#26126;&#26174;&#30340;&#20551;&#38451;&#24615;(FPs)&#65292;&#21487;&#33021;&#35823;&#23548;&#21021;&#32423;&#25918;&#23556;&#31185;&#21307;&#29983;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#23427;&#20204;&#26410;&#33021;&#21033;&#29992;&#20043;&#21069;&#24103;&#20013;&#30340;&#36127;&#38754;&#30151;&#29366;&#65292;&#21363;&#36127;&#38754;&#26102;&#38388;&#32972;&#26223;(NTC)&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36870;&#20809;&#27969;&#25351;&#23548;&#65292;&#20174;&#20043;&#21069;&#30340;&#24103;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;NTC&#12290;&#36890;&#36807;&#32858;&#21512;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#36171;&#20104;&#27169;&#22411;&#21033;&#29992;NTC&#25233;&#21046;FP&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#27169;&#22411;&#31216;&#20026;UltraDet&#12290;&#25152;&#25552;&#20986;&#30340;UltraDet&#22312;&#36229;&#36807;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#22312;https://github&#19978;&#21457;&#24067;&#20102;CVA-BUS&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#12289;&#26816;&#26597;&#28857;&#21644;&#39640;&#36136;&#37327;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
During ultrasonic scanning processes, real-time lesion detection can assist radiologists in accurate cancer diagnosis. However, this essential task remains challenging and underexplored. General-purpose real-time object detection models can mistakenly report obvious false positives (FPs) when applied to ultrasound videos, potentially misleading junior radiologists. One key issue is their failure to utilize negative symptoms in previous frames, denoted as negative temporal contexts (NTC). To address this issue, we propose to extract contexts from previous frames, including NTC, with the guidance of inverse optical flow. By aggregating extracted contexts, we endow the model with the ability to suppress FPs by leveraging NTC. We call the resulting model UltraDet. The proposed UltraDet demonstrates significant improvement over previous state-of-the-arts and achieves real-time inference speed. We release the code, checkpoints, and high-quality labels of the CVA-BUS dataset in https://github
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24102;&#26377;&#30830;&#23450;&#24615;&#31574;&#30053;&#25628;&#32034;&#30340;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#20351;&#29992;&#36825;&#20123;&#23450;&#29702;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#22238;&#25253;&#31163;&#31574;&#30053;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ARO-DDPG&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;$\epsilon$-&#26368;&#20248;&#31283;&#23450;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.12239</link><description>&lt;p&gt;
&#24102;&#26377;&#30830;&#23450;&#24615;&#31574;&#30053;&#25628;&#32034;&#30340;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24102;&#26377;&#30830;&#23450;&#24615;&#31574;&#30053;&#25628;&#32034;&#30340;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#20351;&#29992;&#36825;&#20123;&#23450;&#29702;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#22238;&#25253;&#31163;&#31574;&#30053;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ARO-DDPG&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;$\epsilon$-&#26368;&#20248;&#31283;&#23450;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22238;&#25253;&#20934;&#21017;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#65292;&#22240;&#20026;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#32771;&#34385;&#20102;&#36148;&#29616;&#22238;&#25253;&#20934;&#21017;&#12290;&#36817;&#26399;&#26377;&#19968;&#20123;&#20851;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#24037;&#20316;&#65292;&#20294;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#24179;&#22343;&#22238;&#25253;&#24615;&#33021;&#20934;&#21017;&#30340;&#22522;&#20110;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#29702;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#22238;&#25253;&#31163;&#31574;&#30053;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ARO-DDPG&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;ODE&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32467;&#26524;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#33719;&#24471;&#20102;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#31283;&#23450;&#31574;&#30053;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\Omega(\epsilon^{-2.5})$&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;ARO-DDPG&#31639;&#27861;&#30340;&#24179;&#22343;&#22238;&#25253;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;&#26356;&#22909;&#30340;&#32463;&#39564;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The average reward criterion is relatively less studied as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this work, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first show asymptotic convergence analysis using the ODE-based method. Subsequently, we provide a finite time analysis of the resulting stochastic approximation scheme with linear function approximator and obtain an $\epsilon$-optimal stationary policy with a sample complexity of $\Omega(\epsilon^{-2.5})$. We compare the average reward performance of our proposed ARO-DDPG algorithm and observe better empir
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.09378</link><description>&lt;p&gt;
&#22312;Lenia&#20013;&#25429;&#33719;&#26032;&#20852;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09378
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39033;&#30446;&#25506;&#35752;&#20102;Lenia&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25968;&#23383;&#29983;&#29289;&#31995;&#32479;&#30340;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;&#12290;Lenia&#30340;&#29983;&#24577;&#31995;&#32479;&#30001;&#31616;&#21333;&#30340;&#20154;&#24037;&#29983;&#29289;&#32452;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#31227;&#21160;&#12289;&#28040;&#32791;&#12289;&#29983;&#38271;&#21644;&#32321;&#27542;&#12290;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#21644;&#36827;&#21270;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#34892;&#20026;&#30340;&#22810;&#26679;&#21270;&#29983;&#29289;&#12290;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#22312;Lenia&#20013;&#27979;&#37327;&#22797;&#26434;&#24615;&#65292;&#35782;&#21035;&#27979;&#37327;&#35268;&#21017;&#30340;&#38271;&#26399;&#22797;&#26434;&#24615;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#36827;&#21270;&#20986;&#23578;&#26410;&#21457;&#29616;&#30340;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;&#36951;&#20256;&#31639;&#27861;&#20351;&#29992;&#30456;&#37051;&#21306;&#22495;&#25110;&#26680;&#20316;&#20026;&#22522;&#22240;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;Lenia&#30340;&#20854;&#20182;&#21442;&#25968;&#65288;&#20363;&#22914;&#29983;&#38271;&#20989;&#25968;&#65289;&#19981;&#21464;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#20154;&#21475;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#28982;&#21518;&#27979;&#37327;&#36866;&#24212;&#24230;&#20540;&#20197;&#20915;&#23450;&#25152;&#24471;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#20316;&#20026;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00909</link><description>&lt;p&gt;
&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;&#65306;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22797;&#26434;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#20570;&#27861;&#36890;&#24120;&#26159;&#20808;&#27010;&#36848;&#19968;&#19979;&#25511;&#21046;&#27969;&#31243;&#65292;&#28982;&#21518;&#36845;&#20195;&#36827;&#34892;&#20016;&#23500;&#65292;&#26368;&#32456;&#29983;&#25104;&#19968;&#20123;&#31934;&#24515;&#21152;&#24037;&#30340;&#35821;&#27861;&#32467;&#26500;&#21644;&#23618;&#27425;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#20195;&#30721;&#65292;&#27809;&#26377;&#20013;&#38388;&#29615;&#33410;&#65292;&#20197;&#21453;&#26144;"&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;"&#30340;&#32467;&#26500;&#21270;&#24605;&#32500;&#36807;&#31243;&#12290;&#21463;&#21040;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChainCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#31243;&#24207;&#32508;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36880;&#27493;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#21363;&#20174;&#31895;&#21040;&#32454;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#35299;&#26512;&#23558;&#28304;&#20195;&#30721;&#20998;&#35299;&#20026;&#24067;&#23616;&#26694;&#26550;&#32452;&#20214;&#21644;&#38468;&#20214;&#32452;&#20214;&#65292;&#20197;&#26500;&#24314;&#23618;&#27425;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#39044;&#27979;&#30446;&#26631;&#37325;&#26032;&#21551;&#21160;&#65292;&#24418;&#25104;&#22810;&#27425;&#36890;&#36807;&#30446;&#26631;&#65292;&#27599;&#27425;&#29983;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#20018;&#32852;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
&lt;/p&gt;</description></item><item><title>ChatGPT&#20855;&#22791;&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12562</link><description>&lt;p&gt;
&#23545;ChatGPT&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Evaluation of ChatGPT in Requirements Information Retrieval. (arXiv:2304.12562v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12562
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20855;&#22791;&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#35768;&#22810;&#35828;&#26126;&#24615;&#23454;&#20363;&#26174;&#31034;&#20102;ChatGPT&#22312;&#25191;&#34892;&#32534;&#31243;&#20219;&#21153;&#21644;&#22238;&#31572;&#19968;&#33324;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30446;&#30340;&#22312;&#20110;&#23454;&#35777;&#35780;&#20215;ChatGPT&#22312;&#38656;&#27714;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#27492;&#27934;&#23519;&#30001;ChatGPT&#20195;&#34920;&#30340;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#26041;&#27861;&#26159;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#27969;&#31243;&#65292;&#21253;&#25324;&#20004;&#20010;&#24120;&#35265;&#30340;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#12289;&#21253;&#21547;&#20004;&#31181;&#20856;&#22411;&#38656;&#27714;&#24037;&#20214;&#30340;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#22266;&#23450;&#20219;&#21153;&#25552;&#31034;&#26597;&#35810;ChatGPT&#65292;&#20197;&#21450;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#37117;&#36798;&#21040;&#20102;&#38646;-shot&#35774;&#32622;&#19979;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;$F\beta$&#20540;&#12290;&#23450;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;ChatGPT&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#26377;&#38480;&#30340;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#30693;&#35782;&#12290;&#32467;&#35770;&#26159;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#33719;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#31361;&#26174;&#20102;&#20854;&#25552;&#39640;&#38656;&#27714;&#24037;&#31243;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Recently, many illustrative examples have shown ChatGPT's impressive ability to perform programming tasks and answer general domain questions.  Objective: We empirically evaluate how ChatGPT performs on requirements analysis tasks to derive insights into how generative large language model, represented by ChatGPT, influence the research and practice of natural language processing for requirements engineering.  Method: We design an evaluation pipeline including two common requirements information retrieval tasks, four public datasets involving two typical requirements artifacts, querying ChatGPT with fixed task prompts, and quantitative and qualitative results analysis.  Results: Quantitative results show that ChatGPT achieves comparable or better $F\beta$ values in all datasets under a zero-shot setting. Qualitative analysis further illustrates ChatGPT's powerful natural language processing ability and limited requirements engineering domain knowledge.  Conclusion: The evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#23398;&#20064;&#36866;&#29992;&#20110;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19977;&#20803;&#32452;&#36873;&#25321;&#31574;&#30053;&#21644;&#19977;&#20803;&#32452;&#25439;&#22833;&#26469;&#22312;&#26032;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#21457;&#29616;&#21160;&#20316;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26102;&#38388;&#36793;&#30028;&#24674;&#22797;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.06403</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#21160;&#20316;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging triplet loss for unsupervised action segmentation. (arXiv:2304.06403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#23398;&#20064;&#36866;&#29992;&#20110;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19977;&#20803;&#32452;&#36873;&#25321;&#31574;&#30053;&#21644;&#19977;&#20803;&#32452;&#25439;&#22833;&#26469;&#22312;&#26032;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#21457;&#29616;&#21160;&#20316;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26102;&#38388;&#36793;&#30028;&#24674;&#22797;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#23398;&#20064;&#36866;&#29992;&#20110;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#25805;&#20316;&#30456;&#20284;&#24230;&#20998;&#24067;&#30340;&#19977;&#20803;&#32452;&#25439;&#22833;&#21644;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#26102;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#20197;&#22312;&#26032;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#21457;&#29616;&#21160;&#20316;&#30340;&#19977;&#20803;&#32452;&#36873;&#25321;&#31574;&#30053;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#23398;&#20064;&#21040;&#30340;&#21160;&#20316;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#36793;&#30028;&#65292;&#36136;&#37327;&#26356;&#39640;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#19978;&#24212;&#29992;&#36890;&#29992;&#32858;&#31867;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel fully unsupervised framework that learns action representations suitable for the action segmentation task from the single input video itself, without requiring any training data. Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions and a novel triplet selection strategy that effectively models temporal and semantic priors to discover actions in the new representational space. Under these circumstances, we successfully recover temporal boundaries in the learned action representations with higher quality compared with existing unsupervised approaches. The proposed method is evaluated on two widely used benchmark datasets for the action segmentation task and it achieves competitive performance by applying a generic clustering algorithm on the learned representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SNB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#20004;&#20010;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.13126</link><description>&lt;p&gt;
MagicFusion&#65306;&#36890;&#36807;&#34701;&#21512;&#25193;&#25955;&#27169;&#22411;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models. (arXiv:2303.13126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SNB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#20004;&#20010;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;AI&#31038;&#21306;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#24378;&#22823;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Saliency-aware Noise Blending (SNB)&#8221; &#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#34701;&#21512;&#30340;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20998;&#31867;&#22120;&#33258;&#30001;&#25351;&#23548;&#30340;&#21709;&#24212;&#19982;&#29983;&#25104;&#22270;&#20687;&#30340;&#26174;&#30528;&#24615;&#39640;&#24230;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26174;&#30528;&#24615;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#28151;&#21512;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#30340;&#39044;&#27979;&#22122;&#22768;&#26469;&#20449;&#20219;&#20854;&#19987;&#19994;&#39046;&#22495;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;SNB&#26159;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23436;&#25104;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;DDIM&#37319;&#26679;&#36807;&#31243;&#20013;&#33258;&#21160;&#23545;&#40784;&#20004;&#20010;&#22122;&#22768;&#31354;&#38388;&#30340;&#35821;&#20041;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#20363;&#22914;&#25513;&#27169;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;SNB&#30340;&#26174;&#30528;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The advent of open-source AI communities has produced a cornucopia of powerful text-guided diffusion models that are trained on various datasets. While few explorations have been conducted on ensembling such models to combine their strengths. In this work, we propose a simple yet effective method called Saliency-aware Noise Blending (SNB) that can empower the fused text-guided diffusion models to achieve more controllable generation. Specifically, we experimentally find that the responses of classifier-free guidance are highly related to the saliency of generated images. Thus we propose to trust different models in their areas of expertise by blending the predicted noises of two diffusion models in a saliency-aware manner. SNB is training-free and can be completed within a DDIM sampling process. Additionally, it can automatically align the semantics of two noise spaces without requiring additional annotations such as masks. Extensive experiments show the impressive effectiveness of SNB
&lt;/p&gt;</description></item><item><title>Sionna RT&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#24494;&#20998;&#30340;&#20809;&#32447;&#36861;&#36394;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#12290;&#36825;&#20010;&#21151;&#33021;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#19982;&#22810;&#20010;&#31995;&#32479;&#21644;&#29615;&#22659;&#21442;&#25968;&#26377;&#20851;&#30340;&#37327;&#30340;&#26799;&#24230;&#65292;&#23545;&#20110;&#35832;&#22914;&#23398;&#20064;&#26080;&#32447;&#30005;&#26448;&#26009;&#21644;&#20248;&#21270;&#21457;&#23556;&#26426;&#26041;&#21521;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#21516;&#26102;&#65292;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#23545;&#20110;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#22914;&#25968;&#23383;&#23402;&#29983;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25512;&#21160;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.11103</link><description>&lt;p&gt;
Sionna RT&#65306;&#26080;&#32447;&#30005;&#20256;&#25773;&#24314;&#27169;&#30340;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling. (arXiv:2303.11103v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11103
&lt;/p&gt;
&lt;p&gt;
Sionna RT&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#24494;&#20998;&#30340;&#20809;&#32447;&#36861;&#36394;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#12290;&#36825;&#20010;&#21151;&#33021;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#19982;&#22810;&#20010;&#31995;&#32479;&#21644;&#29615;&#22659;&#21442;&#25968;&#26377;&#20851;&#30340;&#37327;&#30340;&#26799;&#24230;&#65292;&#23545;&#20110;&#35832;&#22914;&#23398;&#20064;&#26080;&#32447;&#30005;&#26448;&#26009;&#21644;&#20248;&#21270;&#21457;&#23556;&#26426;&#26041;&#21521;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#21516;&#26102;&#65292;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#23545;&#20110;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#22914;&#25968;&#23383;&#23402;&#29983;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25512;&#21160;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sionna&#26159;&#19968;&#20010;&#22522;&#20110;TensorFlow&#30340;GPU&#21152;&#36895;&#24320;&#28304;&#24211;&#65292;&#29992;&#20110;&#38142;&#36335;&#32423;&#27169;&#25311;&#12290;&#33258;v0.14&#29256;&#26412;&#20197;&#26469;&#65292;&#23427;&#38598;&#25104;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#20809;&#32447;&#36861;&#36394;&#22120;&#65288;RT&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#12290;&#36825;&#20010;&#29420;&#29305;&#21151;&#33021;&#20801;&#35768;&#35745;&#31639;&#19982;&#35768;&#22810;&#31995;&#32479;&#21644;&#29615;&#22659;&#21442;&#25968;&#26377;&#20851;&#30340;&#20449;&#36947;&#20914;&#28608;&#21709;&#24212;&#21644;&#20854;&#20182;&#30456;&#20851;&#37327;&#30340;&#26799;&#24230;&#65292;&#20363;&#22914;&#26448;&#26009;&#29305;&#24615;&#12289;&#22825;&#32447;&#22270;&#26696;&#12289;&#38453;&#21015;&#20960;&#20309;&#12289;&#21457;&#23556;&#26426;&#21644;&#25509;&#25910;&#26426;&#30340;&#26041;&#21521;&#21644;&#20301;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;Sionna RT&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#26080;&#32447;&#30005;&#26448;&#26009;&#21644;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#21457;&#23556;&#26426;&#26041;&#21521;&#31561;&#31034;&#20363;&#24212;&#29992;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20809;&#32447;&#36861;&#36394;&#23545;&#20110;6G&#30740;&#31350;&#35838;&#39064;&#22914;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#12289;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#20197;&#21450;&#29992;&#25143;&#23450;&#20301;&#26159;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#26159;&#35768;&#22810;&#26032;&#39062;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#20851;&#38190;&#25512;&#21160;&#32773;&#65292;&#20363;&#22914;&#25968;&#23383;&#23402;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. Since release v0.14 it integrates a differentiable ray tracer (RT) for the simulation of radio wave propagation. This unique feature allows for the computation of gradients of the channel impulse response and other related quantities with respect to many system and environment parameters, such as material properties, antenna patterns, array geometries, as well as transmitter and receiver orientations and positions. In this paper, we outline the key components of Sionna RT and showcase example applications such as learning radio materials and optimizing transmitter orientations by gradient descent. While classic ray tracing is a crucial tool for 6G research topics like reconfigurable intelligent surfaces, integrated sensing and communications, as well as user localization, differentiable ray tracing is a key enabler for many novel and exciting research directions, for example, digital twins.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#24335;&#25512;&#29702;&#30340;&#26032;&#22411;&#25512;&#29702;&#33539;&#24335;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21069;&#21521;&#26041;&#26696;&#23398;&#20064;&#28436;&#32462;&#22320;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22270;&#21305;&#37197;&#31574;&#30053;&#23558;&#22270;&#20687;&#30340;&#35270;&#35273;&#27010;&#24565;&#19982;&#22330;&#26223;&#21360;&#35937;&#30456;&#20851;&#32852;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19987;&#29992;&#26550;&#26500;&#31216;&#20026;SchemaNet&#65292;&#20197;&#24314;&#27169;&#36755;&#20837;&#23454;&#20363;&#30340;&#35270;&#35273;&#35821;&#20041;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#23398;&#20064;&#25277;&#35937;&#24819;&#35937;&#12290;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Feat2Graph&#26041;&#26696;&#20197;&#25429;&#25417;&#21644;&#21033;&#29992;&#35270;&#35273;&#35821;&#20041;&#30340;&#32452;&#21512;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.06635</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#30340;&#27169;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Schema Inference for Interpretable Image Classification. (arXiv:2303.06635v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#24335;&#25512;&#29702;&#30340;&#26032;&#22411;&#25512;&#29702;&#33539;&#24335;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21069;&#21521;&#26041;&#26696;&#23398;&#20064;&#28436;&#32462;&#22320;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22270;&#21305;&#37197;&#31574;&#30053;&#23558;&#22270;&#20687;&#30340;&#35270;&#35273;&#27010;&#24565;&#19982;&#22330;&#26223;&#21360;&#35937;&#30456;&#20851;&#32852;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19987;&#29992;&#26550;&#26500;&#31216;&#20026;SchemaNet&#65292;&#20197;&#24314;&#27169;&#36755;&#20837;&#23454;&#20363;&#30340;&#35270;&#35273;&#35821;&#20041;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#23398;&#20064;&#25277;&#35937;&#24819;&#35937;&#12290;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Feat2Graph&#26041;&#26696;&#20197;&#25429;&#25417;&#21644;&#21033;&#29992;&#35270;&#35273;&#35821;&#20041;&#30340;&#32452;&#21512;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#24335;&#25512;&#29702;&#30340;&#26032;&#22411;&#25512;&#29702;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#37325;&#24314;&#20808;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21069;&#21521;&#26041;&#26696;&#26469;&#23398;&#20064;&#28436;&#32462;&#22320;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#65292;&#20197;&#27169;&#24335;&#30340;&#21746;&#23398;&#35748;&#30693;&#27010;&#24565;&#20026;&#25351;&#23548;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#23558;&#20256;&#32479;&#30340;&#27169;&#22411;&#25512;&#29702;&#27969;&#31243;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#31181;&#22270;&#21305;&#37197;&#31574;&#30053;&#65292;&#36890;&#36807;&#31867;&#27604;&#20154;&#31867;&#25512;&#29702;&#26426;&#21046;&#20013;&#30340;&#21360;&#35937;&#21305;&#37197;&#65292;&#23558;&#22270;&#20687;&#30340;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#19982;&#39044;&#20808;&#35745;&#31639;&#30340;&#22330;&#26223;&#21360;&#35937;&#30456;&#20851;&#32852;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#24515;&#26500;&#24314;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;SchemaNet&#65292;&#20316;&#20026;&#25152;&#25552;&#20986;&#30340;&#27169;&#24335;&#25512;&#29702;&#27010;&#24565;&#30340;&#19987;&#29992;&#23454;&#20363;&#65292;&#23427;&#23558;&#36755;&#20837;&#23454;&#20363;&#30340;&#35270;&#35273;&#35821;&#20041;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#23398;&#20064;&#25277;&#35937;&#24819;&#35937;&#37117;&#24314;&#27169;&#20026;&#25299;&#25169;&#20851;&#31995;&#22270;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20840;&#23616;&#35270;&#22270;&#20013;&#25429;&#25417;&#21644;&#21033;&#29992;&#35270;&#35273;&#35821;&#20041;&#30340;&#32452;&#21512;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#22312;SchemaNet&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Feat2Graph&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a novel inference paradigm, termed as schema inference, that learns to deductively infer the explainable predictions by rebuilding the prior deep neural network (DNN) forwarding scheme, guided by the prevalent philosophical cognitive concept of schema. We strive to reformulate the conventional model inference pipeline into a graph matching policy that associates the extracted visual concepts of an image with the pre-computed scene impression, by analogy with human reasoning mechanism via impression matching. To this end, we devise an elaborated architecture, termed as SchemaNet, as a dedicated instantiation of the proposed schema inference concept, that models both the visual semantics of input instances and the learned abstract imaginations of target categories as topological relational graphs. Meanwhile, to capture and leverage the compositional contributions of visual semantics in a global view, we also introduce a universal Feat2Graph scheme in SchemaNet to 
&lt;/p&gt;</description></item><item><title>CO-BED&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#20449;&#24687;&#29702;&#35770;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#20248;&#21270;&#12290;&#23427;&#37319;&#29992;&#40657;&#31665;&#21464;&#20998;&#26041;&#27861;&#21516;&#26102;&#20272;&#35745;&#21644;&#20248;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#24212;&#31163;&#25955;&#21160;&#20316;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.14015</link><description>&lt;p&gt;
CO-BED&#65306;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#20449;&#24687;&#29702;&#35770;&#19978;&#19979;&#25991;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design. (arXiv:2302.14015v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14015
&lt;/p&gt;
&lt;p&gt;
CO-BED&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#20449;&#24687;&#29702;&#35770;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#20248;&#21270;&#12290;&#23427;&#37319;&#29992;&#40657;&#31665;&#21464;&#20998;&#26041;&#27861;&#21516;&#26102;&#20272;&#35745;&#21644;&#20248;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#24212;&#31163;&#25955;&#21160;&#20316;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#35270;&#35282;&#23545;&#19978;&#19979;&#25991;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;CO-BED - &#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#21407;&#21017;&#35774;&#35745;&#19978;&#19979;&#25991;&#23454;&#39564;&#12290;&#22312;&#21046;&#23450;&#21512;&#36866;&#30340;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21464;&#20998;&#26041;&#27861;&#22312;&#21333;&#19968;&#38543;&#26426;&#26799;&#24230;&#26041;&#26696;&#20013;&#21516;&#26102;&#20272;&#35745;&#21644;&#20248;&#21270;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36866;&#24212;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#31163;&#25955;&#21160;&#20316;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#36830;&#32493;&#26494;&#24347;&#26041;&#26696;&#65292;&#36825;&#21487;&#20197;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#25105;&#20204;&#21464;&#20998;&#30446;&#26631;&#20013;&#12290;&#22240;&#27492;&#65292;CO-BED&#20026;&#21508;&#31181;&#19978;&#19979;&#25991;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#23454;&#39564;&#20013;&#28436;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#19982;&#23450;&#21046;&#30340;&#12289;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#65292;CO-BED&#20063;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formalize the problem of contextual optimization through the lens of Bayesian experimental design and propose CO-BED -- a general, model-agnostic framework for designing contextual experiments using information-theoretic principles. After formulating a suitable information-based objective, we employ black-box variational methods to simultaneously estimate it and optimize the designs in a single stochastic gradient scheme. In addition, to accommodate discrete actions within our framework, we propose leveraging continuous relaxation schemes, which can naturally be integrated into our variational objective. As a result, CO-BED provides a general and automated solution to a wide range of contextual optimization problems. We illustrate its effectiveness in a number of experiments, where CO-BED demonstrates competitive performance even when compared to bespoke, model-specific alternatives.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20851;&#20110;&#22312;&#32570;&#20047;&#30495;&#23454;&#35299;&#37322;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21487;&#38752;&#20272;&#31639;&#35299;&#37322;&#26041;&#27861;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#36136;&#37327;&#20272;&#35745;&#22120;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#21033;&#29992;MetaQuantus&#26694;&#26550;&#20998;&#26512;&#20102;&#20272;&#35745;&#22120;&#30340;&#38887;&#24615;&#21644;&#21453;&#24212;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#36341;&#32773;&#36873;&#25321;&#26368;&#20339;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.07265</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20803;&#35780;&#20272;&#38382;&#39064;&#65306;&#20351;&#29992;MetaQuantus&#35782;&#21035;&#21487;&#38752;&#30340;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus. (arXiv:2302.07265v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20851;&#20110;&#22312;&#32570;&#20047;&#30495;&#23454;&#35299;&#37322;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21487;&#38752;&#20272;&#31639;&#35299;&#37322;&#26041;&#27861;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#36136;&#37327;&#20272;&#35745;&#22120;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#21033;&#29992;MetaQuantus&#26694;&#26550;&#20998;&#26512;&#20102;&#20272;&#35745;&#22120;&#30340;&#38887;&#24615;&#21644;&#21453;&#24212;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#36341;&#32773;&#36873;&#25321;&#26368;&#20339;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#23450;&#22312;&#27809;&#26377;&#30495;&#23454;&#35299;&#37322;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26368;&#21487;&#38752;&#22320;&#20272;&#31639;&#35299;&#37322;&#26041;&#27861;&#30340;&#36136;&#37327;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#31454;&#20105;&#35780;&#20272;&#26041;&#27861;&#65288;&#25110;&#8220;&#36136;&#37327;&#20272;&#35745;&#22120;&#8221;&#65289;&#29983;&#25104;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#26088;&#22312;&#34913;&#37327;&#35299;&#37322;&#26041;&#27861;&#30340;&#30456;&#21516;&#24615;&#36136;&#65292;&#32463;&#24120;&#21576;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#25490;&#21517;&#12290;&#36825;&#26679;&#30340;&#20998;&#27495;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#24456;&#38590;&#35299;&#37322;&#65292;&#20174;&#32780;&#20351;&#20182;&#20204;&#38590;&#20197;&#36873;&#25321;&#34920;&#29616;&#26368;&#22909;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;XAI&#20013;&#30340;&#19981;&#21516;&#36136;&#37327;&#20272;&#35745;&#22120;&#36827;&#34892;&#20803;&#35780;&#20272;&#65288;"&#35780;&#20272;&#35780;&#20272;&#26041;&#27861;&#30340;&#36807;&#31243;"&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;MetaQuantus&#20998;&#26512;&#20102;&#36136;&#37327;&#20272;&#35745;&#22120;&#30340;&#20004;&#20010;&#20114;&#34917;&#24615;&#24615;&#33021;&#29305;&#24449;&#65306;&#23545;&#22122;&#22768;&#30340;&#38887;&#24615;&#21644;&#23545;&#38543;&#26426;&#24615;&#30340;&#21453;&#24212;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#30495;&#23454;&#26631;&#31614;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to most reliably estimate the quality of an explanation method in the absence of ground truth explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes generated by competing evaluation methods (or ''quality estimators''), which aim at measuring the same property of an explanation method, frequently present conflicting rankings. Such disagreements can be challenging for practitioners to interpret, thereby complicating their ability to select the best-performing explanation method. We address this problem through a meta-evaluation of different quality estimators in XAI, which we define as ''the process of evaluating the evaluation method''. Our novel framework, MetaQuantus, analyses two complementary performance characteristics of a quality estimator: its resilience to noise and reactivity to randomness, thus circumventing the need for ground truth labels. We demonstr
&lt;/p&gt;</description></item><item><title>CIPER&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21464;&#21644;&#31561;&#21464;&#23398;&#20064;&#30446;&#26631;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#29305;&#23450;&#25968;&#25454;&#22686;&#24378;&#35201;&#27714;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;</title><link>http://arxiv.org/abs/2302.02330</link><description>&lt;p&gt;
CIPER: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#39044;&#27979;&#23398;&#20064;&#32467;&#21512;&#19981;&#21464;&#21644;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning. (arXiv:2302.02330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02330
&lt;/p&gt;
&lt;p&gt;
CIPER&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21464;&#21644;&#31561;&#21464;&#23398;&#20064;&#30446;&#26631;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#29305;&#23450;&#25968;&#25454;&#22686;&#24378;&#35201;&#27714;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#20110;&#23398;&#20064;&#23545;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#20855;&#26377;&#19981;&#21464;&#24615;&#25110;&#31561;&#21464;&#24615;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#19981;&#21464;&#25110;&#31561;&#21464;&#29305;&#24449;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#21462;&#20915;&#20110;&#25152;&#36873;&#25321;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;&#24403;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#31526;&#21512;&#20219;&#21153;&#35201;&#27714;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#33021;&#22815;&#25805;&#20316;&#23545;&#35937;&#35270;&#22270;&#24182;&#30693;&#36947;&#29983;&#25104;&#27599;&#20010;&#35270;&#22270;&#30340;&#21160;&#20316;&#30340;&#20027;&#21160;&#35266;&#23519;&#32773;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#19981;&#21464;&#21644;&#39044;&#27979;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#65288;CIPER&#65289;&#12290;CIPER&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;&#22120;&#21644;&#20004;&#20010;&#19981;&#21516;&#30340;&#36755;&#20986;&#22836;&#37096;&#30340;&#19981;&#21464;&#21644;&#31561;&#21464;&#23398;&#20064;&#30446;&#26631;&#12290;&#19968;&#20010;&#36755;&#20986;&#22836;&#37096;&#26159;&#19968;&#20010;&#20855;&#26377;&#26368;&#20808;&#36827;&#23545;&#27604;&#30446;&#26631;&#30340;&#25237;&#24433;&#22836;&#37096;&#65292;&#20197;&#40723;&#21169;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning (SSRL) methods have shown great success in computer vision. In recent studies, augmentation-based contrastive learning methods have been proposed for learning representations that are invariant or equivariant to pre-defined data augmentation operations. However, invariant or equivariant features favor only specific downstream tasks depending on the augmentations chosen. They may result in poor performance when the learned representation does not match task requirements. Here, we consider an active observer that can manipulate views of an object and has knowledge of the action(s) that generated each view. We introduce Contrastive Invariant and Predictive Equivariant Representation learning (CIPER). CIPER comprises both invariant and equivariant learning objectives using one shared encoder and two different output heads on top of the encoder. One output head is a projection head with a state-of-the-art contrastive objective to encourage invariance 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#32452;&#21512;&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#32452;&#21512;&#31616;&#21333;&#30340;&#20302;&#32423;&#29983;&#25104;&#22120;&#26469;&#26500;&#24314;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#21019;&#20316;&#65292;&#20174;&#32780;&#20248;&#21270;&#30446;&#26631;&#24182;&#35774;&#35745;&#22797;&#26434;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.01561</link><description>&lt;p&gt;
&#26500;&#24314;&#22797;&#26434;&#32467;&#26500;&#30340;&#23618;&#32423;&#32452;&#25104;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchically Composing Level Generators for the Creation of Complex Structures. (arXiv:2302.01561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#32452;&#21512;&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#32452;&#21512;&#31616;&#21333;&#30340;&#20302;&#32423;&#29983;&#25104;&#22120;&#26469;&#26500;&#24314;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#21019;&#20316;&#65292;&#20174;&#32780;&#20248;&#21270;&#30446;&#26631;&#24182;&#35774;&#35745;&#22797;&#26434;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#29983;&#25104;&#20869;&#23481;&#65288;PCG&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20855;&#26377;&#22312;&#35270;&#39057;&#28216;&#25103;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#20197;&#36739;&#20302;&#25104;&#26412;&#21019;&#24314;&#26356;&#22909;&#28216;&#25103;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;PCG&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#30456;&#23545;&#31616;&#21333;&#30340;&#28216;&#25103;&#20013;&#30340;&#20851;&#21345;&#65292;&#22240;&#20026;&#35774;&#35745;&#21487;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#23545;&#20110;&#22797;&#26434;&#29615;&#22659;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#38480;&#21046;&#20102;PCG&#22312;&#26356;&#22797;&#26434;&#21644;&#29616;&#20195;&#21270;&#30340;&#28216;&#25103;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#24037;&#19994;&#30028;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32452;&#25104;&#23618;&#32423;&#29983;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#36882;&#24402;&#22320;&#32452;&#25104;&#31616;&#21333;&#30340;&#20302;&#32423;&#29983;&#25104;&#22120;&#20197;&#26500;&#24314;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#21019;&#36896;&#29289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24341;&#29992;&#36739;&#20302;&#32423;&#21035;&#32452;&#20214;&#26469;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35774;&#35745;&#22797;&#26434;&#32467;&#26500;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38750;&#32452;&#21512;&#22522;&#20934;&#32447;&#65292;&#26356;&#20934;&#30830;&#22320;&#28385;&#36275;&#35774;&#35745;&#24072;&#30340;&#21151;&#33021;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural content generation (PCG) is a growing field, with numerous applications in the video game industry and great potential to help create better games at a fraction of the cost of manual creation. However, much of the work in PCG is focused on generating relatively straightforward levels in simple games, as it is challenging to design an optimisable objective function for complex settings. This limits the applicability of PCG to more complex and modern titles, hindering its adoption in industry. Our work aims to address this limitation by introducing a compositional level generation method that recursively composes simple low-level generators to construct large and complex creations. This approach allows for easily-optimisable objectives and the ability to design a complex structure in an interpretable way by referencing lower-level components. We empirically demonstrate that our method outperforms a non-compositional baseline by more accurately satisfying a designer's functiona
&lt;/p&gt;</description></item><item><title>ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.11596</link><description>&lt;p&gt;
ThoughtSource:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25968;&#25454;&#30340;&#20013;&#22830;&#26530;&#32445;&#12290;
&lt;/p&gt;
&lt;p&gt;
ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11596
&lt;/p&gt;
&lt;p&gt;
ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#19978;&#20173;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#19981;&#36879;&#26126;&#65292;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#20107;&#23454;&#65292;&#24182;&#19988;&#23384;&#22312;&#20854;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35753;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#34920;&#36798;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ThoughtSource&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#12290;ThoughtSource&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26469;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;ThoughtSource&#30340;&#39318;&#27425;&#21457;&#24067;&#38598;&#25104;&#20102;&#20845;&#20010;&#31185;&#23398;/&#21307;&#23398;&#12289;&#19977;&#20010;&#36890;&#29992;&#39046;&#22495;&#21644;&#20116;&#20010;&#25968;&#23398;&#39064;&#31572;&#26696;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.10551</link><description>&lt;p&gt;
Lego-MT: &#36208;&#21521;&#21487;&#25286;&#21368;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation. (arXiv:2212.10551v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;MNMT&#21333;&#20307;&#27169;&#22411;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;:&#35821;&#35328;&#20043;&#38388;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20302;&#25928;&#25512;&#29702;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#27599;&#31181;&#35821;&#35328;(&#25110;&#35821;&#35328;&#32452;)&#20998;&#37197;&#32473;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#21333;&#29420;&#20998;&#25903;&#65292;&#24320;&#21457;&#20986;&#21487;&#25286;&#21368;&#27169;&#22411;&#12290;&#20026;&#20102;&#28385;&#36275;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#20026;&#25152;&#26377;&#35821;&#35328;&#23398;&#20064;&#34920;&#31034;&#30340;&#38656;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#27492;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#21487;&#25286;&#21368;&#27169;&#22411;&#65292;Lego-MT&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#27491;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#20174;OPUS&#25910;&#38598;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;433&#31181;&#35821;&#35328;&#21644;13&#20159;&#20010;&#24179;&#34892;&#25968;&#25454;&#30340;&#32763;&#35793;&#22522;&#20934;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21442;&#25968;&#20026;12&#20159;&#30340;Lego-MT&#24102;&#26469;&#20102;3.2&#20010;spBLEU&#30340;&#24179;&#22343;&#22686;&#30410;&#12290;&#23427;&#29978;&#33267;&#32988;&#36807;&#20102;&#21442;&#25968;&#20026;120&#20159;&#30340;M2M-100&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#37197;&#26041;&#27604;&#24182;&#34892;&#35757;&#32451;&#25552;&#36895;&#20102;28.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT. For a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. The proposed training recipe brings a 28.2$\times$ speedup over the co
&lt;/p&gt;</description></item><item><title>API-Miner&#26159;&#19968;&#31181;API&#21040;API&#35268;&#33539;&#25512;&#33616;&#24341;&#25806;&#65292;&#20855;&#26377;&#20174;OpenAPI&#35268;&#33539;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12289;&#20248;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#20197;&#21450;&#32467;&#21512;&#22810;&#31181;&#20449;&#21495;&#30340;&#23545;&#25968;&#32447;&#24615;&#27010;&#29575;&#27169;&#22411;&#31561;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2212.07253</link><description>&lt;p&gt;
API-Miner: &#19968;&#31181;API&#21040;API&#35268;&#33539;&#25512;&#33616;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
API-Miner: an API-to-API Specification Recommendation Engine. (arXiv:2212.07253v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07253
&lt;/p&gt;
&lt;p&gt;
API-Miner&#26159;&#19968;&#31181;API&#21040;API&#35268;&#33539;&#25512;&#33616;&#24341;&#25806;&#65292;&#20855;&#26377;&#20174;OpenAPI&#35268;&#33539;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12289;&#20248;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#20197;&#21450;&#32467;&#21512;&#22810;&#31181;&#20449;&#21495;&#30340;&#23545;&#25968;&#32447;&#24615;&#27010;&#29575;&#27169;&#22411;&#31561;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#22823;&#22411;&#39033;&#30446;&#30340;&#26032;API&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#30830;&#20445;&#20195;&#30721;&#24211;&#33021;&#22815;&#21487;&#25345;&#32493;&#22686;&#38271;&#12290;&#20026;&#20102;&#30830;&#20445;&#26032;&#30340;API&#32452;&#20214;&#35774;&#35745;&#33391;&#22909;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#20511;&#37492;&#29616;&#26377;&#30340;API&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;API&#35774;&#35745;&#27604;&#36739;&#26041;&#27861;&#65292;&#36825;&#20010;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#32791;&#26102;&#19988;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;API-Miner&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#20102;&#35299;&#65292;&#36825;&#26159;&#39318;&#20010;API&#21040;API&#35268;&#33539;&#25512;&#33616;&#24341;&#25806;&#20043;&#19968;&#12290;API-Miner&#26816;&#32034;&#19982;OpenAPI&#65288;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#25551;&#36848;Web API&#30340;&#35821;&#35328;&#65289;&#32534;&#20889;&#30340;&#30456;&#20851;&#35268;&#33539;&#32452;&#20214;&#12290;API-Miner&#25552;&#20986;&#20102;&#20960;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#20174;OpenAPI&#35268;&#33539;&#20013;&#22788;&#29702;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#65292;&#65288;2&#65289;&#20026;&#39640;&#24230;&#25216;&#26415;&#30340;API&#35268;&#33539;&#39046;&#22495;&#20248;&#21270;&#30340;&#21019;&#26032;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#31181;&#32467;&#21512;&#22810;&#31181;&#20449;&#21495;&#20197;&#26816;&#32034;&#30456;&#20851;&#20869;&#23481;&#30340;&#26032;&#22411;&#23545;&#25968;&#32447;&#24615;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
When designing a new API for a large project, developers need to make smart design choices so that their code base can grow sustainably. To ensure that new API components are well designed, developers can learn from existing API components. However, the lack of standardized methods for comparing API designs makes this learning process time-consuming and difficult. To address this gap we developed API-Miner, to the best of our knowledge, one of the first API-to-API specification recommendation engines. API-Miner retrieves relevant specification components written in OpenAPI (a widely adopted language used to describe web APIs). API-miner presents several significant contributions, including: (1) novel methods of processing and extracting key information from OpenAPI specifications, (2) innovative feature extraction techniques that are optimized for the highly technical API specification domain, and (3) a novel log-linear probabilistic model that combines multiple signals to retrieve rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SurCo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32447;&#24615;&#20195;&#29702;&#25104;&#26412;&#65292;&#23558;&#32452;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26799;&#24230;&#26041;&#27861;&#21644;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.12547</link><description>&lt;p&gt;
SurCo&#65306;&#23398;&#20064;&#29992;&#20110;&#32452;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32447;&#24615;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SurCo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32447;&#24615;&#20195;&#29702;&#25104;&#26412;&#65292;&#23558;&#32452;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26799;&#24230;&#26041;&#27861;&#21644;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#38750;&#32447;&#24615;&#20195;&#20215;&#20989;&#25968;&#21644;&#32452;&#21512;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#19982;&#20854;&#32447;&#24615;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38590;&#20197;&#39640;&#25928;&#27714;&#35299;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SurCo&#65292;&#23427;&#23398;&#20064;&#32447;&#24615;&#20195;&#29702;&#25104;&#26412;&#65292;&#21487;&#29992;&#20110;&#29616;&#26377;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#20197;&#36755;&#20986;&#21407;&#22987;&#38750;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#32447;&#24615;&#20195;&#29702;&#27714;&#35299;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#24494;&#20998;&#65292;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#19982;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#30340;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;SurCo&#21464;&#20307;&#65306;SurCo-zero&#29992;&#20110;&#21333;&#20010;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;SurCo-prior&#29992;&#20110;&#38382;&#39064;&#20998;&#24067;&#65292;SurCo-hybrid&#29992;&#20110;&#32467;&#21512;&#20998;&#24067;&#21644;&#38382;&#39064;&#29305;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#29702;&#35770;&#19978;&#30340;&#30452;&#35273;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Optimization problems with nonlinear cost functions and combinatorial constraints appear in many real-world applications but remain challenging to solve efficiently compared to their linear counterparts. To bridge this gap, we propose $\textbf{SurCo}$ that learns linear $\underline{\text{Sur}}$rogate costs which can be used in existing $\underline{\text{Co}}$mbinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem. The surrogate costs are learned end-to-end with nonlinear loss by differentiating through the linear surrogate solver, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We propose three $\texttt{SurCo}$ variants: $\texttt{SurCo}-\texttt{zero}$ for individual nonlinear problems, $\texttt{SurCo}-\texttt{prior}$ for problem distributions, and $\texttt{SurCo}-\texttt{hybrid}$ to combine both distribution and problem-specific information. We give theoretical intuition motiv
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>http://arxiv.org/abs/2208.10967</link><description>&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10967
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26399;&#26395;&#38543;&#30528;&#31867;&#20284;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#20943;&#23567;&#65307;&#32780;&#38543;&#30528;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#65288;OOD&#65289;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#22686;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21453;&#30452;&#35273;&#30340;&#29616;&#35937;&#65306;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#26159;&#26679;&#26412;&#20174;OOD&#20219;&#21153;&#20013;&#30340;&#25968;&#37327;&#30340;&#38750;&#21333;&#35843;&#20989;&#25968;&#12290;&#38543;&#30528;OOD&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30446;&#26631;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#20043;&#21069;&#20250;&#20808;&#20943;&#23567;&#21518;&#22686;&#22823;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20351;&#29992;&#23569;&#37327;OOD&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;Fisher&#32447;&#24615;&#21028;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#12289;CIFAR-10&#12289;CINIC-10&#12289;PACS&#21644;DomainNet&#65289;&#19978;&#30340;&#28145;&#24230;&#32593;&#32476;&#26469;&#23637;&#31034;&#21644;&#20998;&#26512;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#26679;&#26412;&#23646;&#20110;OOD&#30340;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#30446;&#26631;&#21644;OOD&#32463;&#39564;&#39118;&#38505;&#30340;&#36866;&#24403;&#21152;&#26435;&#30446;&#26631;&#26469;&#21033;&#29992;&#36825;&#20123;&#38750;&#21333;&#35843;&#36235;&#21183;&#12290;&#23613;&#31649;&#23454;&#38469;&#24212;&#29992;&#26377;&#38480;&#65292;&#20294;&#36825;&#34920;&#26126;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26816;&#27979;&#21040;OOD&#26679;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can det
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07734</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#65306;&#31934;&#24515;&#31579;&#36873;&#30340;&#33258;&#30417;&#30563;&#23545;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25104;&#21151;&#20135;&#29983;&#20102;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07734
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#21019;&#24314;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#24040;&#22823;&#25104;&#26412;&#12290;&#23545;&#20110;&#26631;&#35760;&#24322;&#24120;&#31232;&#32570;&#25110;&#20960;&#20046;&#19981;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;SSL&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#36807;&#21435;&#24050;&#32463;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#36827;&#34892;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#30340;&#31867;&#22411;&#23545;&#20934;&#30830;&#24615;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#26816;&#27979;&#27169;&#22411;&#21644;420&#20010;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25968;&#23383;&#21644;&#21487;&#35270;&#35777;&#25454;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;SSAD&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#32780;&#22312;&#32570;&#20047;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;SSL&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20851;&#20110;&#22270;&#20687;&#22411;SSAD&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#20272;&#35745;&#30340;&#34892;&#21160;&#25104;&#26412;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22810;&#20010;&#20272;&#35745;&#22120;&#26469;&#24179;&#34913;&#35745;&#31639;&#26102;&#38388;&#21644;&#26377;&#30028;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#35268;&#21010;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.04166</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#20272;&#35745;&#30340;&#34892;&#21160;&#25104;&#26412;&#36827;&#34892;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Dynamically Estimated Action Costs. (arXiv:2206.04166v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#20272;&#35745;&#30340;&#34892;&#21160;&#25104;&#26412;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22810;&#20010;&#20272;&#35745;&#22120;&#26469;&#24179;&#34913;&#35745;&#31639;&#26102;&#38388;&#21644;&#26377;&#30028;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#35268;&#21010;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#25104;&#26412;&#30340;&#20449;&#24687;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#19981;&#20165;&#20381;&#36182;&#20110;&#22768;&#26126;&#24615;&#34892;&#21160;&#27169;&#22411;&#65292;&#36824;&#20351;&#29992;&#40657;&#30418;&#22806;&#37096;&#34892;&#21160;&#25104;&#26412;&#20272;&#35745;&#22120;&#65292;&#36890;&#24120;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#35268;&#21010;&#38454;&#27573;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#20135;&#29983;&#19981;&#30830;&#23450;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#34892;&#21160;&#25104;&#26412;&#36827;&#34892;&#24191;&#20041;&#21270;&#30340;&#30830;&#23450;&#24615;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#20272;&#35745;&#22120;&#20043;&#38388;&#36873;&#25321;&#34892;&#21160;&#25104;&#26412;&#65292;&#20197;&#22312;&#35745;&#31639;&#26102;&#38388;&#19982;&#26377;&#30028;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#36825;&#26679;&#21487;&#20197;&#23454;&#29616;&#26356;&#20016;&#23500;&#21644;&#26356;&#29616;&#23454;&#30340;&#38382;&#39064;&#34920;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20801;&#35768;&#35268;&#21010;&#22120;&#38480;&#21046;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25628;&#32034;&#31639;&#27861;&#65292;&#23545;$A^*$&#36827;&#34892;&#27867;&#21270;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#26679;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20854;&#20182;&#31639;&#27861;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information about action costs is critical for real-world AI planning applications. Rather than rely solely on declarative action models, recent approaches also use black-box external action cost estimators, often learned from data, that are applied during the planning phase. These, however, can be computationally expensive, and produce uncertain values. In this paper we suggest a generalization of deterministic planning with action costs that allows selecting between multiple estimators for action cost, to balance computation time against bounded estimation uncertainty. This enables a much richer -- and correspondingly more realistic -- problem representation. Importantly, it allows planners to bound plan accuracy, thereby increasing reliability, while reducing unnecessary computational burden, which is critical for scaling to large problems. We introduce a search algorithm, generalizing $A^*$, that solves such planning problems, and additional algorithmic extensions. In addition to t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#25216;&#33021;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#24182;&#23558;&#20854;&#32508;&#21512;&#21040;&#26032;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#38271;&#26399;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#26234;&#33021;&#20307;&#22312;&#38590;&#20197;&#35299;&#20915;&#30340;&#38271;&#26399;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2206.03597</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Parameterized Skills. (arXiv:2206.03597v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03597
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#25216;&#33021;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#24182;&#23558;&#20854;&#32508;&#21512;&#21040;&#26032;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#38271;&#26399;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#26234;&#33021;&#20307;&#22312;&#38590;&#20197;&#35299;&#20915;&#30340;&#38271;&#26399;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#24182;&#23558;&#23427;&#20204;&#32508;&#21512;&#21040;&#25903;&#25345;&#38271;&#26399;&#20219;&#21153;&#39640;&#25928;&#23398;&#20064;&#30340;&#26032;&#21160;&#20316;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31163;&#31574;&#30053;&#20803;&#24378;&#21270;&#23398;&#20064;&#19982;&#20197;&#36712;&#36857;&#20026;&#20013;&#24515;&#30340;&#24179;&#28369;&#39033;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#19968;&#32452;&#21442;&#25968;&#21270;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#26500;&#24314;&#19968;&#20010;&#19977;&#32423;&#23618;&#27425;&#32467;&#26500;&#26694;&#26550;&#65292;&#27169;&#25311;&#26102;&#38388;&#25193;&#23637;&#21442;&#25968;&#21270;&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#35299;&#20915;&#19968;&#32452;&#22256;&#38590;&#30340;&#38271;&#26399;&#20219;&#21153;&#65288;&#38556;&#30861;&#35838;&#31243;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel parameterized skill-learning algorithm that aims to learn transferable parameterized skills and synthesize them into a new action space that supports efficient learning in long-horizon tasks. We propose to leverage off-policy Meta-RL combined with a trajectory-centric smoothness term to learn a set of parameterized skills. Our agent can use these learned skills to construct a three-level hierarchical framework that models a Temporally-extended Parameterized Action Markov Decision Process. We empirically demonstrate that the proposed algorithms enable an agent to solve a set of difficult long-horizon (obstacle-course and robot manipulation) tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#27010;&#29575;&#20844;&#24179;&#31574;&#30053;ProbFair&#65292;&#22914;&#20309;&#22312;&#19981;&#30830;&#23450;&#24615;&#36138;&#23146;&#36172;&#21338;&#38382;&#39064;&#20013;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#65292;&#24182;&#22312;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;ProbFair&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2106.07677</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#36138;&#23146;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#20998;&#37197;&#35268;&#21010;&#65306;&#27010;&#29575;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#27010;&#29575;&#20844;&#24179;&#31574;&#30053;ProbFair&#65292;&#22914;&#20309;&#22312;&#19981;&#30830;&#23450;&#24615;&#36138;&#23146;&#36172;&#21338;&#38382;&#39064;&#20013;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#65292;&#24182;&#22312;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;ProbFair&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#21463;&#38480;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#23849;&#28291;&#36172;&#21338;&#27169;&#22411;&#26469;&#25551;&#36848;&#20855;&#26377;&#21160;&#20316;&#30456;&#20851;&#36716;&#31227;&#27010;&#29575;&#30340;&#24773;&#22659;&#65292;&#20363;&#22914;&#22312;&#24739;&#32773;&#20043;&#38388;&#20998;&#37197;&#20581;&#24247;&#24178;&#39044;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20010;&#35268;&#21010;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#30340;Whittle&#25351;&#25968;&#26041;&#27861;&#35201;&#20040;&#19981;&#32771;&#34385;&#36172;&#21338;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#65292;&#35201;&#20040;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#26102;&#28608;&#21169;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;ProbFair&#65292;&#19968;&#31181;&#27010;&#29575;&#20844;&#24179;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22870;&#21169;&#65292;&#24182;&#30830;&#20445;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#34987;&#36873;&#25321;&#30340;&#27010;&#29575;&#20855;&#26377;&#20005;&#26684;&#30340;&#27491;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312;&#36825;&#20010;&#24212;&#29992;&#20013;&#65292;&#20171;&#20837;&#25514;&#26045;&#25903;&#25345;&#24739;&#32773;&#38388;&#25345;&#32493;&#30340;&#27491;&#21387;&#36890;&#27668;&#65288;CPAP&#65289;&#30103;&#27861;&#30340;&#20381;&#20174;&#24615;&#65292;&#20197;&#21450;&#22312;&#26356;&#24191;&#27867;&#30340;&#21512;&#25104;&#36716;&#31227;&#30697;&#38453;&#31867;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;ProbFair&#22312;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless and collapsing bandits are often used to model budget-constrained resource allocation in settings where arms have action-dependent transition probabilities, such as the allocation of health interventions among patients. However, state-of-the-art Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it. We thus introduce ProbFair, a probabilistically fair policy that maximizes total expected reward and satisfies the budget constraint while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among patients, as well as on a broader class of synthetic transition matrices. We find that ProbFair preserves utility while providing fairness guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item></channel></rss>