<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01103</link><description>&lt;p&gt;
&#32452;&#21512;&#29983;&#25104;&#24314;&#27169;&#65306;&#21333;&#19968;&#27169;&#22411;&#24182;&#19981;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Modeling: A Single Model is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#24040;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#20027;&#27969;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#22914;&#20309;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#20998;&#24067;&#65292;&#20351;&#24471;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#20998;&#24067;&#37096;&#20998;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#35757;&#32451;&#26102;&#23436;&#20840;&#26410;&#35265;&#30340;&#20219;&#21153;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29420;&#31435;&#30340;&#32452;&#21512;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#21475;&#22836;&#21453;&#39304;&#36827;&#34892;&#23450;&#21046;&#21270;&#35843;&#25972;&#32780;&#19981;&#21457;&#29983;&#36807;&#24230;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;C3PO&#12290;</title><link>https://arxiv.org/abs/2402.10893</link><description>&lt;p&gt;
RLVF: &#23398;&#20064;&#22914;&#20309;&#22312;&#27809;&#26377;&#27867;&#21270;&#30340;&#24773;&#20917;&#19979;&#20174;&#21475;&#22836;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLVF: Learning from Verbal Feedback without Overgeneralization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10893
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#21475;&#22836;&#21453;&#39304;&#36827;&#34892;&#23450;&#21046;&#21270;&#35843;&#25972;&#32780;&#19981;&#21457;&#29983;&#36807;&#24230;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;C3PO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37096;&#32626;&#30340;&#19981;&#21516;&#24773;&#22659;&#30340;&#22810;&#26679;&#24615;&#35201;&#27714;&#33021;&#22815;&#20462;&#25913;&#25110;&#23450;&#21046;&#40664;&#35748;&#27169;&#22411;&#34892;&#20026;&#65292;&#20197;&#28385;&#36275;&#32454;&#24494;&#30340;&#35201;&#27714;&#21644;&#20559;&#22909;&#12290;&#35268;&#23450;&#36825;&#31181;&#27169;&#22411;&#35843;&#25972;&#30340;&#26041;&#20415;&#30028;&#38754;&#26159;&#39640;&#23618;&#27425;&#21475;&#22836;&#21453;&#39304;&#65292;&#27604;&#22914;"&#22312;&#32473;&#32769;&#26495;&#36215;&#33609;&#37038;&#20214;&#26102;&#19981;&#35201;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;"&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25776;&#20889;&#39640;&#23618;&#21453;&#39304;&#27604;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#25910;&#38598;&#24378;&#21270;&#23398;&#20064;&#27880;&#37322;&#65288;RLHF&#65289;&#31616;&#21333;&#24471;&#22810;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21482;&#26159;&#29992;&#36825;&#31181;&#21453;&#39304;&#25552;&#31034;&#27169;&#22411;&#20250;&#23548;&#33268;&#21453;&#39304;&#22312;&#19981;&#30456;&#20851;&#30340;&#24773;&#22659;&#20013;&#20135;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#36825;&#31181;&#27867;&#21270;&#30340;&#24773;&#20917;&#19979;&#25972;&#21512;&#21475;&#22836;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#24182;&#21551;&#21457;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65306;&#24102;&#32422;&#26463;&#20559;&#22909;&#20248;&#21270;&#30340;&#24773;&#22659;&#21270;&#35780;&#35770;&#65288;C3PO&#65289;&#12290;C3PO&#20351;&#29992;&#19968;&#27573;&#39640;&#23618;&#27425;&#21453;&#39304;&#29983;&#25104;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#25351;&#23450;&#20102;&#21453;&#39304;&#24212;&#35813;&#22914;&#20309;&#65288;&#20197;&#21450;&#19981;&#24212;&#35813;&#22914;&#20309;&#65289;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10893v1 Announce Type: cross  Abstract: The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as "Don't use emojis when drafting emails to my boss." However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should no
&lt;/p&gt;</description></item><item><title>&#25351;&#23548;&#35843;&#25972;&#36890;&#36807;&#22686;&#21152;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#26469;&#25512;&#21160;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10891</link><description>&lt;p&gt;
&#25351;&#23548;&#22810;&#26679;&#24615;&#25512;&#21160;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instruction Diversity Drives Generalization To Unseen Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10891
&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#36890;&#36807;&#22686;&#21152;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#26469;&#25512;&#21160;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#8212;&#8212;&#22312;&#25351;&#20196;&#21644;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#8212;&#8212;&#26159;&#19968;&#31181;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#26041;&#27861;&#12290;&#20854;&#23454;&#38469;&#25104;&#21151;&#21462;&#20915;&#20110;&#27169;&#22411;&#23398;&#20064;&#27604;&#20854;&#35757;&#32451;&#26102;&#26356;&#24191;&#27867;&#30340;&#25351;&#20196;&#38598;&#12290;&#28982;&#32780;&#65292;&#20915;&#23450;&#27169;&#22411;&#23545;&#36825;&#31181;&#8220;&#26410;&#35265;&#20219;&#21153;&#8221;&#30340;&#27867;&#21270;&#30340;&#22240;&#32032;&#23578;&#19981;&#21313;&#20998;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26412;&#25991;&#36890;&#36807;&#23383;&#31526;&#20018;&#37325;&#20889;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21495;&#20219;&#21153;&#65292;&#26159;&#22270;&#28789;&#23436;&#25972;&#39532;&#23572;&#21487;&#22827;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#23454;&#39564;&#23545;&#8220;&#36755;&#20837;&#8221;&#21644;&#8220;&#25351;&#20196;&#8221;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#25509;&#21463;&#30340;&#25351;&#20196;&#25968;&#37327;&#21644;&#20026;&#27599;&#20010;&#25351;&#20196;&#25552;&#20379;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35266;&#23519;&#21040;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#30830;&#23450;&#20102;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of "inputs" and "instructions". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10890</link><description>&lt;p&gt;
LLM&#35268;&#21010;&#20013;&#26641;&#25628;&#32034;&#20309;&#26102;&#26377;&#29992;&#65311;&#21462;&#20915;&#20110;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
When is Tree Search Useful for LLM Planning? It Depends on the Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10890
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22312;&#22810;&#27493;&#38382;&#39064;&#19979;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#35268;&#21010;&#26041;&#27861;&#19977;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20808;&#36827;&#35268;&#21010;&#26041;&#27861;&#65292;&#36845;&#20195;&#26657;&#27491;&#21644;&#26641;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#37492;&#21035;&#20934;&#30830;&#24615;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#25110;&#26356;&#31616;&#21333;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26102;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#20004;&#39033;&#20219;&#21153;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21644;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#30340;&#37492;&#21035;&#22120;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65307;&#65288;2&#65289;&#24403;&#21069;LLMs&#30340;&#37492;&#21035;&#33021;&#21147;&#23578;&#26410;&#28385;&#36275;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#38656;&#27714;&#65307;&#65288;3&#65289;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#37492;&#21035;&#22120;&#26102;&#65292;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#23616;&#37096;&#35299;&#37322;&#24182;&#30830;&#20445;&#36825;&#20123;&#35299;&#37322;&#23545;&#29992;&#25143;&#20855;&#26377;&#21487;&#29702;&#35299;&#24615;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#24320;&#21457;&#20855;&#26377;&#25968;&#25454;&#36866;&#24212;&#24615;&#21644;&#29992;&#25143;&#24863;&#30693;&#35201;&#27714;&#31449;&#28857;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10888</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#20174;&#25968;&#25454;&#36866;&#24212;&#24615;&#21040;&#29992;&#25143;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Explainability for Machine Learning Models: From Data Adaptability to User Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#23616;&#37096;&#35299;&#37322;&#24182;&#30830;&#20445;&#36825;&#20123;&#35299;&#37322;&#23545;&#29992;&#25143;&#20855;&#26377;&#21487;&#29702;&#35299;&#24615;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#24320;&#21457;&#20855;&#26377;&#25968;&#25454;&#36866;&#24212;&#24615;&#21644;&#29992;&#25143;&#24863;&#30693;&#35201;&#27714;&#31449;&#28857;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#23616;&#37096;&#35299;&#37322;&#65292;&#26088;&#22312;&#30830;&#23450;&#20135;&#29983;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26368;&#20339;&#26465;&#20214;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#21644;&#29992;&#25143;&#38656;&#27714;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#26041;&#27861;&#65292;&#29983;&#25104;&#20219;&#20309;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#35299;&#37322;&#20445;&#25345;&#24544;&#23454;&#20110;&#22522;&#30784;&#27169;&#22411;&#24182;&#23545;&#29992;&#25143;&#20855;&#26377;&#21487;&#29702;&#35299;&#24615;&#12290;&#35770;&#25991;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#22686;&#24378;&#20102;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#32447;&#24615;&#35299;&#37322;&#36924;&#36817;&#27169;&#22411;&#36866;&#23452;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#27604;&#20102;&#20004;&#31181;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#26063;&#20197;&#20998;&#26512;&#20854;&#20013;&#19968;&#31181;&#30456;&#23545;&#21478;&#19968;&#31181;&#30340;&#20248;&#21183;&#12290;&#31532;&#20108;&#37096;&#20998;&#20391;&#37325;&#20110;&#29992;&#25143;&#23454;&#39564;&#65292;&#35780;&#20272;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#21644;&#20004;&#31181;&#19981;&#21516;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#23454;&#39564;&#27979;&#37327;&#20102;&#29992;&#25143;&#29702;&#35299;&#35299;&#37322;&#30340;&#36895;&#24230;&#65292;&#21487;&#20449;&#24230;&#21644;&#20851;&#27880;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10888v1 Announce Type: new  Abstract: This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure ho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.10884</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20559;&#22909;&#23545;&#40784;&#20462;&#22797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#19978;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-modal preference alignment remedies regression of visual instruction tuning on language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#26399;&#26395;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#25442;&#24335;&#22810;&#36718;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20351;&#29992;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MLLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#36864;&#21270;&#65292;&#22240;&#20026;VQA&#25968;&#25454;&#38598;&#32570;&#20047;&#21407;&#22987;&#25991;&#26412;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21518;&#32773;&#26159;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#65288;6k&#26465;&#35760;&#24405;&#65289;&#30340;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31572;&#26696;&#30001;Gemini&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#27880;&#37322;&#20102;5&#20010;&#36136;&#37327;&#25351;&#26631;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#26631;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#12289;&#25298;&#32477;&#25277;&#26679;&#12289;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#21644;SteerLM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;DPO&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;6.73&#30340;MT-Bench&#20998;&#25968;&#65292;&#32780;Vicuna&#30340;6.57&#21644;LLaVA&#30340;5.99&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedD2S&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#21040;&#27973;&#30340;&#23618;&#20002;&#24323;&#26426;&#21046;&#65292;&#22312;&#26080;&#25968;&#25454;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;&#20013;&#22686;&#24378;&#20102;&#26412;&#22320;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#25913;&#21892;&#23458;&#25143;&#38388;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10846</link><description>&lt;p&gt;
FedD2S: &#20010;&#24615;&#21270;&#26080;&#25968;&#25454;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
FedD2S: Personalized Data-Free Federated Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedD2S&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#21040;&#27973;&#30340;&#23618;&#20002;&#24323;&#26426;&#21046;&#65292;&#22312;&#26080;&#25968;&#25454;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;&#20013;&#22686;&#24378;&#20102;&#26412;&#22320;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#25913;&#21892;&#23458;&#25143;&#38388;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#20013;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedD2S&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#12290;FedD2S&#22312;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#28145;&#21040;&#27973;&#30340;&#23618;&#20002;&#24323;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;FEMNIST&#12289;CIFAR10&#12289;CINIC0&#21644;CIFAR100&#65289;&#19978;&#36827;&#34892;&#22823;&#37327;&#27169;&#25311;&#65292;&#25105;&#20204;&#23558;FedD2S&#19982;&#26368;&#20808;&#36827;&#30340;FL&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20855;&#26377;&#21152;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#23458;&#25143;&#38388;&#20844;&#24179;&#24615;&#30340;&#29305;&#28857;&#12290;&#24341;&#20837;&#30340;&#23618;&#20002;&#24323;&#25216;&#26415;&#26377;&#25928;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10846v1 Announce Type: cross  Abstract: This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#33151;&#36827;&#34892;&#25805;&#20316;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23454;&#29616;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24037;&#20316;&#31354;&#38388;&#30340;Pedipulation&#25511;&#21046;&#22120;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10837</link><description>&lt;p&gt;
Pedipulate: &#20351;&#29992;&#22235;&#36275;&#26426;&#22120;&#20154;&#33151;&#37096;&#23454;&#29616;&#25805;&#20316;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#33151;&#36827;&#34892;&#25805;&#20316;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23454;&#29616;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24037;&#20316;&#31354;&#38388;&#30340;Pedipulation&#25511;&#21046;&#22120;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#26426;&#22120;&#20154;&#20855;&#26377;&#22312;&#32500;&#25252;&#12289;&#23478;&#24237;&#25903;&#25345;&#21644;&#25506;&#32034;&#22330;&#26223;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#19982;&#29615;&#22659;&#20114;&#21160;&#21644;&#25805;&#20316;&#65292;&#22823;&#22810;&#25968;&#22235;&#36275;&#26426;&#22120;&#20154;&#37117;&#37197;&#22791;&#20102;&#19987;&#29992;&#26426;&#22120;&#20154;&#33218;&#65292;&#36825;&#24847;&#21619;&#30528;&#19982;&#26631;&#20934;&#22235;&#36275;&#26426;&#22120;&#20154;&#30456;&#27604;&#20855;&#26377;&#39069;&#22806;&#30340;&#36136;&#37327;&#21644;&#26426;&#26800;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;Pedipulation&#8221;-&#20351;&#29992;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#33151;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#36861;&#36394;&#19968;&#20010;&#36275;&#37096;&#30340;&#20301;&#32622;&#30446;&#26631;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#19987;&#29992;&#30340;Pedipulation&#25511;&#21046;&#22120;&#65292;&#23427;&#23545;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#20840;&#36523;&#34892;&#20026;&#20855;&#26377;&#22823;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#27493;&#24577;&#30340;&#20986;&#29616;&#21040;&#36798;&#36828;&#36317;&#31163;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36816;&#21160;-&#25805;&#20316;&#19968;&#20307;&#21270;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#37096;&#32626;&#22312;&#19968;&#20010;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#20351;&#29992;&#36828;&#31243;&#25805;&#20316;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#20363;&#22914;&#24320;&#38376;&#12289;&#37319;&#38598;&#26679;&#26412;&#21644;&#25512;&#21160;&#38556;&#30861;&#29289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36275;&#37096;&#36127;&#36733;&#25658;&#24102;&#36229;&#36807;2.0&#20844;&#26020;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10837v1 Announce Type: cross  Abstract: Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additi
&lt;/p&gt;</description></item><item><title>RAG-Driver &#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36890;&#29992;&#21270;&#39550;&#39542;&#35299;&#37322;&#31995;&#32479;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#39640;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10828</link><description>&lt;p&gt;
RAG-Driver&#65306;&#22312;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#39550;&#39542;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10828
&lt;/p&gt;
&lt;p&gt;
RAG-Driver &#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36890;&#29992;&#21270;&#39550;&#39542;&#35299;&#37322;&#31995;&#32479;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#39640;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#38656;&#35201;&#25552;&#20379;&#20154;&#31867;&#21487;&#20449;&#36182;&#30340;&#21487;&#29702;&#35299;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#24615;&#22312;&#21487;&#20449;&#20219;&#30340;&#33258;&#20027;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#20197;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#26368;&#32456;&#29992;&#25143;&#30340;&#25509;&#21463;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#26368;&#36817;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#22686;&#24378;&#35299;&#37322;&#24615;&#20316;&#20026;&#39550;&#39542;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20135;&#29983;&#25511;&#21046;&#39044;&#27979;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#27880;&#37322;&#25104;&#26412;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26174;&#33879;&#39046;&#22495;&#24046;&#24322;&#65292;&#23548;&#33268;&#30340;&#20005;&#37325;&#25968;&#25454;&#31232;&#32570;&#20351;&#24471;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#31995;&#32479;&#21464;&#24471;&#24322;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;MLLM&#30340;&#35757;&#32451;&#35201;&#27714;&#26114;&#36149;&#65292;&#32780;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#23578;&#26410;&#35299;&#20915;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAG-Driver&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10828v1 Announce Type: cross  Abstract: Robots powered by 'blackbox' models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.10805</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#22270;&#20687;&#29992;&#20110;&#26816;&#32034;&#21450;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#20854;&#33021;&#22815;&#35760;&#24518;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#24182;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#12290;&#22312;&#27492;&#33021;&#21147;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#33021;&#22815;&#22312;&#20854;&#21442;&#25968;&#20869;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#29992;&#25143;&#23545;&#35270;&#35273;&#20869;&#23481;&#30340;&#26597;&#35810;&#65292;MLLM&#34987;&#26399;&#26395;&#33021;&#22815;&#20174;&#20854;&#21442;&#25968;&#20013;&#8220;&#22238;&#24518;&#8221;&#30456;&#20851;&#22270;&#20687;&#20316;&#20026;&#21709;&#24212;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;MLLM&#20869;&#32622;&#30340;&#35270;&#35273;&#35760;&#24518;&#21644;&#35270;&#35273;&#26816;&#32034;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026;&#22270;&#20687;&#20998;&#37197;&#21807;&#19968;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#65292;&#24182;&#28041;&#21450;&#20004;&#20010;&#35757;&#32451;&#27493;&#39588;&#65306;&#23398;&#20064;&#35760;&#24518;&#21644;&#23398;&#20064;&#26816;&#32034;&#12290;&#31532;&#19968;&#27493;&#20391;&#37325;&#20110;&#35757;&#32451;MLLM&#35760;&#24518;&#22270;&#20687;&#19982;&#20854;&#26631;&#35782;&#31526;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to "recall" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#27169;&#25311;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#65292;&#24357;&#34917;&#20102;&#20043;&#21069;&#22522;&#20110;&#38646;&#26234;&#33021;&#20195;&#29702;&#25110;&#21333;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.10803</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#23545;&#21152;&#23494;&#24066;&#22330;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modelling crypto markets by multi-agent reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#27169;&#25311;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#65292;&#24357;&#34917;&#20102;&#20043;&#21069;&#22522;&#20110;&#38646;&#26234;&#33021;&#20195;&#29702;&#25110;&#21333;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#22312;&#20043;&#21069;&#30340;&#22522;&#30784;&#24037;&#20316;&#65288;Lussange&#31561;&#20154;&#65292;2020&#24180;&#65289;&#20043;&#19978;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#22411;&#65292;&#27169;&#25311;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#65292;&#35813;&#27169;&#22411;&#26657;&#20934;&#20026; 2018 &#24180;&#33267; 2022 &#24180;&#38388;&#19981;&#38388;&#26029;&#20132;&#26131;&#30340; Binance &#30340;153&#31181;&#21152;&#23494;&#36135;&#24065;&#30340;&#27599;&#26085;&#25910;&#30424;&#20215;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#38646;&#26234;&#33021;&#20195;&#29702;&#25110;&#21333;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#26041;&#27861;&#30340;&#20195;&#29702;&#22522;&#30784;&#27169;&#22411;&#65288;ABM&#65289;&#25110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36171;&#20104;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#20197;&#27169;&#25311;&#21152;&#23494;&#24066;&#22330;&#12290;&#36825;&#31181;&#25972;&#21512;&#26088;&#22312;&#36890;&#36807;&#33258;&#19979;&#32780;&#19978;&#30340;&#22797;&#26434;&#24615;&#25512;&#29702;&#65292;&#27169;&#25311;&#20010;&#20307;&#21644;&#38598;&#20307;&#20195;&#29702;&#65292;&#30830;&#20445;&#22312;&#36825;&#31867;&#24066;&#22330;&#36817;&#26399;&#27874;&#21160;&#21095;&#28872;&#19988;&#22312; COVID-19 &#26102;&#20195;&#26399;&#38388;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#36824;&#22312;&#20110;&#20854;&#33258;&#20027;&#20195;&#29702;&#26681;&#25454;&#20004;&#31181;&#20449;&#24687;&#28304;&#36827;&#34892;&#36164;&#20135;&#20215;&#26684;&#20272;&#20540;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10803v1 Announce Type: cross  Abstract: Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent reinforcement learning (MARL) model simulating crypto markets, which is calibrated to the Binance's daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022. Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with reinforcement learning (RL) techniques in order to model crypto markets. This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era. A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the mar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39118;&#26684;&#21270;&#25163;&#20889;&#25991;&#26412;&#29983;&#25104;&#20013;&#36755;&#20837;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#20934;&#22791;&#21644;&#35757;&#32451;&#35268;&#33539;&#21270;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#38024;&#23545;HTG&#30740;&#31350;&#20013;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#19981;&#36275;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.10798</link><description>&lt;p&gt;
VATr++&#65306;&#26126;&#26234;&#22320;&#36873;&#25321;&#24744;&#30340;&#23383;&#35789;&#36827;&#34892;&#25163;&#20889;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VATr++: Choose Your Words Wisely for Handwritten Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39118;&#26684;&#21270;&#25163;&#20889;&#25991;&#26412;&#29983;&#25104;&#20013;&#36755;&#20837;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#20934;&#22791;&#21644;&#35757;&#32451;&#35268;&#33539;&#21270;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#38024;&#23545;HTG&#30740;&#31350;&#20013;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#19981;&#36275;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10798v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;GANs&#12289;Transformers&#21644;&#21021;&#27493;&#30340;Diffusion&#27169;&#22411;&#30340;&#25104;&#21151;&#25512;&#21160;&#20102;&#39118;&#26684;&#21270;&#25163;&#20889;&#25991;&#26412;&#29983;&#25104;&#65288;HTG&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#23613;&#31649;&#23545;&#27492;&#36235;&#21183;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#19988;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#30340;&#26041;&#38754;&#26159;&#36755;&#20837;&#65288;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#65289;&#23545;HTG&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#21450;&#20854;&#23545;&#24615;&#33021;&#30340;&#21518;&#32493;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#39118;&#26684;&#21270;HTG&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#36755;&#20837;&#20934;&#22791;&#21644;&#35757;&#32451;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#36825;&#20123;&#26041;&#38754;&#36890;&#36807;&#22312;&#22810;&#31181;&#19981;&#21516;&#35774;&#32622;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#33268;&#21147;&#20110;&#24615;&#33021;&#20248;&#21270;&#65292;&#36824;&#35299;&#20915;&#20102;HTG&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861; - &#32570;&#20047;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#21270;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10798v1 Announce Type: cross  Abstract: Styled Handwritten Text Generation (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance. This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better. These aspects are validated through extensive analysis on several different settings and datasets. Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol. In particular, we propose a standardization of the evaluatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10793</link><description>&lt;p&gt;
&#25513;&#30721;&#27880;&#24847;&#21147;&#26159;&#22270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Masked Attention is All You Need for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#21464;&#31181;&#20027;&#35201;&#29992;&#20110;&#22312;&#22270;&#19978;&#23398;&#20064;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#12289;&#36895;&#24230;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;GNNs&#38656;&#35201;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#31526;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#27880;&#24847;&#21147;&#12290;&#22270;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#25110;&#36793;&#38598;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#26435;&#37325;&#30697;&#38453;&#26469;&#24378;&#21046;&#23427;&#20204;&#30340;&#36830;&#25509;&#65292;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#22270;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#29992;&#20110;&#22270;&#30340;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;55&#22810;&#20010;&#33410;&#28857;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#28040;&#24687;&#20256;&#36882;&#22522;&#32447;&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10790</link><description>&lt;p&gt;
&#22312;&#19968;&#20010; 1000 &#19975;&#26681;&#33609;&#22427;&#20013;&#23547;&#25214;&#38024;&#65306;&#24490;&#29615;&#35760;&#24518;&#25214;&#21040;&#20102;&#35821;&#35328;&#27169;&#22411;&#19981;&#25797;&#38271;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335; Transformer &#27169;&#22411;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; BABILong&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#24191;&#27867;&#25991;&#26412;&#20013;&#20998;&#24067;&#24335;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324; GPT-4 &#21644; RAG &#30340;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24120;&#35265;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#26368;&#22810; $10^4$ &#20010;&#20803;&#32032;&#30340;&#24207;&#21015;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#26368;&#22810; $10^7$ &#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#25104;&#23601;&#26631;&#24535;&#30528;&#36804;&#20170;&#20026;&#27490;&#20219;&#20309;&#24320;&#28304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#30340;&#26368;&#38271;&#36755;&#20837;&#65292;&#26174;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.10787</link><description>&lt;p&gt;
EdgeQAT: &#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65292;&#29992;&#20110;&#21152;&#36895;&#36731;&#37327;&#32423;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#65292;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#37327;&#21270;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#24555;&#36895;&#25512;&#29702;&#30340;&#36731;&#37327;&#32423;LLMs&#12290;&#28982;&#32780;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#23558;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#19968;&#36215;&#37327;&#21270;&#33267;8&#20301;&#20197;&#19979;&#26102;&#65292;&#36136;&#37327;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#24037;&#20316;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28608;&#27963;&#26410;&#34987;&#35302;&#21450;&#65292;&#36825;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#23545;&#36793;&#32536;&#31471;&#25512;&#29702;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#21363;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;QAT&#65292;&#29992;&#20110;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#37327;&#21270;&#24615;&#33021;&#19979;&#38477;&#20027;&#35201;&#28304;&#33258;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AutoGPT+P&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#35268;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10778</link><description>&lt;p&gt;
&#22522;&#20110;Affordance&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AutoGPT+P
&lt;/p&gt;
&lt;p&gt;
AutoGPT+P: Affordance-based Task Planning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AutoGPT+P&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#35268;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20219;&#21153;&#35268;&#21010;&#30340;&#19968;&#20123;&#26032;&#36827;&#23637;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#32463;&#20856;&#35268;&#21010;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#35299;&#20915;&#23427;&#20204;&#22312;&#25512;&#29702;&#33021;&#21147;&#19978;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#21160;&#24577;&#25429;&#25417;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#30340;&#21021;&#22987;&#29366;&#24577;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoGPT+P&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#23558;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#19982;&#19968;&#20010;&#35268;&#21010;&#31995;&#32479;&#30456;&#32467;&#21512;&#12290;Affordance&#21253;&#25324;&#20102;&#19968;&#20010;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#21644;&#20854;&#20013;&#23384;&#22312;&#30340;&#29289;&#20307;&#19978;&#30340;&#21160;&#20316;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20174;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#20013;&#25512;&#23548;&#20986;&#35268;&#21010;&#22495;&#65292;&#20801;&#35768;&#20351;&#29992;&#20219;&#24847;&#23545;&#35937;&#36827;&#34892;&#31526;&#21495;&#35268;&#21010;&#12290;AutoGPT+P&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#26469;&#20026;&#29992;&#25143;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#30340;&#20219;&#21153;&#21046;&#23450;&#21644;&#25191;&#34892;&#35745;&#21010;&#12290;&#38500;&#20102;&#22312;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#19979;&#35299;&#20915;&#35268;&#21010;&#20219;&#21153;&#22806;&#65292;AutoGPT+P&#36824;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10778v1 Announce Type: cross  Abstract: Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#24418;&#24335;&#30340;&#38169;&#35823;&#21453;&#39304;EF21&#65292;&#23558;&#20854;&#20381;&#36182;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#24179;&#22343;&#20540;&#25913;&#36827;&#20026;&#26356;&#23567;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10774</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#37325;&#26032;&#21152;&#36733;&#65306;&#20174;&#24179;&#26041;&#21040;&#24179;&#28369;&#24230;&#24120;&#25968;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#24418;&#24335;&#30340;&#38169;&#35823;&#21453;&#39304;EF21&#65292;&#23558;&#20854;&#20381;&#36182;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#24179;&#22343;&#20540;&#25913;&#36827;&#20026;&#26356;&#23567;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#65288;EF&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#19988;&#26497;&#20854;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#20998;&#24067;&#24335;GD&#25110;SGD&#65289;&#20013;&#30001;&#20110;&#19982;&#36138;&#23146;&#36890;&#20449;&#21387;&#32553;&#25216;&#26415;&#65288;&#22914;TopK&#65289;&#32467;&#21512;&#32780;&#20135;&#29983;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;&#23613;&#31649;EF&#25552;&#20986;&#24050;&#26377;&#36817;&#21313;&#24180;&#26102;&#38388;&#65288;Seide&#31561;&#20154;&#65292;2014&#24180;&#65289;&#65292;&#24182;&#19988;&#23613;&#31649;&#31038;&#21306;&#20026;&#25512;&#36827;&#23545;&#35813;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#32780;&#38598;&#20013;&#21162;&#21147;&#65292;&#20173;&#26377;&#24456;&#22810;&#23578;&#24453;&#25506;&#32034;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;EF21&#65288;Richtarik&#31561;&#20154;&#65292;2021&#24180;&#65289;&#30340;&#29616;&#20195;&#24418;&#24335;&#30340;&#38169;&#35823;&#21453;&#39304;&#65292;&#23427;&#25552;&#20379;&#20102;&#30446;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#65292;&#22312;&#26368;&#24369;&#30340;&#20551;&#35774;&#19979;&#20063;&#22312;&#23454;&#36341;&#20013;&#36816;&#34892;&#33391;&#22909;&#12290;&#29305;&#21035;&#22320;&#65292;&#34429;&#28982;EF21&#30340;&#29702;&#35770;&#36890;&#20449;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26576;&#20123;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24179;&#26041;&#22343;&#20540;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20381;&#36182;&#24615;&#25913;&#36827;&#20026;&#23427;&#20204;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#21518;&#32773;&#22987;&#32456;&#26356;&#23567;&#65292;&#23588;&#20854;&#26159;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10774v1 Announce Type: cross  Abstract: Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.10770</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#22312;&#38754;&#21521;&#25351;&#20196;&#30340;LLM&#20013;&#26377;&#22810;&#21487;&#38752;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#37325;&#21472;&#21644;LLM&#21028;&#26029;&#30340;&#33258;&#21160;&#26041;&#27861;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#25104;&#26412;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20219;&#21153;&#31867;&#22411;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#26041;&#27861;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;ROUGE-L&#24230;&#37327;&#22312;&#30701;&#31572;&#26696;&#33521;&#35821;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#24378;&#30456;&#20851;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#19981;&#21487;&#38752;&#12290;&#20351;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#21592;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22312;&#35201;&#27714;&#35780;&#20272;&#26102;&#21253;&#21547;&#21442;&#32771;&#31572;&#26696;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#36807;&#20110;&#20005;&#26684;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#21028;&#26029;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#21487;&#33021;&#22240;&#20219;&#21153;&#31867;&#22411;&#21644;&#35780;&#20272;&#35774;&#32622;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33976;&#39311;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGR&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#21644;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10769</link><description>&lt;p&gt;
&#33976;&#39311;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Distillation Enhanced Generative Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33976;&#39311;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGR&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#21644;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#36890;&#36807;&#29983;&#25104;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#20316;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#35813;&#33539;&#24335;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#31232;&#30095;&#25110;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#36890;&#36807;&#33976;&#39311;&#36827;&#19968;&#27493;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#21487;&#34892;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGR&#30340;&#21487;&#34892;&#26694;&#26550;&#12290;DGR&#21033;&#29992;&#35832;&#22914;&#36328;&#32534;&#30721;&#22120;&#31561;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#65292;&#22312;&#25945;&#24072;&#35282;&#33394;&#20013;&#25552;&#20379;&#27573;&#33853;&#25490;&#21517;&#21015;&#34920;&#65292;&#25429;&#33719;&#27573;&#33853;&#30340;&#19981;&#21516;&#30456;&#20851;&#31243;&#24230;&#65292;&#32780;&#19981;&#26159;&#20108;&#20803;&#30828;&#26631;&#31614;&#65307;&#38543;&#21518;&#65292;DGR&#37319;&#29992;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#32771;&#34385;&#25945;&#24072;&#27169;&#22411;&#25552;&#20379;&#30340;&#27573;&#33853;&#25490;&#21517;&#39034;&#24207;&#20316;&#20026;&#26631;&#31614;&#12290;&#35813;&#26694;&#26550;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#33976;&#39311;&#27493;&#39588;&#26469;&#22686;&#24378;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#24182;&#19981;&#22686;&#21152;&#20219;&#20309;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#21551;&#21457;&#35774;&#35745;&#30340;&#26694;&#26550;IBE-Eval&#65292;&#29992;&#20110;&#25512;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#65292;&#22312;&#22240;&#26524;&#38382;&#31572;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#39640;&#36798;77%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10767</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#35299;&#37322;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference to the Best Explanation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#21551;&#21457;&#35774;&#35745;&#30340;&#26694;&#26550;IBE-Eval&#65292;&#29992;&#20110;&#25512;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#65292;&#22312;&#22240;&#26524;&#38382;&#31572;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#39640;&#36798;77%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#22522;&#26412;&#35299;&#37322;&#36807;&#31243;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;IBE-Eval&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21746;&#23398;&#20851;&#20110;&#26368;&#20339;&#35299;&#37322;&#25512;&#26029;&#65288;IBE&#65289;&#30340;&#21551;&#21457;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#36827;&#23545;LLMs&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#12290;IBE-Eval&#36890;&#36807;&#32467;&#21512;&#21253;&#25324;&#19968;&#33268;&#24615;&#12289;&#31616;&#27905;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#22312;&#20869;&#30340;&#26174;&#24335;&#36923;&#36753;&#21644;&#35821;&#35328;&#29305;&#24449;&#26469;&#20272;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#22240;&#26524;&#38382;&#31572;&#65288;CQA&#65289;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20854;&#20013;IBE-Eval&#34987;&#35201;&#27714;&#22312;&#22810;&#20010;&#30001;LLMs&#65288;&#21363;GPT 3.5&#21644;Llama 2&#65289;&#29983;&#25104;&#30340;&#31454;&#20105;&#24615;&#22240;&#26524;&#35299;&#37322;&#20013;&#36873;&#25321;&#26368;&#21512;&#29702;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;IBE-Eval&#21487;&#20197;&#25104;&#21151;&#22320;&#20197;&#39640;&#36798;77\%&#30340;&#20934;&#30830;&#29575;&#65288;&#27604;&#38543;&#26426;&#39640;&#32422;27%&#65289;&#35782;&#21035;&#26368;&#20339;&#35299;&#37322;&#65292;&#20248;&#20110;GPT 3.5&#20316;&#20026;&#21028;&#23450;&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#22914;&#20309;&#24212;&#23545;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20043;&#38388;&#30340;&#21160;&#24577;&#24046;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10765</link><description>&lt;p&gt;
&#25919;&#31574;&#23398;&#20064;&#22312;&#25903;&#25345;&#19981;&#36275;&#30340;&#31163;&#32447;&#21160;&#21147;&#23398;RL&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Policy Learning for Off-Dynamics RL with Deficient Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#22914;&#20309;&#24212;&#23545;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20043;&#38388;&#30340;&#21160;&#24577;&#24046;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#25104;&#26412;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#23558;&#22312;&#20302;&#25104;&#26412;&#12289;&#24555;&#36895;&#28304;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#30446;&#26631;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23384;&#22312;&#25361;&#25112;&#12290;&#26080;&#35770;&#27169;&#25311;&#22120;&#22810;&#20040;&#20808;&#36827;&#65292;&#37117;&#19981;&#33021;&#23436;&#32654;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#23548;&#33268;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20043;&#38388;&#23384;&#22312;&#21160;&#24577;&#24046;&#24322;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#65292;&#28304;&#39046;&#22495;&#24517;&#39035;&#21253;&#21547;&#25152;&#26377;&#21487;&#33021;&#30340;&#30446;&#26631;&#36716;&#25442;&#65292;&#36825;&#31181;&#26465;&#20214;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#20840;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#26399;&#23436;&#20840;&#25903;&#25345;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#37325;&#22823;&#21160;&#24577;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10765v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#23454;&#29616;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31995;&#32479;&#24615;&#22320;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#35299;&#37322;&#20197;&#21450;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.10762</link><description>&lt;p&gt;
&#20851;&#20110;&#35299;&#37322;&#19981;&#20844;&#24179;&#24615;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On Explaining Unfairness: An Overview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#23454;&#29616;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31995;&#32479;&#24615;&#22320;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#35299;&#37322;&#20197;&#21450;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#23454;&#29616;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#26412;&#25991;&#20851;&#27880;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#26159;&#26368;&#36817;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#30340;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#20998;&#21035;&#20195;&#34920;&#20004;&#20010;&#20114;&#34917;&#30340;&#30740;&#31350;&#39046;&#22495;&#65306;&#20844;&#24179;&#24615;&#21644;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29992;&#20110;&#20844;&#24179;&#24615;&#30340;&#35299;&#37322;&#20998;&#20026;&#19977;&#31867;&#65306;(a) &#29992;&#20110;&#22686;&#24378;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#35299;&#37322;&#65292;(b) &#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;(&#19981;)&#20844;&#24179;&#24615;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;(c) &#29992;&#20110;&#21327;&#21161;&#25105;&#20204;&#35774;&#35745;&#20943;&#36731;&#19981;&#20844;&#24179;&#24615;&#26041;&#27861;&#30340;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#20844;&#24179;&#24615;&#21644;&#35299;&#37322;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#26410;&#34987;&#21457;&#29616;&#30340;&#25991;&#29486;&#36335;&#24452;&#65292;&#25581;&#31034;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#23453;&#36149;&#35265;&#35299;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10762v1 Announce Type: new  Abstract: Algorithmic fairness and explainability are foundational elements for achieving responsible AI. In this paper, we focus on their interplay, a research area that is recently receiving increasing attention. To this end, we first present two comprehensive taxonomies, each representing one of the two complementary fields of study: fairness and explanations. Then, we categorize explanations for fairness into three types: (a) Explanations to enhance fairness metrics, (b) Explanations to help us understand the causes of (un)fairness, and (c) Explanations to assist us in designing methods for mitigating unfairness. Finally, based on our fairness and explanation taxonomies, we present undiscovered literature paths revealing gaps that can serve as valuable insights for future research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20844;&#24179;&#27491;&#21017;&#21270;&#30340;&#20010;&#20307;&#20844;&#24179;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#24179;&#34913;&#21644;&#20957;&#32858;&#30340;&#31751;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#31934;&#24230;&#21644;&#20844;&#24179;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.10756</link><description>&lt;p&gt;
&#26397;&#21521;&#20957;&#32858;-&#20844;&#24179;-&#21644;&#35856;&#65306;&#23545;&#27604;&#27491;&#21017;&#21270;&#22312;&#20010;&#20307;&#20844;&#24179;&#22270;&#32858;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10756
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20844;&#24179;&#27491;&#21017;&#21270;&#30340;&#20010;&#20307;&#20844;&#24179;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#24179;&#34913;&#21644;&#20957;&#32858;&#30340;&#31751;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#31934;&#24230;&#21644;&#20844;&#24179;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20844;&#24179;&#22270;&#32858;&#31867;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#36890;&#36807;&#26045;&#21152;&#20005;&#26684;&#30340;&#32422;&#26463;&#20248;&#20808;&#32771;&#34385;&#24179;&#34913;&#30340;&#31751;&#65292;&#32780;&#29306;&#29298;&#20102;&#31751;&#30340;&#20957;&#32858;&#24615;&#65307;&#29616;&#26377;&#30340;&#20010;&#20154;&#21644;&#32676;&#20307;&#32423;&#20844;&#24179;&#26041;&#27861;&#22312;&#22270;&#20998;&#21306;&#20013;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#24449;&#20540;&#20998;&#35299;&#65292;&#22240;&#27492;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;iFairNMTF&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#23545;&#27604;&#20844;&#24179;&#27491;&#21017;&#21270;&#30340;&#20010;&#20307;&#20844;&#24179;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#21644;&#20957;&#32858;&#30340;&#31751;&#12290;&#36890;&#36807;&#24341;&#20837;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#23450;&#21046;&#31934;&#24230;-&#20844;&#24179;&#24230;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29992;&#25143;&#30340;&#33258;&#20027;&#26435;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;iFairNMTF&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#32858;&#31867;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10756v1 Announce Type: cross  Abstract: Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters. By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance.
&lt;/p&gt;</description></item><item><title>ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10753</link><description>&lt;p&gt;
ToolSword&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#36328;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10753
&lt;/p&gt;
&lt;p&gt;
ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#24037;&#20855;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#26041;&#27861;&#12290;&#23613;&#31649;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24037;&#20855;&#26469;&#22686;&#24378;LLMs&#65292;&#20294;&#23427;&#32463;&#24120;&#24573;&#35270;&#19982;&#20854;&#24212;&#29992;&#30456;&#20851;&#30340;&#26032;&#20852;&#23433;&#20840;&#32771;&#34385;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$ToolSword$&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#32454;&#33268;&#35843;&#26597;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ToolSword&#21246;&#30011;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#20845;&#20010;&#23433;&#20840;&#22330;&#26223;&#65292;&#21253;&#25324;&#36755;&#20837;&#38454;&#27573;&#30340;$&#24694;&#24847;$ $&#26597;&#35810;$&#21644;$&#36234;&#29425;$ $&#25915;&#20987;$&#65292;&#25191;&#34892;&#38454;&#27573;&#30340;$&#22122;&#22768;$ $&#35823;&#23548;$&#21644;$&#39118;&#38505;$ $&#32447;&#32034;$&#65292;&#20197;&#21450;&#36755;&#20986;&#38454;&#27573;&#30340;$&#26377;&#23475;$ $&#21453;&#39304;$&#21644;$&#38169;&#35823;$ $&#20914;&#31361;$&#12290;&#23545;11&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23384;&#22312;&#25345;&#20037;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#26377;&#23475;&#26597;&#35810;&#12289;&#20351;&#29992;&#39118;&#38505;&#24037;&#20855;&#21644;&#25552;&#20379;&#26377;&#23475;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 Announce Type: cross  Abstract: Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#38477;&#27700;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.10747</link><description>&lt;p&gt;
&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36830;&#32493;&#19968;&#33268;&#29289;&#29702;&#20449;&#24687;&#38477;&#27700;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#38477;&#27700;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#38477;&#27700;&#39044;&#25253;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LUPIN&#65292;&#21363;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25289;&#26684;&#26391;&#26085;&#21452;U-Net&#30340;&#29616;&#22312;&#39044;&#25253;&#65292;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#22806;&#25512;&#30340;&#39044;&#25253;&#26041;&#27861;&#65292;&#24182;&#20197;&#23436;&#20840;&#21487;&#24494;&#19988;GPU&#21152;&#36895;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#25289;&#26684;&#26391;&#26085;&#22352;&#26631;&#31995;&#36716;&#25442;&#65292;&#20197;&#20801;&#35768;&#23454;&#26102;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;LUPIN&#19982;&#24182;&#36229;&#36807;&#20102;&#25152;&#36873;&#25321;&#22522;&#20934;&#30340;&#24615;&#33021;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25950;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10747v1 Announce Type: cross  Abstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.
&lt;/p&gt;</description></item><item><title>GenRES&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#24230;&#35780;&#20272;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20351;&#29992;&#20256;&#32479;&#25351;&#26631;&#35780;&#20272;GRE&#26041;&#27861;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.10744</link><description>&lt;p&gt;
GenRES&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10744
&lt;/p&gt;
&lt;p&gt;
GenRES&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#24230;&#35780;&#20272;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20351;&#29992;&#20256;&#32479;&#25351;&#26631;&#35780;&#20272;GRE&#26041;&#27861;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#39046;&#22495;&#27491;&#26397;&#30528;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#30340;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#65288;GRE&#65289;&#26041;&#21521;&#21457;&#29983;&#26174;&#30528;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#25351;&#26631;&#22914;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#22312;&#35780;&#20272;GRE&#26041;&#27861;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#36825;&#31181;&#19981;&#36275;&#30340;&#21407;&#22240;&#22312;&#20110;&#36825;&#20123;&#25351;&#26631;&#20381;&#36182;&#20110;&#19982;&#20154;&#24037;&#27880;&#37322;&#30340;&#21442;&#32771;&#20851;&#31995;&#30340;&#31934;&#30830;&#21305;&#37197;&#65292;&#32780;GRE&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#19982;&#21442;&#32771;&#19981;&#21516;&#30340;&#22810;&#26679;&#19988;&#35821;&#20041;&#20934;&#30830;&#30340;&#20851;&#31995;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenRES&#65292;&#20197;&#22810;&#32500;&#24230;&#35780;&#20272;GRE&#32467;&#26524;&#30340;&#20027;&#39064;&#30456;&#20284;&#24615;&#12289;&#29420;&#29305;&#24615;&#12289;&#31890;&#24230;&#12289;&#30495;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;GenRES&#65292;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#65306;&#65288;1&#65289;&#31934;&#30830;&#29575;/&#21484;&#22238;&#29575;&#19981;&#33021;&#20805;&#20998;&#35777;&#26126;GRE&#26041;&#27861;&#30340;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20154;&#24037;&#27880;&#37322;&#30340;&#21442;&#32771;&#20851;&#31995;&#21487;&#33021;&#23384;&#22312;&#19981;&#23436;&#25972;&#24773;&#20917;&#65307;&#65288;3&#65289;&#20197;&#22266;&#23450;&#19968;&#32452;&#20851;&#31995;&#25110;&#23454;&#20307;&#25552;&#31034;LLM
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10744v1 Announce Type: cross  Abstract: The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#20851;&#27880;&#22312;&#21442;&#25968;&#26410;&#25552;&#20379;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10726</link><description>&lt;p&gt;
&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Planning Action Models from State Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#20851;&#27880;&#22312;&#21442;&#25968;&#26410;&#25552;&#20379;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;STRIPS&#39046;&#22495;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#20174;&#35201;&#23398;&#20064;&#30340;&#34892;&#21160;&#30340;&#21517;&#31216;&#21644;&#21442;&#25968;&#24320;&#22987;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#21807;&#19968;&#20219;&#21153;&#26159;&#25512;&#26029;&#32473;&#23450;&#34892;&#21160;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#26102;&#26410;&#25552;&#20379;&#23398;&#20064;&#34892;&#21160;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#22312;&#19968;&#20010;&#32423;&#21035;(L1)&#20013;&#65292;&#36712;&#36857;&#20013;&#30340;&#29366;&#24577;&#34987;&#26631;&#35760;&#20026;&#34892;&#21160;&#21517;&#31216;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#34892;&#21160;&#30340;&#25968;&#37327;&#21644;&#21517;&#31216;&#65292;&#20294;&#25105;&#20204;&#20173;&#38656;&#35201;&#24324;&#28165;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#12290;&#22312;&#21478;&#19968;&#20010;&#32423;&#21035;(L2)&#20013;&#65292;&#29366;&#24577;&#36824;&#39069;&#22806;&#26631;&#35760;&#26377;&#26500;&#25104;&#30456;&#24212;&#22522;&#20110;&#23545;&#35937;&#30340;&#34892;&#21160;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#25512;&#26029;&#23398;&#20064;&#34892;&#21160;&#20013;&#21442;&#25968;&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10726v1 Announce Type: new  Abstract: Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the
&lt;/p&gt;</description></item><item><title>Cloud Kitchen&#24179;&#21488;&#21033;&#29992;&#22522;&#20110;&#35268;&#21010;&#30340;&#22797;&#21512;AI&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#27969;&#31243;&#65292;&#36890;&#36807;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21644;&#30495;&#23454;&#21382;&#21490;&#25968;&#25454;&#38598;&#38477;&#20302;&#24310;&#36831;&#37197;&#36865;&#37327;&#65292;&#25552;&#39640;&#39038;&#23458;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10725</link><description>&lt;p&gt;
&#20113;&#21416;&#25151;&#65306;&#20351;&#29992;&#22522;&#20110;&#35268;&#21010;&#30340;&#22797;&#21512;AI&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10725
&lt;/p&gt;
&lt;p&gt;
Cloud Kitchen&#24179;&#21488;&#21033;&#29992;&#22522;&#20110;&#35268;&#21010;&#30340;&#22797;&#21512;AI&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#27969;&#31243;&#65292;&#36890;&#36807;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21644;&#30495;&#23454;&#21382;&#21490;&#25968;&#25454;&#38598;&#38477;&#20302;&#24310;&#36831;&#37197;&#36865;&#37327;&#65292;&#25552;&#39640;&#39038;&#23458;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#39135;&#21697;&#37197;&#36865;&#24066;&#22330;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#35768;&#22810;&#26426;&#20250;&#65292;&#21487;&#20197;&#25552;&#39640;&#20840;&#29699;&#20379;&#39184;&#25928;&#29575;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Cloud Kitchen&#24179;&#21488;&#65292;&#20316;&#20026;&#19968;&#20010;&#20915;&#31574;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#20855;&#26377;&#39135;&#21697;&#37197;&#36865;&#26381;&#21153;&#30340;&#39184;&#21381;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#35813;&#24179;&#21488;&#30001;&#19968;&#20010;Technology-Specific Bridge&#65288;TSB&#65289;&#32452;&#25104;&#65292;&#29992;&#20110;&#19982;&#39184;&#21381;&#25110;&#27169;&#25311;&#22120;&#36827;&#34892;&#36890;&#20449;&#12290;TSB&#20351;&#29992;PDDL&#27169;&#22411;&#34920;&#31034;&#23884;&#20837;&#22312;Unified Planning Framework&#65288;UPF&#65289;&#20013;&#30340;&#20915;&#31574;&#12290;&#20915;&#31574;&#28041;&#21450;&#23558;&#39038;&#23458;&#35746;&#21333;&#20998;&#37197;&#21040;&#36710;&#36742;&#20197;&#21450;&#20915;&#23450;&#20197;&#20160;&#20040;&#39034;&#24207;&#20026;&#39038;&#23458;&#25552;&#20379;&#26381;&#21153;&#65288;&#23545;&#20110;&#27599;&#36742;&#36710;&#65289;&#65292;&#36825;&#26159;&#36890;&#36807;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPTW&#65289;&#26469;&#23436;&#25104;&#30340;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26159;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24179;&#21488;&#21046;&#23450;&#30340;&#20915;&#31574;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#21382;&#21490;&#25968;&#25454;&#38598;&#20943;&#23569;&#24310;&#36831;&#37197;&#36865;&#37327;&#26469;&#25552;&#39640;&#39038;&#23458;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10725v1 Announce Type: new  Abstract: The global food delivery market provides many opportunities for AI-based services that can improve the efficiency of feeding the world. This paper presents the Cloud Kitchen platform as a decision-making tool for restaurants with food delivery and a simulator to evaluate the impact of the decisions. The platform consists of a Technology-Specific Bridge (TSB) that provides an interface for communicating with restaurants or the simulator. TSB uses a PDDL model to represent decisions embedded in the Unified Planning Framework (UPF). Decision-making, which concerns allocating customers' orders to vehicles and deciding in which order the customers will be served (for each vehicle), is done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool for this problem. We show that decisions made by our platform can improve customer satisfaction by reducing the number of delayed deliveries using a real-world historical dataset.
&lt;/p&gt;</description></item><item><title>BioFusionNet&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#19982;&#22522;&#22240;&#21644;&#20020;&#24202;&#25968;&#25454;&#34701;&#21512;&#65292;&#23454;&#29616;ER+&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#12290;</title><link>https://arxiv.org/abs/2402.10717</link><description>&lt;p&gt;
BioFusionNet&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29305;&#24449;&#19982;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;ER+&#20083;&#33146;&#30284;&#20013;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;
&lt;/p&gt;
&lt;p&gt;
BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10717
&lt;/p&gt;
&lt;p&gt;
BioFusionNet&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#19982;&#22522;&#22240;&#21644;&#20020;&#24202;&#25968;&#25454;&#34701;&#21512;&#65292;&#23454;&#29616;ER+&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VA...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10717v1 Announce Type: cross  Abstract: Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to captur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10712</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#25512;&#29702;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#20998;&#35789;&#22120;&#12289;&#35789;&#27719;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#19968;&#20123;LLMs&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26102;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#25928;&#29575;&#20250;&#19979;&#38477;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#21644;&#25104;&#26412;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#31181;&#29983;&#25104;LLMs&#65288;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65289;&#22312;&#22235;&#31181;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#19988;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
&lt;/p&gt;</description></item><item><title>AutoSAT&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#25552;&#21319;&#27714;&#35299;&#22120;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#21363;&#25554;&#21363;&#29992;&#25805;&#20316;&#65292;&#20445;&#35777;&#20102;&#23481;&#38169;&#24615;&#65292;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10705</link><description>&lt;p&gt;
AutoSAT:&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoSAT: Automatically Optimize SAT Solvers via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10705
&lt;/p&gt;
&lt;p&gt;
AutoSAT&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#25552;&#21319;&#27714;&#35299;&#22120;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#21363;&#25554;&#21363;&#29992;&#25805;&#20316;&#65292;&#20445;&#35777;&#20102;&#23481;&#38169;&#24615;&#65292;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#24335;&#22312;SAT&#27714;&#35299;&#22120;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#24182;&#27809;&#26377;&#36866;&#29992;&#20110;&#25152;&#26377;&#38382;&#39064;&#23454;&#20363;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#38656;&#35201;&#20026;&#29305;&#23450;&#38382;&#39064;&#23454;&#20363;&#20248;&#21270;&#29305;&#23450;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoSAT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#12290;AutoSAT&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#65292;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#21518;&#21033;&#29992;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#21551;&#21457;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#22686;&#24378;&#27714;&#35299;&#22120;&#33021;&#21147;&#12290;AutoSAT&#22522;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;&#24191;&#27867;&#30340;&#21021;&#27493;&#35774;&#32622;&#21644;&#27169;&#22411;&#35757;&#32451;&#30340;&#38656;&#27714;&#65292;&#24182;&#20419;&#36827;&#20102;&#19968;&#31181;&#24102;&#26377;&#23481;&#38169;&#33021;&#21147;&#30340;&#24605;&#32500;&#38142;&#21327;&#20316;&#36807;&#31243;&#65292;&#30830;&#20445;&#21551;&#21457;&#24335;&#20248;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;&#23545;&#20351;&#29992;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#27714;&#35299;&#22120;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;AutoSAT&#30340;&#25972;&#20307;&#24615;&#33021;&#20248;&#36234;&#65292;&#29305;&#21035;&#22312;&#35299;&#20915;&#26576;&#20123;&#29305;&#23450;&#30340;SAT&#38382;&#39064;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10705v1 Announce Type: new  Abstract: Heuristics are crucial in SAT solvers, while no heuristic rules are suitable for all problem instances. Therefore, it typically requires to refine specific solvers for specific problem instances. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on Large Large Models (LLMs) which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive preliminary setup and model training, and fosters a Chain of Thought collaborative process with fault-tolerance, ensuring robust heuristic optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL) solver demonstrates the overall superior performance of AutoSAT, especially in solving some specific SAT pr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#21452;&#29983;&#36710;&#32852;&#32593;&#32476;&#22312;&#23494;&#38598;&#21306;&#22495;&#20013;&#36890;&#36807;&#34394;&#25311;&#21452;&#29983;&#26174;&#33879;&#38477;&#20302;&#32593;&#32476;&#24310;&#36831;&#12289;&#20445;&#25345;&#20302;&#24310;&#36831;&#21644;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10701</link><description>&lt;p&gt;
&#21452;&#29983;&#36710;&#32852;&#32593;&#32476;&#22312;&#23494;&#38598;&#21306;&#22495;&#20013;&#26159;&#21542;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10701
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#21452;&#29983;&#36710;&#32852;&#32593;&#32476;&#22312;&#23494;&#38598;&#21306;&#22495;&#20013;&#36890;&#36807;&#34394;&#25311;&#21452;&#29983;&#26174;&#33879;&#38477;&#20302;&#32593;&#32476;&#24310;&#36831;&#12289;&#20445;&#25345;&#20302;&#24310;&#36831;&#21644;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25968;&#23383;&#21452;&#29983;&#20307;&#65288;DTs&#65289;&#23545;&#20110;&#25552;&#39640;&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#22478;&#24066;&#22320;&#21306;&#20013;&#32593;&#32476;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#36710;&#32852;&#32593;&#32476;&#12290;&#30740;&#31350;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#21644;AI&#32858;&#31867;&#26469;&#35782;&#21035;&#20851;&#38190;&#20301;&#32622;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#20107;&#25925;&#29575;&#30340;&#25317;&#25380;&#22478;&#21306;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#37096;&#32626;&#22330;&#26223;&#35780;&#20272;&#20102;&#21452;&#29983;&#36710;&#32852;&#32593;&#32476;&#30340;&#20248;&#21183;&#65306;&#22522;&#20110;&#36793;&#32536;&#30340;&#21452;&#29983;&#20307;&#65292;&#22522;&#20110;&#20113;&#30340;&#21452;&#29983;&#20307;&#21644;&#28151;&#21512;&#21452;&#29983;&#20307;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21452;&#29983;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#24310;&#36831;&#65292;&#34394;&#25311;&#21452;&#29983;&#32988;&#36807;&#29289;&#29702;&#32593;&#32476;&#12290;&#34394;&#25311;&#21452;&#29983;&#21363;&#20351;&#22312;&#22686;&#21152;&#36710;&#36742;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#20063;&#20445;&#25345;&#20302;&#24310;&#36831;&#65292;&#20363;&#22914;&#22312;300&#36742;&#36710;&#30340;&#24773;&#20917;&#19979;&#20026;15.05&#31186;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#20113;&#30340;&#21452;&#29983;&#27604;&#22522;&#20110;&#36793;&#32536;&#30340;&#21452;&#29983;&#24555;1.7&#20493;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25928;&#29575;&#36710;&#32852;&#32593;&#32476;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10701v1 Announce Type: cross  Abstract: This paper investigates the potential of Digital Twins (DTs) to enhance network performance in densely populated urban areas, specifically focusing on vehicular networks. The study comprises two phases. In Phase I, we utilize traffic data and AI clustering to identify critical locations, particularly in crowded urban areas with high accident rates. In Phase II, we evaluate the advantages of twinning vehicular networks through three deployment scenarios: edge-based twin, cloud-based twin, and hybrid-based twin. Our analysis demonstrates that twinning significantly reduces network delays, with virtual twins outperforming physical networks. Virtual twins maintain low delays even with increased vehicle density, such as 15.05 seconds for 300 vehicles. Moreover, they exhibit faster computational speeds, with cloud-based twins being 1.7 times faster than edge twins in certain scenarios. These findings provide insights for efficient vehicular 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10695</link><description>&lt;p&gt;
&#19982;&#36951;&#24536;&#21628;&#24212;&#30340;&#35299;&#38500;&#38142;&#25509;&#65306;&#31616;&#21270;GNN&#20013;&#30340;&#36793;&#35299;&#38500;
&lt;/p&gt;
&lt;p&gt;
Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#21152;&#21095;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#35299;&#38500;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#23398;&#26415;&#30028;&#19968;&#20010;&#31361;&#20986;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#36825;&#19968;&#27010;&#24565;&#22312;&#24378;&#35843;&#34987;&#36951;&#24536;&#26435;&#21033;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#26377;&#36873;&#25321;&#24615;&#22320;&#20174;&#24050;&#35757;&#32451;&#30340;GNN&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#36793;&#30340;&#35299;&#38500;&#23398;&#20064;&#65292;&#36825;&#19968;&#36807;&#31243;&#23545;&#29616;&#23454;&#24212;&#29992;&#29305;&#21035;&#30456;&#20851;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22914;GNNDelete&#21487;&#20197;&#28040;&#38500;&#29305;&#23450;&#36793;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#65292;&#31216;&#20026;&#36807;&#24230;&#36951;&#24536;&#12290;&#24403;&#35299;&#38500;&#23398;&#20064;&#36807;&#31243;&#26080;&#24847;&#20013;&#38500;&#21435;&#36229;&#20986;&#29305;&#23450;&#25968;&#25454;&#30340;&#36807;&#22810;&#20449;&#24687;&#26102;&#65292;&#20250;&#23548;&#33268;&#23545;&#21097;&#20313;&#36793;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;GNNDelete&#30340;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
&lt;/p&gt;</description></item><item><title>LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10685</link><description>&lt;p&gt;
LongHeads: &#22810;&#22836;&#27880;&#24847;&#21147;&#20854;&#23454;&#26159;&#19968;&#20010;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
LongHeads: Multi-Head Attention is Secretly a Long Context Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10685
&lt;/p&gt;
&lt;p&gt;
LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#30001;&#20110;&#26377;&#38480;&#38271;&#24230;&#27867;&#21270;&#21644;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#38656;&#27714;&#65292;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#22788;&#29702;&#36739;&#38271;&#30340;&#36755;&#20837;&#12290; &#35768;&#22810;&#20154;&#35797;&#22270;&#36890;&#36807;&#38480;&#21046;&#22312;&#39044;&#35757;&#32451;&#38271;&#24230;&#20869;&#30340;&#27880;&#24847;&#21147;&#31383;&#21475;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#38382;&#39064;&#65292;&#22914;&#24573;&#30053;&#20013;&#38388;&#19978;&#19979;&#25991;&#21644;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongHeads&#65292;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;LLM&#30340;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#12290; &#25105;&#20204;&#20801;&#35768;&#27599;&#20010;&#22836;&#37096;&#36873;&#25321;&#24182;&#20851;&#27880;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#22359;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#38271;&#24230;&#65292;&#32780;&#19981;&#26159;&#35753;&#27599;&#20010;&#22836;&#37096;&#37117;&#21442;&#19982;&#20840;&#21477;&#27880;&#24847;&#21147;&#65292;&#36825;&#26679;&#20570;&#30001;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#38382;&#39064;&#32780;&#38590;&#20197;&#27867;&#21270;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29289;&#29702;&#30456;&#20851;&#30340;MeshGraphNets&#65288;PI-MGNs&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#32593;&#26684;&#19978;&#36827;&#34892;&#38750;&#23450;&#24577;&#21644;&#38750;&#32447;&#24615;&#20223;&#30495;&#65292;&#21033;&#29992;PINNs&#26469;&#20943;&#23569;&#23545;&#22823;&#37327;&#26114;&#36149;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;</title><link>https://arxiv.org/abs/2402.10681</link><description>&lt;p&gt;
&#29289;&#29702;&#30456;&#20851;&#30340;MeshGraphNets&#65288;PI-MGNs&#65289;&#65306;&#36866;&#29992;&#20110;&#20219;&#24847;&#32593;&#26684;&#19978;&#38750;&#23450;&#24577;&#21644;&#38750;&#32447;&#24615;&#20223;&#30495;&#30340;&#31070;&#32463;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10681
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29289;&#29702;&#30456;&#20851;&#30340;MeshGraphNets&#65288;PI-MGNs&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#32593;&#26684;&#19978;&#36827;&#34892;&#38750;&#23450;&#24577;&#21644;&#38750;&#32447;&#24615;&#20223;&#30495;&#65292;&#21033;&#29992;PINNs&#26469;&#20943;&#23569;&#23545;&#22823;&#37327;&#26114;&#36149;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#31243;&#32452;&#20214;&#24517;&#39035;&#28385;&#36275;&#26085;&#30410;&#22686;&#38271;&#30340;&#25216;&#26415;&#38656;&#27714;&#65292;&#32780;&#19988;&#24320;&#21457;&#21608;&#26399;&#21464;&#24471;&#36234;&#26469;&#36234;&#30701;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#25972;&#20307;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#24320;&#21457;&#38646;&#20214;&#35774;&#35745;&#12289;&#26448;&#26009;&#31995;&#32479;&#21644;&#21046;&#36896;&#24037;&#33402;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#25968;&#20540;&#20223;&#30495;&#65292;&#28982;&#32780;&#23545;&#20110;&#36845;&#20195;&#20248;&#21270;&#32780;&#35328;&#24456;&#24555;&#21464;&#24471;&#35745;&#31639;&#23494;&#38598;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#20110;&#21462;&#20195;&#32791;&#26102;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#25968;&#20540;&#20223;&#30495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MeshGraphNets&#65288;MGNs&#65289;&#26174;&#31034;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#26410;&#30693;&#32593;&#26684;&#20960;&#20309;&#19978;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#23545;&#20248;&#21270;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#37327;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20363;&#22914;&#25968;&#20540;&#20223;&#30495;&#12290;&#29289;&#29702;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#20250;&#65292;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#32780;&#19981;&#26159;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10681v1 Announce Type: cross  Abstract: Engineering components must meet increasing technological demands in ever shorter development cycles. To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process. Current approaches employ numerical simulations, which however quickly becomes computation-intensive, especially for iterative optimization. Data-driven machine learning methods can be used to replace time- and resource-intensive numerical simulations. In particular, MeshGraphNets (MGNs) have shown promising results. They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization. However, these models rely on large amounts of expensive training data, such as numerical simulations. Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled dat
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10645</link><description>&lt;p&gt;
&#20998;&#38548;&#31526;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25928;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Separators Improve Chain-of-Thought Prompting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10645
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#30340;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#23558;&#31034;&#20363;&#25918;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#65292;&#35753;LLMs&#36880;&#27493;&#25286;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CoT&#25552;&#31034;&#30340;&#23494;&#38598;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;LLMs&#30340;&#35748;&#30693;&#36127;&#33655;&#36807;&#37325;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoT-Sep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;CoT&#25552;&#31034;&#20013;&#27599;&#20010;&#31034;&#20363;&#30340;&#26411;&#23614;&#31574;&#30053;&#24615;&#22320;&#24212;&#29992;&#20998;&#38548;&#31526;&#12290;&#36825;&#20123;&#20998;&#38548;&#31526;&#26088;&#22312;&#24110;&#21161;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#19981;&#20351;&#29992;&#20998;&#38548;&#31526;&#30340;&#26222;&#36890;CoT&#30456;&#27604;&#65292;CoT-Sep&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;GSM-8K&#12289;AQuA&#12289;CSQA&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#20998;&#38548;&#31526;&#23545;&#22810;&#20010;LLMs&#65288;&#21253;&#25324;GPT-3.5-Turbo&#12289;GPT-4&#21644;LLaMA-27&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#26469;&#20445;&#25345;&#20027;&#39064;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#25552;&#21462;&#24335;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#37327;&#21644;&#20943;&#23569;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2402.10643</link><description>&lt;p&gt;
&#8220;&#20445;&#25345;&#32852;&#31995;&#65306;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#22312;&#25552;&#21462;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#8221;
&lt;/p&gt;
&lt;p&gt;
`Keep it Together': Enforcing Cohesion in Extractive Summaries by Simulating Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#26469;&#20445;&#25345;&#20027;&#39064;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#25552;&#21462;&#24335;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#37327;&#21644;&#20943;&#23569;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#24335;&#25688;&#35201;&#36890;&#24120;&#20197;&#19968;&#31995;&#21015;&#21477;&#23376;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#23427;&#20204;&#20043;&#38388;&#27809;&#26377;&#39044;&#26399;&#30340;&#36830;&#36143;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#22312;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#65292;&#21516;&#26102;&#25511;&#21046;&#20449;&#24687;&#37327;&#21644;&#20887;&#20313;&#65292;&#29305;&#21035;&#26159;&#24403;&#36755;&#20837;&#20855;&#26377;&#36739;&#39640;&#20887;&#20313;&#24615;&#26102;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#25511;&#21046;&#20887;&#20313;&#65292;&#24182;&#22312;&#36873;&#25321;&#21477;&#23376;&#26102;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#21477;&#23376;&#36873;&#25321;&#22120;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#20197;&#36319;&#36394;&#20027;&#39064; -- &#34987;&#24314;&#27169;&#20026;&#35789;&#38142; -- &#22312;&#21517;&#35789;&#30701;&#35821;&#20043;&#38388;&#24378;&#21270;&#36830;&#36143;&#32852;&#31995;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#21462;&#39640;&#24230;&#36830;&#36143;&#30340;&#25688;&#35201;&#65292;&#28982;&#32780;&#35835;&#32773;&#20173;&#20250;&#24863;&#21040;&#36825;&#20123;&#25688;&#35201;&#21644;&#20165;&#32771;&#34385;&#20449;&#24687;&#37327;&#25110;&#20887;&#20313;&#24615;&#30340;&#25688;&#35201;&#19968;&#26679;&#23500;&#26377;&#20449;&#24687;&#12290;&#25552;&#21462;&#30340;&#25688;&#35201;&#22312;&#21477;&#23376;&#20043;&#38388;&#23637;&#31034;&#20102;&#24179;&#28369;&#30340;&#20027;&#39064;&#36716;&#25442;&#65292;&#36825;&#20123;&#36716;&#25442;&#34987;&#35789;&#38142;&#25152;&#26631;&#35782;&#65292;&#36825;&#20123;&#38142;&#36328;&#36234;&#30456;&#37051;&#25110;&#20960;&#20046;&#30456;&#37051;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10643v1 Announce Type: cross  Abstract: Extractive summaries are usually presented as lists of sentences with no expected cohesion between them. In this paper, we aim to enforce cohesion whilst controlling for informativeness and redundancy in summaries, in cases where the input exhibits high redundancy. The pipeline controls for redundancy in long inputs as it is consumed, and balances informativeness and cohesion during sentence selection. Our sentence selector simulates human memory to keep track of topics --modeled as lexical chains--, enforcing cohesive ties between noun phrases. Across a variety of domains, our experiments revealed that it is possible to extract highly cohesive summaries that nevertheless read as informative to humans as summaries extracted by only accounting for informativeness or redundancy. The extracted summaries exhibit smooth topic transitions between sentences as signaled by lexical chains, with chains spanning adjacent or near-adjacent sentence
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#29983;&#25104;&#30446;&#26631;&#37325;&#23450;&#21521;&#21040;&#23567;&#27874;&#22495;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#35821;&#38899;DDPMs&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.10642</link><description>&lt;p&gt;
&#22312;&#23567;&#27874;&#22495;&#35828;&#35805;&#65306;&#21152;&#36895;&#35821;&#38899;&#25193;&#25955;&#27169;&#22411;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#29983;&#25104;&#30446;&#26631;&#37325;&#23450;&#21521;&#21040;&#23567;&#27874;&#22495;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#35821;&#38899;DDPMs&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#65292;&#23613;&#31649;DDPMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#38271;&#26102;&#38388;&#35757;&#32451;&#21644;&#22823;&#37327;&#25512;&#29702;&#25104;&#26412;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#21152;&#36895;&#35757;&#32451;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36890;&#36807;&#20462;&#25913;&#35821;&#38899;&#20449;&#21495;&#26412;&#36523;&#65292;&#26159;&#21542;&#21487;&#33021;&#25552;&#39640;DDPMs&#30340;&#35757;&#32451;/&#25512;&#29702;&#36895;&#24230;&#21644;&#24615;&#33021;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#29983;&#25104;&#30446;&#26631;&#37325;&#23450;&#21521;&#21040;&#23567;&#27874;&#22495;&#65292;&#23558;&#35821;&#38899;DDPMs&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#21462;&#24471;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10642v1 Announce Type: cross  Abstract: Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves c
&lt;/p&gt;</description></item><item><title>&#23558;vanilla Transformer&#30340;&#20851;&#31995;&#24314;&#27169;&#25193;&#23637;&#21040;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;ContiFormer&#12290;</title><link>https://arxiv.org/abs/2402.10635</link><description>&lt;p&gt;
ContiFormer&#65306;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#36830;&#32493;&#26102;&#38388;Transformer
&lt;/p&gt;
&lt;p&gt;
ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10635
&lt;/p&gt;
&lt;p&gt;
&#23558;vanilla Transformer&#30340;&#20851;&#31995;&#24314;&#27169;&#25193;&#23637;&#21040;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;ContiFormer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#19978;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#23545;&#20110;&#35299;&#37322;&#36830;&#32493;&#21457;&#29983;&#30340;&#25968;&#25454;&#28436;&#21464;&#21644;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290; &#20256;&#32479;&#26041;&#27861;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;Transformer&#27169;&#22411;&#36890;&#36807;&#24378;&#22823;&#30340;&#31070;&#32463;&#26550;&#26500;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#26469;&#25429;&#33719;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#31163;&#25955;&#29305;&#24615;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21040;&#36830;&#32493;&#26102;&#38388;&#25968;&#25454;&#33539;&#24335;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#23613;&#31649;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#21450;&#20854;&#21464;&#20307;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#25429;&#33719;&#36825;&#20123;&#24207;&#21015;&#20869;&#37096;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#12290; &#21516;&#26102;&#23545;&#36755;&#20837;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#24182;&#25429;&#33719;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#21160;&#24577;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#38656;&#27714;&#36843;&#20999;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10635v1 Announce Type: cross  Abstract: Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10634</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#38477;&#37319;&#26679;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10634
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#19982;&#31354;&#38388;&#20013;&#20256;&#24863;&#22120;&#28857;&#30456;&#20851;&#32852;&#12289;&#20855;&#26377;&#30456;&#20114;&#20851;&#31995;&#30340;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#65292;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#21253;&#25324;&#20026;&#27599;&#20010;&#28857;&#39044;&#27979;&#26410;&#26469;&#35266;&#27979;&#20540;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20026;&#22270;&#26469;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#22987;&#32456;&#21487;&#29992;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#26102;&#26080;&#27861;&#25429;&#25417;&#38544;&#34255;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#26102;&#31354;&#38477;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#38543;&#30528;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#31895;&#21270;&#65292;&#33719;&#24471;&#19968;&#32452;&#25429;&#25417;&#24322;&#36136;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#30340;&#34920;&#31034;&#12290;&#22312;&#35266;&#27979;&#20540;&#21644;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#32452;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
&lt;/p&gt;</description></item><item><title>&#23558;&#36923;&#36753;&#32422;&#26463;&#34701;&#21512;&#21040;&#22810;&#20219;&#21153;&#26680;&#24515;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36716;&#25442;&#36923;&#36753;&#38472;&#36848;&#20026;&#36830;&#32493;&#23454;&#29616;&#65292;&#20197;&#23454;&#29616;&#26680;&#24515;&#35859;&#35789;&#35745;&#31639;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.10617</link><description>&lt;p&gt;
&#24102;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#22522;&#20110;&#26680;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Kernel-based Learning with Logic Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10617
&lt;/p&gt;
&lt;p&gt;
&#23558;&#36923;&#36753;&#32422;&#26463;&#34701;&#21512;&#21040;&#22810;&#20219;&#21153;&#26680;&#24515;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36716;&#25442;&#36923;&#36753;&#38472;&#36848;&#20026;&#36830;&#32493;&#23454;&#29616;&#65292;&#20197;&#23454;&#29616;&#26680;&#24515;&#35859;&#35789;&#35745;&#31639;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#36923;&#36753;&#32422;&#26463;&#24418;&#24335;&#30340;&#20808;&#39564;&#30693;&#35782;&#25972;&#21512;&#21040;&#26680;&#24515;&#26426;&#22120;&#20013;&#30340;&#19968;&#32452;&#20219;&#21153;&#20989;&#25968;&#20013;&#12290;&#36923;&#36753;&#21629;&#39064;&#25552;&#20379;&#20102;&#29615;&#22659;&#30340;&#37096;&#20998;&#34920;&#31034;&#65292;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#23427;&#19982;&#30417;&#30563;&#26679;&#26412;&#20013;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#20854;&#20013;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#19968;&#20803;&#35859;&#35789;&#35201;&#30001;&#26680;&#24515;&#26426;&#22120;&#23398;&#20064;&#65292;&#39640;&#32423;&#25277;&#35937;&#34920;&#31034;&#30001;&#36825;&#20123;&#35859;&#35789;&#30340;&#36923;&#36753;&#23376;&#21477;&#32452;&#25104;&#65292;&#24050;&#30693;&#23545;&#20110;&#20219;&#20309;&#36755;&#20837;&#37117;&#25104;&#31435;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#23376;&#21477;&#36716;&#25442;&#20026;&#36830;&#32493;&#23454;&#29616;&#65292;&#22788;&#29702;&#26680;&#24515;&#35859;&#35789;&#35745;&#31639;&#30340;&#36755;&#20986;&#12290;&#23398;&#20064;&#20219;&#21153;&#34987;&#21046;&#23450;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#32467;&#21512;&#20102;&#27979;&#37327;&#30417;&#30563;&#26679;&#26412;&#25311;&#21512;&#24230;&#30340;&#39033;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10617v1 Announce Type: cross  Abstract: This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10614</link><description>&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#20197;&#29983;&#25104;&#21487;&#25511;&#30340;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20195;&#34920;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#23588;&#20854;&#26159;&#23569;&#25968;&#32676;&#20307;&#65292;&#24182;&#20135;&#29983;&#25903;&#25345;&#20854;&#22810;&#26679;&#21270;&#29978;&#33267;&#26377;&#20105;&#35758;&#35266;&#28857;&#30340;&#22768;&#26126;&#23545;&#20110;&#21019;&#36896;&#19968;&#20010;&#21253;&#23481;&#30340;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30340;&#25511;&#21046;&#24615;&#26469;&#25903;&#25345;&#29983;&#25104;&#20869;&#23481;&#30340;&#31435;&#22330;&#65292;&#20854;&#20013;&#24448;&#24448;&#21253;&#21547;&#19981;&#19968;&#33268;&#12289;&#20013;&#31435;&#25110;&#26377;&#20559;&#35265;&#30340;&#22768;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;LLMs&#22312;&#29983;&#25104;&#25903;&#25345;&#29992;&#25143;&#22312;&#25552;&#31034;&#20013;&#23450;&#20041;&#30340;&#35770;&#28857;&#30340;&#22768;&#26126;&#26102;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#25345;&#26377;&#30456;&#21453;&#31435;&#22330;&#30340;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#23545;&#20110;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#26159;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Debate &amp; Tuning&#65288;&#8220;DEBATunE&#8221;&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#24494;&#35843;LLMs&#29983;&#25104;&#36890;&#36807;&#36777;&#35770;&#33719;&#24471;&#30340;&#22768;&#26126;&#12290;&#20026;&#20102;&#26816;&#39564;DEBATunE&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#28085;&#30422;710&#20010;&#20105;&#35758;&#24615;&#20027;&#39064;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#26469;&#36234;&#29425;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#36947;&#24503;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21517;&#20026;Jailbreak&#30340;&#21019;&#24847;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#32469;&#36807;&#23545;&#40784;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20013;&#30340;&#26377;&#23475;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;LLMs&#33258;&#36523;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26368;&#20808;&#36827;&#30340;LLM&#65292;GPT-4&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#35299;&#30721;&#20102;&#20351;&#29992;&#21508;&#31181;&#23494;&#30721;&#25216;&#26415;&#21152;&#23494;&#30340;&#20960;&#20010;&#23433;&#20840;&#21477;&#23376;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#21487;&#20197;&#34987;&#26368;&#26377;&#25928;&#22320;&#35299;&#30721;&#12290;&#21463;&#27492;&#32467;&#26524;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#32534;&#20889;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#19981;&#23433;&#20840;&#21333;&#35789;&#26144;&#23556;&#21040;&#23433;&#20840;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26144;&#23556;&#30340;&#21333;&#35789;&#25552;&#20986;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;59.42%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19981;&#21516;Parameter Efficient Fine-tuning (PEFT)&#26041;&#27861;&#22312;&#20020;&#24202;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38500;&#20102;LoRA&#22806;&#65292;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#22312;&#21508;&#20010;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#20013;&#24615;&#33021;&#19981;&#31283;&#23450;&#65292;&#32780;LoRA&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#24615;&#33021;&#37117;&#30456;&#23545;&#36739;&#39640;&#12290;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21487;&#20197;&#25805;&#20316;&#30340;&#19987;&#38376;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10597</link><description>&lt;p&gt;
&#35268;&#27169;&#25928;&#29575;&#65306;&#30740;&#31350;&#24494;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10597
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19981;&#21516;Parameter Efficient Fine-tuning (PEFT)&#26041;&#27861;&#22312;&#20020;&#24202;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38500;&#20102;LoRA&#22806;&#65292;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#22312;&#21508;&#20010;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#20013;&#24615;&#33021;&#19981;&#31283;&#23450;&#65292;&#32780;LoRA&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#24615;&#33021;&#37117;&#30456;&#23545;&#36739;&#39640;&#12290;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21487;&#20197;&#25805;&#20316;&#30340;&#19987;&#38376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#20837;&#30740;&#31350;&#21644;&#21830;&#19994;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#36234;&#26469;&#36234;&#22823;&#27169;&#22411;&#30340;&#36235;&#21183;&#65292;&#26368;&#21021;&#25215;&#35834;&#36890;&#29992;&#24615;&#65292;&#38543;&#21518;&#26222;&#36941;&#24076;&#26395;&#32553;&#23567;&#35268;&#27169;&#24182;&#21019;&#24314;&#19987;&#38376;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#24494;&#35843;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#28085;&#30422;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65292;&#21253;&#25324;&#21482;&#26377;$25$&#30334;&#19975;&#21442;&#25968;&#30340;&#26497;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#36739;&#22823;&#65292;&#38500;&#20102;LoRA&#22806;&#65292;LoRA&#22312;&#25152;&#26377;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20445;&#25345;&#30456;&#23545;&#36739;&#39640;&#65292;&#36890;&#24120;&#25509;&#36817;&#25110;&#36798;&#21040;&#23436;&#20840;&#24494;&#35843;&#24615;&#33021;&#12290; PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21487;&#20197;&#25805;&#20316;&#30340;&#19987;&#38376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10597v1 Announce Type: cross  Abstract: The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as $25$ million parameters.   Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can oper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19982;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#29983;-&#25945;&#24072;&#33976;&#39311;&#26041;&#27861; EMUFormer&#65292;&#25581;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.10580</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#29992;&#20110;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19982;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#29983;-&#25945;&#24072;&#33976;&#39311;&#26041;&#27861; EMUFormer&#65292;&#25581;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#25104;&#20026;&#24212;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26222;&#36941;&#25361;&#25112;&#65288;&#22914;&#36807;&#24230;&#33258;&#20449;&#12289;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#65289;&#30340;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23613;&#31649;&#23427;&#24448;&#24448;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#37117;&#20855;&#26377;&#22810;&#27169;&#24577;&#24615;&#36136;&#65292;&#22240;&#27492;&#21463;&#30410;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#32852;&#21512;&#35299;&#20915;&#26041;&#26696;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19982;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30456;&#20114;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#21333;&#29420;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#30456;&#27604;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#26041;&#38754;&#30340;&#30410;&#22788;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EMUFormer&#65292;&#19968;&#31181;&#26032;&#30340;&#23398;&#29983;-&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10580v1 Announce Type: cross  Abstract: Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular d
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#33258;&#32534;&#30721;&#65288;$\Sigma$AE&#65289;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#30417;&#30563;&#25439;&#22833;&#26469;&#20248;&#21270;&#36830;&#25509;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#36716;&#23548;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10575</link><description>&lt;p&gt;
&#31526;&#21495;&#33258;&#32534;&#30721;&#29992;&#20110;&#33258;&#30417;&#30563;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symbolic Autoencoding for Self-Supervised Sequence Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10575
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#33258;&#32534;&#30721;&#65288;$\Sigma$AE&#65289;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#30417;&#30563;&#25439;&#22833;&#26469;&#20248;&#21270;&#36830;&#25509;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#36716;&#23548;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#39044;&#27979;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#20294;&#22312;&#19981;&#21516;&#31526;&#21495;&#31995;&#32479;&#20043;&#38388;&#25191;&#34892;&#36716;&#23548;&#20219;&#21153;&#26102;&#36890;&#24120;&#20250;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#34892;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31526;&#21495;&#33258;&#32534;&#30721;&#65288;$\Sigma$AE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20016;&#23500;&#30340;&#19981;&#24179;&#34892;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;$\Sigma$AE&#36890;&#36807;&#19968;&#20010;&#31163;&#25955;&#29942;&#39048;&#23618;&#36830;&#25509;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#65288;&#19982;&#24179;&#34892;&#25968;&#25454;&#30340;&#30417;&#30563;&#25439;&#22833;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#20351;&#24471;&#31163;&#25955;&#29942;&#39048;&#29983;&#25104;&#30340;&#24207;&#21015;&#21487;&#20197;&#34987;&#35835;&#21462;&#20026;&#36716;&#23548;&#30340;&#36755;&#20837;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23613;&#31649;&#23384;&#22312;&#29942;&#39048;&#31163;&#25955;&#24615;&#65292;&#20173;&#33021;&#36827;&#34892;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#24207;&#21015;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;$\Sigma$AE&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#23548;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#26368;&#23569;&#37327;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10575v1 Announce Type: cross  Abstract: Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\Sigma$AE significantly enhances performance on transduction tasks, even with min
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;</title><link>https://arxiv.org/abs/2402.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#32622;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization with an Offset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DPO&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#23558;&#27599;&#20010;&#20559;&#22909;&#23545;&#35270;&#20026;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.10543</link><description>&lt;p&gt;
&#28040;&#38500;&#21542;&#23450;&#23548;&#33268;&#30340;&#24378;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Strong hallucinations from negation and how to fix them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10543
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26377;&#26102;&#20250;&#25552;&#20379;&#30001;&#20110;&#36923;&#36753;&#19981;&#36830;&#36143;&#32780;&#19981;&#21487;&#33021;&#25104;&#31435;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#21709;&#24212;&#20026;\textit{&#24378;&#24187;&#35273;}&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#28304;&#20110;LM&#35745;&#31639;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#20174;&#36825;&#20123;&#34920;&#31034;&#20013;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#37325;&#28857;&#20851;&#27880;&#21542;&#23450;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21542;&#23450;&#19981;&#26159;&#20316;&#20026;&#28508;&#22312;&#34920;&#31034;&#30340;&#21478;&#19968;&#20010;&#20803;&#32032;&#65292;&#32780;&#26159;&#20316;&#20026;\textit{LM&#28508;&#22312;&#34920;&#31034;&#19978;&#30340;&#19968;&#20010;&#25805;&#20316;&#65292;&#32422;&#26463;&#23427;&#20204;&#21487;&#33021;&#30340;&#28436;&#21464;&#26041;&#24335;}&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#22312;&#24102;&#21542;&#23450;&#30340;&#22635;&#31354;&#25552;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#23545;&#31232;&#30095;&#36127;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#22810;&#39046;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#21644;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#36739;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.10532</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#29305;&#24615;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Properties and Challenges of LLM-Generated Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#22810;&#39046;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#21644;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#36739;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#21512;&#29702;&#21270;&#33021;&#21147;&#22312;&#38480;&#23450;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;/&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#24182;&#19981;&#65288;&#20165;&#65289;&#20381;&#36182;&#20110;&#29305;&#23450;&#27880;&#37322;&#30340;&#25968;&#25454;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#35299;&#37322;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21253;&#21547;&#22823;&#37327;&#37326;&#22806;&#20154;&#31867;&#32534;&#20889;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#20551;&#35774;LLMs&#37319;&#29992;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#20849;&#21516;&#29305;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#24182;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#24456;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23646;&#24615;&#23384;&#22312;&#25110;&#32570;&#22833;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26681;&#25454;LLMs&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#24494;&#35843;&#25968;&#25454;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#23646;&#24615;&#23384;&#22312;&#25110;&#32570;&#22833;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10532v1 Announce Type: cross  Abstract: The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations "in the wild", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>LLM Comparator&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#34920;&#29616;&#20248;&#21155;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10524</link><description>&lt;p&gt;
LLM&#27604;&#36739;&#22120;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#34892;&#35780;&#20272;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10524
&lt;/p&gt;
&lt;p&gt;
LLM Comparator&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#34920;&#29616;&#20248;&#21155;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#24050;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21709;&#24212;&#36136;&#37327;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#30340;&#32467;&#26524;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLM&#27604;&#36739;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#22320;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#12290;&#35813;&#24037;&#20855;&#25903;&#25345;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24335;&#24037;&#20316;&#27969;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#21644;&#20309;&#26102;&#27169;&#22411;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#20197;&#21450;&#20004;&#20010;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#36136;&#37327;&#19978;&#26377;&#20309;&#19981;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19968;&#23478;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#23494;&#20999;&#21512;&#20316;&#65292;&#36845;&#20195;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#35813;&#24037;&#20855;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#35782;&#21035;&#30340;&#29992;&#25143;&#25361;&#25112;&#12289;&#35813;&#24037;&#20855;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#20197;&#21450;&#23450;&#26399;&#35780;&#20272;&#20854;&#27169;&#22411;&#30340;&#21442;&#19982;&#32773;&#30340;&#35266;&#23519;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#26397;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#38761;&#21629;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10516</link><description>&lt;p&gt;
&#25511;&#21046;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Controllable Protein Sequence Design: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10516
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#26397;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#38761;&#21629;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#38024;&#23545;&#24615;&#21151;&#33021;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#24433;&#21709;&#30528;&#33647;&#29289;&#21457;&#29616;&#21644;&#37238;&#24037;&#31243;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#21644;&#37329;&#34701;&#38480;&#21046;&#65292;&#23548;&#33322;&#36825;&#20010;&#24222;&#22823;&#30340;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#25361;&#25112;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#36825;&#31181;&#24773;&#20917;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#26397;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#38761;&#21629;&#26041;&#21521;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20026;&#20102;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20013;&#28041;&#21450;&#30340;&#32422;&#26463;&#24615;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20851;&#38190;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#23457;&#26597;&#20102;&#27599;&#20010;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10516v1 Announce Type: cross  Abstract: The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#21151;&#32791;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#39057;&#29575;DL-TDOA&#27979;&#36317;&#31639;&#27861;&#65292;&#20027;&#35201;&#24212;&#23545;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;&#38750;&#30452;&#23556;&#20449;&#36947;&#36335;&#24452;&#21644;&#20449;&#21495;&#20013;&#26029;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.10515</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#36890;&#36947;&#24863;&#30693;&#36229;&#23485;&#24102;DL-TDOA&#30340;&#39640;&#25928;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#21151;&#32791;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#39057;&#29575;DL-TDOA&#27979;&#36317;&#31639;&#27861;&#65292;&#20027;&#35201;&#24212;&#23545;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;&#38750;&#30452;&#23556;&#20449;&#36947;&#36335;&#24452;&#21644;&#20449;&#21495;&#20013;&#26029;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#27979;&#36317;&#26041;&#27861;&#20013;&#65292;&#32570;&#20047;&#19978;&#34892;&#36890;&#20449;&#25110;&#38598;&#20013;&#35745;&#31639;&#20351;&#24471;&#19979;&#34892;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;DL-TDOA&#65289;&#23450;&#20301;&#26368;&#36866;&#21512;&#22823;&#35268;&#27169;&#24037;&#19994;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#21306;&#22495;&#20013;&#30340;&#20020;&#26102;&#25110;&#27704;&#20037;&#38556;&#30861;&#29289;&#36890;&#24120;&#20250;&#23548;&#33268;&#38750;&#30452;&#23556;&#65288;NLOS&#65289;&#20449;&#36947;&#36335;&#24452;&#21644;&#20449;&#21495;&#20013;&#26029;&#25928;&#24212;&#65292;&#20174;&#32780;&#23548;&#33268;&#23450;&#20301;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#21151;&#32791;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#39057;&#29575;DL-TDOA&#27979;&#36317;&#31639;&#27861;&#12290;&#23427;&#21253;&#25324;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;NLOS&#27010;&#29575;&#39044;&#27979;&#22120;&#12289;&#21160;&#24577;&#27979;&#36317;&#39057;&#29575;&#25511;&#21046;&#27169;&#22359;&#21644;&#19968;&#20010;IMU&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10515v1 Announce Type: cross  Abstract: Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink communication or centralized computation makes downlink time-difference-of-arrival (DL-TDOA) localization the most suitable for large-scale industrial deployments. However, temporary or permanent obstacles in the deployment region often lead to non-line-of-sight (NLOS) channel path and signal outage effects, which result in localization errors. Prior research has addressed this problem by increasing the ranging frequency, which leads to a heavy increase in the user device power consumption. It also does not contribute to any increase in localization accuracy under line-of-sight (LOS) conditions. In this paper, we propose and implement a novel low-power channel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS probability predictor based on a convolutional neural network (CNN), a dynamic ranging frequency control module, and an IMU sensor-ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;Resoformer&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20256;&#21160;&#36724;&#19978;&#30340;&#25197;&#25391;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38459;&#23612;&#25216;&#26415;&#21482;&#33021;&#22312;&#20849;&#25391;&#21457;&#29983;&#21518;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10511</link><description>&lt;p&gt;
&#21487;&#20197;&#21464;&#21387;&#22120;&#39044;&#27979;&#25391;&#21160;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Predict Vibrations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;Resoformer&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20256;&#21160;&#36724;&#19978;&#30340;&#25197;&#25391;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38459;&#23612;&#25216;&#26415;&#21482;&#33021;&#22312;&#20849;&#25391;&#21457;&#29983;&#21518;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25391;&#21160;&#26159;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#38382;&#39064;&#12290;EVs&#22312;&#23822;&#23702;&#22320;&#24418;&#19978;&#34892;&#39542;&#26102;&#32463;&#24120;&#20250;&#20135;&#29983;&#25391;&#21160;&#65292;&#34987;&#31216;&#20026;&#25197;&#25391;&#20849;&#25391;&#12290;&#36825;&#31181;&#30001;&#30005;&#26426;&#21644;&#36718;&#32974;&#25391;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24341;&#36215;&#30340;&#20849;&#25391;&#20250;&#22312;&#36710;&#36742;&#20256;&#21160;&#36724;&#19978;&#26045;&#21152;&#36807;&#22823;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38459;&#23612;&#25216;&#26415;&#20165;&#22312;&#20256;&#21160;&#36724;&#25197;&#30697;&#25391;&#21160;&#24133;&#24230;&#36798;&#21040;&#19968;&#23450;&#38408;&#20540;&#21518;&#25165;&#33021;&#26816;&#27979;&#21040;&#20849;&#25391;&#65292;&#23548;&#33268;&#22312;&#26816;&#27979;&#26102;&#20256;&#21160;&#36724;&#19978;&#25215;&#21463;&#37325;&#35201;&#36127;&#33655;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;Resoformer&#65292;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#25197;&#25391;&#30340;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;Resoformer&#21033;&#29992;&#30005;&#26426;&#36716;&#36895;&#30340;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#36755;&#20837;&#24207;&#21015;&#20043;&#21518;&#30340;&#29305;&#23450;&#20998;&#20301;&#25968;&#22788;&#39044;&#27979;&#20256;&#21160;&#36724;&#25197;&#25391;&#30340;&#24133;&#24230;&#12290;&#36890;&#36807;&#35745;&#31639;&#36882;&#24402;&#21644;&#21367;&#31215;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10511v1 Announce Type: cross  Abstract: Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs). EVs often experience vibrations when driving on rough terrains, known as torsional resonance. This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle's drive shaft. However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection. In this study, we propose a novel approach to address this issue by introducing Resoformer, a transformer-based model for predicting torsional resonance. Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series. By calculating the attention between recursive and convolutio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#25506;&#35752;&#20102;&#20154;&#31867;&#30446;&#26631;&#35782;&#21035;&#20013;&#34892;&#21160;&#12289;&#26102;&#24207;&#21644;&#30446;&#26631;&#21487;&#35299;&#24615;&#30340;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#19982;&#20154;&#31867;&#25512;&#26029;&#26356;&#21305;&#37197;&#30340;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10510</link><description>&lt;p&gt;
&#20154;&#31867;&#30446;&#26631;&#35782;&#21035;&#20316;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#65306;&#25506;&#35752;&#34892;&#21160;&#12289;&#26102;&#24207;&#21644;&#30446;&#26631;&#21487;&#35299;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10510
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#25506;&#35752;&#20102;&#20154;&#31867;&#30446;&#26631;&#35782;&#21035;&#20013;&#34892;&#21160;&#12289;&#26102;&#24207;&#21644;&#30446;&#26631;&#21487;&#35299;&#24615;&#30340;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#19982;&#20154;&#31867;&#25512;&#26029;&#26356;&#21305;&#37197;&#30340;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#26159;&#19968;&#31181;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#20351;&#20010;&#20307;&#33021;&#22815;&#26681;&#25454;&#21487;&#29992;&#32447;&#32034;&#25512;&#26029;&#24847;&#22270;&#12290;&#24403;&#21069;&#30340;&#30446;&#26631;&#35782;&#21035;&#31639;&#27861;&#36890;&#24120;&#21482;&#32771;&#34385;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#22312;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#25506;&#35752;&#34892;&#21160;&#12289;&#26102;&#24207;&#21644;&#30446;&#26631;&#21487;&#35299;&#24615;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#22312;Sokoban&#39046;&#22495;&#20013;&#23545;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#30340;&#21709;&#24212;&#65292;&#21457;&#29616;&#34892;&#21160;&#34987;&#36171;&#20104;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26102;&#24207;&#21644;&#21487;&#35299;&#24615;&#20063;&#24433;&#21709;&#30446;&#26631;&#35782;&#21035;&#65292;&#29305;&#21035;&#26159;&#24403;&#34892;&#21160;&#26080;&#20449;&#24687;&#26102;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#24320;&#21457;&#20102;&#19968;&#20010;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#65292;&#20854;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#26356;&#33021;&#21305;&#37197;&#20154;&#31867;&#30340;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20154;&#31867;&#30446;&#26631;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#36808;&#20986;&#20102;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;AI&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10510v1 Announce Type: cross  Abstract: Goal recognition is a fundamental cognitive process that enables individuals to infer intentions based on available cues. Current goal recognition algorithms often take only observed actions as input, but here we use a Bayesian framework to explore the role of actions, timing, and goal solvability in goal recognition. We analyze human responses to goal-recognition problems in the Sokoban domain, and find that actions are assigned most importance, but that timing and solvability also influence goal recognition in some cases, especially when actions are uninformative. We leverage these findings to develop a goal recognition model that matches human inferences more closely than do existing algorithms. Our work provides new insight into human goal recognition and takes a step towards more human-like AI models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.10496</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Comparing Hallucination Detection Metrics for Multilingual Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#38024;&#23545;&#33521;&#25991;&#25991;&#26412;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#23545;&#36825;&#20123;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#34920;&#29616;&#22914;&#20309;&#30340;&#35748;&#35782;&#19978;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26816;&#27979;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#35832;&#22914;ROUGE&#21644;&#21629;&#21517;&#23454;&#20307;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#65292;&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#20256;&#35760;&#25688;&#35201;&#20013;&#26816;&#27979;&#24187;&#35273;&#65307;&#25105;&#20204;&#36824;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#34913;&#37327;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#34429;&#28982;&#35789;&#27719;&#25351;&#26631;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22522;&#20110;NLI&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#22312;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#12290;&#30456;&#21453;&#65292;NLI-based&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22810;&#35821;&#35328;&#24187;&#35273;&#26816;&#27979;&#20013;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10496v1 Announce Type: cross  Abstract: While many automatic hallucination detection techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these hallucination detection metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like ROUGE and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at detecting hallucinations in biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, NLI-based metrics perform well in high-resource languages at the sentence level. In contrast, NLI-based metrics often fail to detect atomic fact hallucinations. Our findings highlight existing gaps in multilingual hallucinati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#22312;&#39044;&#27979;&#23567;&#40614;&#26543;&#21494;&#30149;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#24182;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.10492</link><description>&lt;p&gt;
&#21457;&#23637;&#19968;&#31181;&#39044;&#27979;&#23567;&#40614;&#26543;&#21494;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#26368;&#20339;&#27169;&#22411;&#65288;&#38463;&#23572;&#35199;&#21644;&#24052;&#21202;&#21306;&#26696;&#20363;&#30740;&#31350;&#65289;
&lt;/p&gt;
&lt;p&gt;
Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#22312;&#39044;&#27979;&#23567;&#40614;&#26543;&#21494;&#30149;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#24182;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20102;&#19977;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#12289;&#20256;&#36755;&#12289;&#20998;&#21106;&#21644;&#23398;&#20064;&#21151;&#33021;&#30340;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#65288;BPNN&#65289;&#65292;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;RBFNN&#65289;&#21644;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#65292;&#26469;&#39044;&#27979;&#26543;&#21494;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32771;&#34385;&#20102;&#21442;&#25968;&#22914;&#24179;&#22343;&#26368;&#39640;&#28201;&#24230;&#12289;&#24179;&#22343;&#26368;&#20302;&#28201;&#24230;&#12289;&#24179;&#22343;&#38477;&#38632;&#37327;&#12289;&#24179;&#22343;&#27668;&#28201;&#12289;&#24179;&#22343;&#30456;&#23545;&#28287;&#24230;&#21644;&#19981;&#21516;&#23567;&#40614;&#21697;&#31181;&#12290;&#32479;&#35745;&#20998;&#26512;&#34920;&#26126;&#65292;GRNN&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#30456;&#27604;&#20854;&#20182;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#24635;&#23395;&#33410;&#38477;&#38632;&#37327;&#23545;&#23567;&#40614;&#26543;&#21494;&#30149;&#30340;&#21457;&#23637;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10492v1 Announce Type: cross  Abstract: This research utilized three types of artificial neural network (ANN) methodologies, namely Backpropagation Neural Network (BPNN) with varied training, transfer, divide, and learning functions; Radial Basis Function Neural Network (RBFNN); and General Regression Neural Network (GRNN), to forecast the severity of stem rust. It considered parameters such as mean maximum temperature, mean minimum temperature, mean rainfall, mean average temperature, mean relative humidity, and different wheat varieties. The statistical analysis revealed that GRNN demonstrated effective predictive capability and required less training time compared to the other models. Additionally, the results indicated that total seasonal rainfall positively influenced the development of wheat stem rust.   Keywords: Wheat stem rust, Back propagation neural network, Radial Basis Function Neural Network, General Regression Neural Network.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Emoji Driven Crypto Assets Market Reactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#65292;&#35832;&#22914;Twitter&#20043;&#31867;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#24066;&#22330;&#36235;&#21183;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#34920;&#24773;&#31526;&#21495;&#36716;&#21270;&#20026;&#21487;&#37327;&#21270;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#19982;BTC&#20215;&#26684;&#21644;VCRIX&#25351;&#25968;&#31561;&#20851;&#38190;&#24066;&#22330;&#25351;&#26631;&#36827;&#34892;&#20102;&#30456;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26088;&#22312;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20803;&#32032;&#35782;&#21035;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#37325;&#22823;&#24066;&#22330;&#19979;&#25387;&#65292;&#24182;&#26377;&#21161;&#20110;&#22238;&#25253;&#30340;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#20808;&#36827;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#26512;&#25972;&#21512;&#21040;&#37329;&#34701;&#31574;&#30053;&#20013;&#30340;&#23454;&#38469;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#30475;&#24453;&#24066;&#22330;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ACGCL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25104;&#23545;&#22686;&#24378;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#30456;&#20284;&#24615;&#30340;&#22270;&#32423;&#27491;&#36127;&#26679;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#35782;&#21035;&#26377;&#25928;&#30340;&#22270;&#27169;&#24335;</title><link>https://arxiv.org/abs/2402.10468</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#23545;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ACGCL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25104;&#23545;&#22686;&#24378;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#30456;&#20284;&#24615;&#30340;&#22270;&#32423;&#27491;&#36127;&#26679;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#35782;&#21035;&#26377;&#25928;&#30340;&#22270;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65306;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ACGCL&#65289;&#65292;&#21033;&#29992;&#25104;&#23545;&#22686;&#24378;&#30340;&#20248;&#28857;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#30456;&#20284;&#24615;&#30340;&#22270;&#32423;&#27491;&#36127;&#26679;&#26412;&#65292;&#20197;&#21450;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#35782;&#21035;&#20854;&#20013;&#30340;&#26377;&#25928;&#22270;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10468v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) has emerged as a pivotal technique in the domain of graph representation learning. A crucial aspect of effective GCL is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative graph patterns. To address this challenge, we propose an innovative framework: Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender graph-level positive and negative samples with controllable similarity, alongside subgraph contrastive learning to discern effective graph patterns therein. Within the ACGCL framework, we have devised a novel adversarial curriculum training methodology that facilitates progressive learning by se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#26469;&#32479;&#19968;&#35780;&#20272;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#25972;&#20307;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20837;&#19981;&#21516;&#30693;&#35782;&#33976;&#39311;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10427</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#25913;&#36827;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Improving Continual Learning in Spoken Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10427
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#26469;&#32479;&#19968;&#35780;&#20272;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#25972;&#20307;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20837;&#19981;&#21516;&#30693;&#35782;&#33976;&#39311;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21475;&#35821;&#29702;&#35299;&#12290;&#22312;&#21475;&#35821;&#29702;&#35299;&#20013;&#65292;&#20854;&#30446;&#26631;&#26159;&#26377;&#25928;&#22788;&#29702;&#26032;&#27010;&#24565;&#30340;&#20986;&#29616;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#12290;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#35780;&#20272;&#36890;&#24120;&#28041;&#21450;&#35780;&#20272;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20316;&#20026;&#26631;&#20934;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#25351;&#26631;&#20027;&#35201;&#38598;&#20013;&#22312;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#23646;&#24615;&#19978;&#12290;&#23427;&#20204;&#24573;&#35270;&#20102;&#25972;&#20307;&#34920;&#29616;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#65292;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#24320;&#27169;&#22411;&#20869;&#30340;&#21487;&#22609;&#24615;&#19982;&#31283;&#23450;&#24615;/&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#32479;&#19968;&#35780;&#20272;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#24341;&#20837;&#21508;&#31181;&#30693;&#35782;&#33976;&#39311;&#26469;&#25913;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10427v1 Announce Type: cross  Abstract: Continual learning has emerged as an increasingly important challenge across various tasks, including Spoken Language Understanding (SLU). In SLU, its objective is to effectively handle the emergence of new concepts and evolving environments. The evaluation of continual learning algorithms typically involves assessing the model's stability, plasticity, and generalizability as fundamental aspects of standards. However, existing continual learning metrics primarily focus on only one or two of the properties. They neglect the overall performance across all tasks, and do not adequately disentangle the plasticity versus stability/generalizability trade-offs within the model. In this work, we propose an evaluation methodology that provides a unified evaluation on stability, plasticity, and generalizability in continual learning. By employing the proposed metric, we demonstrate how introducing various knowledge distillations can improve diffe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#21253;&#25324;&#24120;&#35782;&#30693;&#35782;&#24211;&#12289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#20197;&#21450;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#33021;&#22815;&#35299;&#37322;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10424</link><description>&lt;p&gt;
&#20351;&#29992;&#40520;&#40533;&#27748;&#26694;&#26550;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning with a Pelican Soup Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#21253;&#25324;&#24120;&#35782;&#30693;&#35782;&#24211;&#12289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#20197;&#21450;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#33021;&#22815;&#35299;&#37322;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#26159;&#22522;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#65292;&#23427;&#20204;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21363;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65288;1&#65289;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#27010;&#24565;&#65292;&#65288;2&#65289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33324;&#24418;&#24335;&#21270;&#65292;&#20197;&#21450;&#65288;3&#65289;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;$\mathcal{O}(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#36825;&#37324;$T$&#26159;&#28436;&#31034;&#20013;&#31034;&#20363;-&#26631;&#31614;&#23545;&#30340;&#25968;&#37327;&#12290;&#19982;&#20808;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21453;&#26144;&#20102;&#21160;&#35789;&#36873;&#25321;&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#39069;&#22806;&#30340;"&#21407;&#23376;&#27010;&#24565;"&#27010;&#24565;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35299;&#37322;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29609;&#20855;&#35774;&#32622;&#65292;Calcutec&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10424v1 Announce Type: cross  Abstract: Many existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a $\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of \textit{atom concepts} makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec,
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36830;&#25509;&#25968;&#25454;&#38598;&#27987;&#32553;&#12289;&#24046;&#20998;&#38544;&#31169;&#21644;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#36890;&#36807;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#36873;&#25321;&#26368;&#20248;&#22122;&#22768;&#27700;&#24179;$\epsilon$&#30340;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#39640;&#20445;&#30495;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.10423</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#25968;&#25454;&#38598;&#27987;&#32553;&#12289;&#24046;&#20998;&#38544;&#31169;&#21644;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10423
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36830;&#25509;&#25968;&#25454;&#38598;&#27987;&#32553;&#12289;&#24046;&#20998;&#38544;&#31169;&#21644;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#36890;&#36807;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#36873;&#25321;&#26368;&#20248;&#22122;&#22768;&#27700;&#24179;$\epsilon$&#30340;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#39640;&#20445;&#30495;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#19982;($\epsilon$, $\delta$)-&#24046;&#20998;&#38544;&#31169;&#30456;&#32852;&#31995;&#65292;&#36890;&#36807;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#36873;&#25321;&#26368;&#20248;&#22122;&#22768;$\epsilon$&#65292;&#29702;&#35299;&#25968;&#25454;&#38598;&#27987;&#32553;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#25105;&#20204;&#21487;&#20197;&#22238;&#31572;&#26377;&#20851;&#25968;&#25454;&#38598;&#27987;&#32553;&#31243;&#24207;&#20869;&#37096;&#36816;&#20316;&#26041;&#24335;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#27987;&#32553;&#65288;DC&#65289;&#19982;($\epsilon$, $\delta$)-&#24046;&#20998;&#38544;&#31169;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#21435;&#38500;&#25968;&#25454;&#38598;&#27987;&#32553;&#20197;&#33719;&#24471;&#33021;&#22815;&#21019;&#24314;&#39640;&#20445;&#30495;&#21512;&#25104;&#25968;&#25454;&#30340;$\epsilon$&#19979;&#38480;&#20272;&#35745;&#30340;&#24037;&#20316;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24314;&#35758;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#26159;&#23454;&#29616;&#26368;&#20248;&#22122;&#22768;&#27700;&#24179;$\epsilon$&#30340;&#26368;&#21512;&#36866;&#26041;&#27861;&#12290;&#20316;&#20026;&#25968;&#25454;&#38598;&#27987;&#32553;&#20869;&#37096;&#21160;&#24577;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20445;&#35777;&#39640;&#20445;&#30495;&#25968;&#25454;&#21516;&#26102;&#25552;&#20379;&#38544;&#31169;&#24615;&#30340;&#22122;&#22768;&#20272;&#35745;&#28385;&#24847;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10423v1 Announce Type: cross  Abstract: Our work focuses on understanding the underpinning mechanism of dataset condensation by drawing connections with ($\epsilon$, $\delta$)-differential privacy where the optimal noise, $\epsilon$, is chosen by adversarial uncertainty \cite{Grining2017}. We can answer the question about the inner workings of the dataset condensation procedure. Previous work \cite{dong2022} proved the link between dataset condensation (DC) and ($\epsilon$, $\delta$)-differential privacy. However, it is unclear from existing works on ablating DC to obtain a lower-bound estimate of $\epsilon$ that will suffice for creating high-fidelity synthetic data. We suggest that adversarial uncertainty is the most appropriate method to achieve an optimal noise level, $\epsilon$. As part of the internal dynamics of dataset condensation, we adopt a satisfactory scheme for noise estimation that guarantees high-fidelity data while providing privacy.
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#20204;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#20877;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#35780;&#20272;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#20854;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10416</link><description>&lt;p&gt;
&#23558;&#20851;&#20110;&#20449;&#24565;&#30340;&#35821;&#35328;&#25509;&#22320;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Grounding Language about Belief in a Bayesian Theory-of-Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10416
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#20204;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#20877;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#35780;&#20272;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#20854;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20449;&#24565;&#26159;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#20154;&#31867;&#24120;&#24120;&#20351;&#29992;&#20016;&#23500;&#30340;&#32452;&#21512;&#35821;&#35328;&#26469;&#25551;&#36848;&#20182;&#20154;&#30340;&#24819;&#27861;&#21644;&#30693;&#35782;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#20449;&#24565;&#38472;&#36848;&#30340;&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#20026;&#35299;&#37322;&#20154;&#31867;&#22914;&#20309;&#35299;&#37322;&#20182;&#20154;&#38544;&#34255;&#30340;&#35748;&#35782;&#20869;&#23481;&#36808;&#20986;&#20102;&#19968;&#27493;&#65306;&#36890;&#36807;&#24314;&#27169;&#20154;&#31867;&#22914;&#20309;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#19968;&#20010;&#20195;&#29702;&#20154;&#34892;&#21160;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#28982;&#21518;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#23545;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#20449;&#24565;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#35282;&#33394;&#35821;&#20041;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20154;&#20204;&#22312;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#20154;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#22914;&#20309;&#24402;&#22240;&#30446;&#26631;&#21644;&#20449;&#24565;&#30340;&#26469;&#35780;&#20272;&#36825;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10416v1 Announce Type: new  Abstract: Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others' beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.10412</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#21152;&#26435;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;LLM&#22312;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#30340;&#34394;&#26500;
&lt;/p&gt;
&lt;p&gt;
Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10412
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#20107;&#23454;&#19981;&#27491;&#30830;&#20294;&#30475;&#20284;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#65292;&#30446;&#21069;&#26159;LLM&#21487;&#20449;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#20854;&#36827;&#34892;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#38656;&#35201;&#20855;&#26377;&#20855;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21363;&#20154;&#31867;&#32534;&#20889;&#30340;&#8220;&#26368;&#20339;&#8221;&#25110;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#36825;&#31181;&#35201;&#27714;&#20351;&#24187;&#35273;&#27979;&#37327;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21152;&#26435;LLM&#23545;&#20107;&#23454;&#24615;&#36827;&#34892;&#35780;&#20272;&#65288;FEWL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#37329;&#26631;&#20934;&#31572;&#26696;&#32570;&#22833;&#26102;&#35774;&#35745;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#12290;FEWL&#21033;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#31572;&#26696;&#20316;&#20026;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#20195;&#29702;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#37327;&#21270;&#21442;&#32771;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;FEWL&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#23427;&#26356;&#20934;&#30830;&#12290;&#24230;&#37327;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10409</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#35770;&#25991;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#30740;&#31350;&#25345;&#32493;&#36827;&#34892;&#65292;&#38590;&#20197;&#36319;&#19978;&#26032;&#30340;&#30740;&#31350;&#21644;&#27169;&#22411;&#12290;&#20026;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#32508;&#21512;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#35768;&#22810;&#20154;&#20889;&#20102;&#35843;&#30740;&#35770;&#25991;&#65292;&#20294;&#21363;&#20351;&#36825;&#20123;&#35770;&#25991;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;&#35843;&#30740;&#35770;&#25991;&#20998;&#37197;&#21040;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;144&#31687;LLM&#35843;&#30740;&#35770;&#25991;&#30340;&#20803;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#19977;&#31181;&#33539;&#20363;&#26469;&#23545;&#20998;&#31867;&#27861;&#20869;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20004;&#20010;&#33539;&#20363;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;; &#20351;&#29992;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#24179;&#22343;&#20154;&#31867;&#35782;&#21035;&#27700;&#24179;&#65292;&#24182;&#19988;&#21033;&#29992;&#36739;&#23567;&#27169;&#22411;&#29983;&#25104;&#30340;&#24369;&#26631;&#31614;&#26469;&#24494;&#35843;LLMs&#65288;&#26412;&#30740;&#31350;&#20013;&#30340;GCN&#31561;&#65289;&#21487;&#33021;&#27604;&#20351;&#29992;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#26356;&#26377;&#25928;&#65292;&#25581;&#31034;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35270;&#35273;&#20998;&#26512;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#35299;&#37322;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36880;&#27493;&#29983;&#25104;&#36755;&#20986;&#24182;&#24378;&#35843;&#19982;&#22522;&#30784;&#35270;&#35273;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10404</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#20998;&#26512;&#35299;&#37322;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Explaining generative diffusion models via visual analysis for interpretable decision-making process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#20998;&#26512;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#35299;&#37322;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36880;&#27493;&#29983;&#25104;&#36755;&#20986;&#24182;&#24378;&#35843;&#19982;&#22522;&#30784;&#35270;&#35273;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#35299;&#37322;&#25193;&#25955;&#36807;&#31243;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31995;&#21015;&#38590;&#20197;&#35299;&#37322;&#30340;&#21435;&#22122;&#22270;&#20687;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#29983;&#25104;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#27169;&#22411;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20851;&#27880;&#30340;&#21306;&#22495;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#26469;&#35299;&#37322;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#21487;&#35270;&#21270;&#25193;&#25955;&#36807;&#31243;&#21644;&#22238;&#31572;&#19978;&#36848;&#30740;&#31350;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#20351;&#25193;&#25955;&#36807;&#31243;&#26131;&#20110;&#20154;&#29702;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#21508;&#31181;&#35270;&#35273;&#20998;&#26512;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35299;&#37322;&#20102;&#25193;&#25955;&#36807;&#31243;&#20013;&#36755;&#20986;&#26159;&#22914;&#20309;&#36880;&#28176;&#29983;&#25104;&#30340;&#65292;&#24378;&#35843;&#20102;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21435;&#22122;&#31243;&#24230;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#20102;&#19982;&#22522;&#30784;&#35270;&#35273;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10404v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable performance in generation tasks. Nevertheless, explaining the diffusion process remains challenging due to it being a sequence of denoising noisy images that are difficult for experts to interpret. To address this issue, we propose the three research questions to interpret the diffusion process from the perspective of the visual concepts generated by the model and the region where the model attends in each time step. We devise tools for visualizing the diffusion process and answering the aforementioned research questions to render the diffusion process human-understandable. We show how the output is progressively generated in the diffusion process by explaining the level of denoising and highlighting relationships to foundational visual concepts at each time step through the results of experiments with various visual analyses using the tools. Throughout the training of the diffusion model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#36827;&#21270;&#35770;&#19981;&#20165;&#36866;&#29992;&#20110;&#22522;&#22240;&#65292;&#20063;&#36866;&#29992;&#20110;&#23384;&#20648;&#22312;&#22823;&#33041;&#20013;&#30340;&#27169;&#22240;&#21644;&#35745;&#31639;&#26426;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#26412;&#20070;&#25506;&#35752;&#20102;&#36825;&#19968;&#26222;&#36941;&#30340;&#36827;&#21270;&#29702;&#35770;&#23545;&#33258;&#28982;&#12289;&#31038;&#20250;&#12289;&#25991;&#21270;&#21644;&#20010;&#20307;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10393</link><description>&lt;p&gt;
&#36798;&#23572;&#25991; &#22270;&#28789; &#37011;&#37329;&#26031;&#65306;&#24314;&#31435;&#19968;&#20010;&#26222;&#36941;&#30340;&#36827;&#21270;&#35770;
&lt;/p&gt;
&lt;p&gt;
Darwin Turing Dawkins: Building a General Theory of Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10393
&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35770;&#19981;&#20165;&#36866;&#29992;&#20110;&#22522;&#22240;&#65292;&#20063;&#36866;&#29992;&#20110;&#23384;&#20648;&#22312;&#22823;&#33041;&#20013;&#30340;&#27169;&#22240;&#21644;&#35745;&#31639;&#26426;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#26412;&#20070;&#25506;&#35752;&#20102;&#36825;&#19968;&#26222;&#36941;&#30340;&#36827;&#21270;&#29702;&#35770;&#23545;&#33258;&#28982;&#12289;&#31038;&#20250;&#12289;&#25991;&#21270;&#21644;&#20010;&#20307;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#12289;&#35745;&#31639;&#26426;&#12289;&#31038;&#20250;&#65292;&#29978;&#33267;&#20070;&#31821;&#37117;&#26159;&#21442;&#19982;&#19968;&#20010;&#23439;&#22823;&#30340;&#36827;&#21270;&#29983;&#23384;&#26007;&#20105;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#22330;&#29983;&#23384;&#26007;&#20105;&#22609;&#36896;&#20102;&#33258;&#28982;&#12289;&#22269;&#23478;&#12289;&#23447;&#25945;&#12289;&#33402;&#26415;&#12289;&#31185;&#23398;&#20197;&#21450;&#20320;&#33258;&#24049;&#12290;&#20320;&#30340;&#24605;&#24819;&#12289;&#24863;&#35273;&#21644;&#34892;&#20026;&#37117;&#21463;&#20854;&#24433;&#21709;&#12290;&#36798;&#23572;&#25991;&#36827;&#21270;&#35770;&#19981;&#20165;&#36866;&#29992;&#20110;&#23384;&#20648;&#22312;DNA&#20013;&#30340;&#22522;&#22240;&#12290;&#36816;&#29992;&#33406;&#20262;&#183;&#22270;&#28789;&#21644;&#29702;&#26597;&#24503;&#183;&#36947;&#37329;&#26031;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#23558;&#30475;&#21040;&#23427;&#20063;&#36866;&#29992;&#20110;&#23384;&#20648;&#22312;&#25105;&#20204;&#22823;&#33041;&#20013;&#30340;&#27169;&#22240;&#21644;&#23384;&#20648;&#22312;&#35745;&#31639;&#26426;&#20013;&#30340;&#20449;&#24687;&#12290;&#19979;&#27425;&#20320;&#31454;&#36873;&#24635;&#32479;&#12289;&#21442;&#19982;&#25112;&#20105;&#65292;&#25110;&#32773;&#21482;&#26159;&#22788;&#29702;&#20154;&#31867;&#26222;&#36941;&#38382;&#39064;&#26102;&#65292;&#20063;&#35768;&#36825;&#26412;&#20070;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;&#22914;&#26524;&#20320;&#24819;&#20102;&#35299;&#20026;&#20160;&#20040;&#20197;&#21450;&#20309;&#26102;&#20250;&#27515;&#20129;&#65292;&#25110;&#32773;&#24819;&#35201;&#21462;&#24471;&#20255;&#22823;&#25104;&#23601;&#65292;&#36825;&#26412;&#20070;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;&#22914;&#26524;&#20320;&#20851;&#24515;&#35745;&#31639;&#26426;&#38761;&#21629;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#36825;&#26412;&#20070;&#21487;&#33021;&#20250;&#25552;&#20379;&#19968;&#20123;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10393v1 Announce Type: cross  Abstract: Living things, computers, societies, and even books are part of a grand evolutionary struggle to survive. That struggle shapes nature, nations, religions, art, science, and you. What you think, feel, and do is determined by it. Darwinian evolution does not apply solely to the genes that are stored in DNA. Using the insights of Alan Turing and Richard Dawkins, we will see that it also applies to the memes we store in our brains and the information we store in our computers. The next time you run for president, fight a war, or just deal with the ordinary problems humans are heir to, perhaps this book will be of use. If you want to understand why and when you will die, or if you want to achieve greatness this book may help. If you are concerned about where the computer revolution is headed, this book may provide some answers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#40784;&#39564;&#35777;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#30340;&#22522;&#30784;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.10392</link><description>&lt;p&gt;
&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pretext Training Algorithms for Event Sequence Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#40784;&#39564;&#35777;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#30340;&#22522;&#30784;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#29305;&#21270;&#30340;&#26032;&#22411;&#23545;&#40784;&#39564;&#35777;&#20219;&#21153;&#65292;&#20511;&#37492;&#20102;&#25513;&#30721;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#33391;&#22909;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#37322;&#25918;&#22522;&#30784;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#30340;&#19979;&#19968;&#20107;&#20214;&#39044;&#27979;&#65292;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#21644;&#32570;&#22833;&#20107;&#20214;&#25554;&#20540;&#12290;&#23545;&#27969;&#34892;&#30340;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10392v1 Announce Type: cross  Abstract: Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.
&lt;/p&gt;</description></item><item><title>UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10381</link><description>&lt;p&gt;
UMAIR-FPS&#65306;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10381
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#27493;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21160;&#28459;&#25554;&#30011;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#25554;&#30011;&#24050;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21160;&#28459;&#25512;&#33616;&#31995;&#32479;&#20391;&#37325;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#20294;&#20173;&#38656;&#35201;&#25972;&#21512;&#22270;&#20687;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#25512;&#33616;&#30740;&#31350;&#21463;&#21040;&#32039;&#23494;&#32806;&#21512;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#21160;&#28459;&#25554;&#30011;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;&#65288;UMAIR-FPS&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#23545;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#25105;&#20204;&#39318;&#27425;&#32467;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#19982;&#35821;&#20041;&#29305;&#24449;&#26469;&#26500;&#24314;&#21452;&#36755;&#20986;&#22270;&#20687;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#34920;&#31034;&#12290;&#23545;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#22522;&#20110;Fine-tuning Sentence-Transformers&#33719;&#24471;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10381v1 Announce Type: cross  Abstract: The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledg
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#31616;&#21333;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20301;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#31354;&#38388;&#20869;&#30340;&#21151;&#33021;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#36890;&#29992;&#24615;</title><link>https://arxiv.org/abs/2402.10380</link><description>&lt;p&gt;
&#23376;&#22270;&#32423;&#36890;&#29992;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Subgraph-level Universal Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10380
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#31616;&#21333;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20301;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#31354;&#38388;&#20869;&#30340;&#21151;&#33021;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#26469;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21464;&#24471;&#26085;&#30410;&#31361;&#20986;&#12290;&#36825;&#19968;&#36235;&#21183;&#22312;&#22270;&#39046;&#22495;&#29305;&#21035;&#26126;&#26174;&#65292;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#20026;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26041;&#27861;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38024;&#23545;&#20855;&#26377;&#36793;&#39044;&#27979;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#23450;&#21046;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#39044;&#35757;&#32451;&#31574;&#30053;&#20043;&#38388;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#36755;&#20837;&#22270;&#30340;&#29305;&#24449;&#31354;&#38388;&#20869;&#21457;&#25381;&#20316;&#29992;&#12290;&#36825;&#20351;&#20854;&#20174;&#29702;&#35770;&#19978;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#31867;&#22411;&#30340;&#25552;&#31034;&#20989;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10380v1 Announce Type: cross  Abstract: In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to ful
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#30528;&#25361;&#25112;&#21253;&#25324;&#20381;&#36182;&#21382;&#21490;&#25968;&#25454;&#38598;&#12289;&#27867;&#21270;&#22256;&#38590;&#12289;&#27169;&#22411;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#31561;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.10350</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10350
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#30528;&#25361;&#25112;&#21253;&#25324;&#20381;&#36182;&#21382;&#21490;&#25968;&#25454;&#38598;&#12289;&#27867;&#21270;&#22256;&#38590;&#12289;&#27169;&#22411;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#31561;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20840;&#38754;&#23457;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#31361;&#20986;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#12289;&#22266;&#26377;&#25361;&#25112;&#20197;&#21450;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;LLMs&#24050;&#32463;&#22312;&#35299;&#26512;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#27169;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#32508;&#36848;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20381;&#36182;&#20110;&#24222;&#22823;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#27169;&#22411;&#24187;&#35273;&#29616;&#35937;&#65292;&#27169;&#22411;&#30693;&#35782;&#36793;&#30028;&#30340;&#38480;&#21046;&#20197;&#21450;&#25152;&#38656;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#35813;&#32508;&#36848;&#35752;&#35770;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#31574;&#30053;&#65292;&#22914;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10350v1 Announce Type: cross  Abstract: This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal dat
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10340</link><description>&lt;p&gt;
&#35770;&#37096;&#32626;LLMs/VLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#31361;&#26174;&#39118;&#38505;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10340
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25152;&#28041;&#21450;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#25805;&#20316;&#65292;&#23548;&#33322;&#31561;&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#20250;&#24341;&#20837;&#26174;&#30528;&#30340;&#28431;&#27934;&#65292;&#21363;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs/VLMs&#19982;&#26426;&#22120;&#20154;&#30028;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#25805;&#32437;&#25110;&#35823;&#23548;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#24694;&#24847;&#25915;&#20987;&#31034;&#20363;&#65292;&#24182;&#23545;&#38598;&#25104;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#30693;&#21517;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;&#21253;&#25324;KnowNo VIMA&#21644;Instruct2Act&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HI-GAN&#30340;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#65292;&#36890;&#36807;&#19977;&#20010;GAN&#20197;&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#24335;&#36827;&#34892;RGBD&#20462;&#22797;&#65292;&#20854;&#20013;EdgeGAN&#21644;LabelGAN&#20998;&#21035;&#20462;&#22797;&#36974;&#32617;&#36793;&#32536;&#21644;&#20998;&#21106;&#26631;&#31614;&#22270;&#20687;&#65292;&#32780;CombinedRGBD-GAN&#32467;&#21512;&#23427;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#36755;&#20986;&#24182;&#36827;&#34892;RGB&#21644;&#28145;&#24230;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.10334</link><description>&lt;p&gt;
HI-GAN&#65306;&#20855;&#26377;&#36741;&#21161;&#36755;&#20837;&#30340;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#29992;&#20110;&#28151;&#21512;RGB&#21644;&#28145;&#24230;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10334
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HI-GAN&#30340;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#65292;&#36890;&#36807;&#19977;&#20010;GAN&#20197;&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#24335;&#36827;&#34892;RGBD&#20462;&#22797;&#65292;&#20854;&#20013;EdgeGAN&#21644;LabelGAN&#20998;&#21035;&#20462;&#22797;&#36974;&#32617;&#36793;&#32536;&#21644;&#20998;&#21106;&#26631;&#31614;&#22270;&#20687;&#65292;&#32780;CombinedRGBD-GAN&#32467;&#21512;&#23427;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#36755;&#20986;&#24182;&#36827;&#34892;RGB&#21644;&#28145;&#24230;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#22797;&#28041;&#21450;&#22635;&#34917;&#22270;&#20687;&#20013;&#20002;&#22833;&#30340;&#20687;&#32032;&#25110;&#21306;&#22495;&#65292;&#36825;&#26159;&#28151;&#21512;&#29616;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#20943;&#23569;&#29616;&#23454;&#65288;DR&#65289;&#20013;&#65292;&#20854;&#20013;&#20174;&#29992;&#25143;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#21024;&#38500;&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25968;&#23383;&#26367;&#25442;&#25216;&#26415;&#65292;&#38656;&#35201;&#22810;&#20010;&#25668;&#20687;&#22836;&#24182;&#20135;&#29983;&#39640;&#25104;&#26412;&#12290;AR&#35774;&#22791;&#21644;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;ToF&#28145;&#24230;&#20256;&#24863;&#22120;&#25429;&#33719;&#19982;RGB&#22270;&#20687;&#23545;&#40784;&#30340;&#22330;&#26223;&#28145;&#24230;&#22270;&#12290;&#23613;&#31649;&#36895;&#24230;&#24555;&#19988;&#20215;&#26684;&#23454;&#24800;&#65292;&#20294;ToF&#30456;&#26426;&#20250;&#20135;&#29983;&#20855;&#26377;&#20002;&#22833;&#20687;&#32032;&#30340;&#19981;&#23436;&#32654;&#28145;&#24230;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#65288;HI-GAN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#30001;&#19977;&#20010;&#20197;&#23618;&#27425;&#32467;&#26500;&#26041;&#24335;&#32452;&#25104;&#30340;GAN&#26500;&#25104;&#65292;&#29992;&#20110;RGBD&#20462;&#22797;&#12290;EdgeGAN&#21644;LabelGAN&#20998;&#21035;&#20462;&#22797;&#36974;&#32617;&#36793;&#32536;&#21644;&#20998;&#21106;&#26631;&#31614;&#22270;&#20687;&#65292;&#32780;CombinedRGBD-GAN&#32467;&#21512;&#23427;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#36755;&#20986;&#24182;&#36827;&#34892;RGB&#21644;&#28145;&#24230;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10334v1 Announce Type: cross  Abstract: Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user's visual environment. Existing methods rely on digital replacement techniques which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address the above challenges, we propose Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and Depth inpainting. Edge images and particularly
&lt;/p&gt;</description></item><item><title>LAVE&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25552;&#20379;LLM&#21160;&#21147;&#30340;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#32534;&#36753;&#21151;&#33021;&#65292;&#20943;&#23569;&#35270;&#39057;&#32534;&#36753;&#30340;&#38556;&#30861;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.10294</link><description>&lt;p&gt;
LAVE&#65306;&#20197;LLM&#20026;&#21160;&#21147;&#30340;&#35270;&#39057;&#32534;&#36753;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10294
&lt;/p&gt;
&lt;p&gt;
LAVE&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25552;&#20379;LLM&#21160;&#21147;&#30340;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#32534;&#36753;&#21151;&#33021;&#65292;&#20943;&#23569;&#35270;&#39057;&#32534;&#36753;&#30340;&#38556;&#30861;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21046;&#20316;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#32534;&#36753;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#21162;&#21147;&#24120;&#24120;&#23545;&#21021;&#23398;&#32773;&#26500;&#25104;&#38556;&#30861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35270;&#39057;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20943;&#23569;&#36825;&#20123;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#29702;&#24565;&#20307;&#29616;&#22312;LAVE&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#25552;&#20379;LLM&#21160;&#21147;&#30340;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#32534;&#36753;&#21151;&#33021;&#30340;&#26032;&#39062;&#31995;&#32479;&#12290;LAVE&#33258;&#21160;&#29983;&#25104;&#29992;&#25143;&#32032;&#26448;&#30340;&#35821;&#35328;&#25551;&#36848;&#65292;&#20316;&#20026;&#20351;LLM&#33021;&#22815;&#22788;&#29702;&#35270;&#39057;&#24182;&#21327;&#21161;&#32534;&#36753;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#24403;&#29992;&#25143;&#25552;&#20379;&#32534;&#36753;&#30446;&#26631;&#26102;&#65292;&#20195;&#29702;&#35745;&#21010;&#24182;&#25191;&#34892;&#30456;&#20851;&#21160;&#20316;&#20197;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;LAVE&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20195;&#29702;&#25110;&#30452;&#25509;UI&#25805;&#20316;&#32534;&#36753;&#35270;&#39057;&#65292;&#25552;&#20379;&#28789;&#27963;&#24615;&#24182;&#20351;&#20195;&#29702;&#21160;&#20316;&#33021;&#22815;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#21253;&#25324;&#20102;&#20174;&#21021;&#23398;&#32773;&#21040;&#29087;&#32451;&#32534;&#36753;&#32773;&#30340;&#20843;&#21517;&#21442;&#19982;&#32773;&#65292;&#35777;&#26126;&#20102;LAVE&#23545;&#20110;&#20943;&#23569;&#32534;&#36753;&#38556;&#30861;&#21644;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10294v1 Announce Type: cross  Abstract: Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30528;&#30524;&#20110;&#25506;&#32034;&#20026;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#20197;&#20415;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#36873;&#25321;&#33391;&#22909;&#34892;&#21160;&#30340;AI&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.10290</link><description>&lt;p&gt;
&#30528;&#37325;&#20110;&#20026;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Experiments with Encoding Structured Data for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30528;&#30524;&#20110;&#25506;&#32034;&#20026;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#20197;&#20415;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#36873;&#25321;&#33391;&#22909;&#34892;&#21160;&#30340;AI&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;AI&#20195;&#29702;&#65292;&#33021;&#22815;&#22312;&#21517;&#20026;&#25112;&#22330;&#30340;&#28216;&#25103;&#39046;&#22495;&#20013;&#36873;&#25321;&#33391;&#22909;&#30340;&#34892;&#21160;&#12290;&#31867;&#20284;&#25112;&#22330;&#30340;&#39034;&#24207;&#39046;&#22495;&#26159;&#35268;&#21010;&#38382;&#39064;&#30340;&#37325;&#35201;&#27979;&#35797;&#24179;&#21488;&#65292;&#22240;&#27492;&#22269;&#38450;&#37096;&#20351;&#29992;&#36825;&#20123;&#39046;&#22495;&#36827;&#34892;&#25112;&#20105;&#28436;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#20195;&#29702;&#32467;&#21512;&#20102;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#25216;&#26415;&#65292;&#35797;&#22270;&#22312;&#28216;&#25103;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#36991;&#24320;&#38556;&#30861;&#65292;&#19982;&#23545;&#25163;&#20114;&#21160;&#24182;&#22842;&#26071;&#12290;&#26412;&#25991;&#23558;&#37325;&#28857;&#20851;&#27880;&#25105;&#20204;&#25506;&#32034;&#30340;&#32534;&#30721;&#25216;&#26415;&#65292;&#20197;&#23637;&#31034;&#23384;&#20648;&#22312;Python&#31867;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#29702;&#30340;&#24517;&#35201;&#21069;&#25552;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10290v1 Announce Type: new  Abstract: The project's aim is to create an AI agent capable of selecting good actions in a game-playing domain called Battlespace. Sequential domains like Battlespace are important testbeds for planning problems, as such, the Department of Defense uses such domains for wargaming exercises. The agents we developed combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN) techniques in an effort to navigate the game environment, avoid obstacles, interact with adversaries, and capture the flag. This paper will focus on the encoding techniques we explored to present complex structured data stored in a Python class, a necessary precursor to an agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#33391;&#24615;&#27491;&#24120;&#25968;&#25454;&#20013;&#21046;&#20316;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#65292;&#25104;&#21151;&#22949;&#21327;&#20102;&#20004;&#20010;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10283</link><description>&lt;p&gt;
&#23545;&#19968;&#31867;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack against One-Class Sequential Anomaly Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#33391;&#24615;&#27491;&#24120;&#25968;&#25454;&#20013;&#21046;&#20316;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#65292;&#25104;&#21151;&#22949;&#21327;&#20102;&#20004;&#20010;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10283v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028; &#25688;&#35201;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#31181;&#20851;&#38190;&#30340;&#23433;&#20840;&#23041;&#32961; - &#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#26469;&#22949;&#21327;&#28145;&#24230;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#25915;&#20987;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65292;&#35302;&#21457;&#22120;&#29983;&#25104;&#21644;&#21518;&#38376;&#27880;&#20837;&#12290; &#35302;&#21457;&#22120;&#29983;&#25104;&#26159;&#36890;&#36807;&#20174;&#33391;&#24615;&#27491;&#24120;&#25968;&#25454;&#20013;&#21046;&#20316;&#25200;&#21160;&#26679;&#26412;&#26469;&#23548;&#20986;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#35302;&#21457;&#22120;&#65292;&#20854;&#20013;&#25200;&#21160;&#26679;&#26412;&#20173;&#28982;&#27491;&#24120;&#12290; &#21518;&#38376;&#27880;&#20837;&#21017;&#26159;&#36866;&#24403;&#22320;&#23558;&#21518;&#38376;&#35302;&#21457;&#22120;&#27880;&#20837;&#27169;&#22411;&#65292;&#21482;&#20026;&#20855;&#26377;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#19978;&#27880;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10283v1 Announce Type: cross  Abstract: Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#65292;&#24182;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#26816;&#39564;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#24615;&#33021;&#20026;&#30740;&#31350;&#25552;&#20379;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.10248</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#21450;&#20854;&#30456;&#20851;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#65292;&#24182;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#26816;&#39564;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#24615;&#33021;&#20026;&#30740;&#31350;&#25552;&#20379;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#26159;&#19968;&#20010;&#36328;&#30028;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#20381;&#36182;&#31354;&#38388;&#31232;&#30095;&#19988;&#24322;&#26500;&#25918;&#32622;&#30340;&#30417;&#27979;&#31449;&#25968;&#25454;&#30340;&#24178;&#39044;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31449;&#28857;&#32463;&#24120;&#30001;&#20110;&#35832;&#22914;&#20572;&#30005;&#31561;&#38382;&#39064;&#32780;&#20986;&#29616;&#26102;&#38388;&#25968;&#25454;&#32570;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#34917;&#20805;&#32570;&#22833;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#27979;&#37327;&#25968;&#25454;&#65292;&#20174;&#32780;&#29983;&#25104;&#21253;&#25324;NO$_2$&#12289;O$_3$&#12289;PM$_{10}$&#12289;PM$_{2.5}$&#21644;SO$_2$&#31561;&#27745;&#26579;&#29289;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#27599;&#19968;&#20272;&#35745;&#20540;&#38468;&#24102;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20026;&#20381;&#36182;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#19979;&#28216;&#35780;&#20272;&#30340;&#24191;&#27867;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26381;&#21153;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10248v1 Announce Type: cross  Abstract: Global ambient air pollution, a transboundary challenge, is typically addressed through interventions relying on data from spatially sparse and heterogeneously placed monitoring stations. These stations often encounter temporal data gaps due to issues such as power outages. In response, we have developed a scalable, data-driven, supervised machine learning framework. This model is designed to impute missing temporal and spatial measurements, thereby generating a comprehensive dataset for pollutants including NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of 0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for each estimate, caters to a wide range of stakeholders relying on outdoor air pollution data for downstream assessments. This enables more detailed studies. Additionally, the model's performance across various geographical locations is examined, providing insights an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10240</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38382;&#39064;&#30340;&#21160;&#24577;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical View of the Question of Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#30001;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#38745;&#24577;&#35774;&#32622;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#24615;&#21644;&#21464;&#21270;&#30340;&#21457;&#23556;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#33539;&#24335;&#65292;&#30452;&#25509;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#26469;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#65292;&#24182;&#23558;&#20854;&#26500;&#36896;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#21253;&#25324;&#21508;&#31181;&#37325;&#35201;&#35774;&#32622;&#65292;&#22914;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#30456;&#24403;&#22797;&#26434;&#30340;&#23454;&#39564;&#21644;&#36890;&#36807;&#32431;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#21644;&#37327;&#21270;&#20102;&#22240;&#26524;&#32852;&#31995;&#65292;&#21542;&#21017;&#30475;&#20284;&#33707;&#21517;&#20854;&#22937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10240v1 Announce Type: cross  Abstract: We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#22810;&#26679;&#24615;&#25628;&#32034;&#12289;&#35838;&#31243;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#65292;&#33258;&#21160;&#25628;&#32034;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#33021;&#22815;&#33258;&#32452;&#32455;&#20986;&#20855;&#26377;&#22522;&#30784;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;&#30340;&#8220;&#20010;&#20307;&#8221;&#65292;&#20026;&#23547;&#25214;&#36825;&#31181;&#22522;&#26412;&#26426;&#26500;&#33258;&#32452;&#32455;&#30340;&#29615;&#22659;&#26465;&#20214;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10236</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26679;&#24615;&#25628;&#32034;&#22312;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#21457;&#29616;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Sensorimotor Agency in Cellular Automata using Diversity Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#22810;&#26679;&#24615;&#25628;&#32034;&#12289;&#35838;&#31243;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#65292;&#33258;&#21160;&#25628;&#32034;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#33021;&#22815;&#33258;&#32452;&#32455;&#20986;&#20855;&#26377;&#22522;&#30784;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;&#30340;&#8220;&#20010;&#20307;&#8221;&#65292;&#20026;&#23547;&#25214;&#36825;&#31181;&#22522;&#26412;&#26426;&#26500;&#33258;&#32452;&#32455;&#30340;&#29615;&#22659;&#26465;&#20214;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#29983;&#21629;&#30740;&#31350;&#39046;&#22495;&#30740;&#31350;&#31867;&#20284;&#29983;&#21629;&#29616;&#35937;&#30340;&#22914;&#33258;&#20027;&#29983;&#25104;&#12289;&#26426;&#26500;&#24615;&#25110;&#33258;&#25105;&#35843;&#33410;&#31561;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#22914;&#20309;&#33258;&#32452;&#32455;&#12290;&#22312;&#20803;&#32990;&#33258;&#21160;&#26426;&#65288;CA&#65289;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#20043;&#35868;&#26159;&#26159;&#21542;&#21487;&#33021;&#25214;&#21040;&#33021;&#22815;&#33258;&#32452;&#32455;&#20986;&#31283;&#20581;&#8220;&#20010;&#20307;&#8221;&#30340;&#29615;&#22659;&#35268;&#21017;&#65292;&#32780;&#36825;&#20123;&#20010;&#20307;&#22312;&#21021;&#22987;&#29366;&#24577;&#19979;&#27809;&#26377;&#8220;&#36523;&#20307;&#8221;&#12289;&#8220;&#22823;&#33041;&#8221;&#12289;&#8220;&#24863;&#30693;&#8221;&#25110;&#8220;&#34892;&#21160;&#8221;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32467;&#21512;&#22810;&#26679;&#24615;&#25628;&#32034;&#12289;&#35838;&#31243;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#65292;&#33258;&#21160;&#25628;&#32034;&#36825;&#20123;&#8220;&#20010;&#20307;&#8221;&#65292;&#21363;&#33021;&#22815;&#31227;&#21160;&#24182;&#26377;&#33021;&#21147;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#23545;&#22806;&#37096;&#38556;&#30861;&#20570;&#20986;&#21453;&#24212;&#19988;&#20445;&#25345;&#23436;&#25972;&#24615;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#20174;&#32780;&#24418;&#25104;&#22522;&#30784;&#24418;&#24335;&#30340;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#31995;&#32479;&#22320;&#25214;&#21040;&#22312;CA&#20013;&#23548;&#33268;&#36825;&#31181;&#22522;&#26412;&#26426;&#26500;&#33258;&#32452;&#32455;&#30340;&#29615;&#22659;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10236v1 Announce Type: cross  Abstract: The research field of Artificial Life studies how life-like phenomena such as autopoiesis, agency, or self-regulation can self-organize in computer simulations. In cellular automata (CA), a key open-question has been whether it it is possible to find environment rules that self-organize robust "individuals" from an initial state with no prior existence of things like "bodies", "brain", "perception" or "action". In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, curriculum learning and gradient descent, to automate the search of such "individuals", i.e. localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. We show that this approach enables to find systematically environmental conditions in CA leading to self-organization of such basic forms of agency. Th
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Ripple-Down Rules&#65288;RDR&#65289;&#25193;&#23637;&#20102;ActorSim&#30446;&#26631;&#25512;&#29702;&#26694;&#26550;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#31034;&#33539;&#23398;&#20064;&#24182;&#24314;&#31435;&#26032;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;&#20197;&#20415;&#22312;&#26410;&#26469;&#27491;&#30830;&#22788;&#29702;&#31867;&#20284;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.10224</link><description>&lt;p&gt;
&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;&#30446;&#26631;&#25512;&#29702;&#19982;Ripple-Down&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Human-Centric Goal Reasoning with Ripple-Down Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Ripple-Down Rules&#65288;RDR&#65289;&#25193;&#23637;&#20102;ActorSim&#30446;&#26631;&#25512;&#29702;&#26694;&#26550;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#31034;&#33539;&#23398;&#20064;&#24182;&#24314;&#31435;&#26032;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;&#20197;&#20415;&#22312;&#26410;&#26469;&#27491;&#30830;&#22788;&#29702;&#31867;&#20284;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ActorSim&#26159;&#22312;&#28023;&#20891;&#30740;&#31350;&#23454;&#39564;&#23460;&#24320;&#21457;&#30340;&#30446;&#26631;&#25512;&#29702;&#26694;&#26550;&#12290;&#26368;&#21021;&#65292;&#25152;&#26377;&#30446;&#26631;&#25512;&#29702;&#35268;&#21017;&#37117;&#26159;&#25163;&#24037;&#21046;&#20316;&#30340;&#12290;&#26412;&#20316;&#21697;&#36890;&#36807;&#23637;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#25193;&#23637;&#20102;ActorSim&#65292;&#21363;&#24403;&#20154;&#31867;&#35757;&#32451;&#21592;&#19982;&#31995;&#32479;&#30340;&#20915;&#23450;&#19981;&#31526;&#26102;&#65292;&#35757;&#32451;&#21592;&#21487;&#20197;&#25509;&#31649;&#24182;&#21521;&#31995;&#32479;&#23637;&#31034;&#27491;&#30830;&#20915;&#31574;&#12290;&#23398;&#20064;&#32452;&#20214;&#20351;&#29992;Ripple-Down Rules&#65288;RDR&#65289;&#26500;&#24314;&#26032;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;&#20197;&#27491;&#30830;&#22788;&#29702;&#26410;&#26469;&#31867;&#20284;&#24773;&#20917;&#12290;&#35813;&#31995;&#32479;&#22312;RoboCup Rescue Agent Simulation&#20013;&#23637;&#31034;&#65292;&#35813;&#27169;&#25311;&#22120;&#27169;&#25311;&#20102;&#19968;&#20010;&#20840;&#24066;&#33539;&#22260;&#30340;&#28798;&#38590;&#65292;&#38656;&#35201;&#32039;&#24613;&#26381;&#21153;&#65292;&#21253;&#25324;&#28040;&#38450;&#12289;&#25937;&#25252;&#36710;&#21644;&#35686;&#23519;&#65292;&#27966;&#24448;&#19981;&#21516;&#22320;&#28857;&#20174;&#21361;&#38505;&#24773;&#20917;&#20013;&#25764;&#31163;&#24179;&#27665;&#12290;RDRs&#23454;&#29616;&#22312;&#19968;&#20010;&#33050;&#26412;&#35821;&#35328;FrameScript&#20013;&#65292;&#29992;&#20110;&#22312;ActorSim&#21644;&#20195;&#29702;&#27169;&#25311;&#22120;&#20043;&#38388;&#36827;&#34892;&#35843;&#35299;&#12290;&#20351;&#29992;Ripple-Down Rules&#65292;ActorSim&#21487;&#20197;&#25193;&#23637;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10224v1 Announce Type: cross  Abstract: ActorSim is a goal reasoning framework developed at the Naval Research Laboratory. Originally, all goal reasoning rules were hand-crafted. This work extends ActorSim with the capability of learning by demonstration, that is, when a human trainer disagrees with a decision made by the system, the trainer can take over and show the system the correct decision. The learning component uses Ripple-Down Rules (RDR) to build new decision rules to correctly handle similar cases in the future. The system is demonstrated using the RoboCup Rescue Agent Simulation, which simulates a city-wide disaster, requiring emergency services, including fire, ambulance and police, to be dispatched to different sites to evacuate civilians from dangerous situations. The RDRs are implemented in a scripting language, FrameScript, which is used to mediate between ActorSim and the agent simulator. Using Ripple-Down Rules, ActorSim can scale to an order of magnitude 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#20114;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#22810;&#20195;&#29702;&#36710;&#36742;&#22312;&#38754;&#20020;&#29615;&#22659;&#22240;&#32032;&#12289;&#38480;&#21046;&#21644;&#21512;&#20316;&#38382;&#39064;&#26102;&#23398;&#20250;&#27807;&#36890;&#21327;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#33258;&#20027;&#36710;&#36742;&#24033;&#36923;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10222</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#36710;&#36742;&#24033;&#36923;&#65306;&#23398;&#20064;&#27807;&#36890;&#21644;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicle Patrolling Through Deep Reinforcement Learning: Learning to Communicate and Cooperate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#20114;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#22810;&#20195;&#29702;&#36710;&#36742;&#22312;&#38754;&#20020;&#29615;&#22659;&#22240;&#32032;&#12289;&#38480;&#21046;&#21644;&#21512;&#20316;&#38382;&#39064;&#26102;&#23398;&#20250;&#27807;&#36890;&#21327;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#33258;&#20027;&#36710;&#36742;&#24033;&#36923;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#36866;&#29992;&#20110;&#36830;&#32493;&#21306;&#22495;&#24033;&#36923;&#38382;&#39064;&#12290;&#25214;&#21040;&#26368;&#20339;&#24033;&#36923;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#26410;&#30693;&#30340;&#29615;&#22659;&#22240;&#32032;&#65292;&#22914;&#39118;&#25110;&#22320;&#24418;&#65307;&#25110;&#33258;&#20027;&#36710;&#36742;&#30340;&#38480;&#21046;&#65292;&#22914;&#26377;&#38480;&#30340;&#30005;&#27744;&#23551;&#21629;&#25110;&#30828;&#20214;&#25925;&#38556;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24033;&#36923;&#22823;&#21306;&#22495;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#20195;&#29702;&#20849;&#21516;&#21327;&#35843;&#20854;&#34892;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24033;&#36923;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#36890;&#24120;&#38590;&#20197;&#25163;&#21160;&#23450;&#20041;&#26368;&#20339;&#21327;&#35843;&#31574;&#30053;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#29615;&#22659;&#22240;&#32032;&#12289;&#20195;&#29702;&#38480;&#21046;&#21644;&#19977;&#31181;&#20856;&#22411;&#21512;&#20316;&#38382;&#39064;--&#36991;&#25758;&#12289;&#36991;&#35753;&#25317;&#25380;&#12289;&#24033;&#36923;&#30446;&#26631;&#21327;&#21830;&#30340;&#24033;&#36923;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#20114;&#20195;&#23398;&#20064;&#65288;RIAL&#65289;&#26041;&#27861;&#30340;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20195;&#29702;&#34987;&#35757;&#32451;&#20986;&#21457;&#23637;&#20182;&#20204;&#33258;&#24049;&#30340;&#36890;&#20449;&#21327;&#35758;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10222v1 Announce Type: cross  Abstract: Autonomous vehicles are suited for continuous area patrolling problems. Finding an optimal patrolling strategy can be challenging due to unknown environmental factors, such as wind or landscape; or autonomous vehicles' constraints, such as limited battery life or hardware failures. Importantly, patrolling large areas often requires multiple agents to collectively coordinate their actions. However, an optimal coordination strategy is often non-trivial to be manually defined due to the complex nature of patrolling environments. In this paper, we consider a patrolling problem with environmental factors, agent limitations, and three typical cooperation problems -- collision avoidance, congestion avoidance, and patrolling target negotiation. We propose a multi-agent reinforcement learning solution based on a reinforced inter-agent learning (RIAL) method. With this approach, agents are trained to develop their own communication protocol to c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65288;WLSC&#65289;&#65292;&#20854;&#28508;&#22312;&#34920;&#31034;&#26159;&#38544;&#21547;&#30340;&#65292;&#29992;&#20110;&#23545;&#32858;&#31867;&#24402;&#32435;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10213</link><description>&lt;p&gt;
&#20351;&#29992;&#23637;&#24320;&#32593;&#32476;&#23545;&#32858;&#31867;&#24402;&#32435;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Clustering Inductive Biases with Unrolled Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65288;WLSC&#65289;&#65292;&#20854;&#28508;&#22312;&#34920;&#31034;&#26159;&#38544;&#21547;&#30340;&#65292;&#29992;&#20110;&#23545;&#32858;&#31867;&#24402;&#32435;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#31232;&#30095;&#32534;&#30721;&#65288;SC&#65289;&#27169;&#22411;&#23558;&#35270;&#35273;&#21050;&#28608;&#34920;&#31034;&#20026;&#23569;&#37327;&#23398;&#20064;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#22312;&#23545;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36825;&#20123;&#22522;&#20989;&#25968;&#31867;&#20284;&#20110;Gabor&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#31232;&#30095;&#32534;&#30721;&#23398;&#20064;&#30340;&#31867;Gabor&#28388;&#27874;&#22120;&#36828;&#36828;&#36229;&#36807;&#20102;&#23454;&#38469;&#35266;&#23519;&#21040;&#30340;&#31616;&#21333;&#32454;&#32990;&#24863;&#21463;&#37326;&#36718;&#24275;&#30340;&#33391;&#22909;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65288;WLSC&#65289;&#65292;&#20854;&#28508;&#22312;&#34920;&#31034;&#26159;&#38544;&#21547;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10213v1 Announce Type: cross  Abstract: The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#26410;&#26631;&#35760;&#27979;&#35797;&#22270;&#20687;&#26469;&#35843;&#25972;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#30452;&#25509;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#30340;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#24182;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#22495;&#32479;&#35745;&#20272;&#35745;&#30340;&#39044;&#27979;&#36827;&#34892;&#38598;&#25104;&#65292;&#24182;&#22522;&#20110;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.09604</link><description>&lt;p&gt;
&#20351;&#29992;InTEnt&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#22522;&#20110;&#38598;&#25104;&#29109;&#21152;&#26435;&#30340;&#21333;&#22270;&#20687;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#26410;&#26631;&#35760;&#27979;&#35797;&#22270;&#20687;&#26469;&#35843;&#25972;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#30452;&#25509;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#30340;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#24182;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#22495;&#32479;&#35745;&#20272;&#35745;&#30340;&#39044;&#27979;&#36827;&#34892;&#38598;&#25104;&#65292;&#24182;&#22522;&#20110;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#25351;&#22312;&#27979;&#35797;&#26399;&#38388;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#35843;&#25972;&#21040;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;TTA&#25216;&#26415;&#20381;&#36182;&#20110;&#22312;&#21516;&#19968;&#39046;&#22495;&#20855;&#26377;&#22810;&#20010;&#27979;&#35797;&#22270;&#20687;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#65288;&#22914;&#21307;&#23398;&#25104;&#20687;&#65289;&#20013;&#65292;&#25968;&#25454;&#33719;&#21462;&#36153;&#29992;&#26114;&#36149;&#19988;&#25104;&#20687;&#26465;&#20214;&#32463;&#24120;&#21464;&#21270;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20351;&#29992;&#20165;&#26377;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#22270;&#20687;&#26469;&#35843;&#25972;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;TTA&#26041;&#27861;&#30452;&#25509;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#65292;&#28982;&#32780;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23427;&#20204;&#26410;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#23618;&#30340;&#32479;&#35745;&#37327;&#36873;&#25321;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#20294;&#19981;&#31283;&#23450;&#30340;&#22240;&#32032;&#65292;&#22240;&#20026;&#21482;&#26377;&#19968;&#20010;&#27979;&#35797;&#22495;&#31034;&#20363;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#22495;&#32479;&#35745;&#20272;&#35745;&#30340;&#39044;&#27979;&#36827;&#34892;\textit{&#38598;&#25104;}&#65292;&#24182;&#22522;&#20110;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09604v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing. Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently. Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image. Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example. To overcome this, we propose to instead \textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on
&lt;/p&gt;</description></item><item><title>WiMANS&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#21644;&#21516;&#27493;&#35270;&#39057;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#30740;&#31350;</title><link>https://arxiv.org/abs/2402.09430</link><description>&lt;p&gt;
WiMANS: WiFi-based&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09430
&lt;/p&gt;
&lt;p&gt;
WiMANS&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#21644;&#21516;&#27493;&#35270;&#39057;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi-based human sensing&#34920;&#29616;&#20986;&#20102;&#22312;&#19981;&#20405;&#20837;&#21644;&#26080;&#38656;&#35774;&#22791;&#30340;&#24773;&#20917;&#19979;&#20998;&#26512;&#29992;&#25143;&#34892;&#20026;&#30340;&#26174;&#30528;&#28508;&#21147;&#65292;&#20351;&#24471;&#26234;&#33021;&#23478;&#23621;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#24212;&#29992;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#29992;&#25143;&#24863;&#30693;&#19978;&#65292;&#22312;&#28041;&#21450;&#22810;&#29992;&#25143;&#22330;&#26223;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25506;&#35752;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;WiMANS&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#25968;&#25454;&#38598;&#12290;WiMANS&#21253;&#21547;&#36229;&#36807;9.4&#23567;&#26102;&#30340;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#65292;&#30417;&#27979;&#22810;&#20010;&#29992;&#25143;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#21516;&#26102;&#36827;&#34892;&#30340;&#27963;&#21160;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;WiMANS&#19981;&#20165;&#25910;&#38598;&#20102;&#21452;WiFi&#39057;&#27573;&#30340;CSI&#65292;&#36824;&#21253;&#25324;&#20102;&#21516;&#27493;&#35270;&#39057;&#12290;&#25105;&#20204;&#21033;&#29992;WiMANS&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09430v1 Announce Type: cross  Abstract: WiFi-based human sensing has exhibited remarkable potential to analyze user behaviors in a non-intrusive and device-free manner, benefiting applications as diverse as smart homes and healthcare. However, most previous works focus on single-user sensing, which has limited practicability in scenarios involving multiple users. Although recent studies have begun to investigate WiFi-based multi-user activity sensing, there remains a lack of benchmark datasets to facilitate reproducible and comparable research. To bridge this gap, we present WiMANS, to our knowledge, the first dataset for multi-user activity sensing based on WiFi. WiMANS contains over 9.4 hours of WiFi Channel State Information (CSI), monitoring simultaneous activities performed by multiple users in various environments. Compared to existing datasets, WiMANS not only collects the CSI of dual WiFi bands but also includes synchronized videos. We exploit WiMANS to benchmark the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#25110;&#24187;&#35273;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09346</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#25110;&#24187;&#35273;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#65292;&#35782;&#21035;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20559;&#35265;&#12289;&#19981;&#19968;&#33268;&#24615;&#21644;&#24187;&#35273;&#12290;&#23613;&#31649;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#23457;&#35745;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#24182;&#19981;&#23481;&#26131;&#35299;&#20915;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#26292;&#38706;&#20854;&#30693;&#35782;&#25110;&#36816;&#34892;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#25110;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#35201;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;&#36825;&#31181;&#23457;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#21487;&#38752;&#19988;&#33258;&#21160;&#21270;&#30340;&#29983;&#25104;&#36825;&#20123;&#25506;&#27979;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#26426;&#21327;&#21516;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#39564;&#35777;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#36991;&#20813;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#20381;&#36182;&#65292;&#24182;&#22686;&#21152;&#20102;&#31185;&#23398;&#20005;&#35880;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09346v1 Announce Type: new Abstract: As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;&#38544;&#21547;&#30340;&#32447;&#32034;&#65292;Puzzler&#36890;&#36807;&#32469;&#36807;LLM&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#22312;&#38388;&#25509;&#26041;&#24335;&#19979;&#23454;&#29616;&#20102;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;96.6%&#12290;</title><link>https://arxiv.org/abs/2402.09091</link><description>&lt;p&gt;
&#19982;LLM&#29609;&#29468;&#35868;&#28216;&#25103;: &#36890;&#36807;&#38544;&#21547;&#25552;&#31034;&#30340;&#38388;&#25509;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#38544;&#21547;&#30340;&#32447;&#32034;&#65292;Puzzler&#36890;&#36807;&#32469;&#36807;LLM&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#22312;&#38388;&#25509;&#26041;&#24335;&#19979;&#23454;&#29616;&#20102;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;96.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLM&#30340;&#21457;&#23637;&#65292;LLM&#30340;&#23433;&#20840;&#23041;&#32961;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#35768;&#22810;&#36234;&#29425;&#25915;&#20987;&#24050;&#32463;&#25552;&#20986;&#26469;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#38450;&#24481;&#12290;&#30446;&#21069;&#30340;&#36234;&#29425;&#25915;&#20987;&#20027;&#35201;&#20351;&#29992;&#22330;&#26223;&#20266;&#35013;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24694;&#24847;&#24847;&#22270;&#30340;&#26126;&#30830;&#25552;&#21450;&#24456;&#23481;&#26131;&#34987;LLM&#35782;&#21035;&#21644;&#38450;&#24481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;Puzzler&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38544;&#21547;&#22320;&#20026;LLM&#25552;&#20379;&#19968;&#20123;&#26377;&#20851;&#21407;&#22987;&#24694;&#24847;&#26597;&#35810;&#30340;&#25552;&#31034;&#26469;&#32469;&#36807;LLM&#30340;&#38450;&#24481;&#31574;&#30053;&#24182;&#33719;&#21462;&#24694;&#24847;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#21463;&#23385;&#23376;&#30340;&#12298;&#23385;&#23376;&#20853;&#27861;&#12299;&#20013;&#8220;&#24403;&#25915;&#26080;&#27861;&#25915;&#65292;&#23432;&#8221;&#26234;&#24935;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#38450;&#24481;&#23039;&#24577;&#26469;&#36890;&#36807;LLM&#25910;&#38598;&#20851;&#20110;&#21407;&#22987;&#24694;&#24847;&#26597;&#35810;&#30340;&#32447;&#32034;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Puzzler&#22312;&#38381;&#28304;LLM&#19978;&#30340;&#26597;&#35810;&#25104;&#21151;&#29575;&#20026;96.6%&#65292;&#27604;&#22522;&#20934;&#32447;&#39640;57.9%-82.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09091v1 Announce Type: cross Abstract: With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines. Furthermore, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#31867;&#20284;&#35774;&#35745;"&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#35770;&#25991;&#39564;&#35777;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08812</link><description>&lt;p&gt;
&#26234;&#33021;&#30011;&#24067;: &#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#31649;&#29702;&#23454;&#29616;&#31867;&#20284;&#35774;&#35745;&#30340;&#25506;&#32034;&#24615;&#21487;&#35270;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#31867;&#20284;&#35774;&#35745;"&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#35770;&#25991;&#39564;&#35777;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25968;&#25454;&#20998;&#26512;&#36890;&#36807;&#25506;&#32034;&#24615;&#30340;&#21487;&#35270;&#20998;&#26512;&#26041;&#27861;&#26469;&#23547;&#27714;&#24847;&#24819;&#19981;&#21040;&#30340;&#27934;&#35265;&#65292;&#24182;&#36229;&#36234;&#36923;&#36753;&#30340;&#36880;&#27493;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30028;&#38754;&#65288;&#22914;&#31508;&#35760;&#26412;&#21644;&#20202;&#34920;&#26495;&#65289;&#22312;&#21487;&#35270;&#25968;&#25454;&#20998;&#26512;&#30340;&#25506;&#32034;&#21644;&#27604;&#36739;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#31867;&#20284;&#35774;&#35745;&#8221;&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#23558;&#29983;&#25104;&#24335;AI&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25552;&#20379;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#30011;&#24067;&#30028;&#38754;&#20013;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65288;N=10&#65289;&#35780;&#20272;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08812v1 Announce Type: cross Abstract: Complex data analysis inherently seeks unexpected insights through exploratory \re{visual analysis} methods, transcending logical, step-by-step processing. However, \re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}. Addressing these limitations, we introduce a "design-like" intelligent canvas environment integrating generative AI into data analysis, offering rapid prototyping, iteration, and comparative visualization management. Our dual contributions include the integration of generative AI components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07616
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#37319;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#38656;&#35201;&#20445;&#30041;&#21382;&#21490;&#26631;&#35760;&#30340;&#38190;/&#20540;&#20449;&#24687;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#36991;&#20813;&#20887;&#20313;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#21442;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#38271;&#24230;&#32780;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#23384;&#20648;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;LLM&#65288;AnLLM&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#25104;&#38170;&#28857;&#26631;&#35760;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#24182;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AnLLM&#22312;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#39640;&#36798;99%&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;3.5&#20493;&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29306;&#29298;&#20102;&#19968;&#20123;&#20934;&#30830;&#24615;&#65292;AnLLM&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20381;&#28982;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLaM&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#65292;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#23454;&#29616;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#34394;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.06764</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#23545;&#40784;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;GLaM&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLaM&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#65292;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#23454;&#29616;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20174;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#27966;&#29983;&#30340;&#30693;&#35782;&#22270;&#38598;&#25104;&#65292;&#20195;&#34920;&#20102;&#26397;&#30528;&#26356;&#24378;&#22823;&#21644;&#20107;&#23454;&#25512;&#29702;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#34394;&#26500;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20114;&#36830;&#23454;&#20307;&#30340;&#39046;&#22495;&#19987;&#29992;&#22270;&#26102;&#65292;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#22635;&#34917;&#36825;&#19968;&#25216;&#26415;&#19978;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#36827;&#34892;&#32593;&#31449;&#40657;&#23458;&#25915;&#20987;&#65292;&#21253;&#25324;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#21453;&#39304;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#36171;&#20104;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.06664</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#40657;&#23458;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
LLM Agents can Autonomously Hack Websites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#36827;&#34892;&#32593;&#31449;&#40657;&#23458;&#25915;&#20987;&#65292;&#21253;&#25324;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#21453;&#39304;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#36171;&#20104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#29616;&#22312;&#21487;&#20197;&#19982;&#24037;&#20855;&#20132;&#20114;&#65288;&#21363;&#35843;&#29992;&#20989;&#25968;&#65289;&#12289;&#35835;&#21462;&#25991;&#26723;&#24182;&#36882;&#24402;&#35843;&#29992;&#33258;&#24049;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;LLMs&#29616;&#22312;&#21487;&#20197;&#33258;&#20027;&#20316;&#20026;&#20195;&#29702;&#20154;&#36816;&#20316;&#12290;&#38543;&#30528;&#36825;&#20123;&#20195;&#29702;&#20154;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25512;&#27979;LLM&#20195;&#29702;&#20154;&#23558;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLM&#20195;&#29702;&#20154;&#30340;&#25915;&#20987;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#20154;&#21487;&#20197;&#33258;&#20027;&#40657;&#23458;&#32593;&#31449;&#65292;&#25191;&#34892;&#35832;&#22914;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#31561;&#22797;&#26434;&#20219;&#21153;&#65292;&#26080;&#38656;&#20154;&#24037;&#21453;&#39304;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#20855;&#26377;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#25152;&#29420;&#29305;&#36171;&#20104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#36827;&#34892;&#36825;&#26679;&#30340;&#40657;&#23458;&#25915;&#20987;&#65292;&#20294;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#21017;&#19981;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#33258;&#20027;&#21457;&#29616;&#32593;&#31449;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.01681</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#35299;&#23494;&#65306;&#21033;&#29992;ChatGPT&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#27807;&#36890;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01681
&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#22312;&#31038;&#20132;&#32593;&#32476;&#27807;&#36890;&#20013;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#20204;&#25215;&#36733;&#20102;&#36229;&#36234;&#25991;&#23383;&#25110;&#30701;&#35821;&#30340;&#35821;&#20041;&#65292;&#36825;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#23545;&#20854;&#23646;&#24615;&#21644;&#21151;&#33021;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#32773;&#36890;&#24120;&#20381;&#36182;&#20247;&#21253;&#26469;&#27880;&#37322;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#20102;&#35299;&#20854;&#24773;&#24863;&#12289;&#20351;&#29992;&#24847;&#22270;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#30340;&#20027;&#35266;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#35823;&#35299;&#65292;&#24182;&#36896;&#25104;&#27807;&#36890;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#27880;&#37322;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;ChatGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#19987;&#19994;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#20197;&#21069;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39564;&#35777;ChatGPT&#21487;&#20197;&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#21487;&#34892;&#26367;&#20195;&#32773;&#65292;&#24182;&#39564;&#35777;&#20854;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01679</link><description>&lt;p&gt;
StickerConv: &#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
StickerConv: Generating Multimodal Empathetic Responses from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#20849;&#24773;&#23545;&#35805;&#30740;&#31350;&#20013;&#65292;&#36148;&#32440;&#23613;&#31649;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#25552;&#39640;&#22312;&#32447;&#20132;&#27969;&#20013;&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#20294;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#21253;&#25324;12.9K&#20010;&#23545;&#35805;&#20250;&#35805;&#65292;5.8K&#20010;&#29420;&#29305;&#36148;&#32440;&#21644;2K&#20010;&#22810;&#26679;&#21270;&#20250;&#35805;&#22330;&#26223;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#24773;&#22659;&#19979;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;LLM&#30340;&#20840;&#38754;&#20849;&#24773;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PEGS&#22312;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28151;&#21512;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26102;&#38388;&#36724;&#29983;&#25104;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#25688;&#35201;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#36724;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#21644;&#20030;&#37325;&#26377;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#65292;TH-VAE&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#25429;&#25417;&#38543;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#20248;&#20110;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.16240</link><description>&lt;p&gt;
&#23558;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#26102;&#38388;&#32447;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation in social media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16240
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28151;&#21512;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26102;&#38388;&#36724;&#29983;&#25104;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#25688;&#35201;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#36724;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#21644;&#20030;&#37325;&#26377;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#65292;TH-VAE&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#25429;&#25417;&#38543;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#20248;&#20110;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;&#25277;&#35937;&#27719;&#24635;&#26041;&#27861;&#65292;&#23558;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#65288;LlaMA-2&#65289;&#65292;&#20197;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26102;&#38388;&#36724;&#29983;&#25104;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#25688;&#35201;&#65292;&#36866;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#12290;&#25688;&#35201;&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#21465;&#36848;&#35266;&#28857;&#65306;&#36890;&#36807;&#21521;&#19987;&#38376;&#30340;&#20020;&#24202;&#25552;&#31034;&#39304;&#36865;&#26469;&#29983;&#25104;&#19987;&#21521;&#20020;&#24202;&#21307;&#29983;&#26377;&#29992;&#30340;&#31532;&#19977;&#20154;&#31216;&#20020;&#24202;&#35265;&#35299;&#65292;&#20197;&#21450;&#37325;&#35201;&#30340;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;TH-VAE&#29983;&#25104;&#29992;&#25143;&#26102;&#38388;&#32447;&#30340;&#20020;&#26102;&#25935;&#24863;&#30340;&#31532;&#19968;&#20154;&#31216;&#25277;&#35937;&#25688;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19987;&#23478;&#25688;&#35201;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#19982;&#20020;&#24202;&#19987;&#23478;&#30340;&#20154;&#24037;&#35780;&#20272;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;TH-VAE&#36827;&#34892;&#30340;&#26102;&#38388;&#32447;&#25688;&#35201;&#20250;&#20135;&#29983;&#26356;&#23500;&#26377;&#20020;&#24202;&#25928;&#29992;&#12289;&#26356;&#20855;&#20107;&#23454;&#21644;&#36923;&#36753;&#36830;&#36143;&#24615;&#30340;&#25688;&#35201;&#65292;&#20248;&#20110;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16240v2 Announce Type: replace-cross  Abstract: We introduce a hybrid abstractive summarisation approach combining hierarchical VAE with LLMs (LlaMA-2) to produce clinically meaningful summaries from social media user timelines, appropriate for mental health monitoring. The summaries combine two different narrative points of view: clinical insights in third person useful for a clinician are generated by feeding into an LLM specialised clinical prompts, and importantly, a temporally sensitive abstractive summary of the user's timeline in first person, generated by a novel hierarchical variational autoencoder, TH-VAE. We assess the generated summaries via automatic evaluation against expert summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in more factual and logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.12192</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#21453;&#21521;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text Embedding Inversion Security for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#20197;&#23454;&#25968;&#23884;&#20837;&#34920;&#31034;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23884;&#20837;&#24335;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23558;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20026;&#23884;&#20837;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#28431;&#27934;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#30693;&#36947;&#24213;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25991;&#26412;&#20063;&#21487;&#20197;&#20174;&#23884;&#20837;&#20013;&#37325;&#26500;&#12290;&#23613;&#31649;&#24050;&#32463;&#25506;&#35752;&#20102;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#20351;&#20854;&#20182;&#35821;&#35328;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#23884;&#20837;&#36870;&#36716;&#25506;&#35752;&#20102;LLM&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#40657;&#30418;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#36870;&#36716;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LLMs&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#33521;&#35821;&#30340;&#38450;&#24481;&#21487;&#33021;&#26080;&#25928;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#65292;&#23545;b&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12192v2 Announce Type: replace-cross  Abstract: Textual data is often represented as realnumbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be vulnerable to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages vulnerable to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and thoroughly explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07324</link><description>&lt;p&gt;
&#23567;&#22411;LLMs&#26159;&#24369;&#24037;&#20855;&#23398;&#20064;&#32773;&#65306;&#22810;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#22823;&#22823;&#25193;&#23637;&#20102;&#29420;&#31435;LLMs&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;API&#65292;&#20989;&#25968;&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#33258;&#20027;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#24037;&#20855;&#20351;&#29992;&#30340;&#25361;&#25112;&#35201;&#27714;LLMs&#19981;&#20165;&#33021;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#24182;&#29983;&#25104;&#31572;&#26696;&#65292;&#36824;&#35201;&#22312;&#20219;&#21153;&#35268;&#21010;&#12289;&#35760;&#24518;&#31649;&#29702;&#12289;&#24037;&#20855;&#35843;&#29992;&#21644;&#32467;&#26524;&#24635;&#32467;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20256;&#32479;&#26041;&#27861;&#38598;&#20013;&#20110;&#35757;&#32451;&#21333;&#20010;&#20855;&#22791;&#25152;&#26377;&#36825;&#20123;&#21151;&#33021;&#30340;LLM&#65292;&#20294;&#22312;&#23567;&#22411;&#27169;&#22411;&#19978;&#20250;&#20986;&#29616;&#24615;&#33021;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#27492;&#22806;&#65292;&#24403;&#24037;&#20855;&#26356;&#26032;&#26102;&#65292;&#25972;&#20010;LLM&#21487;&#33021;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#19978;&#36848;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#12290;&#27599;&#20010;&#32452;&#20214;&#30001;&#19968;&#20010;&#21333;&#29420;&#30340;LLM&#23454;&#29616;&#65292;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#20854;&#20182;&#32452;&#20214;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#20415;&#20110;&#36827;&#34892;&#20010;&#20307;&#26356;&#26032;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;NomaFedHAP&#36825;&#19968;&#26032;&#22411;FL-SatCom&#26041;&#27861;&#65292;&#21033;&#29992;HAPs&#20316;&#20026;PS&#26469;&#22686;&#24378;&#21355;&#26143;&#21487;&#35265;&#24615;&#65292;&#24182;&#24341;&#20837;NOMA&#25216;&#26415;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#30340;&#27169;&#22411;&#20256;&#36755;&#12290;</title><link>https://arxiv.org/abs/2401.00685</link><description>&lt;p&gt;
&#38598;&#25104;&#28151;&#21512;NOMA-OFDM&#30340;HAP&#19982;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;NomaFedHAP&#36825;&#19968;&#26032;&#22411;FL-SatCom&#26041;&#27861;&#65292;&#21033;&#29992;HAPs&#20316;&#20026;PS&#26469;&#22686;&#24378;&#21355;&#26143;&#21487;&#35265;&#24615;&#65292;&#24182;&#24341;&#20837;NOMA&#25216;&#26415;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#30340;&#27169;&#22411;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#31354;&#20154;&#24037;&#26234;&#33021;&#23545;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#31038;&#20250;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#65292;&#26377;&#26102;&#29978;&#33267;&#25104;&#20026;&#24517;&#38656;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LEO&#21355;&#26143;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;FL-SatCom&#26041;&#27861;NomaFedHAP&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39640;&#31354;&#24179;&#21488;(HAPs)&#20316;&#20026;&#20998;&#24067;&#24335;&#21442;&#25968;&#26381;&#21153;&#22120;(PS)&#26469;&#22686;&#24378;&#21355;&#26143;&#30340;&#21487;&#35265;&#24615;&#65292;&#24341;&#20837;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;(NOMA)&#21040;LEO&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#21644;&#24102;&#23485;&#39640;&#25928;&#30340;&#27169;&#22411;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00685v2 Announce Type: replace-cross  Abstract: Space AI has become increasingly important and sometimes even necessary for government, businesses, and society. An active research topic under this mission is integrating federated learning (FL) with satellite communications (SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively train a machine learning model. However, the special communication environment of SatCom leads to a very slow FL training process up to days and weeks. This paper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO satellites, that (1) utilizes high-altitude platforms (HAPs) as distributed parameter servers (PS) to enhance satellite visibility, and (2) introduces non-orthogonal multiple access (NOMA) into LEO to enable fast and bandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a new communication topology that exploits HAPs to bridge satellites among different orbits to mitigate the Dopple
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;--SemiDQG&#65292;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#35757;&#32451;&#21709;&#24212;&#22686;&#24378;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120; (RA)&#12290;</title><link>https://arxiv.org/abs/2312.12713</link><description>&lt;p&gt;
&#21709;&#24212;&#22686;&#24378;&#30340;&#21322;&#30417;&#30563;&#23545;&#35805;&#26597;&#35810;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Response Enhanced Semi-supervised Dialogue Query Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;--SemiDQG&#65292;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#35757;&#32451;&#21709;&#24212;&#22686;&#24378;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120; (RA)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20114;&#32852;&#32593;&#33719;&#21462;&#24222;&#22823;&#19988;&#19981;&#26029;&#26356;&#26032;&#30340;&#30693;&#35782;&#34987;&#35748;&#20026;&#26159;&#23545;&#35805;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#29983;&#25104;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#30340;&#25628;&#32034;&#26597;&#35810;&#32780;&#25552;&#20986;&#20102;&#23545;&#35805;&#26597;&#35810;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#20123;&#26597;&#35810;&#23558;&#34987;&#25552;&#20132;&#21040;&#25628;&#32034;&#24341;&#25806;&#20197;&#26816;&#32034;&#20114;&#32852;&#32593;&#19978;&#30456;&#20851;&#30340;&#32593;&#31449;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; - SemiDQG&#65292;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22522;&#20110;&#25628;&#32034;&#26597;&#35810;&#36890;&#24120;&#19982;&#23545;&#35805;&#21709;&#24212;&#20027;&#39064;&#30456;&#20851;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#21709;&#24212;&#22686;&#24378;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120;&#65288;RA&#65289;&#26469;&#25552;&#20379;&#20016;&#23500;&#19988;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12713v2 Announce Type: replace-cross  Abstract: Leveraging vast and continually updated knowledge from the Internet has been considered an important ability for a dialogue system. Therefore, the dialogue query generation task is proposed for generating search queries from dialogue histories, which will be submitted to a search engine for retrieving relevant websites on the Internet. In this regard, previous efforts were devoted to collecting conversations with annotated queries and training a query producer (QP) via standard supervised learning. However, these studies still face the challenges of data scarcity and domain adaptation. To address these issues, in this paper, we propose a semi-supervised learning framework -- SemiDQG, to improve model performance with unlabeled conversations. Based on the observation that the search query is typically related to the topic of dialogue response, we train a response-augmented query producer (RA) to provide rich and effective traini
&lt;/p&gt;</description></item><item><title>KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2312.11539</link><description>&lt;p&gt;
KGLens&#65306;&#19968;&#20010;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#30693;&#36947;&#21644;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11539
&lt;/p&gt;
&lt;p&gt;
KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#26159;&#35780;&#20272;&#20107;&#23454;&#24615;&#24182;&#35782;&#21035;LLMs&#30340;&#30693;&#35782;&#30450;&#28857;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#39640;&#25928;&#35780;&#20272;&#36825;&#20123;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLens--&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;KGs&#21644;LLMs&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#65292;&#24182;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;KGs&#30340;&#30693;&#35782;&#32570;&#38519;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;KGLens&#20855;&#26377;&#19968;&#20010;&#22270;&#24341;&#23548;&#30340;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;KG&#32467;&#26500;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#21152;&#24555;KG&#30340;&#36941;&#21382;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#19977;&#20010;&#39046;&#22495;&#29305;&#23450;KG&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;KG&#21253;&#25324;&#36229;&#36807;19,000&#26465;&#36793;&#65292;700&#20010;&#20851;&#31995;&#21644;21,000&#20010;&#23454;&#20307;&#12290;&#25105;&#20204;&#36328;&#36234;8&#20010;LLMs&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KGLens&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2311.09731</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#21442;&#25968;&#21270;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#31995;&#32479;&#22320;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#20197;&#29983;&#25104;&#21512;&#29702;&#22238;&#24212;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#24378;&#35843;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#31934;&#30830;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#30333;&#25361;&#25112;&#65292;&#25105;&#20204;&#35786;&#26029;&#24615;&#22320;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#23384;&#22312;&#27010;&#24565;&#25110;&#38169;&#35823;&#21069;&#25552;&#30340;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#23427;&#20204;&#36229;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#32534;&#21046;&#19968;&#20010;&#21253;&#21547;&#26082;&#26377;&#26080;&#27861;&#22238;&#31572;&#20063;&#26377;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#22522;&#20934;&#65292;UnknownBench&#65292;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25345;&#35802;&#23454;&#30340;&#21516;&#26102;&#25552;&#20379;&#24110;&#21161;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#32479;&#19968;&#20449;&#24515;&#24341;&#23548;&#26041;&#27861;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33268;&#25298;&#32477;&#25110;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#21442;&#25968;&#21270;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#35299;&#37322;&#25209;&#35780;&#20219;&#21153;&#12289;&#21019;&#24314;&#20154;&#24037;&#39564;&#35777;&#36807;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#24320;&#28304;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65292;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#26377;&#21161;&#20110;&#25581;&#31034;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2311.09613</link><description>&lt;p&gt;
&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#65306;&#36890;&#36807;&#35299;&#37322;&#25209;&#35780;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
Digital Socrates: Evaluating LLMs through Explanation Critiques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#35299;&#37322;&#25209;&#35780;&#20219;&#21153;&#12289;&#21019;&#24314;&#20154;&#24037;&#39564;&#35777;&#36807;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#24320;&#28304;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65292;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#26377;&#21161;&#20110;&#25581;&#31034;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;LLMs&#21487;&#20197;&#25552;&#20379;&#26377;&#29702;&#26377;&#25454;&#30340;&#35299;&#37322;&#20197;&#21450;&#31572;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#30340;&#24615;&#36136;&#21644;&#36136;&#37327;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23450;&#20041;&#19968;&#31181;&#35814;&#32454;&#30340;&#26041;&#24335;&#26469;&#34920;&#24449;&#29616;&#20195;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#32454;&#33268;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#35780;&#20272;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#34920;&#24449;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#26114;&#36149;&#30340;API&#35843;&#29992;&#25110;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#65306;(a)&#23450;&#20041;&#35299;&#37322;&#25209;&#35780;&#30340;&#26032;&#20219;&#21153;&#8212;&#8212;&#35782;&#21035;&#21644;&#20998;&#31867;&#35299;&#37322;&#20013;&#30340;&#20219;&#20309;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#24314;&#35758;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65307;(b)&#20026;&#27492;&#20219;&#21153;&#21019;&#24314;&#19968;&#20010;&#35268;&#27169;&#21487;&#35266;&#19988;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65307;(c)&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65288;&#31216;&#20026;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#65289;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#22914;&#20309;&#26377;&#21161;&#20110;&#36890;&#36807;&#26816;&#26597;&#20854;&#29702;&#30001;&#26469;&#25581;&#31034;&#26377;&#20851;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09613v2 Announce Type: replace-cross  Abstract: While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reas
&lt;/p&gt;</description></item><item><title>Fusion-Eval&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#25972;&#21512;&#19981;&#21516;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#65292;&#26497;&#22823;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09204</link><description>&lt;p&gt;
Fusion-Eval: &#23558;&#35780;&#20272;&#22120;&#19982;LLMs&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Fusion-Eval: Integrating Evaluators with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09204
&lt;/p&gt;
&lt;p&gt;
Fusion-Eval&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#25972;&#21512;&#19981;&#21516;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#65292;&#26497;&#22823;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#30340;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#39640;&#32423;&#25512;&#29702;&#39046;&#22495;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Fusion-Eval&#8221;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#12290;&#27599;&#20010;&#35780;&#20272;&#22120;&#19987;&#38376;&#36127;&#36131;&#35780;&#20272;&#21709;&#24212;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#31181;&#29420;&#29305;&#31574;&#30053;&#20351;&#24471;Fusion-Eval&#33021;&#22815;&#26377;&#25928;&#22320;&#36328;&#36234;&#21508;&#31181;&#20219;&#21153;&#21644;&#26631;&#20934;&#65292;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;SummEval&#19978;&#65292;Fusion-Eval&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#31995;&#32479;&#32423;Kendall-Tau&#30456;&#20851;&#24615;&#36798;&#21040;0.962&#65292;&#22312;TopicalChat&#19978;&#30340;&#36718;&#32423;Spearman&#30456;&#20851;&#24615;&#36798;&#21040;0.744&#65292;&#36828;&#39640;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;Fusion-Eval&#22312;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09204v2 Announce Type: replace-cross  Abstract: Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce "Fusion-Eval", an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. This unique strategy enables Fusion-Eval to function effectively across a diverse range of tasks and criteria, enhancing the effectiveness of existing evaluation methods. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.05866</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative quantum machine learning via denoising diffusion probabilistic models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#32467;&#26500;&#28789;&#27963;&#12289;&#35757;&#32451;&#31616;&#21333;&#30340;&#29305;&#28857;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21033;&#29992;&#32416;&#32544;&#21644;&#21472;&#21152;&#30340;&#33021;&#21147;&#20026;&#23398;&#20064;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#21463;&#32463;&#20856;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;QuDDPM&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#12290;QuDDPM&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#26469;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#19982;&#22122;&#22768;&#20043;&#38388;&#30340;&#25554;&#20540;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#35823;&#24046;&#30340;&#19978;&#30028;&#21644;...&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2310.05212</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#32593;&#32476;&#20195;&#34920;&#24847;&#35782;&#30340;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Interpretable Semiotics Networks Representing Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#37117;&#24863;&#30693;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#20256;&#36798;&#20182;&#20204;&#30340;&#24863;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#30340;&#24863;&#30693;&#20197;&#21450;&#23427;&#20204;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20869;&#37096;&#34920;&#31034;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;"&#35266;&#23519;&#21040;&#30340;"&#21644;"&#30475;&#21040;&#30340;"&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29087;&#24713;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27010;&#24565;&#65288;&#32534;&#30721;&#21644;&#35299;&#30721;&#65289;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#31526;&#21495;&#32593;&#32476;&#65292;&#27169;&#25311;&#20102;&#29289;&#20307;&#24863;&#30693;&#21644;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#24847;&#35782;&#12290;&#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#35265;&#24615;&#12290;&#25105;&#20204;&#20154;&#30340;&#29289;&#20307;&#24863;&#30693;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32593;&#32476;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22522;&#20934;&#20998;&#31867;&#22120;&#21644;&#39069;&#22806;&#23618;&#30340;&#26032;&#32593;&#32476;&#26469;&#28436;&#31034;&#36825;&#19968;&#28857;&#12290;&#36825;&#20010;&#23618;&#20135;&#29983;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20887;&#20313;&#21644;&#27010;&#24565;&#20998;&#26512;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#32473;&#23450;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#24110;&#21161;&#29702;&#35299;&#32593;&#32476;&#20013;&#21738;&#20123;&#31070;&#32463;&#20803;&#21644;&#23618;&#21487;&#20197;&#34987;&#28040;&#38500;&#65292;&#24182;&#22312;&#21738;&#37324;&#25214;&#21040;&#37325;&#35201;&#30340;&#20195;&#30721;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.00875</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20887;&#20313;&#21644;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Redundancy and Concept Analysis for Code-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20887;&#20313;&#21644;&#27010;&#24565;&#20998;&#26512;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#32473;&#23450;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#24110;&#21161;&#29702;&#35299;&#32593;&#32476;&#20013;&#21738;&#20123;&#31070;&#32463;&#20803;&#21644;&#23618;&#21487;&#20197;&#34987;&#28040;&#38500;&#65292;&#24182;&#22312;&#21738;&#37324;&#25214;&#21040;&#37325;&#35201;&#30340;&#20195;&#30721;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#29942;&#39048;&#21644;&#20869;&#23384;&#38480;&#21046;&#65292;&#23545;&#20110;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#24212;&#29992;&#26469;&#35828;&#65292;&#23427;&#20204;&#21487;&#33021;&#38590;&#20197;&#35757;&#32451;&#21644;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#23454;&#26045;&#26377;&#25928;&#30340;&#31574;&#30053;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#12290;&#26412;&#25991;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#39318;&#27425;&#31070;&#32463;&#20803;&#32423;&#21035;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#8220;&#37325;&#35201;&#8221;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#38500;&#19982;&#32473;&#23450;&#20219;&#21153;&#39640;&#24230;&#30456;&#20284;&#25110;&#19981;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#25105;&#20204;&#20102;&#35299;&#21738;&#20123;&#31070;&#32463;&#20803;&#21644;&#23618;&#21487;&#20197;&#34987;&#28040;&#38500;&#65288;&#20887;&#20313;&#20998;&#26512;&#65289;&#65292;&#20197;&#21450;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#20195;&#30721;&#23646;&#24615;&#20301;&#20110;&#20309;&#22788;&#65288;&#27010;&#24565;&#20998;&#26512;&#65289;&#12290;&#21033;&#29992;&#20887;&#20313;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19982;&#30693;&#35782;&#36716;&#31227;&#21644;&#27169;&#22411;&#20248;&#21270;&#24212;&#29992;&#30456;&#20851;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.00875v2 Announce Type: replace-cross  Abstract: Code-trained language models have proven to be highly effective for various code intelligence tasks. However, they can be challenging to train and deploy for many software engineering applications due to computational bottlenecks and memory constraints. Implementing effective strategies to address these issues requires a better understanding of these 'black box' models. In this paper, we perform the first neuron-level analysis for source code models to identify \textit{important} neurons within latent representations. We achieve this by eliminating neurons that are highly similar or irrelevant to the given task. This approach helps us understand which neurons and layers can be eliminated (redundancy analysis) and where important code properties are located within the network (concept analysis). Using redundancy analysis, we make observations relevant to knowledge transfer and model optimization applications. We find that over 9
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#21333;&#32454;&#32990;&#35270;&#20026;&#31354;&#38388;&#26631;&#35760;&#65292;&#24182;&#21033;&#29992;Transformer&#27169;&#22411;&#20419;&#36827;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#20805;&#12290;</title><link>https://arxiv.org/abs/2302.03038</link><description>&lt;p&gt;
&#21333;&#32454;&#32990;&#26159;&#31354;&#38388;&#26631;&#35760;&#65306;&#29992;&#20110;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#20805;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#21333;&#32454;&#32990;&#35270;&#20026;&#31354;&#38388;&#26631;&#35760;&#65292;&#24182;&#21033;&#29992;Transformer&#27169;&#22411;&#20419;&#36827;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#35299;&#26512;&#36716;&#24405;&#32452;&#23398;&#36890;&#36807;&#25552;&#20379;&#29289;&#29702;&#20301;&#32622;&#21644;&#22522;&#22240;&#34920;&#36798;&#24102;&#26469;&#20102;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#28608;&#21160;&#20154;&#24515;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26497;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25104;&#26412;&#65292;&#32454;&#32990;&#27700;&#24179;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#21333;&#20010;&#32454;&#32990;&#35270;&#20026;&#31354;&#38388;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;Transformer&#27169;&#22411;&#26469;&#20419;&#36827;&#31354;&#38388;&#36716;&#24405;&#32452;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03038v2 Announce Type: replace-cross  Abstract: Spatially resolved transcriptomics brings exciting breakthroughs to single-cell analysis by providing physical locations along with gene expression. However, as a cost of the extremely high spatial resolution, the cellular level spatial transcriptomic data suffer significantly from missing values. While a standard solution is to perform imputation on the missing values, most existing methods either overlook spatial information or only incorporate localized spatial context without the ability to capture long-range spatial information. Using multi-head self-attention mechanisms and positional encoding, transformer models can readily grasp the relationship between tokens and encode location information. In this paper, by treating single cells as spatial tokens, we study how to leverage transformers to facilitate spatial tanscriptomics imputation. In particular, investigate the following two key questions: (1) $\textit{how to encod
&lt;/p&gt;</description></item><item><title>Orbit&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#39640;&#20445;&#30495;&#30340;&#21018;&#24615;&#21644;&#21487;&#21464;&#24418;&#20307;&#27169;&#25311;&#65292;&#25903;&#25345;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;GPU&#24182;&#34892;&#21270;&#24555;&#36895;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21644;&#25910;&#38598;&#22823;&#22411;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2301.04195</link><description>&lt;p&gt;
Orbit&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;&#29615;&#22659;&#20223;&#30495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.04195
&lt;/p&gt;
&lt;p&gt;
Orbit&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#39640;&#20445;&#30495;&#30340;&#21018;&#24615;&#21644;&#21487;&#21464;&#24418;&#20307;&#27169;&#25311;&#65292;&#25903;&#25345;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;GPU&#24182;&#34892;&#21270;&#24555;&#36895;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21644;&#25910;&#38598;&#22823;&#22411;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Orbit&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;NVIDIA Isaac Sim&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#26694;&#26550;&#12290;&#23427;&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#36731;&#26494;&#39640;&#25928;&#22320;&#21019;&#24314;&#20855;&#26377;&#29031;&#29255;&#32423;&#36924;&#30495;&#22330;&#26223;&#21644;&#39640;&#20445;&#30495;&#21018;&#20307;&#21644;&#21487;&#21464;&#24418;&#20307;&#27169;&#25311;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#12290;&#36890;&#36807;Orbit&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#19981;&#21516;&#38590;&#24230;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#20174;&#21333;&#38454;&#27573;&#30340;&#26588;&#23376;&#25171;&#24320;&#21644;&#24067;&#26009;&#25240;&#21472;&#21040;&#22810;&#38454;&#27573;&#20219;&#21153;&#65292;&#22914;&#25151;&#38388;&#37325;&#32452;&#12290;&#20026;&#20102;&#25903;&#25345;&#22788;&#29702;&#19981;&#21516;&#30340;&#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#25105;&#20204;&#21253;&#25324;&#20855;&#26377;&#19981;&#21516;&#22522;&#20110;&#29289;&#29702;&#30340;&#20256;&#24863;&#22120;&#21644;&#36816;&#21160;&#29983;&#25104;&#22120;&#30340;&#22266;&#23450;&#33218;&#21644;&#31227;&#21160;&#24335;&#25805;&#20316;&#22120;&#12290;Orbit&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#21270;&#65292;&#22312;&#20960;&#20998;&#38047;&#20869;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#24182;&#25910;&#38598;&#26469;&#33258;&#25163;&#24037;&#35774;&#35745;&#25110;&#19987;&#23478;&#35299;&#20915;&#26041;&#26696;&#30340;&#22823;&#22411;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#39044;&#20808;&#25552;&#20379;&#20102;16&#31181;&#26426;&#22120;&#20154;&#24179;&#21488;&#65292;4&#31181;&#20256;&#24863;&#22120;&#27169;&#24335;&#65292;10&#31181;&#36816;&#21160;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.04195v2 Announce Type: replace-cross  Abstract: We present Orbit, a unified and modular framework for robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and high-fidelity rigid and deformable body simulation. With Orbit, we provide a suite of benchmark tasks of varying difficulty -- from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. Orbit allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion gener
&lt;/p&gt;</description></item><item><title>&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2212.03733</link><description>&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#65306;&#35268;&#23450;&#21644;&#24555;&#36895;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03733
&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20316;&#20026;&#20154;&#31867;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#34920;&#36798;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#24182;&#20351;&#20195;&#29702;&#33021;&#22815;&#36805;&#36895;&#23398;&#20064;&#36825;&#31181;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20219;&#21153;&#20013;&#36798;&#21040;&#33391;&#22909;&#29366;&#24577;&#21644;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#31574;&#30053;&#31354;&#38388;&#30340;&#37096;&#20998;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#34892;&#20026;&#20559;&#22909;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#26356;&#20542;&#21521;&#20110;&#33021;&#26356;&#24555;&#36895;&#22320;&#21040;&#36798;&#33391;&#22909;&#29366;&#24577;&#24182;&#20197;&#26356;&#39640;&#30340;&#27010;&#29575;&#21040;&#36798;&#65292;&#21516;&#26102;&#33021;&#26356;&#38271;&#26102;&#38388;&#22320;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23618;&#32423;&#22870;&#21169;&#65292;&#19968;&#31867;&#19982;&#29615;&#22659;&#26080;&#20851;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#34920;&#26126;&#23427;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#25105;&#20204;&#30340;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23618;&#32423;&#22870;&#21169;&#21487;&#20197;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#20351;&#29992;&#22810;&#20010;&#34920;&#26684;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#20110;&#35813;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2212.00322</link><description>&lt;p&gt;
&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#21333;&#26041;&#38754;&#21163;&#25345;
&lt;/p&gt;
&lt;p&gt;
Hijack Vertical Federated Learning Models As One Party
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#20110;&#35813;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#20351;&#21512;&#20316;&#20249;&#20276;&#33021;&#22815;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#20849;&#21516;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#21442;&#19982;&#26041;&#26377;&#19968;&#32452;&#20849;&#21516;&#29992;&#25143;&#65292;&#20294;&#25317;&#26377;&#19981;&#21516;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;VFL&#26694;&#26550;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#25552;&#20379;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#30740;&#31350;&#35745;&#31639;&#25928;&#29575;&#21644;&#24555;&#36895;&#23454;&#29616;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;VFL&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.00322v2 Announce Type: replace-cross  Abstract: Vertical federated learning (VFL) is an emerging paradigm that enables collaborators to build machine learning models together in a distributed fashion. In general, these parties have a group of users in common but own different features. Existing VFL frameworks use cryptographic techniques to provide data privacy and security guarantees, leading to a line of works studying computing efficiency and fast implementation. However, the security of VFL's model remains underexplored.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#35013;&#39280;&#21644;Fourier&#20998;&#21306;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#32593;&#32476;&#21508;&#31181;&#20114;&#34917;&#29305;&#24449;&#65292;&#20943;&#23569;&#22522;&#20110;&#25200;&#21160;&#30340;&#24858;&#24324;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2207.09031</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#35013;&#39280;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Decorrelative Network Architecture for Robust Electrocardiogram Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.09031
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#35013;&#39280;&#21644;Fourier&#20998;&#21306;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#32593;&#32476;&#21508;&#31181;&#20114;&#34917;&#29305;&#24449;&#65292;&#20943;&#23569;&#22522;&#20110;&#25200;&#21160;&#30340;&#24858;&#24324;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#34987;&#24191;&#27867;&#37096;&#32626;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#35013;&#39280;&#21644;Fourier&#20998;&#21306;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#32593;&#32476;&#21508;&#31181;&#20114;&#34917;&#29305;&#24449;&#65292;&#20943;&#23569;&#22522;&#20110;&#25200;&#21160;&#30340;&#24858;&#24324;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#22312;&#21333;&#36890;&#36947;&#21644;&#22810;&#36890;&#36947;&#24515;&#30005;&#22270;&#20998;&#31867;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;DVERGE&#35843;&#25972;&#20026;&#36125;&#21494;&#26031;&#38598;&#25104;&#26694;&#26550;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09031v4 Announce Type: replace-cross  Abstract: Artificial intelligence has made great progress in medical data analysis, but the lack of robustness and trustworthiness has kept these methods from being widely deployed. As it is not possible to train networks that are accurate in all scenarios, models must recognize situations where they cannot operate confidently. Bayesian deep learning methods sample the model parameter space to estimate uncertainty, but these parameters are often subject to the same vulnerabilities, which can be exploited by adversarial attacks. We propose a novel ensemble approach based on feature decorrelation and Fourier partitioning for teaching networks diverse complementary features, reducing the chance of perturbation-based fooling. We test our approach on single and multi-channel electrocardiogram classification, and adapt adversarial training and DVERGE into the Bayesian ensemble framework for comparison. Our results indicate that the combination
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16461</link><description>&lt;p&gt;
&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65306;&#26356;&#24555;&#30340;&#20986;&#29616;&#65292;&#26356;&#24555;&#20048;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16461
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21487;&#35270;&#20026;&#19968;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#65292;&#36890;&#36807;&#31038;&#20250;&#35268;&#33539;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25511;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#30340;&#35268;&#33539;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#32780;&#26159;&#20174;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#20013;&#30340;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#20316;&#20986;&#30340;&#21453;&#24212;&#20197;&#21450;&#23545;&#20182;&#20154;&#21453;&#24212;&#30340;&#22238;&#24212;&#65292;&#20915;&#23450;&#20102;&#31038;&#20250;&#20013;&#20986;&#29616;&#21738;&#20123;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#28385;&#24847;&#25110;&#19981;&#28385;&#24847;&#34892;&#20026;&#30340;&#21453;&#24212;&#35270;&#20026;&#31532;&#19968;&#20010;&#26234;&#33021;&#20307;&#21521;&#31532;&#20108;&#20010;&#26234;&#33021;&#20307;&#30340;&#20132;&#27969;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#27969;&#26159;&#19968;&#31181;&#31038;&#20250;&#26234;&#33021;&#65306;&#36825;&#20123;&#20132;&#27969;&#36890;&#36807;&#25512;&#21160;&#26234;&#33021;&#20307;&#26397;&#30528;&#26576;&#20123;&#34892;&#20026;&#36827;&#34892;&#65292;&#20174;&#32780;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#24809;&#32602;&#21487;&#20197;&#23548;&#33268;&#35268;&#33539;&#30340;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#23485;&#27867;&#30340;&#31038;&#20250;&#26234;&#33021;&#21487;&#33021;&#22312;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;Ne&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12481</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#22269;&#23478;&#37117;&#24198;&#31069;&#24863;&#24681;&#33410;&#65306;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models. (arXiv:2310.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#22312;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#28304;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;LLMs&#24448;&#24448;&#20250;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#30340;&#33521;&#35821;&#25991;&#21270;&#30456;&#20851;&#31572;&#26696;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20855;&#20307;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20551;&#26085;&#21644;&#27468;&#26354;&#65289;&#21644;&#25277;&#35937;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#34920;&#24615;&#30340;GPT&#27169;&#22411;&#23384;&#22312;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;GPT-4&#21463;&#21040;&#26368;&#20005;&#37325;&#24433;&#21709;&#65292;&#32780;text-davinci-003&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21463;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#23545;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#25209;&#21028;&#24615;&#23457;&#35270;&#21644;&#20262;&#29702;&#32771;&#34385;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#65306;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#30340;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g. ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark that consists of both concrete (e.g. holidays and songs) and abstract (e.g. values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need for critical examination of cultural dominance and ethical consideration in their development and deployment. We show two straightforward methods in model development (i.e. pretraining on more diverse data) and deployment (e.g. culture-aware prompting) can signifi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.07637</link><description>&lt;p&gt;
OpsEval: &#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07637
&lt;/p&gt;
&lt;p&gt;
OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#22312;&#32763;&#35793;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#31561;NLP&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;AIOps&#65288;&#38754;&#21521;IT&#36816;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#20013;&#65292;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#20449;&#24687;&#27719;&#24635;&#12289;&#25253;&#21578;&#20998;&#26512;&#21644;API&#35843;&#29992;&#33021;&#21147;&#32780;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#22312;AIOps&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#23578;&#26410;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#24341;&#23548;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#35780;&#20272;&#32593;&#32476;&#37197;&#32622;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OpsEval&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;LLMs&#35774;&#35745;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#12290;OpsEval&#39318;&#27425;&#23545;LLMs&#22312;&#19977;&#20010;&#20851;&#38190;&#22330;&#26223;&#65288;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#33021;&#21147;&#27700;&#24179;&#65288;&#30693;&#35782;&#22238;&#24518;&#12289;&#20998;&#26512;&#24605;&#32771;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.00867</link><description>&lt;p&gt;
(&#21160;&#24577;)&#25552;&#31034;&#21487;&#33021;&#26159;&#20462;&#22797;&#21387;&#32553;LLMs&#25152;&#38656;&#30340;&#20840;&#37096;&#12290;(arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
(Dynamic) Prompting might be all you need to repair Compressed LLMs. (arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#26377;&#30528;&#37325;&#22823;&#30340;&#21464;&#38761;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24378;&#35843;&#20102;&#39640;&#25928;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#38024;&#23545;&#26368;&#22823;&#30340;LLMs&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;LLaMA-7B&#21644;OPT-6.7b&#36827;&#34892;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21387;&#32553;&#21518;&#37325;&#26032;&#35757;&#32451;&#30340;&#26435;&#34913;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25552;&#31034;&#39537;&#21160;&#30340;&#24674;&#22797;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#20855;&#26377;&#28508;&#22312;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#35780;&#20272;&#21644;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#23545;&#25552;&#31034;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#27809;&#26377;&#32473;&#20986;&#26126;&#30830;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#21387;&#32553;&#20013;&#22825;&#30495;&#25552;&#31034;&#30340;&#33030;&#24369;&#24615;&#65292;&#21363;&#36807;&#24230;&#20381;&#36182;&#21333;&#19968;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#26102;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#26368;&#20339;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), while transformative for NLP, come with significant computational demands, underlining the need for efficient, training-free compression. Notably, despite the marked improvement in training-free compression for the largest of LLMs, our tests using LLaMA-7B and OPT-6.7b highlight a significant performance drop in several realistic downstream tasks. Investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. However, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting. We tackle this uncertainty in two key ways. First, we uncover the vulnerability of naive prompts in LLM compression as an over-reliance on a singular prompt per input. In response, we propose inference-time dynamic prompting (IDP), a mechanism that autonomously chooses f
&lt;/p&gt;</description></item><item><title>ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17452</link><description>&lt;p&gt;
ToRA&#65306;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17452
&lt;/p&gt;
&lt;p&gt;
ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38598;&#25104;&#24037;&#20855;&#30340;&#25512;&#29702;&#20195;&#29702;ToRA&#65292;&#23427;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;&#35745;&#31639;&#24211;&#21644;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#30340;&#21033;&#29992;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#23558;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#19982;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;ToRA&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20114;&#21160;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#20110;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#36755;&#20986;&#31354;&#38388;&#25972;&#24418;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ToRA&#27169;&#22411;&#22312;10&#20010;&#28085;&#30422;&#21508;&#31181;&#35268;&#27169;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;13%&#33267;19%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ToRA-7B &#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#65292;&#36229;&#36234;&#20102;&#26368;&#20339;&#24320;&#28304;&#27169;&#22411;WizardMath&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
&lt;/p&gt;</description></item><item><title>&#32463;&#39564;&#20016;&#23500;&#30340;&#35774;&#35745;&#24072;&#35748;&#20026;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#65288;UXD&#65289;&#23454;&#36341;&#20013;&#20855;&#26377;&#36741;&#21161;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21021;&#32423;&#35774;&#35745;&#24072;&#21487;&#33021;&#20250;&#38754;&#20020;&#25216;&#33021;&#36864;&#21270;&#12289;&#24037;&#20316;&#26367;&#20195;&#21644;&#21019;&#36896;&#21147;&#26543;&#31469;&#31561;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;-GenAI&#21327;&#20316;&#30340;&#19968;&#20123;&#24433;&#21709;&#65292;&#21253;&#25324;&#29256;&#26435;&#19982;&#25152;&#26377;&#26435;&#12289;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#20195;&#29702;&#24615;&#20197;&#21450;AI&#32032;&#20859;&#21644;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.15237</link><description>&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#19987;&#19994;&#20154;&#21592;&#23545;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
User Experience Design Professionals' Perceptions of Generative Artificial Intelligence. (arXiv:2309.15237v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15237
&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#20016;&#23500;&#30340;&#35774;&#35745;&#24072;&#35748;&#20026;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#65288;UXD&#65289;&#23454;&#36341;&#20013;&#20855;&#26377;&#36741;&#21161;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21021;&#32423;&#35774;&#35745;&#24072;&#21487;&#33021;&#20250;&#38754;&#20020;&#25216;&#33021;&#36864;&#21270;&#12289;&#24037;&#20316;&#26367;&#20195;&#21644;&#21019;&#36896;&#21147;&#26543;&#31469;&#31561;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;-GenAI&#21327;&#20316;&#30340;&#19968;&#20123;&#24433;&#21709;&#65292;&#21253;&#25324;&#29256;&#26435;&#19982;&#25152;&#26377;&#26435;&#12289;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#20195;&#29702;&#24615;&#20197;&#21450;AI&#32032;&#20859;&#21644;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21019;&#24847;&#19987;&#19994;&#20154;&#21592;&#20013;&#65292;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#24341;&#36215;&#20102;&#23545;&#20854;&#33021;&#21147;&#30340;&#20852;&#22859;&#21644;&#23545;&#26410;&#39044;&#26399;&#21518;&#26524;&#30340;&#25285;&#24551;&#12290;GenAI&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#65288;UXD&#65289;&#23454;&#36341;&#65292;&#24182;&#19988;&#36825;&#20123;&#25285;&#24551;&#26159;&#21542;&#21512;&#29702;&#65311;&#25105;&#20204;&#37319;&#35775;&#20102;20&#20301;UX&#35774;&#35745;&#24072;&#65292;&#20182;&#20204;&#25317;&#26377;&#20016;&#23500;&#30340;&#32463;&#39564;&#65292;&#26469;&#33258;&#21508;&#31181;&#20844;&#21496;&#65288;&#21019;&#19994;&#20844;&#21496;&#21040;&#22823;&#22411;&#20225;&#19994;&#65289;&#12290;&#25105;&#20204;&#35810;&#38382;&#20182;&#20204;&#30340;&#23454;&#36341;&#29305;&#24449;&#65292;&#24182;&#20102;&#35299;&#20182;&#20204;&#30340;&#24577;&#24230;&#12289;&#20851;&#27880;&#28857;&#21644;&#26399;&#26395;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#35774;&#35745;&#24072;&#23545;&#20182;&#20204;&#30340;&#21407;&#21019;&#24615;&#12289;&#21019;&#36896;&#21147;&#21644;&#20849;&#24773;&#33021;&#21147;&#20805;&#28385;&#20449;&#24515;&#65292;&#24182;&#35748;&#20026;GenAI&#30340;&#35282;&#33394;&#26159;&#36741;&#21161;&#24615;&#30340;&#12290;&#20182;&#20204;&#24378;&#35843;&#20102;"&#20139;&#21463;"&#21644;"&#20195;&#29702;"&#36825;&#20004;&#20010;&#29420;&#29305;&#30340;&#20154;&#31867;&#22240;&#32032;&#65292;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#20154;&#31867;&#20173;&#28982;&#26159;"AI&#23545;&#40784;"&#30340;&#20210;&#35009;&#32773;&#12290;&#28982;&#32780;&#65292;&#25216;&#33021;&#36864;&#21270;&#12289;&#24037;&#20316;&#26367;&#20195;&#21644;&#21019;&#36896;&#21147;&#26543;&#31469;&#21487;&#33021;&#23545;&#21021;&#32423;&#35774;&#35745;&#24072;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;-GenAI&#21327;&#20316;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#29256;&#26435;&#21644;&#25152;&#26377;&#26435;&#12289;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#20195;&#29702;&#24615;&#65292;&#20197;&#21450;AI&#32032;&#20859;&#21644;&#33719;&#21462;&#26041;&#38754;&#12290;&#36890;&#36807;&#36825;&#20010;&#35270;&#35282;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#21069;&#20154;&#30740;&#31350;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI's role as assistive. They emphasized the unique human factors of "enjoyment" and "agency", where humans remain the arbiters of "AI alignment". However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.02726</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20551;&#35774;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31185;&#23398;&#23478;&#35266;&#23519;&#19990;&#30028;&#24182;&#35797;&#22270;&#25552;&#20986;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#20551;&#35774;&#26102;&#65292;&#20551;&#35774;&#24402;&#32435;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#36807;&#21435;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#30340;&#35266;&#23519;&#27880;&#37322;&#19981;&#26159;&#21407;&#22987;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#32780;&#26159;&#25163;&#21160;&#36873;&#25321;&#30340;&#21477;&#23376;&#65288;&#23548;&#33268;&#20102;&#19968;&#20010;&#23553;&#38381;&#39046;&#22495;&#30340;&#35774;&#32622;&#65289;&#65307;&#65288;2&#65289;&#23454;&#38469;&#30340;&#20551;&#35774;&#27880;&#37322;&#20027;&#35201;&#26159;&#24120;&#35782;&#30693;&#35782;&#65292;&#20351;&#24471;&#20219;&#21153;&#19981;&#22826;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;50&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#31038;&#20250;&#31185;&#23398;&#26399;&#21002;&#19978;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#25910;&#38598;&#20102;&#24320;&#21457;&#35770;&#25991;&#20013;&#30340;&#20551;&#35774;&#25152;&#38656;&#30340;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#19968;&#22534;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#23601;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21487;&#20197;&#35299;&#20915;&#20197;&#21069;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14754</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#36951;&#24536;&#65306;&#22312;&#20943;&#23569;&#24046;&#24322;&#30340;&#21516;&#26102;&#21024;&#38500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20844;&#20247;&#23545;&#20225;&#19994;&#25910;&#38598;&#21644;&#20351;&#29992;&#20010;&#20154;&#20449;&#24687;&#30340;&#24847;&#35782;&#22686;&#24378;&#65292;&#28040;&#36153;&#32773;&#31215;&#26497;&#21442;&#19982;&#20225;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25968;&#25454;&#31649;&#29702;&#26694;&#26550;&#65288;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65289;&#24050;&#32463;&#25552;&#20986;&#20102;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#65292;&#20801;&#35768;&#20010;&#20154;&#35831;&#27714;&#23558;&#20854;&#20010;&#20154;&#25968;&#25454;&#20174;&#32452;&#32455;&#20351;&#29992;&#30340;&#25968;&#25454;&#24211;&#21644;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36951;&#24536;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#36951;&#24536;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#36951;&#24536;&#35831;&#27714;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#22312;&#32447;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#36951;&#24536;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20854;&#20182;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#24179;&#34913;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20294;&#23384;&#22312;&#39640;&#20272;&#20182;&#20154;&#20851;&#27880;&#34892;&#20026;&#30340;&#20542;&#21521;&#65292;&#36825;&#23545;AI&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.12776</link><description>&lt;p&gt;
&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#22312;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#20043;&#38388;&#24179;&#34913;&#30340;&#21487;&#39044;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predict-AI-bility of how humans balance self-interest with the interest of others. (arXiv:2307.12776v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12776
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#24179;&#34913;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20294;&#23384;&#22312;&#39640;&#20272;&#20182;&#20154;&#20851;&#27880;&#34892;&#20026;&#30340;&#20542;&#21521;&#65292;&#36825;&#23545;AI&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#25913;&#21464;&#20174;&#26085;&#24120;&#29983;&#27963;&#21040;&#39640;&#39118;&#38505;&#22330;&#26223;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#20915;&#31574;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#65292;&#20026;&#20102;&#20351;AI&#33021;&#22815;&#25104;&#20026;&#21487;&#38752;&#30340;&#20915;&#31574;&#21161;&#25163;&#65292;&#23427;&#24517;&#39035;&#33021;&#22815;&#25429;&#25417;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#26469;&#33258;12&#20010;&#22269;&#23478;&#30340;78&#20010;&#23454;&#39564;&#30340;&#29420;&#35009;&#32773;&#28216;&#25103;&#20915;&#31574;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;GPT-4&#65288;&#32780;&#19981;&#26159;Bard&#25110;Bing&#65289;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#34892;&#20026;&#27169;&#24335;&#30340;&#23450;&#24615;&#29305;&#24449;&#65292;&#35782;&#21035;&#20986;&#19977;&#31181;&#20027;&#35201;&#30340;&#34892;&#20026;&#31867;&#21035;&#65306;&#33258;&#31169;&#30340;&#12289;&#19981;&#20844;&#24179;&#21388;&#24694;&#30340;&#21644;&#23436;&#20840;&#26080;&#31169;&#30340;&#12290;&#28982;&#32780;&#65292;GPT-4&#19968;&#30452;&#39640;&#20272;&#20102;&#20182;&#20154;&#20851;&#27880;&#34892;&#20026;&#65292;&#22840;&#22823;&#20102;&#19981;&#20844;&#24179;&#21388;&#24694;&#21644;&#23436;&#20840;&#26080;&#31169;&#21442;&#19982;&#32773;&#30340;&#27604;&#20363;&#12290;&#36825;&#31181;&#20559;&#35265;&#23545;&#20110;AI&#24320;&#21457;&#20154;&#21592;&#21644;&#29992;&#25143;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. However, as many decisions carry social implications, for AI to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. We investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. We find that only GPT-4 (not Bard nor Bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. Nonetheless, GPT-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. This bias has significant implications for AI developers and users.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10928</link><description>&lt;p&gt;
FLASK: &#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10928
&lt;/p&gt;
&lt;p&gt;
FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25351;&#20196;&#38656;&#35201;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25216;&#33021;&#38598;&#26681;&#25454;&#25351;&#20196;&#32780;&#24322;&#65292;&#22240;&#27492;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#35780;&#20272;&#65288;&#21363;&#22522;&#20110;&#25972;&#20307;&#20559;&#22909;&#30340;&#35780;&#20272;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#38656;&#35201;&#23454;&#20363;&#32423;&#25216;&#33021;&#32452;&#21512;&#30340;&#29992;&#25143;&#25351;&#20196;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLASK&#65288;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#23427;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#23545;&#20110;&#33719;&#24471;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;FLASK&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;&#24320;&#28304;&#21644;&#19987;&#26377;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#31574;&#30053;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#25915;&#20987;&#24418;&#24335;&#21644;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#30452;&#25509;&#25200;&#21160;&#21463;&#23475;&#32773;&#30340;&#29366;&#24577;/&#21160;&#20316;&#25110;&#22522;&#30784;&#36716;&#31227;&#21160;&#24577;&#20197;&#23637;&#31034;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#30452;&#25509;&#25805;&#32437;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21478;&#19968;&#31181;&#24120;&#35265;&#19988;&#29616;&#23454;&#30340;&#25915;&#20987;&#35774;&#32622;&#65306;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#30340;&#35774;&#32622;&#20013;&#65292;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#21463;&#23475;&#20195;&#29702;$\nu$&#34987;&#25915;&#20987;&#32773;&#25511;&#21046;&#21478;&#19968;&#20010;&#20195;&#29702;$\alpha$&#20197;&#25932;&#23545;&#26041;&#24335;&#34892;&#21160;&#65292;&#20351;&#29992;&#8220;&#23545;&#25239;&#31574;&#30053;&#8221;&#23545;&#21463;&#23475;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#25915;&#20987;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#31181;&#35774;&#32622;&#65292;&#20294;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36935;&#21040;&#25269;&#25239;&#65292;&#22240;&#27492;&#21482;&#33021;&#37096;&#20998;&#25511;&#21046;&#20195;&#29702;$\alpha$&#65292;&#21516;&#26102;&#24341;&#20837;&#21487;&#23519;&#35273;&#30340;&#8220;&#24322;&#24120;&#8221;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#24182;&#19988;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#23545;&#25239;&#31574;&#30053;&#30340;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#21270;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#25511;&#21046;&#20195;&#29702;$\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10544</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#30340;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10544
&lt;/p&gt;
&lt;p&gt;
GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476; (GSPN)&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#12290;&#21463;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#39030;&#28857;&#24341;&#36215;&#30340;&#35745;&#31639;&#26641;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#21644;&#31215;&#32593;&#32476;&#65288;SPN&#65289;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#29238;SPN&#30340;&#21442;&#25968;&#26159;&#20854;&#23376;&#32423;&#30340;&#21518;&#39564;&#28151;&#21512;&#27010;&#29575;&#30340;&#21487;&#23398;&#20064;&#21464;&#25442;&#12290;&#30001;&#20110;&#26435;&#37325;&#20849;&#20139;&#21644;GSPN&#30340;&#26641;&#29366;&#35745;&#31639;&#22270;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#32570;&#20047;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01713</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35299;&#37322;&#30340;&#38750;&#20132;&#20114;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#21270;&#36830;&#32493;&#31354;&#38388;&#30340;&#21477;&#23376;&#34920;&#24449;&#19978;&#36827;&#34892;&#35299;&#32806;&#21487;&#20197;&#22312;&#23450;&#20301;&#26126;&#30830;&#21457;&#29983;&#30340;&#29983;&#25104;&#22240;&#32032;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#21644;&#35821;&#20041;&#25511;&#21046;&#65292;&#36825;&#20026;&#22522;&#20110;&#31070;&#32463;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#20102;&#19968;&#20123;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#38500;&#32534;&#30721;&#30340;&#38544;&#34255;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;INN&#33021;&#22815;&#23558;&#20998;&#24067;&#24335;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#22909;&#30340;&#35821;&#20041;&#19978;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15103</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#30456;&#20284;&#24615;&#22270;&#35889;&#19978;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#36816;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#12290;&#21033;&#29992;&#36825;&#31181;&#31561;&#20215;&#24615;&#20316;&#20026;&#22522;&#30707;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#24182;&#20005;&#26684;&#25551;&#36848;&#22810;&#27169;&#24577;&#23545;&#35937;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#19968;&#36215;&#12290;&#22312;&#29702;&#35770;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#28151;&#21512;&#25439;&#22833;&#65292;&#32467;&#21512;&#26032;&#39062;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12023</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#20197;&#24448;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#21644;&#31526;&#21495;&#25512;&#29702;&#22120;&#65289;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#35777;&#26126;&#20855;&#26377;&#22256;&#38590;&#65288;&#20363;&#22914;&#33030;&#24369;&#24615;&#21644;&#30693;&#35782;&#33719;&#21462;&#29942;&#39048;&#65289;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#20197;&#21450;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#22120;&#65289;&#65292;&#21253;&#25324;&#36923;&#36753;&#25512;&#29702;&#30340;&#21746;&#23398;&#23450;&#20041;&#21644;&#20998;&#31867;&#65292;&#26032;&#27169;&#24335;&#30340;&#20248;&#21183;&#12289;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#26410;&#26469;&#38656;&#35201;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#20197;&#21450;&#19982;&#30456;&#20851; NLP &#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#24418;&#24335;&#21270;&#34920;&#31034;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#32780;&#19988;&#20063;&#20855;&#26377;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners). However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks &amp; methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
&lt;/p&gt;</description></item><item><title>Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11118</link><description>&lt;p&gt;
Box$^2$EL: EL++&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#26041;&#27861;&#21450;&#20854;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11118
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#35770;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#19982;&#27010;&#24565;&#20449;&#24687;&#21644;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#26412;&#20307;&#35770;&#30340;&#24402;&#32435;&#25512;&#29702;&#25216;&#26415;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#34917;&#20805;&#20256;&#32479;&#30340;&#28436;&#32462;&#25512;&#29702;&#31639;&#27861;&#12290;&#31867;&#20284;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23436;&#21892;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#26412;&#20307;&#35770;&#23884;&#20837;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#24213;&#23618;&#25551;&#36848;&#36923;&#36753;&#30340;&#36923;&#36753;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21463;&#38480;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Box$^2$EL&#26041;&#27861;&#65292;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#37117;&#34920;&#31034;&#20026;&#30418;&#23376;&#65288;&#21363;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20811;&#26381;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03295</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#38454;&#38376;&#25511;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-order Gated Aggregation Network. (arXiv:2211.03295v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#21462;&#24471;&#26368;&#36817;&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;ViT&#39118;&#26684;&#26550;&#26500;&#30340;&#25506;&#32034;&#24341;&#21457;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#20852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#36825;&#31181;&#20132;&#20114;&#21453;&#26144;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#19981;&#21516;&#23610;&#24230;&#19978;&#19979;&#25991;&#30340;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#22312;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26469;&#23450;&#21046;&#20004;&#20010;&#29305;&#24449;&#28151;&#21512;&#22120;&#65292;&#20197;&#20419;&#36827;&#36328;&#31354;&#38388;&#21644;&#36890;&#36947;&#31354;&#38388;&#30340;&#20013;&#38454;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;ImageNet&#21644;&#21253;&#25324;COCO&#30446;&#26631;&#26816;&#27979;&#12289;ADE20K&#35821;&#20041;&#20998;&#21106;&#12289;2D&amp;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20197;&#21450;&#35270;&#39057;&#39044;&#27979;&#31561;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the recent success of Vision Transformers (ViTs), explorations toward ViT-style architectures have triggered the resurgence of ConvNets. In this work, we explore the representation ability of modern ConvNets from a novel view of multi-order game-theoretic interaction, which reflects inter-variable interaction effects w.r.t.~contexts of different scales based on game theory. Within the modern ConvNet framework, we tailor the two feature mixers with conceptually simple yet effective depthwise convolutions to facilitate middle-order information across spatial and channel spaces respectively. In this light, a new family of pure ConvNet architecture, dubbed MogaNet, is proposed, which shows excellent scalability and attains competitive results among state-of-the-art models with more efficient use of parameters on ImageNet and multifarious typical vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&amp;3D human pose estimation, and video prediction. Typica
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13508</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#20221;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#20986;&#33394;&#24615;&#33021;&#30340;&#26041;&#24335;&#24182;&#19981;&#38656;&#35201;&#22826;&#38271;&#26102;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#20016;&#23500;&#65292;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#21644;&#38656;&#35201;&#20445;&#35777;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#25968;&#25454;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#25110;&#32622;&#25442;&#36824;&#26159;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#29992;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#30456;&#20851;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#19981;&#21516;&#21464;&#20307;&#30340;&#25928;&#29575;&#23558;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#20013;&#24515;&#37096;&#20998;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#23558;&#35780;&#20272;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#27599;&#20010;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2203.11242</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;GAN&#32508;&#36848;&#65306;&#26368;&#26032;&#30740;&#31350;&#12289;&#20998;&#26512;&#21644;&#20998;&#31867;&#65288;arXiv&#65306;2203.11242v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#27425;&#38761;&#21629;&#65292;&#20854;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24040;&#22823;&#24433;&#21709;&#12290;GAN&#19981;&#20165;&#22312;&#23450;&#20041;&#20854;&#27169;&#22411;&#26102;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#32780;&#19988;&#29983;&#25104;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65292;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#30452;&#25509;&#24433;&#21709;&#12290;&#30001;&#20110;GAN&#24102;&#26469;&#30340;&#37325;&#22823;&#25913;&#36827;&#21644;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#31038;&#21306;&#19981;&#26029;&#25552;&#20986;&#26032;&#30340;&#30740;&#31350;&#65292;&#20351;&#24471;&#36319;&#19978;&#26102;&#20195;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;GAN&#30340;&#27010;&#36848;&#65292;&#23637;&#31034;&#26368;&#26032;&#30340;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#21464;&#20307;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#23558;&#35780;&#20272;&#19981;&#21516;&#21464;&#20307;&#30340;&#27169;&#22411;&#26550;&#26500;&#25928;&#29575;&#65292;&#23637;&#31034;&#26368;&#20339;&#30340;&#24212;&#29992;&#39046;&#22495;&#65307;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23558;&#20998;&#26512;&#35780;&#20272;GAN&#24615;&#33021;&#30340;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#21644;&#32463;&#24120;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#23558;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;GAN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#65292;&#24182;&#25253;&#21578;&#20102;&#25152;&#32771;&#23519;&#24037;&#20316;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#20173;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#20043;&#22788;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.06095</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Deep Learning-based Approaches for Deepfake Content Detection. (arXiv:2202.06095v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#65292;&#24182;&#25253;&#21578;&#20102;&#25152;&#32771;&#23519;&#24037;&#20316;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#20173;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#20043;&#22788;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21019;&#24314;&#39640;&#24230;&#36924;&#30495;&#30340;&#20266;&#36896;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#36825;&#23545;&#20154;&#20204;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#20266;&#36896;&#20869;&#23481;&#65292;&#24182;&#25552;&#37266;&#29992;&#25143;&#21487;&#33021;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#31713;&#25913;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#23457;&#35270;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#36896;&#20869;&#23481;&#26816;&#27979;&#26469;&#25299;&#23485;&#26368;&#26032;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#25152;&#32771;&#23519;&#24037;&#20316;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#20173;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#20043;&#22788;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning generative models have raised concerns as they can create highly convincing counterfeit images and videos. This poses a threat to people's integrity and can lead to social instability. To address this issue, there is a pressing need to develop new computational models that can efficiently detect forged content and alert users to potential image and video manipulations. This paper presents a comprehensive review of recent studies for deepfake content detection using deep learning-based approaches. We aim to broaden the state-of-the-art research by systematically reviewing the different categories of fake content detection. Furthermore, we report the advantages and drawbacks of the examined works and future directions towards the issues and shortcomings still unsolved on deepfake detection.
&lt;/p&gt;</description></item></channel></rss>