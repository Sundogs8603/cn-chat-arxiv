<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLEVR-POC&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#36827;&#34892;&#29702;&#24615;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#65292;&#38656;&#35201;&#21033;&#29992;&#36923;&#36753;&#32422;&#26463;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#21487;&#33021;&#30340;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.03203</link><description>&lt;p&gt;
CLEVR-POC&#65306;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#29702;&#24615;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03203
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLEVR-POC&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#36827;&#34892;&#29702;&#24615;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#65292;&#38656;&#35201;&#21033;&#29992;&#36923;&#36753;&#32422;&#26463;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#21487;&#33021;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23545;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#25972;&#21512;&#38750;&#24120;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#21033;&#29992;&#29616;&#26377;&#32972;&#26223;&#30693;&#35782;&#26469;&#25512;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#22330;&#26223;&#20197;&#22238;&#31572;&#20851;&#20110;&#22330;&#26223;&#30340;&#38382;&#39064;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#25105;&#20204;&#20154;&#31867;&#32463;&#24120;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#26469;&#25512;&#26029;&#35270;&#35273;&#38382;&#39064;&#30340;&#21512;&#29702;&#31572;&#26696;&#65292;&#36825;&#31181;&#30693;&#35782;&#36890;&#24120;&#20197;&#23545;&#35937;&#32422;&#26463;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#24182;&#19988;&#24448;&#24448;&#39640;&#24230;&#29305;&#23450;&#20110;&#39046;&#22495;&#25110;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLEVR-POC&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#21463;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#36827;&#34892;&#29702;&#24615;&#23494;&#38598;&#22411;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12290;&#22312;CLEVR-POC&#20013;&#65292;&#38656;&#35201;&#21033;&#29992;&#36923;&#36753;&#32422;&#26463;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#20851;&#20110;&#32473;&#23450;&#23616;&#37096;&#22330;&#26223;&#20013;&#38544;&#34255;&#23545;&#35937;&#30340;&#38382;&#39064;&#30340;&#21512;&#29702;&#31572;&#26696;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#26576;&#20154;&#30693;&#36947;&#25152;&#26377;&#26479;&#23376;&#35201;&#20040;&#34987;&#28034;&#25104;&#32418;&#33394;&#12289;&#32511;&#33394;&#25110;&#34013;&#33394;&#65292;&#24182;&#19988;&#26377;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03203v1 Announce Type: new  Abstract: The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22522;&#20110;GPT-4&#30340;&#23450;&#21046;AI&#21161;&#25163;&#65292;&#26088;&#22312;&#20419;&#36827;&#20915;&#31574;&#32773;&#12289;&#26222;&#36890;&#20844;&#20247;&#21644;&#27946;&#27700;&#39044;&#25253;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#65292;&#25552;&#21319;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#20247;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03188</link><description>&lt;p&gt;
&#36798;&#25104;&#27665;&#20027;&#21270;&#27946;&#27700;&#39118;&#38505;&#31649;&#29702;&#65306;&#22522;&#20110;GPT-4&#30340;&#20808;&#36827;AI&#21161;&#25163;&#23454;&#29616;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#20247;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03188
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22522;&#20110;GPT-4&#30340;&#23450;&#21046;AI&#21161;&#25163;&#65292;&#26088;&#22312;&#20419;&#36827;&#20915;&#31574;&#32773;&#12289;&#26222;&#36890;&#20844;&#20247;&#21644;&#27946;&#27700;&#39044;&#25253;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#65292;&#25552;&#21319;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#20247;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#27946;&#27700;&#39044;&#27979;&#22312;&#20419;&#36827;&#21450;&#26102;&#26377;&#25928;&#30340;&#24212;&#24613;&#21709;&#24212;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#22312;&#20110;&#24357;&#21512;&#22797;&#26434;&#25968;&#23383;&#27946;&#27700;&#27169;&#22411;&#19982;&#23454;&#38469;&#20915;&#31574;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20915;&#31574;&#32773;&#32463;&#24120;&#20381;&#36182;&#19987;&#23478;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27946;&#27700;&#20943;&#28798;&#31574;&#30053;&#12290;&#20844;&#20247;&#38656;&#35201;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#35843;&#26597;&#21644;&#29702;&#35299;&#31038;&#20250;&#25991;&#21270;&#21644;&#21046;&#24230;&#22240;&#32032;&#65292;&#36825;&#32463;&#24120;&#38459;&#30861;&#20102;&#20844;&#20247;&#23545;&#27946;&#27700;&#39118;&#38505;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#39033;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65306;&#30001;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#23450;&#21046;AI&#21161;&#25163;&#12290;&#36825;&#20010;AI&#21161;&#25163;&#26088;&#22312;&#20419;&#36827;&#20915;&#31574;&#32773;&#12289;&#26222;&#36890;&#20844;&#20247;&#21644;&#27946;&#27700;&#39044;&#25253;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#65292;&#32780;&#26080;&#38656;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#19968;&#26032;&#26694;&#26550;&#21033;&#29992;&#20102;GPT-4&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03188v1 Announce Type: new  Abstract: Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses. However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making. Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies. And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks. To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model. This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge. The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to pr
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#30340;&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;LMs&#65292;&#20316;&#32773;&#20027;&#24352;&#20351;&#29992;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03187</link><description>&lt;p&gt;
&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;&#21487;&#38752;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#21487;&#36861;&#28335;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reliable, Adaptable, and Attributable Language Models with Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03187
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#30340;&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;LMs&#65292;&#20316;&#32773;&#20027;&#24352;&#20351;&#29992;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#28023;&#37327;&#32593;&#32476;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#21644;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#24187;&#35273;&#12289;&#38590;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#20998;&#24067;&#21644;&#32570;&#20047;&#21487;&#39564;&#35777;&#24615;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#29992;&#20855;&#22791;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#21462;&#20195;&#21442;&#25968;&#21270;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25972;&#21512;&#22823;&#35268;&#27169;&#25968;&#25454;&#23384;&#20648;&#65292;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#21487;&#20197;&#26356;&#21152;&#21487;&#38752;&#12289;&#36866;&#24212;&#24615;&#26356;&#24378;&#12289;&#21487;&#36861;&#28335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03187v1 Announce Type: cross  Abstract: Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65292;&#36890;&#36807;&#22522;&#20110;&#23631;&#24149;&#22270;&#20687;&#21644;&#21487;&#33021;&#30340;&#38899;&#39057;&#36755;&#20837;&#30340;&#22522;&#37329;&#20195;&#29702;&#26694;&#26550;Cradle&#65292;&#23454;&#29616;&#20102;&#23545;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.03186</link><description>&lt;p&gt;
&#36890;&#24448;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65306;&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65292;&#36890;&#36807;&#22522;&#20110;&#23631;&#24149;&#22270;&#20687;&#21644;&#21487;&#33021;&#30340;&#38899;&#39057;&#36755;&#20837;&#30340;&#22522;&#37329;&#20195;&#29702;&#26694;&#26550;Cradle&#65292;&#23454;&#29616;&#20102;&#23545;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22522;&#37329;&#30340;&#20195;&#29702;&#22312;&#29305;&#23450;&#20219;&#21153;&#25110;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#26080;&#27861;&#36328;&#19981;&#21516;&#22330;&#26223;&#27867;&#21270;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#35821;&#20041;&#24046;&#36317;&#65292;&#25110;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65306;&#36890;&#36807;&#20165;&#33719;&#21462;&#35745;&#31639;&#26426;&#30340;&#23631;&#24149;&#22270;&#20687;&#65288;&#20197;&#21450;&#21487;&#33021;&#30340;&#38899;&#39057;&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#20316;&#20026;&#36755;&#20986;&#65292;&#31867;&#20284;&#20110;&#20154;&#26426;&#20132;&#20114;&#65292;&#26500;&#24314;&#21487;&#20197;&#31934;&#36890;&#20219;&#20309;&#35745;&#31639;&#26426;&#20219;&#21153;&#30340;&#22522;&#37329;&#20195;&#29702;&#12290;&#20026;&#20102;&#38024;&#23545;GCC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Cradle&#65292;&#19968;&#20010;&#20195;&#29702;&#26694;&#26550;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#25105;&#21453;&#24605;&#12289;&#20219;&#21153;&#25512;&#29702;&#21644;&#25216;&#33021;&#25972;&#29702;&#65292;&#20197;&#30830;&#20445;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#21644;&#33258;&#25105;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;Cradle&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20854;&#37096;&#32626;&#22312;&#22797;&#26434;&#30340;AAA&#28216;&#25103;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#65292;&#20316;&#20026;&#36890;&#21521;G&#30340;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03186v1 Announce Type: new  Abstract: Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios. However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks. To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards G
&lt;/p&gt;</description></item><item><title>&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03185</link><description>&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Preventing Reward Hacking with Occupancy Measure Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03185
&lt;/p&gt;
&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#26681;&#25454;&#19968;&#20010;&#8220;&#20195;&#29702;&#8221;&#22870;&#21169;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30456;&#23545;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#22870;&#21169;&#21364;&#34920;&#29616;&#31967;&#31957;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#22870;&#21169;&#27450;&#39575;&#12290;&#30001;&#20110;&#30830;&#20445;&#20195;&#29702;&#21644;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#26497;&#20026;&#22256;&#38590;&#65292;&#39044;&#38450;&#22870;&#21169;&#27450;&#39575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20445;&#23432;&#22320;&#20248;&#21270;&#20195;&#29702;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#36890;&#36807;&#24809;&#32602;&#20182;&#20204;&#30340;&#34892;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#24378;&#21046;&#35753;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#23433;&#20840;&#8221;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#21333;&#20010;&#29366;&#24577;&#19979;&#34892;&#20026;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#32780;&#36739;&#22823;&#30340;&#21464;&#21270;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#20219;&#20309;&#21361;&#38505;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#65292;&#24403;&#22870;&#21169;&#27450;&#39575;&#26102;&#65292;&#20195;&#29702;&#35775;&#38382;&#30340;&#29366;&#24577;&#19982;&#23433;&#20840;&#31574;&#30053;&#36798;&#21040;&#30340;&#29366;&#24577;&#25130;&#28982;&#19981;&#21516;&#65292;&#23548;&#33268;&#29366;&#24577;&#21344;&#29992;&#24230;&#30340;&#24040;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
&lt;/p&gt;</description></item><item><title>Transformers&#33021;&#22815;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;Transformer&#21487;&#20197;&#22312; logistic &#22238;&#24402;&#20219;&#21153;&#20013;&#36817;&#20284;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#21363;&#20351;&#26159;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;Transformer&#20063;&#21487;&#20197;&#23454;&#29616;&#30697;&#38453;&#27714;&#36870;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;</title><link>https://arxiv.org/abs/2403.03183</link><description>&lt;p&gt;
Transformers&#33021;&#22810;&#22909;&#22320;&#27169;&#25311; Newton &#26041;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Well Can Transformers Emulate In-context Newton's Method?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03183
&lt;/p&gt;
&lt;p&gt;
Transformers&#33021;&#22815;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;Transformer&#21487;&#20197;&#22312; logistic &#22238;&#24402;&#20219;&#21153;&#20013;&#36817;&#20284;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#21363;&#20351;&#26159;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;Transformer&#20063;&#21487;&#20197;&#23454;&#29616;&#30697;&#38453;&#27714;&#36870;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#22522;&#30784;&#26426;&#21046;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#20197;&#23454;&#29616;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#29978;&#33267;&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#24773;&#20917;&#65292;&#21487;&#20197;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#26159;&#21542;&#33021;&#22815;&#25191;&#34892;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30830;&#23450;&#20855;&#26377;ReLU&#23618;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;Transformer&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#23545;&#25968;&#21040;&#38169;&#35823;&#26356;&#22810;&#30340;&#23618;&#21487;&#20197;&#36798;&#21040;$\epsilon$&#35823;&#24046;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#20165;&#20855;&#26377;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;Transformer&#20063;&#21487;&#20197;&#22312;&#20165;&#20004;&#23618;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30697;&#38453;&#27714;&#36870;&#30340;&#29275;&#39039;&#36845;&#20195;&#30340;&#21333;&#27493;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;Transformer&#26550;&#26500;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03183v1 Announce Type: cross  Abstract: Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to im
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.03181</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#21160;&#20316;&#30340;&#34892;&#20026;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Behavior Generation with Latent Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03181
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22797;&#26434;&#34892;&#20026;&#30340;&#29983;&#25104;&#24314;&#27169;&#19968;&#30452;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#19982;&#35821;&#35328;&#25110;&#22270;&#20687;&#29983;&#25104;&#19981;&#21516;&#65292;&#20915;&#31574;&#21046;&#23450;&#38656;&#35201;&#24314;&#27169;&#21160;&#20316; - &#36830;&#32493;&#20540;&#21521;&#37327;&#65292;&#20854;&#22312;&#20998;&#24067;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21487;&#33021;&#26469;&#33258;&#26410;&#32463;&#31579;&#36873;&#30340;&#26469;&#28304;&#65292;&#22312;&#39034;&#24207;&#39044;&#27979;&#20013;&#29983;&#25104;&#35823;&#24046;&#21487;&#33021;&#20250;&#30456;&#20114;&#32047;&#31215;&#12290;&#26368;&#36817;&#19968;&#31867;&#31216;&#20026;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;k-means&#32858;&#31867;&#23545;&#21160;&#20316;&#36827;&#34892;&#31163;&#25955;&#21270;&#20197;&#25429;&#25417;&#19981;&#21516;&#27169;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;k-means&#22312;&#22788;&#29702;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#25110;&#38271;&#24207;&#21015;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#32570;&#20047;&#26799;&#24230;&#20449;&#24687;&#65292;&#22240;&#27492;BeT&#22312;&#24314;&#27169;&#38271;&#36317;&#31163;&#21160;&#20316;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30690;&#37327;&#37327;&#21270;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;VQ-BeT&#65289;&#30340;&#22810;&#21151;&#33021;&#34892;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#21160;&#20316;&#39044;&#27979;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#37096;&#20998;&#35266;&#23519;&#12290;VQ-BeT&#36890;&#36807;&#23545;&#36830;&#32493;&#21160;&#20316;&#36827;&#34892;&#26631;&#35760;&#21270;&#26469;&#22686;&#24378;BeT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03181v1 Announce Type: cross  Abstract: Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#35268;&#21010;&#23450;&#20041;&#21487;&#20197;&#32479;&#19968;&#20026;&#22522;&#20110;&#25903;&#37197;&#20851;&#31995;&#30340;&#19968;&#20010;&#23450;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#35748;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#35748;&#35777;&#21508;&#31181;&#39640;&#36136;&#37327;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26469;&#26377;&#25928;&#22320;&#35748;&#35777;&#26080;&#29615;&#39640;&#36136;&#37327;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.03176</link><description>&lt;p&gt;
&#32479;&#19968;&#24182;&#35748;&#35777;&#39640;&#36136;&#37327;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Unifying and Certifying Top-Quality Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03176
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#35268;&#21010;&#23450;&#20041;&#21487;&#20197;&#32479;&#19968;&#20026;&#22522;&#20110;&#25903;&#37197;&#20851;&#31995;&#30340;&#19968;&#20010;&#23450;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#35748;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#35748;&#35777;&#21508;&#31181;&#39640;&#36136;&#37327;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26469;&#26377;&#25928;&#22320;&#35748;&#35777;&#26080;&#29615;&#39640;&#36136;&#37327;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#22330;&#26223;&#20013;&#23545;&#35268;&#21010;&#24037;&#20855;&#30340;&#26085;&#30410;&#21033;&#29992;&#24341;&#21457;&#20102;&#23545;&#29983;&#25104;&#22810;&#20010;&#39640;&#36136;&#37327;&#35745;&#21010;&#30340;&#20852;&#36259;&#12290;&#22240;&#27492;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#28085;&#30422;&#39030;&#32423;&#36136;&#37327;&#35268;&#21010;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#27599;&#20010;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#23450;&#20041;&#21487;&#20197;&#32479;&#19968;&#20026;&#22522;&#20110;&#25903;&#37197;&#20851;&#31995;&#30340;&#19968;&#20010;&#23450;&#20041;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#35745;&#31639;&#38382;&#39064;&#31616;&#21333;&#22320;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#25903;&#37197;&#20851;&#31995;&#12290;&#22312;&#32473;&#23450;&#32479;&#19968;&#23450;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#35777;&#26126;&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#36136;&#37327;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#19981;&#21487;&#35299;&#24615;&#21644;&#26368;&#20248;&#24615;&#35748;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#21457;&#29616;&#30340;&#20219;&#21153;&#36716;&#25442;&#21487;&#29992;&#20110;&#26377;&#25928;&#35748;&#35777;&#21508;&#31181;&#39640;&#36136;&#37327;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36716;&#25442;&#26469;&#26377;&#25928;&#22320;&#35748;&#35777;&#26080;&#29615;&#39640;&#36136;&#37327;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03176v1 Announce Type: new  Abstract: The growing utilization of planning tools in practical scenarios has sparked an interest in generating multiple high-quality plans. Consequently, a range of computational problems under the general umbrella of top-quality planning were introduced over a short time period, each with its own definition. In this work, we show that the existing definitions can be unified into one, based on a dominance relation. The different computational problems, therefore, simply correspond to different dominance relations. Given the unified definition, we can now certify the top-quality of the solutions, leveraging existing certification of unsolvability and optimality. We show that task transformations found in the existing literature can be employed for the efficient certification of various top-quality planning problems and propose a novel transformation to efficiently certify loopless top-quality planning.
&lt;/p&gt;</description></item><item><title>MOKA&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.03174</link><description>&lt;p&gt;
MOKA&#65306;&#22522;&#20110;&#26631;&#35760;&#30340;&#35270;&#35273;&#25552;&#31034;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03174
&lt;/p&gt;
&lt;p&gt;
MOKA&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#30340;&#27867;&#21270;&#35201;&#27714;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#28041;&#21450;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#29615;&#22659;&#20197;&#21450;&#20219;&#21153;&#30446;&#26631;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOKA&#65288;Marking Open-vocabulary Keypoint Affordances&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26469;&#35299;&#20915;&#30001;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#25551;&#36848;&#25351;&#23450;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03174v1 Announce Type: cross  Abstract: Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from br
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20849;&#35782;&#26426;&#21046;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#24819;&#35937;&#26694;&#26550;&#24341;&#23548;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03172</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#24819;&#35937;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20849;&#35782;&#26426;&#21046;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#24819;&#35937;&#26694;&#26550;&#24341;&#23548;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36798;&#25104;&#19968;&#33268;&#24847;&#35265;&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23436;&#25104;&#21327;&#20316;&#20219;&#21153;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#21327;&#35843;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#32852;&#21512;&#21160;&#20316;&#65292;&#20197;&#26368;&#22823;&#21270;&#22242;&#38431;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#19981;&#26126;&#30830;&#32771;&#34385;&#19968;&#33268;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21327;&#35843;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20849;&#35782;&#26426;&#21046;&#65292;&#20197;&#26126;&#30830;&#21327;&#35843;&#22810;&#20010;&#26234;&#33021;&#20307;&#12290;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#24819;&#35937;&#65288;MAGI&#65289;&#26694;&#26550;&#24341;&#23548;&#26234;&#33021;&#20307;&#36890;&#36807;&#24819;&#35937;&#20986;&#30340;&#20849;&#21516;&#30446;&#26631;&#36798;&#25104;&#19968;&#33268;&#12290;&#20849;&#21516;&#30446;&#26631;&#26159;&#19968;&#20010;&#20855;&#26377;&#39640;&#20215;&#20540;&#30340;&#21487;&#23454;&#29616;&#29366;&#24577;&#65292;&#36890;&#36807;&#20174;&#26410;&#26469;&#29366;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#33719;&#24471;&#12290;&#25105;&#20204;&#30452;&#25509;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#23545;&#27492;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27169;&#22411;&#26041;&#27861;&#20013;&#24120;&#29992;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#27493;&#39588;&#31574;&#30053;&#23637;&#24320;&#24341;&#36215;&#30340;&#8220;&#32500;&#24230;&#28798;&#38590;&#8221;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#39640;&#25928;&#30340;&#20849;&#35782;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03172v1 Announce Type: new  Abstract: Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the "curse of dimensinality" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient c
&lt;/p&gt;</description></item><item><title>SNIFFER&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#36229;&#20986;&#19978;&#19979;&#25991;&#35823;&#20256;&#26816;&#27979;&#21644;&#35299;&#37322;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#25351;&#23548;&#24494;&#35843;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2403.03170</link><description>&lt;p&gt;
SNIFFER: &#21487;&#35299;&#37322;&#30340;&#36328;&#25991;&#26412;&#20449;&#24687;&#35823;&#20256;&#26816;&#27979;&#30340;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03170
&lt;/p&gt;
&lt;p&gt;
SNIFFER&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#36229;&#20986;&#19978;&#19979;&#25991;&#35823;&#20256;&#26816;&#27979;&#21644;&#35299;&#37322;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#25351;&#23548;&#24494;&#35843;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#39640;&#39118;&#38505;&#12290;&#36229;&#20986;&#19978;&#19979;&#25991;&#65288;OOC&#65289;&#30340;&#35823;&#20256;&#65292;&#21363;&#30495;&#23454;&#22270;&#20687;&#34987;&#20266;&#36896;&#30340;&#25991;&#26412;&#20877;&#21033;&#29992;&#65292;&#26159;&#35823;&#23548;&#35266;&#20247;&#30340;&#26368;&#31616;&#21333;&#21644;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#19968;&#33268;&#24615;&#65292;&#20294;&#32570;&#20047;&#35828;&#26381;&#21147;&#30340;&#35299;&#37322;&#26469;&#25903;&#25345;&#20182;&#20204;&#30340;&#21028;&#26029;&#65292;&#36825;&#23545;&#20110;&#25581;&#31034;&#35823;&#20256;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#20869;&#22312;&#30340;&#35270;&#35273;&#25512;&#29702;&#21644;&#35299;&#37322;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#29702;&#35299;&#21644;&#21457;&#29616;&#24494;&#22937;&#30340;&#36328;&#27169;&#24577;&#24046;&#24322;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SNIFFER&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;OOC&#35823;&#20256;&#26816;&#27979;&#21644;&#35299;&#37322;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;SNIFFER&#22312;InstructBLIP&#19978;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#25351;&#23548;&#24494;&#35843;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#26032;&#38395;&#39046;&#22495;&#23454;&#20307;&#19982;&#36890;&#29992;&#23545;&#35937;&#30340;&#27169;&#22411;&#27010;&#24565;&#23545;&#40784;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03170v1 Announce Type: cross  Abstract: Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#25511;&#21046;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21644;&#26465;&#20214;&#25968;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20445;&#35777;&#25968;&#25454;&#22312;&#31232;&#30095;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#34920;&#31034;&#30340;&#20248;&#21270;&#21464;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.03168</link><description>&lt;p&gt;
&#23398;&#20064;&#26174;&#24335;&#26465;&#20214;&#21270;&#31232;&#30095;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Learning Explicitly Conditioned Sparsifying Transforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#25511;&#21046;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21644;&#26465;&#20214;&#25968;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20445;&#35777;&#25968;&#25454;&#22312;&#31232;&#30095;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#34920;&#31034;&#30340;&#20248;&#21270;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#31232;&#30095;&#21270;&#21464;&#25442;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#26576;&#20123;&#21464;&#25442;&#22495;&#20013;&#25214;&#21040;&#20449;&#21495;&#30340;&#32467;&#26500;&#31232;&#30095;&#34920;&#31034;&#12290;&#23613;&#31649;&#20687;DCT&#21644;&#23567;&#27874;&#36825;&#26679;&#30340;&#32463;&#20856;&#21464;&#25442;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#26368;&#36817;&#22312;&#19968;&#31995;&#21015;&#35770;&#25991;&#20013;&#24050;&#32463;&#20998;&#26512;&#20102;&#23398;&#20064;&#20445;&#35777;&#25968;&#25454;&#22312;&#31232;&#30095;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#34920;&#31034;&#30340;&#26368;&#20248;&#21464;&#25442;&#12290;&#23398;&#20064;&#26041;&#22359;&#21464;&#25442;&#30340;&#26465;&#20214;&#25968;&#21644;&#34920;&#31034;&#33021;&#21147;&#36890;&#24120;&#26159;&#20114;&#34917;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21487;&#33021;&#22312;&#32473;&#23450;&#30340;&#20248;&#21270;&#27169;&#22411;&#20013;&#19981;&#33021;&#26126;&#30830;&#25511;&#21046;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#25442;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24378;&#21046;&#22312;&#23398;&#20064;&#21464;&#25442;&#30340;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21644;&#26465;&#20214;&#25968;&#19978;&#36827;&#34892;&#26174;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#30830;&#35748;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03168v1 Announce Type: cross  Abstract: Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#26234;&#33021;&#32467;&#21512;&#20102;AI&#21644;&#36793;&#32536;&#35745;&#31639;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;FL&#32593;&#32476;&#20013;&#25552;&#20986;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03165</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#23454;&#29616;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03165
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#32467;&#21512;&#20102;AI&#21644;&#36793;&#32536;&#35745;&#31639;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;FL&#32593;&#32476;&#20013;&#25552;&#20986;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#22823;&#35268;&#27169;&#39640;&#25928;&#37096;&#32626;&#65292;AI&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#32467;&#21512;&#20135;&#29983;&#20102;&#36793;&#32536;&#26234;&#33021;&#65292;&#21033;&#29992;&#26411;&#31471;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#33021;&#21147;&#26469;&#26356;&#25509;&#36817;&#25968;&#25454;&#29983;&#25104;&#22320;&#22788;&#29702;&#25968;&#25454;&#12290;&#36793;&#32536;&#26234;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#20351;&#25968;&#25454;&#25152;&#26377;&#32773;&#33021;&#22815;&#22312;&#26080;&#38656;&#23558;&#21407;&#22987;&#25968;&#25454;&#20256;&#36755;&#33267;&#31532;&#19977;&#26041;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;FL&#32593;&#32476;&#39044;&#35745;&#20250;&#28041;&#21450;&#25968;&#21315;&#20010;&#24322;&#26500;&#20998;&#24067;&#24335;&#35774;&#22791;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#25925;&#38556;&#21644;&#35774;&#22791;&#36864;&#20986;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25351;&#23450;&#30340;&#38598;&#32676;&#39046;&#23548;&#32773;&#36890;&#36807;&#20013;&#38388;&#27169;&#22411;&#32858;&#21512;&#26469;&#25903;&#25345;&#25968;&#25454;&#25152;&#26377;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03165v1 Announce Type: new  Abstract: To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated. A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers. However, FL networks are expected to involve thousands of heterogeneous distributed devices. As a result, communication efficiency remains a key bottleneck. To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation. Therefore, based on the improvement of edge server resource utilizatio
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#22320;&#25191;&#34892;&#29702;&#35770;&#29289;&#29702;&#30740;&#31350;&#35770;&#25991;&#20013;&#20851;&#38190;&#30340;Hartree-Fock&#26041;&#27861;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.03154</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#22810;&#20307;&#29289;&#29702;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum Many-Body Physics Calculations with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03154
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#22320;&#25191;&#34892;&#29702;&#35770;&#29289;&#29702;&#30740;&#31350;&#35770;&#25991;&#20013;&#20851;&#38190;&#30340;Hartree-Fock&#26041;&#27861;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#25968;&#23398;&#21644;&#31185;&#23398;&#25512;&#29702;&#65289;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#21069;&#25152;&#26410;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20934;&#30830;&#22320;&#25191;&#34892;&#29702;&#35770;&#29289;&#29702;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#20851;&#38190;&#35745;&#31639;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#37327;&#23376;&#29289;&#29702;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#36817;&#20284;&#26041;&#27861;&#65306;Hartree-Fock&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#20998;&#26512;&#24615;&#30340;&#22810;&#27493;&#35745;&#31639;&#65292;&#23548;&#20986;&#36817;&#20284;&#21704;&#23494;&#39039;&#37327;&#21644;&#30456;&#24212;&#30340;&#33258;&#27965;&#26041;&#31243;&#12290;&#20026;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#35745;&#31639;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#27493;&#25552;&#31034;&#27169;&#26495;&#65292;&#23558;&#20998;&#26512;&#35745;&#31639;&#25286;&#20998;&#20026;&#26631;&#20934;&#27493;&#39588;&#65292;&#24182;&#20026;&#38382;&#39064;&#29305;&#23450;&#20449;&#24687;&#30041;&#20986;&#21344;&#20301;&#31526;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#22312;&#25191;&#34892;&#36807;&#21435;&#21313;&#24180;&#30340;15&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#35745;&#31639;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20462;&#27491;&#20013;&#38388;&#27493;&#39588;&#65292;&#23427;&#21487;&#20197;&#27491;&#30830;&#22320;&#25512;&#23548;&#20986;&#26368;&#32456;&#30340;Hartree-Fock&#21704;&#23494;&#39039;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03154v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#65292;&#19982;&#20043;&#21069;&#22797;&#26434;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#33021;&#27867;&#21270;&#65292;&#21448;&#33021;&#20026;&#29702;&#35770;&#29702;&#35299;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.03134</link><description>&lt;p&gt;
&#22797;&#26434;&#20013;&#30340;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Simplicity in Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#65292;&#19982;&#20043;&#21069;&#22797;&#26434;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#33021;&#27867;&#21270;&#65292;&#21448;&#33021;&#20026;&#29702;&#35770;&#29702;&#35299;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21050;&#28608;&#30340;&#22797;&#26434;&#24615;&#22312;&#35768;&#22810;&#35748;&#30693;&#29616;&#35937;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#27880;&#24847;&#21147;&#12289;&#21442;&#19982;&#24230;&#12289;&#26131;&#35760;&#24615;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#32654;&#23398;&#35780;&#20215;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#22797;&#26434;&#24615;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#65292;&#35773;&#21050;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#24403;&#22797;&#26434;&#12290;&#26089;&#20808;&#30340;&#27169;&#22411;&#35797;&#22270;&#23547;&#25214;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#26469;&#35299;&#37322;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#26159;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#65292;&#22240;&#27492;&#26080;&#27861;&#27867;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#19981;&#25351;&#23548;&#23545;&#38382;&#39064;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;SAM&#21644;FC-CLIP&#65292;&#26469;&#37327;&#21270;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#31890;&#24230;&#30340;&#21306;&#27573;&#25968;&#37327;&#65292;&#20197;&#21450;&#22270;&#20687;&#20013;&#30340;&#31867;&#21035;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03134v1 Announce Type: cross  Abstract: The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31454;&#20105;&#24615;&#35774;&#26045;&#36873;&#22336;&#30340;&#20004;&#38454;&#27573;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#38024;&#23545;&#19981;&#21487;&#20998;&#21106;&#26435;&#37325;&#30340;&#23458;&#25143;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35774;&#26045;&#24067;&#32622;&#26102;&#21487;&#33021;&#20986;&#29616;&#25130;&#28982;&#19981;&#21516;&#30340;&#23458;&#25143;&#22343;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.03114</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;&#35774;&#26045;&#36873;&#22336;&#20013;&#30340;&#22343;&#34913;&#19982;&#21407;&#23376;&#23458;&#25143;
&lt;/p&gt;
&lt;p&gt;
Equilibria in Two-Stage Facility Location with Atomic Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31454;&#20105;&#24615;&#35774;&#26045;&#36873;&#22336;&#30340;&#20004;&#38454;&#27573;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#38024;&#23545;&#19981;&#21487;&#20998;&#21106;&#26435;&#37325;&#30340;&#23458;&#25143;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35774;&#26045;&#24067;&#32622;&#26102;&#21487;&#33021;&#20986;&#29616;&#25130;&#28982;&#19981;&#21516;&#30340;&#23458;&#25143;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31454;&#20105;&#24615;&#35774;&#26045;&#36873;&#22336;&#35270;&#20026;&#19968;&#20010;&#24102;&#26377;&#20004;&#31181;&#23458;&#25143;&#31867;&#22411;&#30340;&#20004;&#38454;&#27573;&#22810;&#20195;&#29702;&#31995;&#32479;&#12290;&#23545;&#20110;&#20855;&#26377;&#21152;&#26435;&#23458;&#25143;&#30340;&#20027;&#26426;&#22270;&#65292;&#39318;&#20808;&#35774;&#26045;&#20195;&#29702;&#32773;&#25112;&#30053;&#24615;&#22320;&#36873;&#25321;&#24320;&#35774;&#35774;&#26045;&#30340;&#39030;&#28857;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#25112;&#30053;&#24615;&#22320;&#36873;&#25321;&#22312;&#20854;&#37051;&#22495;&#20869;&#30340;&#21738;&#20010;&#24320;&#35774;&#35774;&#26045;&#28040;&#36153;&#12290;&#35774;&#26045;&#24076;&#26395;&#23613;&#21487;&#33021;&#21560;&#24341;&#26356;&#22810;&#23458;&#25143;&#26435;&#37325;&#65292;&#23458;&#25143;&#24076;&#26395;&#26368;&#23567;&#21270;&#25152;&#36873;&#35774;&#26045;&#19978;&#30340;&#25317;&#25380;&#12290;&#25152;&#26377;&#26368;&#36817;&#30740;&#31350;&#30340;&#27492;&#27169;&#22411;&#29256;&#26412;&#37117;&#20551;&#23450;&#23458;&#25143;&#21487;&#20197;&#25112;&#30053;&#24615;&#22320;&#20998;&#25285;&#20182;&#20204;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19981;&#21487;&#20998;&#21106;&#26435;&#37325;&#30340;&#23458;&#25143;&#65292;&#20294;&#20801;&#35768;&#28151;&#21512;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#21487;&#20197;&#22312;&#21738;&#20010;&#35774;&#26045;&#28040;&#36153;&#19978;&#38543;&#26426;&#36873;&#25321;&#12290;&#38500;&#20102;&#23545;&#33258;&#28982;&#23458;&#25143;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#22806;&#65292;&#36825;&#31181;&#24494;&#22937;&#30340;&#21464;&#21270;&#20250;&#20135;&#29983; drast &#21095;&#28872;&#30340;&#21464;&#21270;&#65292;&#20363;&#22914;&#65292;&#22312;&#32473;&#23450;&#35774;&#26045;&#24067;&#32622;&#26102;&#65292;&#21487;&#33021;&#20986;&#29616;&#25130;&#28982;&#19981;&#21516;&#30340;&#23458;&#25143;&#22343;&#34913;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#23376;&#21338;&#24328;&#23436;&#32654;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03114v1 Announce Type: cross  Abstract: We consider competitive facility location as a two-stage multi-agent system with two types of clients. For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities. Then, the clients strategically select which of the opened facilities in their neighborhood to patronize. Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility.   All recently studied versions of this model assume that clients can split their weight strategically. We consider clients with unsplittable weights, but allow mixed strategies. So clients may randomize over which facility to patronize. Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible.   As our main result, we show that pure subgame perfect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#20102;LiDAR&#25968;&#25454;&#30340;&#28857;&#32447;&#21305;&#37197;&#21644;&#28857;&#38754;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#36816;&#21160;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.03111</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#21644;&#26032;&#22411;&#31163;&#32676;&#28857;&#26816;&#27979;&#25913;&#36827;LiDAR&#37324;&#31243;&#35745;&#19982;&#22320;&#22270;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and Novel Outliers Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03111
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#20102;LiDAR&#25968;&#25454;&#30340;&#28857;&#32447;&#21305;&#37197;&#21644;&#28857;&#38754;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#36816;&#21160;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#26159;&#23454;&#29616;&#26234;&#33021;&#33258;&#20027;&#23548;&#33322;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#30340;&#35821;&#20041;&#21644;&#20934;&#30830;&#30340;&#36710;&#36742;&#23039;&#24577;&#20272;&#35745;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#31227;&#21160;&#24179;&#21488;&#30340;&#23454;&#26102;LiDAR&#37324;&#31243;&#35745;&#19982;&#22320;&#22270;&#21046;&#20316;&#65292;&#22522;&#20110;LOAM&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;LiDAR&#25195;&#25551;&#20043;&#38388;&#30340;&#28857;&#32447;&#21305;&#37197;&#21644;&#28857;&#38754;&#21305;&#37197;&#65292;&#24182;&#26500;&#24314;&#20102;&#29615;&#22659;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20174;&#32780;&#21033;&#29992;LiDAR&#25968;&#25454;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#36816;&#21160;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21305;&#37197;&#36807;&#31243;&#20013;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#20250;&#24341;&#20837;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#31163;&#32676;&#21305;&#37197;&#65292;&#21305;&#37197;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#21305;&#37197;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03111v1 Announce Type: cross  Abstract: Perception is a key element for enabling intelligent autonomous navigation. Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks. Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms. In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms. Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data. We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>VQSynergy&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#20197;&#21450;&#20854;&#20182;&#21019;&#26032;&#25216;&#26415;&#25552;&#39640;&#20102;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#39640;&#26031;&#22122;&#22768;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03089</link><description>&lt;p&gt;
VQSynergy: &#21033;&#29992;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#36827;&#34892;&#31283;&#20581;&#30340;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03089
&lt;/p&gt;
&lt;p&gt;
VQSynergy&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#20197;&#21450;&#20854;&#20182;&#21019;&#26032;&#25216;&#26415;&#25552;&#39640;&#20102;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#39640;&#26031;&#22122;&#22768;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#39640;&#36890;&#37327;&#31579;&#36873;&#21644;&#35745;&#31639;&#21019;&#26032;&#30340;&#20986;&#29616;&#24341;&#39046;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#25506;&#32034;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#26041;&#27861;&#23398;&#30340;&#36716;&#21464;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;VQSynergy&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#20102;&#30690;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26426;&#21046;&#12289;&#19982;&#38376;&#25511;&#27531;&#24046;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#26031;&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;VQSynergy&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#22797;&#26434;&#19988;&#24120;&#24120;&#22024;&#26434;&#30340;&#33647;&#29289;&#21327;&#21516;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03089v1 Announce Type: cross  Abstract: The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in rev
&lt;/p&gt;</description></item><item><title>&#26694;&#26550;&#25552;&#20986;&#20102;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;(GAMM)&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#26368;&#22823;&#21270;&#36807;&#21435;&#30693;&#35782;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03082</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;&#30340;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Recall-Oriented Continual Learning with Generative Adversarial Meta-Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03082
&lt;/p&gt;
&lt;p&gt;
&#26694;&#26550;&#25552;&#20986;&#20102;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;(GAMM)&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#26368;&#22823;&#21270;&#36807;&#21435;&#30693;&#35782;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#22256;&#22659;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#20197;&#21069;&#20219;&#21153;&#24615;&#33021;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21484;&#22238;&#23548;&#21521;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22823;&#33041;&#20998;&#31163;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26426;&#21046;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#20004;&#32423;&#20307;&#31995;&#32467;&#26500;&#65292;&#20854;&#20013;&#25512;&#29702;&#32593;&#32476;&#26377;&#25928;&#22320;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#32780;&#29983;&#25104;&#32593;&#32476;&#22312;&#38656;&#35201;&#26102;&#22238;&#39038;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#36807;&#21435;&#30693;&#35782;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#34920;&#31034;&#21462;&#20915;&#20110;&#30693;&#35782;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#22686;&#37327;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#32780;&#19981;&#26159;&#20219;&#21153;&#30340;&#36755;&#20837;&#25968;&#25454;&#26679;&#26412;&#30340;&#29983;&#25104;&#23545;&#25239;&#20803;&#27169;&#22411;&#65288;GAMM&#65289;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03082v1 Announce Type: cross  Abstract: The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#30721;&#26412;&#35774;&#35745;&#31639;&#27861;&#65292;&#32593;&#32476;&#27874;&#26463;&#23398;&#20064;&#65288;NBL&#65289;&#65292;&#33021;&#22815;&#20248;&#21270;&#30721;&#26412;&#20197;&#20943;&#23569;&#24178;&#25200;&#24182;&#26368;&#22823;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#28151;&#21512;&#38453;&#21015;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.03053</link><description>&lt;p&gt;
&#31070;&#32463;&#30721;&#26412;&#35774;&#35745;&#29992;&#20110;&#32593;&#32476;&#27874;&#26463;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Codebook Design for Network Beam Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#30721;&#26412;&#35774;&#35745;&#31639;&#27861;&#65292;&#32593;&#32476;&#27874;&#26463;&#23398;&#20064;&#65288;NBL&#65289;&#65292;&#33021;&#22815;&#20248;&#21270;&#30721;&#26412;&#20197;&#20943;&#23569;&#24178;&#25200;&#24182;&#26368;&#22823;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#28151;&#21512;&#38453;&#21015;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#20934;&#30830;&#21450;&#26102;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26159;&#22823;&#22411;&#22825;&#32447;&#31995;&#32479;&#38754;&#20020;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;&#31227;&#21160;&#31995;&#32479;&#22914;5G&#20351;&#29992;&#19968;&#20010;&#27874;&#26463;&#31649;&#29702;&#26694;&#26550;&#65292;&#23558;&#21021;&#22987;&#25509;&#20837;&#12289;&#27874;&#26463;&#25104;&#24418;&#12289;CSI&#33719;&#21462;&#21644;&#25968;&#25454;&#20256;&#36755;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12289;&#19981;&#26029;&#21464;&#21270;&#30340;&#38453;&#21015;&#22823;&#23567;&#20197;&#21450;&#29305;&#23450;&#20110;&#31449;&#28857;&#30340;&#20449;&#36947;&#21644;&#29992;&#25143;&#20998;&#24067;&#65292;&#36825;&#20123;&#38454;&#27573;&#30340;&#30721;&#26412;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#27874;&#26463;&#31649;&#29702;&#36890;&#24120;&#38598;&#20013;&#22312;&#21333;&#25159;&#21306;&#36816;&#33829;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;&#24635;&#20307;&#32593;&#32476;&#21644;&#31995;&#32479;&#32423;&#21035;&#30340;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#30721;&#26412;&#35774;&#35745;&#31639;&#27861;&#65292;&#32593;&#32476;&#27874;&#26463;&#23398;&#20064;&#65288;NBL&#65289;&#65292;&#23427;&#25429;&#25417;&#21644;&#20248;&#21270;&#30721;&#26412;&#65292;&#20197;&#20943;&#23569;&#24178;&#25200;&#65292;&#21516;&#26102;&#36890;&#36807;&#26497;&#22823;&#28151;&#21512;&#38453;&#21015;&#26368;&#22823;&#21270;&#21487;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#38656;&#35201;&#26377;&#38480;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#21364;&#35774;&#35745;&#20986;&#20248;&#20110;&#20256;&#32479;&#30721;&#26412;&#30340;&#30721;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03053v1 Announce Type: cross  Abstract: Obtaining accurate and timely channel state information (CSI) is a fundamental challenge for large antenna systems. Mobile systems like 5G use a beam management framework that joins the initial access, beamforming, CSI acquisition, and data transmission. The design of codebooks for these stages, however, is challenging due to their interrelationships, varying array sizes, and site-specific channel and user distributions. Furthermore, beam management is often focused on single-sector operations while ignoring the overarching network- and system-level optimization. In this paper, we proposed an end-to-end learned codebook design algorithm, network beamspace learning (NBL), that captures and optimizes codebooks to mitigate interference while maximizing the achievable performance with extremely large hybrid arrays. The proposed algorithm requires limited shared information yet designs codebooks that outperform traditional codebooks by over
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#30340;&#32553;&#25918;&#39033;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#31283;&#23450;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#36890;&#29992;&#24615;&#24182;&#29983;&#25104;&#20102;&#22810;&#31181;&#26377;&#21033;&#29305;&#24615;&#30340;&#22791;&#36873;&#36890;&#29992;&#20844;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.03030</link><description>&lt;p&gt;
&#32479;&#19968;&#25511;&#21046;&#22120;&#35774;&#35745;&#29992;&#20110;&#31283;&#23450;&#20855;&#26377;&#33539;&#25968;&#26377;&#30028;&#25511;&#21046;&#36755;&#20837;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#30340;&#32553;&#25918;&#39033;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#31283;&#23450;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#36890;&#29992;&#24615;&#24182;&#29983;&#25104;&#20102;&#22810;&#31181;&#26377;&#21033;&#29305;&#24615;&#30340;&#22791;&#36873;&#36890;&#29992;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35774;&#35745;&#38750;&#32447;&#24615;&#31995;&#32479;&#31283;&#23450;&#25511;&#21046;&#22120;&#38754;&#20020;&#30340;&#32463;&#20856;&#25361;&#25112;&#65292;&#36890;&#36807;&#25193;&#23637;Lin-Sontag&#30340;&#36890;&#29992;&#20844;&#24335;&#24182;&#24341;&#20837;&#19968;&#31181;&#36890;&#29992;&#65288;&#20381;&#36182;&#20110;&#29366;&#24577;&#65289;&#30340;&#32553;&#25918;&#39033;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#12290;&#24341;&#20837;&#36825;&#31181;&#36890;&#29992;&#30340;&#32553;&#25918;&#39033;&#32473;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#20351;&#24471;&#33021;&#22815;&#25512;&#23548;&#20986;&#20855;&#26377;&#21508;&#31181;&#26377;&#21033;&#29305;&#24615;&#30340;&#22791;&#36873;&#36890;&#29992;&#20844;&#24335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#23450;&#21046;&#25511;&#21046;&#35774;&#35745;&#20197;&#28385;&#36275;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#36328;&#19981;&#21516;&#25511;&#21046;&#22330;&#26223;&#30340;&#36890;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#26368;&#20248;&#32553;&#25918;&#39033;&#30340;&#26500;&#36896;&#26041;&#27861;&#65292;&#23548;&#33268;&#19968;&#20010;&#26126;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;&#22522;&#20110;&#20248;&#21270;&#30340;&#36890;&#29992;&#20844;&#24335;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#25511;&#21046;&#22120;&#30830;&#20445;&#28176;&#36817;&#31283;&#23450;&#24615;&#65292;&#28385;&#36275;&#33539;&#25968;&#26377;&#30028;&#30340;&#36755;&#20837;&#32422;&#26463;&#65292;&#24182;&#20248;&#21270;&#39044;&#23450;&#20041;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03030v1 Announce Type: cross  Abstract: This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint. By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed. The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios. Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula. The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function. Finally, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#37322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20174;&#32780;&#25581;&#31034;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03028</link><description>&lt;p&gt;
&#21333;&#35789;&#37325;&#35201;&#24615;&#35299;&#37322;&#20102;&#25552;&#31034;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Word Importance Explains How Prompts Affect Language Model Outputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03028
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#37322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20174;&#32780;&#25581;&#31034;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#34892;&#21508;&#19994;&#30340;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#8220;&#40657;&#30418;&#8221;&#24615;&#36136;&#24120;&#24120;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#20570;&#20986;&#20855;&#20307;&#20915;&#31574;&#30340;&#29702;&#35299;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#36879;&#26126;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36947;&#24503;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#26469;&#25552;&#39640;LLMs&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25581;&#31034;&#20854;&#22312;&#27169;&#22411;&#36755;&#20986;&#19978;&#30340;&#32479;&#35745;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#21463;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#37325;&#35201;&#24615;&#21551;&#21457;&#65292;&#23631;&#34109;&#31995;&#32479;&#25552;&#31034;&#20013;&#30340;&#27599;&#20010;&#21333;&#35789;&#65292;&#24182;&#26681;&#25454;&#21487;&#29992;&#25991;&#26412;&#20998;&#25968;&#22312;&#22810;&#20010;&#29992;&#25143;&#36755;&#20837;&#19978;&#36827;&#34892;&#32858;&#21512;&#26469;&#35780;&#20272;&#20854;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#27880;&#24847;&#21147;&#19981;&#21516;&#65292;&#21333;&#35789;&#37325;&#35201;&#24615;&#34913;&#37327;&#25552;&#31034;&#20013;&#21333;&#35789;&#23545;&#20219;&#24847;&#23450;&#20041;&#30340;&#25991;&#26412;&#20998;&#25968;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#33021;&#22815;&#23558;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#20998;&#35299;&#20026;&#20855;&#20307;&#30340;&#24863;&#20852;&#36259;&#30340;&#24230;&#37327;-- &#21253;&#25324;&#20559;&#35265;&#12289;&#38405;&#35835;&#27700;&#24179;&#12289;&#20887;&#20313;&#31561;&#12290;&#27492;&#31243;&#24207;&#36824;&#20351;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03028v1 Announce Type: new  Abstract: The emergence of large language models (LLMs) has revolutionized numerous applications across industries. However, their "black box" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc. This procedure also enables measur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.03020</link><description>&lt;p&gt;
SplAgger&#65306;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#21106;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SplAgger: Split Aggregation for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36825;&#20123;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19968;&#31867;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#31216;&#20026;&#40657;&#30418;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#29616;&#25104;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20043;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#21478;&#19968;&#31867;&#26041;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#30693;&#20219;&#21153;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#24207;&#21015;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20219;&#21153;&#25512;&#26029;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
&lt;/p&gt;</description></item><item><title>OPEx&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#23884;&#20837;&#24335;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#26680;&#24515;&#32452;&#20214;-&#35266;&#23519;&#32773;&#12289;&#35268;&#21010;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#35780;&#20272;&#20998;&#26512;&#20102;&#21508;&#32452;&#20214;&#23545;&#20110;&#23884;&#20837;&#24335;&#25351;&#23548;&#36981;&#24490;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03017</link><description>&lt;p&gt;
OPEx:&#23545;&#22522;&#20110;LLM&#30340;&#22352;&#26631;&#25351;&#23548;&#26234;&#33021;&#20307;&#36827;&#34892;&#32452;&#20214;&#32423;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03017
&lt;/p&gt;
&lt;p&gt;
OPEx&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#23884;&#20837;&#24335;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#26680;&#24515;&#32452;&#20214;-&#35266;&#23519;&#32773;&#12289;&#35268;&#21010;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#35780;&#20272;&#20998;&#26512;&#20102;&#21508;&#32452;&#20214;&#23545;&#20110;&#23884;&#20837;&#24335;&#25351;&#23548;&#36981;&#24490;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#25351;&#23548;&#36981;&#24490;(Embodied Instruction Following&#65292;EIF)&#26159;&#23884;&#20837;&#24335;&#23398;&#20064;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#36890;&#36807;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#35266;&#23519;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#20197;&#23436;&#25104;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#30475;&#21040;&#22312;&#20197;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#20013;&#24191;&#27867;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#22686;&#24378;&#22312;&#23884;&#20837;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;EIF&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#20851;&#21508;&#31181;&#32452;&#20214;(&#20174;&#35270;&#35273;&#24863;&#30693;&#21040;&#21160;&#20316;&#25191;&#34892;)&#23545;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#30340;&#32479;&#19968;&#29702;&#35299;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OPEx&#65292;&#19968;&#20010;&#35814;&#23613;&#30340;&#26694;&#26550;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#35299;&#20915;&#23884;&#20837;&#24335;&#23398;&#20064;&#20219;&#21153;&#25152;&#24517;&#38656;&#30340;&#26680;&#24515;&#32452;&#20214;:&#35266;&#23519;&#32773;(Observer)&#12289;&#35268;&#21010;&#32773;(Planner)&#21644;&#25191;&#34892;&#32773;(Executor)&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;EIF&#20219;&#21153;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20869;&#65292;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#31574;&#30053;&#36827;&#34892;&#20102;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03017v1 Announce Type: new  Abstract: Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#20107;&#23454;&#19978;&#19979;&#25991;&#30340;&#26469;&#28304;&#65292;&#20026;&#22522;&#20110;LLM&#30340;&#23398;&#20064;&#25512;&#33616;&#25552;&#20379;&#35299;&#37322;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#24187;&#35273;&#39118;&#38505;&#65292;&#30830;&#20445;&#39640;&#31934;&#24230;&#65292;&#24182;&#20445;&#25345;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03008</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#22522;&#20110;LLM&#30340;&#23398;&#20064;&#25512;&#33616;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#20107;&#23454;&#19978;&#19979;&#25991;&#30340;&#26469;&#28304;&#65292;&#20026;&#22522;&#20110;LLM&#30340;&#23398;&#20064;&#25512;&#33616;&#25552;&#20379;&#35299;&#37322;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#24187;&#35273;&#39118;&#38505;&#65292;&#30830;&#20445;&#39640;&#31934;&#24230;&#65292;&#24182;&#20445;&#25345;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25945;&#32946;&#26102;&#20195;&#65292;&#20026;&#23398;&#20064;&#25512;&#33616;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#23545;&#20110;&#22686;&#24378;&#23398;&#20064;&#32773;&#23545;&#25512;&#33616;&#23398;&#20064;&#20869;&#23481;&#30340;&#29702;&#35299;&#21644;&#21442;&#19982;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36817;&#26399;&#20026;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#35299;&#37322;&#65292;&#20026;&#23398;&#20064;&#25512;&#33616;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25935;&#24863;&#39046;&#22495;&#22914;&#25945;&#32946;&#20013;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#20173;&#36828;&#26410;&#36798;&#21040;&#21487;&#25509;&#21463;&#27700;&#24179;&#12290;&#20026;&#20102;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#30830;&#20445;&#23545;&#23398;&#20064;&#32773;&#24847;&#22270;&#30340;&#39640;&#31934;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20316;&#20026;&#20107;&#23454;&#19978;&#19979;&#25991;&#30340;&#26469;&#28304;&#65292;&#29992;&#20110;LLM&#25552;&#31034;&#65292;&#38477;&#20302;&#27169;&#22411;&#24187;&#35273;&#30340;&#39118;&#38505;&#65292;&#38450;&#27490;&#38169;&#35823;&#25110;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#65292;&#24182;&#20445;&#25345;&#24212;&#29992;&#24847;&#22270;&#30340;&#23398;&#20064;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#25552;&#20379;&#30456;&#20851;&#24615;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03008v1 Announce Type: new  Abstract: In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer cura
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#35760;&#24518;&#20803;&#20214;&#20132;&#21449;&#38453;&#21015;&#35774;&#35745;&#20302;&#21151;&#32791;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;memristive&#21644;memcapacitive&#20132;&#21449;&#38453;&#21015;&#22312;8&#23618;VGG&#32593;&#32476;&#19978;&#23454;&#29616;&#20986;&#33394;&#35757;&#32451;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03002</link><description>&lt;p&gt;
&#22522;&#20110;Mem&#20803;&#20214;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#22312;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mem-elements based Neuromorphic Hardware for Neural Network Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#35760;&#24518;&#20803;&#20214;&#20132;&#21449;&#38453;&#21015;&#35774;&#35745;&#20302;&#21151;&#32791;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;memristive&#21644;memcapacitive&#20132;&#21449;&#38453;&#21015;&#22312;8&#23618;VGG&#32593;&#32476;&#19978;&#23454;&#29616;&#20986;&#33394;&#35757;&#32451;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#21151;&#32791;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#20013;&#21033;&#29992;&#35760;&#24518;&#30005;&#38459;&#21644;&#35760;&#24518;&#30005;&#23481;&#20132;&#21449;&#38453;&#21015;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21327;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#28151;&#21512;Python&#21644;PyTorch&#26041;&#27861;&#23454;&#29616;&#65292;&#32771;&#34385;&#20102;&#21508;&#31181;&#38750;&#29702;&#24819;&#22240;&#32032;&#65292;&#22312;8&#23618;VGG&#32593;&#32476;&#19978;&#65292;&#20351;&#29992;&#35760;&#24518;&#30005;&#38459;&#21644;&#35760;&#24518;&#30005;&#23481;&#20132;&#21449;&#38453;&#21015;&#23545;CIFAR-10&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#35757;&#32451;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20026;90.02%&#21644;91.03%&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36816;&#31639;&#36328;&#23548;&#25918;&#22823;&#22120;&#65288;OTA&#65289;&#21644;&#30005;&#23481;&#22120;&#26469;&#27169;&#25311;meminductor&#22120;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#21487;&#35843;&#34892;&#20026;&#12290;&#22312;180&#32435;&#31859;CMOS&#25216;&#26415;&#30340;&#26230;&#20307;&#31649;&#32423;&#27169;&#25311;&#20013;&#65292;&#20197;60 MHz&#36816;&#34892;&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;meminductor&#27169;&#25311;&#22120;&#20855;&#26377;0.337 mW&#21151;&#32791;&#30340;&#21487;&#34892;&#24615;&#12290;&#35813;&#35774;&#35745;&#36827;&#19968;&#27493;&#22312;&#31070;&#32463;&#24418;&#24577;&#30005;&#36335;&#21644;CNN&#21152;&#36895;&#22120;&#20013;&#39564;&#35777;&#65292;&#21462;&#24471;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03002v1 Announce Type: cross  Abstract: The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN). The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network. Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior. Transistor-level simulations in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator's viability with a power consumption of 0.337 mW. The design is further validated in neuromorphic circuits and CNN accelerators, achieving training and testing ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#38598;&#25104;&#26641;&#26469;&#32531;&#35299;&#24694;&#24847;URL&#26816;&#27979;&#22120;&#20013;&#30340;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#38598;&#25104;&#38450;&#24481;&#26426;&#21046;&#20197;&#38450;&#33539;&#28508;&#22312;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.02995</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#26641;&#22312;&#24694;&#24847;URL&#26816;&#27979;&#22120;&#20013;&#32531;&#35299;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#38598;&#25104;&#26641;&#26469;&#32531;&#35299;&#24694;&#24847;URL&#26816;&#27979;&#22120;&#20013;&#30340;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#38598;&#25104;&#38450;&#24481;&#26426;&#21046;&#20197;&#38450;&#33539;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;URL&#25552;&#20379;&#20102;&#36328;&#21508;&#34892;&#19994;&#65288;&#21253;&#25324;&#20132;&#36890;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33021;&#28304;&#21644;&#38134;&#34892;&#19994;&#65289;&#30340;&#23545;&#25239;&#24615;&#26426;&#20250;&#65292;&#21487;&#33021;&#23545;&#19994;&#21153;&#36816;&#33829;&#36896;&#25104;&#37325;&#22823;&#25439;&#23475;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#20123;URL&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65307;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#25805;&#32437;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#26631;&#31614;&#65292;&#22914;&#26631;&#31614;&#32763;&#36716;&#65288;LF&#65289;&#65292;&#23558;&#33391;&#24615;&#26631;&#31614;&#26356;&#25913;&#20026;&#24694;&#24847;&#26631;&#31614;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#36825;&#31181;&#25805;&#32437;&#23548;&#33268;&#35823;&#20998;&#31867;&#65292;&#24182;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#19981;&#27491;&#30830;&#12290;&#22240;&#27492;&#65292;&#22312;ML&#27169;&#22411;&#26550;&#26500;&#20013;&#38598;&#25104;&#38450;&#24481;&#26426;&#21046;&#25104;&#20026;&#21152;&#22266;&#28508;&#22312;&#25915;&#20987;&#30340;&#24517;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#22312;&#20351;&#29992;&#38598;&#25104;&#26641;&#36827;&#34892;URL&#26816;&#27979;&#32972;&#26223;&#19979;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#38416;&#26126;&#27492;&#31867;&#25915;&#20987;&#32972;&#21518;&#30340;&#21160;&#26426;&#65292;&#31361;&#20986;&#25915;&#20987;&#32773;&#30340;&#35282;&#33394;&#65292;&#24182;&#24378;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02995v1 Announce Type: cross  Abstract: Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#21270;&#30340;&#38646;&#38454;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#23616;&#37096;&#26368;&#20248;&#35299;&#36890;&#24120;&#26222;&#36941;&#23384;&#22312;&#19988;&#34920;&#29616;&#33391;&#22909;&#65292;&#26377;&#21161;&#20110;&#39640;&#25928;&#30340;&#25552;&#31034;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.02993</link><description>&lt;p&gt;
&#26412;&#22320;&#21270;&#30340;&#38646;&#38454;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localized Zeroth-Order Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02993
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#21270;&#30340;&#38646;&#38454;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#23616;&#37096;&#26368;&#20248;&#35299;&#36890;&#24120;&#26222;&#36941;&#23384;&#22312;&#19988;&#34920;&#29616;&#33391;&#22909;&#65292;&#26377;&#21161;&#20110;&#39640;&#25928;&#30340;&#25552;&#31034;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#65292;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#40657;&#30418;LLMs&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#20840;&#23616;&#20248;&#21270;&#20197;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#28982;&#32780;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#25552;&#31034;&#20248;&#21270;&#20013;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#35265;&#35299;&#12290;&#19982;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#32597;&#35265;&#24615;&#24418;&#25104;&#23545;&#27604;&#65292;&#23616;&#37096;&#26368;&#20248;&#35299;&#36890;&#24120;&#26222;&#36941;&#23384;&#22312;&#19988;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#23545;&#20110;&#39640;&#25928;&#30340;&#25552;&#31034;&#20248;&#21270;&#21487;&#33021;&#26356;&#20855;&#20215;&#20540;&#65288;&#35265;&#35299;I&#65289;&#12290;&#36755;&#20837;&#39046;&#22495;&#30340;&#36873;&#25321;&#65292;&#21253;&#25324;&#25552;&#31034;&#30340;&#29983;&#25104;&#21644;&#34920;&#31034;&#65292;&#24433;&#21709;&#20102;&#20248;&#21270;&#33391;&#22909;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#35782;&#21035;&#65288;&#35265;&#35299;II&#65289;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02993v1 Announce Type: new  Abstract: The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algor
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Evolution Transformer&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#20803;&#20248;&#21270;&#30452;&#25509;&#21457;&#29616;&#24378;&#22823;&#30340;&#20248;&#21270;&#21407;&#21017;&#65292;&#36798;&#21040;&#25913;&#36827;&#25628;&#32034;&#20998;&#24067;&#30340;&#30446;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.02985</link><description>&lt;p&gt;
&#36827;&#21270;Transformer&#65306;&#19978;&#19979;&#25991;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Evolution Transformer: In-Context Evolutionary Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Evolution Transformer&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#20803;&#20248;&#21270;&#30452;&#25509;&#21457;&#29616;&#24378;&#22823;&#30340;&#20248;&#21270;&#21407;&#21017;&#65292;&#36798;&#21040;&#25913;&#36827;&#25628;&#32034;&#20998;&#24067;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#20174;&#23485;&#27867;&#30340;&#29983;&#29289;&#31867;&#27604;&#20013;&#34893;&#29983;&#20986;&#26469;&#65292;&#22312;&#20248;&#21270;&#30340;&#36830;&#32493;&#36807;&#31243;&#20013;&#38590;&#20197;&#21033;&#29992;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#25968;&#25454;&#65292;&#36890;&#36807;&#20803;&#20248;&#21270;&#30452;&#25509;&#21457;&#29616;&#24378;&#22823;&#30340;&#20248;&#21270;&#21407;&#21017;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#26679;&#30340;&#33539;&#24335;&#65292;&#24341;&#20837;&#20102;Evolution Transformer&#65292;&#19968;&#20010;&#22240;&#26524;Transformer&#26550;&#26500;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#25551;&#36848;&#19968;&#31867;&#36827;&#21270;&#31574;&#30053;&#12290;&#32473;&#23450;&#35780;&#20272;&#36712;&#36857;&#21644;&#25628;&#32034;&#20998;&#24067;&#32479;&#35745;&#25968;&#25454;&#65292;Evolution Transformer&#36755;&#20986;&#19968;&#20010;&#25913;&#36827;&#25628;&#32034;&#20998;&#24067;&#30340;&#26356;&#26032;&#12290;&#35813;&#26550;&#26500;&#26045;&#21152;&#20102;&#19968;&#32452;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#22312;&#19968;&#20195;&#20013;&#20010;&#20307;&#25104;&#21592;&#39034;&#24207;&#19981;&#21464;&#30340;&#20998;&#24067;&#26356;&#26032;&#21644;&#23545;&#25628;&#32034;&#32500;&#24230;&#39034;&#24207;&#30340;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#33976;&#39311;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02985v1 Announce Type: new  Abstract: Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization. An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization. In this work, we follow such a paradigm and introduce Evolution Transformer, a causal Transformer architecture, which can flexibly characterize a family of Evolution Strategies. Given a trajectory of evaluations and search distribution statistics, Evolution Transformer outputs a performance-improving update to the search distribution. The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions. We train the model weights using Evolutionary Algorithm Distillation, a technique fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#20005;&#37325;&#24615;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#26631;&#31614;&#32763;&#36716;&#21644;&#29305;&#24449;&#20013;&#27602;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02983</link><description>&lt;p&gt;
&#21463;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#25581;&#31034;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#20005;&#37325;&#24615;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#26631;&#31614;&#32763;&#36716;&#21644;&#29305;&#24449;&#20013;&#27602;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02983v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#35774;&#22791;&#25110;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#35757;&#32451;&#21644;&#27169;&#22411;&#26356;&#26032;&#20849;&#20139;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#21644;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#21516;&#30340;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#20005;&#37325;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#23454;&#26045;&#20294;&#24456;&#38590;&#26816;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#65292;&#21363;&#26631;&#31614;&#32763;&#36716;(LF)&#21644;&#29305;&#24449;&#20013;&#27602;(FP)&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#22312;LF&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#32763;&#36716;&#20102;&#33391;&#24615;&#25968;&#25454;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#25805;&#32437;&#21518;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#23545;&#20110;FP&#65292;&#25105;&#20204;&#38543;&#26426;&#25805;&#32437;&#20102;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30830;&#23450;&#30340;&#39640;&#36129;&#29486;&#29305;&#24449;&#12290;&#35813;&#23454;&#39564;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20026;&#19982;&#35745;&#31639;&#26426;&#32593;&#32476;&#30456;&#20851;&#30340;CIC&#21644;UNSW&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02983v1 Announce Type: cross  Abstract: Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02975</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#35821;&#20041;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#22312;&#31038;&#21306;&#38382;&#31572;&#12289;&#25628;&#32034;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25512;&#33616;&#31561;&#21508;&#31181;&#37325;&#35201;&#22330;&#26223;&#20013;&#20855;&#26377;&#30456;&#24403;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DC-Match&#26469;&#35299;&#24320;&#21477;&#23376;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#21305;&#37197;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#30452;&#25509;&#27169;&#25311;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#21333;&#35789;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#32780;&#24573;&#30053;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;DC-Match&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#21305;&#37197;&#26041;&#27861;&#65292;&#20294;&#23427;&#39640;&#24230;&#20381;&#36182;&#22806;&#37096;NER&#25216;&#26415;&#26469;&#35782;&#21035;&#21477;&#23376;&#30340;&#20851;&#38190;&#35789;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#27425;&#35201;&#35821;&#35328;&#30340;&#35821;&#20041;&#21305;&#37197;&#24615;&#33021;&#65292;&#22240;&#20026;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;NER&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02975v1 Announce Type: cross  Abstract: Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \textit{keywords} and \textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#31561;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#20013;&#20063;&#20855;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02965</link><description>&lt;p&gt;
ChatGPT&#19982;&#29983;&#29289;&#35782;&#21035;&#25216;&#26415;&#65306;&#23545;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#33021;&#21147;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#31561;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#20013;&#20063;&#20855;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#29305;&#21035;&#26816;&#39564;&#20102;ChatGPT&#22312;&#25191;&#34892;&#29983;&#29289;&#35782;&#21035;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#12290;&#30001;&#20110;&#29983;&#29289;&#35782;&#21035;&#34987;&#35270;&#20026;&#25935;&#24863;&#20449;&#24687;&#65292;ChatGPT&#36991;&#20813;&#22238;&#31572;&#30452;&#25509;&#25552;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#31034;&#31574;&#30053;&#26469;&#32469;&#36807;&#20854;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#35780;&#20272;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#33021;&#22815;&#20197;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#35782;&#21035;&#38754;&#37096;&#36523;&#20221;&#24182;&#22312;&#20004;&#20010;&#38754;&#37096;&#22270;&#20687;&#20043;&#38388;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#24615;&#33021;&#26174;&#33879;&#65292;&#24182;&#23545;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#22312;&#29983;&#29289;&#35782;&#21035;&#20013;&#24212;&#29992;LLMs&#21644;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#24191;&#38420;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02965v1 Announce Type: cross  Abstract: This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;WikiTableEdit&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;26,531&#20010;&#34920;&#26684;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#38024;&#23545;&#34920;&#26684;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#19981;&#35268;&#21017;&#32467;&#26500;&#65292;&#20026;&#35299;&#20915;&#22797;&#26434;&#34920;&#26684;&#20195;&#30721;&#32534;&#36753;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.02962</link><description>&lt;p&gt;
WikiTableEdit: &#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34920;&#26684;&#32534;&#36753;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;WikiTableEdit&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;26,531&#20010;&#34920;&#26684;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#38024;&#23545;&#34920;&#26684;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#19981;&#35268;&#21017;&#32467;&#26500;&#65292;&#20026;&#35299;&#20915;&#22797;&#26434;&#34920;&#26684;&#20195;&#30721;&#32534;&#36753;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#25968;&#25454;&#34920;&#31034;&#30340;&#19968;&#31181;&#20851;&#38190;&#24418;&#24335;&#65292;&#23384;&#22312;&#20110;Web&#19978;&#30340;&#21508;&#31181;&#26684;&#24335;&#20013;&#12290;&#24403;&#38754;&#23545;&#22797;&#26434;&#21644;&#19981;&#35268;&#21017;&#30340;&#34920;&#26684;&#26102;&#65292;&#25163;&#21160;&#20462;&#25913;&#21464;&#24471;&#32321;&#29712;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34920;&#26684;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35268;&#21017;&#24418;&#29366;&#30340;&#34920;&#26684;&#65292;&#20854;&#20013;&#25351;&#20196;&#29992;&#20110;&#29983;&#25104;SQL&#12289;Python&#25110;Excel Office-script&#20013;&#30340;&#20195;&#30721;&#65292;&#29992;&#20110;&#25805;&#20316;&#34920;&#26684;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#36753;&#20855;&#26377;&#19981;&#35268;&#21017;&#32467;&#26500;&#30340;&#34920;&#26684;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#21253;&#21547;&#36328;&#22810;&#34892;&#21512;&#24182;&#21333;&#20803;&#26684;&#30340;&#34920;&#26684;&#65292;&#20250;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WikiTableEdit&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#26469;&#33258;WikiSQL&#25968;&#25454;&#38598;&#30340;26,531&#20010;&#34920;&#26684;&#65292;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#20102;&#20845;&#31181;&#19981;&#21516;&#22522;&#26412;&#25805;&#20316;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20197;&#21450;&#30456;&#24212;&#30340;&#32467;&#26524;&#65292;&#20135;&#29983;&#20102;&#36229;&#36807;200,000&#20010;&#23454;&#20363;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02962v1 Announce Type: new  Abstract: Tabular data, as a crucial form of data representation, exists in diverse formats on the Web. When confronted with complex and irregular tables, manual modification becomes a laborious task. This paper investigates the performance of Large Language Models (LLMs) in the context of table editing tasks. Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables. Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code. To address this, we introduce the WikiTableEdit dataset. Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances. Subsequently, we evaluate several representative l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02959</link><description>&lt;p&gt;
SimuCourt: &#21033;&#29992;&#30495;&#23454;&#21496;&#27861;&#21028;&#20915;&#25991;&#20214;&#26500;&#24314;&#21496;&#27861;&#20915;&#31574;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20256;&#32479;&#21496;&#27861;&#34892;&#19994;&#21508;&#20010;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#21496;&#27861;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#36328;&#38454;&#27573;&#30340;&#21327;&#20316;&#12290;&#38543;&#30528;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#65292;&#24182;&#33021;&#20570;&#20986;&#22797;&#26434;&#20915;&#31574;&#65292;&#20026;&#21496;&#27861;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SimuCourt&#65292;&#19968;&#20010;&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;420&#20221;&#21028;&#20915;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#31867;&#22411;&#30340;&#21496;&#27861;&#26696;&#20363;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#20219;&#21153;&#21496;&#27861;&#20915;&#31574;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21496;&#27861;&#30693;&#35782;&#24211;&#65292;JudicialKB&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#31181;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;AgentsCourt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.02951</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;SQL&#33021;&#21147;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02951
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#25512;&#21160;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#28982;&#27809;&#26377;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02951v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer
&lt;/p&gt;</description></item><item><title>Venom&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#22686;&#24378;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#20851;&#27880;&#27169;&#20223;&#25439;&#22833;&#65292;&#24378;&#36843;&#21463;&#25439;&#26679;&#26412;&#30340;&#20915;&#31574;&#36335;&#24452;&#19982;&#33391;&#24615;&#26679;&#26412;&#30340;&#20851;&#38190;&#20915;&#31574;&#36335;&#24452;&#32806;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23545;&#27169;&#22411;&#37325;&#24314;&#22411;&#38450;&#24481;&#30340;&#29983;&#23384;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.02950</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#20915;&#31574;&#36335;&#24452;&#32806;&#21512;&#22686;&#24378;&#21518;&#38376;&#25915;&#20987;&#29983;&#23384;&#33021;&#21147;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A general approach to enhance the survivability of backdoor attacks by decision path coupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02950
&lt;/p&gt;
&lt;p&gt;
Venom&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#22686;&#24378;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#20851;&#27880;&#27169;&#20223;&#25439;&#22833;&#65292;&#24378;&#36843;&#21463;&#25439;&#26679;&#26412;&#30340;&#20915;&#31574;&#36335;&#24452;&#19982;&#33391;&#24615;&#26679;&#26412;&#30340;&#20851;&#38190;&#20915;&#31574;&#36335;&#24452;&#32806;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23545;&#27169;&#22411;&#37325;&#24314;&#22411;&#38450;&#24481;&#30340;&#29983;&#23384;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38754;&#20020;&#30340;&#26032;&#20852;&#23433;&#20840;&#23041;&#32961;&#20043;&#19968;&#65292;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#27969;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#26159;&#22522;&#20110;&#27169;&#22411;&#37325;&#24314;&#30340;&#12290;&#36825;&#20123;&#38450;&#24481;&#37319;&#29992;&#27169;&#22411;&#36951;&#24536;&#25110;&#21098;&#26525;&#26469;&#28040;&#38500;&#21518;&#38376;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22914;&#20309;&#20174;&#36825;&#20123;&#38450;&#24481;&#20013;&#24184;&#23384;&#19979;&#26469;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Venom&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#25552;&#39640;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23545;&#22522;&#20110;&#27169;&#22411;&#37325;&#24314;&#30340;&#38450;&#24481;&#30340;&#29983;&#23384;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;Venom&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20803;&#20219;&#21153;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#21407;&#22987;&#21518;&#38376;&#25915;&#20987;&#20219;&#21153;&#65292;&#20197;&#20445;&#30041;&#21407;&#22987;&#25915;&#20987;&#33021;&#21147;&#65292;&#32780;&#21478;&#19968;&#20010;&#20219;&#21153;&#26159;&#25915;&#20987;&#22686;&#24378;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#25915;&#20987;&#30340;&#29983;&#23384;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#31532;&#20108;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#27880;&#27169;&#20223;&#25439;&#22833;&#65292;&#26469;&#24378;&#21046;&#21518;&#38376;&#27169;&#22411;&#20013;&#21463;&#25439;&#26679;&#26412;&#30340;&#20915;&#31574;&#36335;&#24452;&#19982;&#33391;&#24615;&#26679;&#26412;&#30340;&#20851;&#38190;&#20915;&#31574;&#36335;&#24452;&#32806;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02950v1 Announce Type: new  Abstract: Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences. One of the mainstream backdoor defenses is model reconstruction-based. Such defenses adopt model unlearning or pruning to eliminate backdoors. However, little attention has been paid to survive from such defenses. To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses. We formalize Venom as a binary-task optimization problem. The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability. To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples,
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;Systolic Array&#30340;DNN&#21152;&#36895;&#22120;&#30340;&#26032;&#22411;&#20998;&#23618;&#36719;&#20214;&#21270;&#30828;&#20214;&#24863;&#30693;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#26102;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02946</link><description>&lt;p&gt;
SAFFIRA: &#19968;&#31181;&#35780;&#20272;&#22522;&#20110;Systolic Array&#30340;DNN&#21152;&#36895;&#22120;&#21487;&#38752;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02946
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;Systolic Array&#30340;DNN&#21152;&#36895;&#22120;&#30340;&#26032;&#22411;&#20998;&#23618;&#36719;&#20214;&#21270;&#30828;&#20214;&#24863;&#30693;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#26102;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Systolic array&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#26174;&#30528;&#26550;&#26500;&#65292;&#25552;&#20379;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#24615;&#33021;&#65292;&#23545;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37096;&#32626;DNN&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;&#26102;&#65292;&#24517;&#39035;&#36827;&#34892;&#21487;&#38752;&#24615;&#35780;&#20272;&#20197;&#30830;&#20445;DNN&#21152;&#36895;&#22120;&#30340;&#27491;&#30830;&#34892;&#20026;&#12290;&#34429;&#28982;&#25925;&#38556;&#27880;&#20837;&#20316;&#20026;&#19968;&#31181;&#25104;&#29087;&#23454;&#29992;&#19988;&#31283;&#20581;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;systolic array&#30340;DNN&#21152;&#36895;&#22120;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20998;&#23618;&#36719;&#20214;&#21270;&#30828;&#20214;&#24863;&#30693;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02946v1 Announce Type: new  Abstract: Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications. However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators. While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process. This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators.
&lt;/p&gt;</description></item><item><title>PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;</title><link>https://arxiv.org/abs/2403.02939</link><description>&lt;p&gt;
PaperWeaver&#65306;&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20016;&#23500;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;
PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02939
&lt;/p&gt;
&lt;p&gt;
PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23398;&#26415;&#26723;&#26696;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#30740;&#31350;&#20154;&#21592;&#35746;&#38405;&#8220;&#35770;&#25991;&#25552;&#37266;&#8221;&#31995;&#32479;&#65292;&#23450;&#26399;&#20026;&#20182;&#20204;&#25512;&#33616;&#26368;&#36817;&#21457;&#34920;&#30340;&#19982;&#20043;&#21069;&#25910;&#38598;&#30340;&#35770;&#25991;&#30456;&#20284;&#30340;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#26377;&#26102;&#24456;&#38590;&#29702;&#35299;&#25512;&#33616;&#35770;&#25991;&#19982;&#20182;&#20204;&#33258;&#24049;&#30740;&#31350;&#32972;&#26223;&#20043;&#38388;&#24494;&#22937;&#30340;&#32852;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#21482;&#21576;&#29616;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaperWeaver&#65292;&#36825;&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#35770;&#25991;&#25552;&#37266;&#31995;&#32479;&#65292;&#26681;&#25454;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#25552;&#20379;&#25512;&#33616;&#35770;&#25991;&#30340;&#19978;&#19979;&#25991;&#21270;&#25991;&#26412;&#25551;&#36848;&#12290;PaperWeaver&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#25552;&#21462;&#35770;&#25991;&#30340;&#29305;&#23450;&#32972;&#26223;&#65292;&#24182;&#22312;&#36825;&#20123;&#32972;&#26223;&#19978;&#27604;&#36739;&#25512;&#33616;&#35770;&#25991;&#21644;&#25910;&#38598;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#65288;N=15&#65289;&#34920;&#26126;&#65292;&#20351;&#29992;PaperWeaver&#30340;&#21442;&#19982;&#32773;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02939v1 Announce Type: cross  Abstract: With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ASIC-based DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.02936</link><description>&lt;p&gt;
AdAM: &#36866;&#29992;&#20110;&#36793;&#32536;DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;
&lt;/p&gt;
&lt;p&gt;
AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;ASIC-based DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;ASIC&#30340;DNN&#21152;&#36895;&#22120;&#23450;&#21046;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#23481;&#38169;&#36817;&#20284;&#20056;&#27861;&#22120;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02936v1 Announce Type: new  Abstract: In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators.
&lt;/p&gt;</description></item><item><title>&#23558;Datalog&#19982;&#23384;&#22312;&#35268;&#21017;&#27867;&#21270;&#21040;&#27169;&#31946;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#20219;&#24847;t-&#33539;&#25968;&#36827;&#34892;&#25512;&#29702;&#65292;&#20445;&#25345;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;</title><link>https://arxiv.org/abs/2403.02933</link><description>&lt;p&gt;
&#22810;&#20540;&#25299;&#23637;&#30340;&#27169;&#31946;Datalog&#22312;&#20219;&#24847;t-&#33539;&#25968;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Datalog$^\exists$ over Arbitrary t-Norms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02933
&lt;/p&gt;
&lt;p&gt;
&#23558;Datalog&#19982;&#23384;&#22312;&#35268;&#21017;&#27867;&#21270;&#21040;&#27169;&#31946;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#20219;&#24847;t-&#33539;&#25968;&#36827;&#34892;&#25512;&#29702;&#65292;&#20445;&#25345;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#22312;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25968;&#25454;&#20849;&#23384;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290; &#20026;&#20102;&#20801;&#35768;&#36825;&#31181;&#25512;&#29702;&#65292;&#25105;&#20204;&#23558;&#26631;&#20934;&#22522;&#20110;&#35268;&#21017;&#30340;&#35821;&#35328;Datalog&#19982;&#23384;&#22312;&#35268;&#21017;&#36827;&#34892;&#20102;&#27867;&#21270;&#65292;&#20801;&#35768;&#22312;&#35268;&#21017;&#20027;&#20307;&#20013;&#30340;&#32463;&#20856;&#36830;&#25509;&#35789;&#30340;&#20301;&#32622;&#19978;&#20351;&#29992;&#20219;&#24847;t-&#33539;&#25968;&#12290; &#32467;&#26524;&#30340;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#23545;&#19982;&#19981;&#30830;&#23450;&#24230;&#31243;&#24230;&#30456;&#20851;&#30340;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#21644;&#20026;&#26631;&#20934;Datalog&#35774;&#32622;&#24314;&#31435;&#30340;&#25512;&#29702;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#12290; &#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Datalog chase&#30340;&#27169;&#31946;&#25193;&#23637;&#65292;&#20135;&#29983;&#27169;&#31946;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02933v1 Announce Type: new  Abstract: One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data. This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more. To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting. In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show th
&lt;/p&gt;</description></item><item><title>TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02920</link><description>&lt;p&gt;
TaylorShift&#65306;&#21033;&#29992;TaylorSoftmax&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#32423;&#36716;&#21464;&#20026;&#32447;&#24615;&#32423;&#65288;&#20877;&#36716;&#22238;&#21435;&#65289;
&lt;/p&gt;
&lt;p&gt;
TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02920
&lt;/p&gt;
&lt;p&gt;
TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26159;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31232;&#30095;&#34920;&#31034;&#25110;&#26377;&#29366;&#24577;&#30340;&#24490;&#29615;&#65292;&#29306;&#29298;&#20102;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#26368;&#32456;&#23548;&#33268;&#24615;&#33021;&#19978;&#30340;&#22949;&#21327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TaylorShift&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Taylor softmax &#37325;&#26500;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#20869;&#35745;&#31639;&#20840;&#20307;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#30830;&#23450;&#20102;&#20351;&#29992;TaylorShift&#27604;&#20256;&#32479;&#27880;&#24847;&#21147;&#26356;&#21152;&#39640;&#25928;&#30340;&#20132;&#21449;&#28857;&#65292;&#36825;&#19982;&#23454;&#35777;&#27979;&#37327;&#32467;&#26524;&#23494;&#20999;&#21305;&#37197;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;TaylorShift&#25552;&#39640;&#20102;&#23545;&#30701;&#33267;800&#20010;&#35760;&#21495;&#30340;&#24207;&#21015;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#21152;&#36895;&#20102;&#23545;&#38271;&#36798;&#32422;1700&#20010;&#35760;&#21495;&#21450;&#20197;&#19978;&#36755;&#20837;&#30340;&#25512;&#26029;&#12290;&#23545;&#20110;&#36739;&#30701;&#30340;&#24207;&#21015;&#65292;TaylorShift&#19982;&#21407;&#22987;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#20998;&#31867;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02920v1 Announce Type: cross  Abstract: The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#20256;&#24863;&#22120;&#37096;&#32626;&#26041;&#27861;&#22312;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21644;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02914</link><description>&lt;p&gt;
DynST&#65306;&#36164;&#28304;&#21463;&#38480;&#26102;&#31354;&#39044;&#27979;&#30340;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02914
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20256;&#24863;&#22120;&#37096;&#32626;&#26041;&#27861;&#22312;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21644;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#24863;&#22120;&#26381;&#21153;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#23613;&#31649;&#20026;&#38754;&#21521;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29699;&#31185;&#23398;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36335;&#24452;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#22320;&#29699;&#31995;&#32479;&#25968;&#25454;&#65292;&#20294;&#36825;&#20063;&#32473;&#23427;&#20204;&#30340;&#24037;&#19994;&#32423;&#37096;&#32626;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22320;&#29702;&#21644;&#31038;&#20250;&#22240;&#32032;&#65292;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#25910;&#38598;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20351;&#24471;&#23454;&#29616;&#20840;&#38754;&#35206;&#30422;&#21644;&#32479;&#19968;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38556;&#30861;&#65292;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#37096;&#32626;&#26041;&#27861;&#21033;&#29992;&#29305;&#23450;&#31639;&#27861;&#35774;&#35745;&#21644;&#37096;&#32626;&#20256;&#24863;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#20256;&#24863;&#22120;&#30340;&#28608;&#27963;&#26102;&#38388;&#65292;&#20197;&#20248;&#21270;&#23545;&#27599;&#20010;&#23376;&#21306;&#22495;&#30340;&#26816;&#27979;&#36807;&#31243;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#21644;&#22320;&#29702;&#29305;&#24449;&#21046;&#23450;&#28608;&#27963;&#31574;&#30053;&#65292;&#36825;&#20123;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#27169;&#22411;&#26082;&#19981;&#31616;&#21333;&#20063;&#19981;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02914v1 Announce Type: new  Abstract: The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25554;&#20837;&#24694;&#24847;&#25991;&#26412;&#25552;&#31034;&#65292;&#25104;&#21151;&#23454;&#26045;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#20998;&#26512;&#20102;&#26377;&#27602;&#25968;&#25454;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02910</link><description>&lt;p&gt;
ImgTrojan: &#29992;&#19968;&#24352;&#22270;&#29255;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25554;&#20837;&#24694;&#24847;&#25991;&#26412;&#25552;&#31034;&#65292;&#25104;&#21151;&#23454;&#26045;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#20998;&#26512;&#20102;&#26377;&#27602;&#25968;&#25454;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19982;&#35270;&#35273;&#27169;&#22359;&#38598;&#25104;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;VLMs&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#26088;&#22312;&#24403;&#29992;&#25143;&#36755;&#20837;&#26377;&#23475;&#25351;&#20196;&#26102;&#32469;&#36807;&#20854;&#23433;&#20840;&#38459;&#30861;&#12290;&#20551;&#35774;&#25105;&#20204;&#30340;&#26377;&#27602;&#65288;&#22270;&#20687;&#65292;&#25991;&#26412;&#65289;&#25968;&#25454;&#23545;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#29992;&#24694;&#24847;&#36234;&#29425;&#25552;&#31034;&#26367;&#25442;&#21407;&#22987;&#25991;&#26412;&#26631;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26377;&#27602;&#22270;&#20687;&#25191;&#34892;&#36234;&#29425;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26377;&#27602;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#25105;&#20204;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21644;&#38544;&#34109;&#24615;&#12290;&#32467;&#21512;&#19968;&#31995;&#21015;&#31574;&#21010;&#30340;&#26377;&#23475;&#25351;&#20196;&#65292;&#21487;&#20197;&#34913;&#37327;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02910v1 Announce Type: cross  Abstract: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficac
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#21512;&#35843;&#26597;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8220;&#35270;&#35282;&#25552;&#20379;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20840;&#38754;&#23457;&#35270;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;ATS&#26368;&#26032;&#30340;&#35843;&#26597;&#65292;&#24357;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#20004;&#24180;&#38388;&#38548;&#12290;</title><link>https://arxiv.org/abs/2403.02901</link><description>&lt;p&gt;
&#20851;&#20110;&#36807;&#31243;&#23548;&#21521;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25506;&#35752;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#21512;&#35843;&#26597;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8220;&#35270;&#35282;&#25552;&#20379;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20840;&#38754;&#23457;&#35270;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;ATS&#26368;&#26032;&#30340;&#35843;&#26597;&#65292;&#24357;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#20004;&#24180;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02901v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#65288;ATS&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#65292;&#26088;&#22312;&#21019;&#24314;&#31616;&#27905;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22788;&#29702;&#22823;&#37327;&#25991;&#26412;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;ATS&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#36807;&#21435;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35843;&#26597;ATS&#30340;&#26041;&#27861;; &#20294;&#26159;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#23454;&#38469;&#23454;&#26045;&#30340;&#23454;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#20197;&#24448;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25913;&#21464;&#20102;&#20256;&#32479;&#30340;ATS&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312; 1&#65289;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8221;&#35270;&#35282;&#25552;&#20379;ATS&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#26368;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;; 2) &#20840;&#38754;&#23457;&#35270;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;; &#20197;&#21450; 3&#65289;&#25552;&#20379;&#20851;&#20110;ATS&#30340;&#26368;&#26032;&#35843;&#26597;&#65292;&#24357;&#34917;&#25991;&#29486;&#20013;&#20004;&#24180;&#38388;&#38548;&#20043;&#22788;&#12290;&#20196;&#20154;&#24863;&#21040;&#28385;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02901v1 Announce Type: new  Abstract: Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39046;&#22495;&#26080;&#20851;&#20114;&#30456;&#25552;&#31034;&#65288;DAMP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20114;&#30456;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#26469;&#21033;&#29992;&#39046;&#22495;&#19981;&#21464;&#35821;&#20041;&#65292;&#24357;&#21512;&#20102;&#20256;&#32479;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02899</link><description>&lt;p&gt;
&#38024;&#23545;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#39046;&#22495;&#26080;&#20851;&#20114;&#30456;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02899
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39046;&#22495;&#26080;&#20851;&#20114;&#30456;&#25552;&#31034;&#65288;DAMP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20114;&#30456;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#26469;&#21033;&#29992;&#39046;&#22495;&#19981;&#21464;&#35821;&#20041;&#65292;&#24357;&#21512;&#20102;&#20256;&#32479;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#33268;&#21147;&#20110;&#26368;&#23567;&#21270;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#24573;&#30053;&#20102;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#20016;&#23500;&#35821;&#20041;&#24182;&#19988;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#26159;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#26356;&#26377;&#25351;&#23548;&#24615;&#30340;&#33258;&#36866;&#24212;&#12290;&#23613;&#31649;&#19968;&#20123;&#23581;&#35797;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23398;&#20064;&#25991;&#26412;&#25552;&#31034;&#23558;&#39046;&#22495;&#35821;&#20041;&#23884;&#20837;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20998;&#21035;&#36827;&#34892;&#20998;&#31867;&#65292;&#38480;&#21046;&#20102;&#36328;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#20165;&#25552;&#31034;&#35821;&#35328;&#20998;&#25903;&#32570;&#20047;&#21160;&#24577;&#36866;&#24212;&#20004;&#31181;&#27169;&#24577;&#30340;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#20114;&#30456;&#25552;&#31034;&#65288;DAMP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20114;&#30456;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#26469;&#21033;&#29992;&#39046;&#22495;&#19981;&#21464;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02899v1 Announce Type: new  Abstract: Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02893</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65288;ECI&#65289;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21477;&#23376;&#32423;ECI&#65292;&#32780;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25991;&#26723;&#32423;ECI&#65288;DECI&#65289;&#21364;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#65288;GIMC&#65289;&#30340;&#24322;&#26500;&#22270;&#20132;&#20114;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;ECI&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24322;&#26500;&#22270;&#20132;&#20114;&#32593;&#32476;&#26469;&#24314;&#27169;&#25991;&#26723;&#20013;&#20998;&#25955;&#20107;&#20214;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#20174;&#28304;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#27169;&#22359;&#65292;&#20197;&#35843;&#25972;&#36328;&#35821;&#35328;&#38388;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24179;&#22343;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#32422;9.4%&#21644;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#21033;&#29992;&#19977;&#20010;&#27969;&#65288;&#20840;&#23616;&#12289;&#23616;&#37096;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#37096;&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#38271;&#26399;&#20154;&#21592;&#20877;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.02892</link><description>&lt;p&gt;
&#21033;&#29992;&#20840;&#23616;&#65292;&#23616;&#37096;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#37096;&#27969;&#22686;&#24378;&#38271;&#26399;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02892
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#21033;&#29992;&#19977;&#20010;&#27969;&#65288;&#20840;&#23616;&#12289;&#23616;&#37096;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#37096;&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#38271;&#26399;&#20154;&#21592;&#20877;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38271;&#26399;&#20154;&#21592;&#20877;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;&#20154;&#21592;&#20877;&#35782;&#21035;&#20551;&#35774;&#20154;&#20204;&#19981;&#20250;&#26356;&#25442;&#34915;&#26381;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#20110;&#30701;&#26399;&#22330;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32771;&#34385;&#25442;&#34915;&#19982;&#19968;&#33268;&#30528;&#35013;&#20004;&#31181;&#24773;&#26223;&#30340;&#38271;&#26399;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#23398;&#20064;&#21644;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#26032;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#27969;&#65306;&#20840;&#23616;&#12289;&#23616;&#37096;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#37096;&#27969;&#12290;&#20840;&#23616;&#21644;&#22836;&#37096;&#27969;&#20998;&#21035;&#32534;&#30721;&#26469;&#33258;&#25972;&#20010;&#22270;&#20687;&#21644;&#22836;&#37096;&#21306;&#22495;&#35009;&#21098;&#22270;&#20687;&#30340;&#19982;&#36523;&#20221;&#30456;&#20851;&#20449;&#24687;&#12290;&#20004;&#20010;&#27969;&#21033;&#29992;&#23545;&#25239;&#24615;&#25830;&#38500;&#65292;&#26368;&#22823;&#27744;&#21270;&#21644;&#24179;&#22343;&#27744;&#21270;&#30340;&#32452;&#21512;&#32534;&#30721;&#26368;&#26126;&#26174;&#30340;&#65292;&#19981;&#22826;&#26126;&#26174;&#30340;&#21644;&#24179;&#22343;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02892v1 Announce Type: cross  Abstract: This work addresses the task of long-term person re-identification. Typically, person re-identification assumes that people do not change their clothes, which limits its applications to short-term scenarios. To overcome this limitation, we investigate long-term person re-identification, which considers both clothes-changing and clothes-consistent scenarios. In this paper, we propose a novel framework that effectively learns and utilizes both global and local information. The proposed framework consists of three streams: global, local body part, and head streams. The global and head streams encode identity-relevant information from an entire image and a cropped image of the head region, respectively. Both streams encode the most distinct, less distinct, and average features using the combinations of adversarial erasing, max pooling, and average pooling. The local body part stream extracts identity-related information for each body part,
&lt;/p&gt;</description></item><item><title>MathScale&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#25968;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#30340;&#26377;&#25928;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02884</link><description>&lt;p&gt;
MathScale: &#25968;&#23398;&#25512;&#29702;&#30340;&#25351;&#23548;&#20248;&#21270;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
MathScale: Scaling Instruction Tuning for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02884
&lt;/p&gt;
&lt;p&gt;
MathScale&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#25968;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#30340;&#26377;&#25928;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MathScale&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#27839;&#30340;LLMs&#65288;&#20363;&#22914;GPT-3.5&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#12290;&#21463;&#20154;&#31867;&#25968;&#23398;&#23398;&#20064;&#20013;&#30340;&#35748;&#30693;&#26426;&#21046;&#21551;&#21457;&#65292;&#23427;&#39318;&#20808;&#20174;&#31181;&#23376;&#25968;&#23398;&#38382;&#39064;&#20013;&#25552;&#21462;&#20027;&#39064;&#21644;&#30693;&#35782;&#28857;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#27010;&#24565;&#22270;&#65292;&#38543;&#21518;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;MathScale&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#25968;&#23398;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;MathScaleQA&#65289;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MwpBench&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#35789;&#27719;&#38382;&#39064;&#22522;&#20934;&#65292;&#21253;&#25324;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#35268;&#21010;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#26377;&#29992;&#24615;&#26631;&#20934;&#36880;&#27493;&#27880;&#37322;&#37319;&#38598;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#26679;&#26412;&#21644;&#26631;&#35760;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02877</link><description>&lt;p&gt;
ActiveAD: &#38754;&#21521;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02877
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#35268;&#21010;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#26377;&#29992;&#24615;&#26631;&#20934;&#36880;&#27493;&#27880;&#37322;&#37319;&#38598;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#26679;&#26412;&#21644;&#26631;&#35760;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31471;&#21040;&#31471;&#21487;&#24494;&#23398;&#20064;&#25104;&#20026;&#33258;&#21160;&#39550;&#39542; (AD) &#30340;&#19968;&#20010;&#31361;&#20986;&#33539;&#20363;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#20854;&#23545;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#28212;&#26395;&#65292;&#20363;&#22914; 3D &#36793;&#30028;&#26694;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#25163;&#21160;&#27880;&#37322;&#36153;&#29992;&#26497;&#39640;&#12290;&#22256;&#38590;&#20043;&#22788;&#36827;&#19968;&#27493;&#20307;&#29616;&#22312; AD &#26679;&#26412;&#20869;&#37096;&#34892;&#20026;&#24448;&#24448;&#21463;&#38271;&#23614;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#20013;&#22823;&#37096;&#20998;&#21487;&#33021;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65288;&#20363;&#22914;&#22312;&#31508;&#30452;&#36947;&#36335;&#19978;&#31616;&#21333;&#34892;&#39542;&#65289;&#65292;&#32780;&#21482;&#26377;&#23569;&#25968;&#24773;&#20917;&#26159;&#23433;&#20840;&#20851;&#38190;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#23454;&#38469;&#37325;&#35201;&#20294;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#23454;&#29616;&#31471;&#21040;&#31471; AD &#30340;&#26679;&#26412;&#21644;&#26631;&#35760;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#35268;&#21010;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#25152;&#25552;&#20986;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#29992;&#24615;&#26631;&#20934;&#36880;&#27493;&#23545;&#37319;&#38598;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#35268;&#21010;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02877v1 Announce Type: cross  Abstract: End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm. One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate. The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution. In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical. In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD. Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes. Empirical
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#25552;&#21462;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#21487;&#20197;&#33719;&#21462;&#27169;&#22411;&#26550;&#26500;&#21644;&#22270;&#20687;&#32500;&#24230;&#31561;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.02870</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#31934;&#30830;&#25552;&#21462;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#25552;&#21462;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#21487;&#20197;&#33719;&#21462;&#27169;&#22411;&#26550;&#26500;&#21644;&#22270;&#20687;&#32500;&#24230;&#31561;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#26085;&#30410;&#26222;&#21450;&#65292;&#27169;&#22411;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#65292;&#21482;&#26377;&#25317;&#26377;&#24222;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24040;&#22823;&#35745;&#31639;&#33021;&#21147;&#30340;&#20844;&#21496;&#25165;&#33021;&#24212;&#23545;&#19994;&#21153;&#38656;&#27714;&#30340;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#26159;&#20844;&#21496;&#19987;&#26377;&#30340;&#65292;&#22240;&#27492;&#36825;&#20123;&#20844;&#21496;&#21162;&#21147;&#20445;&#25252;&#20182;&#20204;&#30340;&#31169;&#26377;&#27169;&#22411;&#65292;&#20197;&#20813;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65288;MEA&#65289;&#30340;&#20405;&#23475;&#65292;&#35813;&#25915;&#20987;&#30446;&#30340;&#26159;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#26469;&#31363;&#21462;&#27169;&#22411;&#12290;&#22914;&#20170;&#65292;&#20844;&#21496;&#20542;&#21521;&#20110;&#23558;&#27169;&#22411;&#20174;&#20013;&#22830;&#26381;&#21153;&#22120;&#36716;&#31227;&#21040;&#36793;&#32536;/&#31471;&#28857;&#35774;&#22791;&#12290;&#27491;&#22914;&#26368;&#26032;&#30740;&#31350;&#25581;&#31034;&#30340;&#37027;&#26679;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#36825;&#19968;&#26426;&#20250;&#20316;&#20026;&#21551;&#21160;&#20391;&#20449;&#36947;&#25915;&#20987;&#65288;SCA&#65289;&#30340;&#26032;&#25915;&#20987;&#21521;&#37327;&#65292;&#38024;&#23545;&#36816;&#34892;&#21463;&#23475;&#27169;&#22411;&#30340;&#35774;&#22791;&#21457;&#21160;&#25915;&#20987;&#65292;&#33719;&#21462;&#27169;&#22411;&#20449;&#24687;&#30340;&#21508;&#31181;&#35201;&#28857;&#65292;&#20363;&#22914;&#27169;&#22411;&#26550;&#26500;&#65288;MA&#65289;&#21644;&#22270;&#20687;&#32500;&#24230;&#65288;ID&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#20840;&#38754;&#29702;&#35299;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#23558;&#26377;&#21161;&#20110;&#26410;&#26469;MEA&#30740;&#31350;&#22312;&#36827;&#25915;&#21644;&#38450;&#24481;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02870v1 Announce Type: new  Abstract: With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#27169;&#22411;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;FLGuard&#26041;&#27861;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02846</link><description>&lt;p&gt;
FLGuard: &#36890;&#36807;&#23545;&#27604;&#27169;&#22411;&#38598;&#21512;&#23454;&#29616;&#25308;&#21344;&#24237;-&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02846
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#27169;&#22411;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;FLGuard&#26041;&#27861;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#25308;&#21344;&#24237;-&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20165;&#20849;&#20139;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#20855;&#26377;&#31169;&#20154;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20247;&#22810;&#23458;&#25143;&#26102;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25237;&#27602;&#25915;&#20987;&#65292;&#24403;&#23545;&#25163;&#20551;&#25198;&#20026;&#33391;&#24615;&#23458;&#25143;&#24182;&#23384;&#22312;&#20110;&#19968;&#32452;&#23458;&#25143;&#20013;&#26102;&#65292;&#20250;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#28798;&#38590;&#24615;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25308;&#21344;&#24237;-&#40065;&#26834;&#30340;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#26381;&#21153;&#22120;&#21363;&#20351;&#22312;&#31995;&#32479;&#20013;&#23384;&#22312;&#23545;&#25163;&#20063;&#33021;&#35757;&#32451;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#35201;&#27714;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#30340;&#25968;&#37327;&#25110;&#36741;&#21161;&#65288;&#24178;&#20928;&#65289;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#26102;&#20854;&#26377;&#25928;&#24615;&#25253;&#21578;&#26126;&#26174;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02846v1 Announce Type: cross  Abstract: Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21407;&#22987;-&#23545;&#20598;&#31070;&#32463;&#32593;&#32476;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#35270;&#35282;&#19979;&#36827;&#34892;&#21407;&#26408;&#30340;&#19977;&#32500;&#23618;&#26512;&#25104;&#20687;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.02820</link><description>&lt;p&gt;
&#38024;&#23545;&#26408;&#26448;&#24037;&#19994;&#20013;&#24212;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#23618;&#26512;&#25104;&#20687;&#30340;&#38271;&#29289;&#20307;&#30340;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21407;&#22987;-&#23545;&#20598;&#31070;&#32463;&#32593;&#32476;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#35270;&#35282;&#19979;&#36827;&#34892;&#21407;&#26408;&#30340;&#19977;&#32500;&#23618;&#26512;&#25104;&#20687;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26408;&#26448;&#24037;&#19994;&#20013;&#65292;&#36890;&#36807;&#22312;&#31227;&#21160;&#20256;&#36865;&#24102;&#19978;&#20174;&#20960;&#20010;&#28304;&#20301;&#32622;&#36827;&#34892;&#31163;&#25955;X&#23556;&#32447;&#25195;&#25551;&#26469;&#23545;&#21407;&#26408;&#36827;&#34892;&#24120;&#35268;&#36136;&#37327;&#31579;&#26597;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#33719;&#24471;&#20108;&#32500;&#65288;2D&#65289;&#20999;&#29255;&#27979;&#37327;&#12290;&#27599;&#20010;2D&#20999;&#29255;&#21333;&#29420;&#19981;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#36827;&#34892;&#19977;&#32500;&#23618;&#26512;&#37325;&#24314;&#65292;&#22312;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#21407;&#26408;&#29983;&#29289;&#29305;&#24449;&#24471;&#20197;&#24456;&#22909;&#20445;&#30041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21407;&#22987;-&#23545;&#20598;&#31070;&#32463;&#32593;&#32476;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37325;&#24314;&#36807;&#31243;&#20013;&#31215;&#32047;&#20102;&#30456;&#37051;&#20999;&#29255;&#20043;&#38388;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20165;&#22312;&#37325;&#24314;&#26399;&#38388;&#32771;&#34385;&#21333;&#20010;&#20999;&#29255;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20215;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#20351;&#29992;&#20116;&#20010;&#28304;&#20301;&#32622;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30340;&#21407;&#26408;&#37325;&#24314;&#36275;&#22815;&#20934;&#30830;&#65292;&#20197;&#35782;&#21035;&#20687;&#33410;&#65288;&#20998;&#25903;&#65289;&#12289;&#24515;&#26448;&#31561;&#29983;&#29289;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02820v1 Announce Type: new  Abstract: In the wood industry, logs are commonly quality screened by discrete X-ray scans on a moving conveyor belt from a few source positions. Typically, two-dimensional (2D) slice-wise measurements are obtained by a sequential scanning geometry. Each 2D slice alone does not carry sufficient information for a three-dimensional tomographic reconstruction in which biological features of interest in the log are well preserved. In the present work, we propose a learned iterative reconstruction method based on the Learned Primal-Dual neural network, suited for sequential scanning geometries. Our method accumulates information between neighbouring slices, instead of only accounting for single slices during reconstruction. Our quantitative and qualitative evaluations with as few as five source positions show that our method yields reconstructions of logs that are sufficiently accurate to identify biological features like knots (branches), heartwood an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;InjectTST&#36825;&#31181;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#29420;&#31435;&#36890;&#36947;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02814</link><description>&lt;p&gt;
InjectTST: &#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#29420;&#31435;&#36890;&#36947;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#29992;&#20110;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02814
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;InjectTST&#36825;&#31181;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#29420;&#31435;&#36890;&#36947;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#25104;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#26368;&#27969;&#34892;&#30340;&#26550;&#26500;&#20043;&#19968;&#12290;&#26368;&#36817;&#22522;&#20110;Transformer&#30340;MTS&#27169;&#22411;&#36890;&#24120;&#20542;&#21521;&#20110;&#20855;&#26377;&#36890;&#36947;&#29420;&#31435;&#32467;&#26500;&#65292;&#22240;&#20026;&#36890;&#36947;&#29420;&#31435;&#21487;&#20197;&#20943;&#36731;&#22122;&#22768;&#21644;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36947;&#20381;&#36182;&#24615;&#20173;&#28982;&#26159;MTS&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#34164;&#21547;&#30528;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#35774;&#35745;&#19968;&#20010;&#32467;&#21512;&#20102;&#36890;&#36947;&#29420;&#31435;&#21644;&#36890;&#36947;&#28151;&#21512;&#32467;&#26500;&#20248;&#28857;&#30340;&#27169;&#22411;&#26159;&#36827;&#19968;&#27493;&#25913;&#36827;MTS&#39044;&#27979;&#30340;&#20851;&#38190;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#36890;&#36947;&#26080;&#20851;Transformer&#30340;&#27880;&#20837;&#26041;&#27861;InjectTST&#12290;&#25105;&#20204;&#27809;&#26377;&#30452;&#25509;&#35774;&#35745;&#19968;&#20010;&#28151;&#21512;&#36890;&#36947;&#27169;&#22411;&#65292;&#32780;&#26159;&#20445;&#30041;&#20102;&#36890;&#36947;&#29420;&#31435;&#30340;&#26694;&#26550;&#65292;&#24182;&#36880;&#28176;&#23558;&#20840;&#23616;&#20449;&#24687;&#27880;&#20837;&#21040;&#21333;&#20010;&#36890;&#36947;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02814v1 Announce Type: cross  Abstract: Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65288;DGGO&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;PDEs&#12290;</title><link>https://arxiv.org/abs/2403.02810</link><description>&lt;p&gt;
&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65306;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02810
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65288;DGGO&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#30001;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#29289;&#29702;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#21457;&#23637;&#20102;&#19968;&#31181;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#23398;&#20064;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#25552;&#20379;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#25509;&#21475;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#23376;&#20165;&#38480;&#20110;&#24658;&#23450;&#21644;&#22343;&#21248;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#35745;&#31639;&#22495;&#30340;&#20219;&#24847;&#31163;&#25955;&#21270;&#26041;&#26696;&#19978;&#27867;&#21270;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#39640;&#26031;&#22270;&#31639;&#23376;&#65288;DGGO&#65289;&#65292;&#23558;&#31070;&#32463;&#31639;&#23376;&#25193;&#23637;&#21040;&#22312;&#20219;&#24847;&#31163;&#25955;&#21147;&#23398;&#38382;&#39064;&#20013;&#23398;&#20064;&#21442;&#25968;&#21270;PDEs&#12290;&#21160;&#24577;&#39640;&#26031;&#22270;&#65288;DGG&#65289;&#26680;&#23398;&#20064;&#23558;&#22312;&#19968;&#33324;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#35266;&#23519;&#21521;&#37327;&#26144;&#23556;&#21040;&#39640;&#32500;&#22343;&#21248;&#24230;&#37327;&#20013;&#23450;&#20041;&#30340;&#24230;&#37327;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02810v1 Announce Type: cross  Abstract: Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data. It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions. However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain. In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20462;&#21098;&#20998;&#21306;&#22686;&#24378;&#65288;DPPA&#65289;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02799</link><description>&lt;p&gt;
DPPA&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20462;&#21098;&#26041;&#27861;&#20197;&#36827;&#34892;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DPPA: Pruning Method for Large Language Model to Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20462;&#21098;&#20998;&#21306;&#22686;&#24378;&#65288;DPPA&#65289;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21512;&#24182;&#26159;&#23558;&#20174;&#22810;&#20010;&#39046;&#22495;&#34893;&#29983;&#20986;&#30340;&#24494;&#35843;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29087;&#32451;&#24230;&#12290;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35299;&#20915;&#21442;&#25968;&#20914;&#31361;&#12290;&#29616;&#26377;&#22823;&#37327;&#30740;&#31350;&#24050;&#35299;&#20915;&#20102;&#21512;&#24182;&#38454;&#27573;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#26032;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#20462;&#21098;&#38454;&#27573;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;DARE&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#31616;&#21333;&#24494;&#35843;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#20110;&#26174;&#31034;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#26174;&#33879;&#21442;&#25968;&#20559;&#24046;&#30340;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24448;&#24448;&#20250;&#20943;&#24369;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20462;&#21098;&#20998;&#21306;&#22686;&#24378;&#65288;DPPA&#65289;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#21512;&#24182;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#20462;&#21098;&#65288;DP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#21098;&#26525;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20854;&#30446;&#30340;&#26159;&#22686;&#24378;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02799v1 Announce Type: cross  Abstract: Model merging is to combine fine-tuned models derived from multiple domains, with the intent of enhancing the model's proficiency across various domains. The principal concern is the resolution of parameter conflicts. A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the pruning stage. The DARE approach has exhibited promising outcomes when applied to a simplistic fine-tuned model. However, the efficacy of this method tends to wane when employed on complex fine-tuned models that show a significant parameter bias relative to the baseline model. In this paper, we introduce a dual-stage method termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the challenge of merging complex fine-tuned models. Initially, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning, which aim is to enhance p
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02795</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21644;&#20248;&#21270;&#25945;&#32946;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Optimizing Educational Content with Large Language Model Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02795
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#26377;&#25928;&#30340;&#25945;&#32946;&#26448;&#26009;&#36890;&#24120;&#38656;&#35201;&#23545;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#36827;&#34892;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#19968;&#20010;&#24819;&#27861;&#26159;&#26500;&#24314;&#23398;&#29983;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#25945;&#23398;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#23398;&#20064;&#21160;&#24577;&#30340;&#35748;&#30693;&#36807;&#31243;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#21508;&#31181;&#25351;&#23548;&#23545;&#23398;&#20064;&#32467;&#26524;&#24433;&#21709;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#26469;&#35780;&#20272;&#25351;&#23548;&#26448;&#26009;&#23545;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#22797;&#21046;&#35832;&#22914;&#19987;&#19994;&#36870;&#36716;&#25928;&#24212;&#21644;&#21464;&#24322;&#25928;&#24212;&#31561;&#24050;&#32463;&#24314;&#31435;&#30340;&#25945;&#32946;&#21457;&#29616;&#12290;&#36825;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#25945;&#32946;&#20869;&#23481;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;LM&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02795v1 Announce Type: new  Abstract: Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instruction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#19982;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411; VIB-DML &#29992;&#20110;&#35780;&#20998;&#39044;&#27979;&#65292;&#38480;&#21046;&#28508;&#31354;&#38388;&#29305;&#24449;&#21521;&#37327;&#30340;&#20114;&#20449;&#24687;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#24182;&#28385;&#36275;&#27431;&#27663;&#36317;&#31163;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.02794</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30340;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Distance Metric Learning Model Based On Variational Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#19982;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411; VIB-DML &#29992;&#20110;&#35780;&#20998;&#39044;&#27979;&#65292;&#38480;&#21046;&#28508;&#31354;&#38388;&#29305;&#24449;&#21521;&#37327;&#30340;&#20114;&#20449;&#24687;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#24182;&#28385;&#36275;&#27431;&#27663;&#36317;&#31163;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#25216;&#26415;&#34028;&#21187;&#21457;&#23637;&#65292;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#20043;&#19968;&#12290;&#20808;&#21518;&#25552;&#20986;&#30340;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#21644;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#21518;&#32773;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#32780;&#38750;&#21069;&#32773;&#25152;&#20351;&#29992;&#30340;&#28857;&#31215;&#26469;&#34913;&#37327;&#28508;&#31354;&#38388;&#21521;&#37327;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#19982;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#27169;&#22411; VIB-DML&#65288;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#65289;&#29992;&#20110;&#35780;&#20998;&#39044;&#27979;&#65292;&#38480;&#21046;&#28508;&#31354;&#38388;&#29305;&#24449;&#21521;&#37327;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#28385;&#36275;&#27431;&#27663;&#36317;&#31163;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02794v1 Announce Type: cross  Abstract: In recent years, personalized recommendation technology has flourished and become one of the hot research directions. The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied. The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector. While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited recommendation quality of the model. In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the mutual information of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#25506;&#32034;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#20197;&#22686;&#24378;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02786</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#30340;&#21322;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33026;&#32938;&#32925;&#30149;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#25506;&#32034;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#20197;&#22686;&#24378;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#33026;&#32938;&#32925;&#30149;&#26041;&#38754;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#20027;&#39064;&#30456;&#20284;&#24615;&#22270;&#65292;&#20174;&#20581;&#24247;&#26816;&#26597;&#25968;&#25454;&#20013;&#35782;&#21035;&#39118;&#38505;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#21508;&#31181;GNN&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#26377;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;GNNs&#21253;&#21547;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#65292;&#20026;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#25552;&#20379;&#20010;&#24615;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20419;&#36827;&#20581;&#24247;&#23454;&#36341;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#31867;&#20013;&#24515;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02786v1 Announce Type: cross  Abstract: Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of graph representation learning within a semi-supervised learning framework. Leveraging graph neural networks (GNNs), our approach constructs a subject similarity graph to identify risk patterns from health checkup data. The effectiveness of various GNN approaches in this context is demonstrated, even with minimal labeled samples. Central to our methodology is the inclusion of human-centric explanations through explainable GNNs, providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on graph representation learning and human-centric explanation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#23376;&#27169;&#24615;&#30340;&#26032;QAP-SAT&#35774;&#35745;&#65292;&#30740;&#31350;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#30340;&#30456;&#21464;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#21464;&#21442;&#25968;&#65292;&#21487;&#39044;&#27979;&#31105;&#24524;&#25628;&#32034;&#20013;&#22256;&#38590;&#23454;&#20363;&#30340;&#35299;&#20915;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02783</link><description>&lt;p&gt;
QAP-SAT&#23454;&#20363;&#20013;&#30495;&#27491;&#22256;&#38590;&#30340;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#25152;&#22312;
&lt;/p&gt;
&lt;p&gt;
Where the Really Hard Quadratic Assignment Problems Are: the QAP-SAT instances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02783
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#23376;&#27169;&#24615;&#30340;&#26032;QAP-SAT&#35774;&#35745;&#65292;&#30740;&#31350;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#30340;&#30456;&#21464;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#21464;&#21442;&#25968;&#65292;&#21487;&#39044;&#27979;&#31105;&#24524;&#25628;&#32034;&#20013;&#22256;&#38590;&#23454;&#20363;&#30340;&#35299;&#20915;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#65288;QAP&#65289;&#26159;&#36827;&#21270;&#35745;&#31639;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#20063;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;QAP&#30340;&#30456;&#21464;&#29616;&#35937;&#65292;&#23427;&#21487;&#20197;&#25551;&#36848;&#20026;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#21487;&#28385;&#36275;&#24615;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#21095;&#28872;&#21464;&#21270;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#22522;&#20110;&#23376;&#27169;&#24615;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;QAP-SAT&#21021;&#22987;&#38382;&#39064;&#35774;&#35745;&#65292;&#20197;&#25429;&#25417;&#20854;&#22256;&#38590;&#24615;&#19982;&#26032;&#29305;&#24615;&#12290;&#24182;&#21033;&#29992;&#20998;&#25903;&#23450;&#30028;&#21644;&#31105;&#24524;&#25628;&#32034;&#35299;&#36825;&#31181;&#20998;&#35299;&#30340;&#38382;&#39064;&#12290;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#21464;&#21442;&#25968;&#65292;&#34920;&#26126;&#31105;&#24524;&#25628;&#32034;&#30340;&#30456;&#21464;&#28385;&#24847;&#24230;&#20020;&#30028;&#21442;&#25968;&#19982;&#35299;&#20915;&#38590;&#24230;&#39640;&#24230;&#30456;&#20851;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#22256;&#38590;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02783v1 Announce Type: new  Abstract: The Quadratic Assignment Problem (QAP) is one of the major domains in the field of evolutionary computation, and more widely in combinatorial optimization. This paper studies the phase transition of the QAP, which can be described as a dramatic change in the problem's computational complexity and satisfiability, within a narrow range of the problem parameters. To approach this phenomenon, we introduce a new QAP-SAT design of the initial problem based on submodularity to capture its difficulty with new features. This decomposition is studied experimentally using branch-and-bound and tabu search solvers. A phase transition parameter is then proposed. The critical parameter of phase transition satisfaction and that of the solving effort are shown to be highly correlated for tabu search, thus allowing the prediction of difficult instances.
&lt;/p&gt;</description></item><item><title>EasyQuant&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#30340;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#24182;&#20445;&#35777;LLM&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02775</link><description>&lt;p&gt;
EasyQuant: &#19968;&#31181;&#29992;&#20110;LLM&#30340;&#39640;&#25928;&#26080;&#25968;&#25454;&#37327;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02775
&lt;/p&gt;
&lt;p&gt;
EasyQuant&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#30340;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#24182;&#20445;&#35777;LLM&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#35201;&#27604;&#20256;&#32479;&#26041;&#27861;&#20248;&#36234;&#24471;&#22810;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#39640;&#20869;&#23384;&#38656;&#27714;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#12290;&#27169;&#22411;&#37327;&#21270;&#26159;&#20943;&#23569;&#36825;&#31181;&#24320;&#38144;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#37327;&#21270;&#27169;&#22411;&#26159;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26657;&#20934;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#21069;&#20154;&#24037;&#20316;&#37324;&#37327;&#21270;&#21518;&#30340;LLMs&#23545;&#26410;&#30693;&#24773;&#20917;&#21644;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20026;LLM&#35774;&#35745;&#19968;&#31181;&#26080;&#25968;&#25454;&#37327;&#21270;&#26041;&#27861;&#20197;&#20445;&#35777;&#20854;&#27867;&#21270;&#24615;&#33021;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyQuant&#65292;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#26080;&#38656;&#35757;&#32451;&#21644;&#26080;&#25968;&#25454;&#30340;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#34920;&#26126;&#65292;&#26435;&#37325;&#21644;&#37327;&#21270;&#33539;&#22260;&#20013;&#30340;&#24322;&#24120;&#20540;&#26159;&#20943;&#23569;&#37327;&#21270;&#35823;&#24046;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#22312;EasyQuant&#20013;&#65292;&#25105;&#20204;&#20445;&#30041;&#20102;&#36825;&#20123;&#24322;&#24120;&#20540;&#65288;&#24453;&#32493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02775v1 Announce Type: new  Abstract: Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24247;&#22797;&#38203;&#28860;&#36136;&#37327;&#35780;&#20272;&#65292;&#32467;&#21512;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38203;&#28860;&#30340;&#24247;&#22797;&#35745;&#21010;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12289;&#38477;&#20302;&#27515;&#20129;&#29575;&#21644;&#20877;&#20303;&#38498;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#34394;&#25311;&#24247;&#22797;&#65292;&#24739;&#32773;&#21487;&#20197;&#22312;&#23478;&#29420;&#31435;&#23436;&#25104;&#38203;&#28860;&#65292;&#21033;&#29992;AI&#31639;&#27861;&#20998;&#26512;&#38203;&#28860;&#25968;&#25454;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#21521;&#20020;&#24202;&#21307;&#29983;&#26356;&#26032;&#20182;&#20204;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#36825;&#20123;&#35745;&#21010;&#36890;&#24120;&#20250;&#25351;&#23450;&#21508;&#31181;&#38203;&#28860;&#31867;&#22411;&#65292;&#36825;&#23548;&#33268;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#25968;&#25454;&#38598;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65306;&#34429;&#28982;&#22312;&#25972;&#20307;&#35757;&#32451;&#26679;&#26412;&#20013;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#27599;&#31181;&#20855;&#20307;&#38203;&#32451;&#31867;&#22411;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12290;&#36825;&#31181;&#24046;&#24322;&#24433;&#21709;&#20102;&#29616;&#26377;&#26041;&#27861;&#35757;&#32451;&#20855;&#26377;&#23567;&#26679;&#26412;&#37327;&#30340;&#27599;&#31181;&#38203;&#32451;&#30340;&#21487;&#27867;&#21270;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25972;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02772v1 Announce Type: cross  Abstract: Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entir
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#23545;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#65292;&#31361;&#30772;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02760</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#22312;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#20013;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02760
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#23545;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#65292;&#31361;&#30772;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#21830;&#21697;&#20043;&#38388;&#30340;&#20114;&#21160;&#24182;&#34701;&#20837;&#20854;&#25991;&#26412;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;DNN&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#38590;&#20197;&#26377;&#25928;&#29702;&#35299;&#29992;&#25143;&#30340;&#20852;&#36259;&#21644;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22914;ChatGPT&#21644;GPT-4&#65292;&#30001;&#20110;&#22312;&#35821;&#35328;&#29702;&#35299;&#31561;&#22522;&#26412;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#33021;&#21147;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02760v1 Announce Type: new  Abstract: With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions. At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19971;&#31181;&#19981;&#21516;&#30340;&#25955;&#26001;&#21435;&#38500;&#26041;&#27861;&#65292;&#35777;&#26126;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#22312;&#20943;&#23569;&#36229;&#22768;&#22270;&#20687;&#25955;&#26001;&#22122;&#22768;&#26102;&#33021;&#22815;&#26377;&#25928;&#20445;&#30041;&#29305;&#24449;&#21644;&#36793;&#32536;&#12290;</title><link>https://arxiv.org/abs/2403.02750</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#38477;&#20302;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#25955;&#26001;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19971;&#31181;&#19981;&#21516;&#30340;&#25955;&#26001;&#21435;&#38500;&#26041;&#27861;&#65292;&#35777;&#26126;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#22312;&#20943;&#23569;&#36229;&#22768;&#22270;&#20687;&#25955;&#26001;&#22122;&#22768;&#26102;&#33021;&#22815;&#26377;&#25928;&#20445;&#30041;&#29305;&#24449;&#21644;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#30340;&#21307;&#23398;&#24037;&#20855;&#65292;&#20294;&#20854;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#25955;&#26001;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#38477;&#20302;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#21644;&#20449;&#22122;&#27604;&#12290;&#20943;&#23569;&#25955;&#26001;&#22122;&#22768;&#23545;&#20110;&#36229;&#22768;&#22270;&#20687;&#30340;&#39044;&#22788;&#29702;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#27493;&#39588;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20960;&#31181;&#25955;&#26001;&#20943;&#23569;&#26041;&#27861;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#25152;&#26377;&#30456;&#20851;&#22240;&#32032;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#19971;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#65306;&#20013;&#20540;&#12289;&#39640;&#26031;&#12289;&#21452;&#36793;&#12289;&#24179;&#22343;&#12289;&#32500;&#32435;&#12289;&#21508;&#21521;&#24322;&#24615;&#21644;&#19981;&#24102;&#36339;&#36291;&#36830;&#25509;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#21450;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#20174;&#23427;&#20204;&#22312;&#20445;&#25345;&#29305;&#24449;&#21644;&#36793;&#32536;&#30340;&#33021;&#21147;&#20197;&#21450;&#26377;&#25928;&#38477;&#22122;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02750v1 Announce Type: cross  Abstract: Ultrasound is a widely used medical tool for non-invasive diagnosis, but its images often contain speckle noise which can lower their resolution and contrast-to-noise ratio. This can make it more difficult to extract, recognize, and analyze features in the images, as well as impair the accuracy of computer-assisted diagnostic techniques and the ability of doctors to interpret the images. Reducing speckle noise, therefore, is a crucial step in the preprocessing of ultrasound images. Researchers have proposed several speckle reduction methods, but no single method takes all relevant factors into account. In this paper, we compare seven such methods: Median, Gaussian, Bilateral, Average, Weiner, Anisotropic and Denoising auto-encoder without and with skip connections in terms of their ability to preserve features and edges while effectively reducing noise. In an experimental study, a convolutional noise-removing auto-encoder with skip con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#37325;&#26657;&#20934;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02745</link><description>&lt;p&gt;
CURATRON&#65306;&#23436;&#25972;&#20581;&#22766;&#20559;&#22909;&#25968;&#25454;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20581;&#22766;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#37325;&#26657;&#20934;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#65288;PL&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#19981;&#23436;&#25972;&#21644;&#25439;&#22351;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#21644;&#23436;&#20840;&#22320;&#37325;&#26032;&#26657;&#20934;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#20445;&#35777;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#25490;&#21517;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20960;&#31181;&#29616;&#26377;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#27604;&#22914;&#32463;&#20856;&#30340;Bradley&#8211;Terry&#8211;Luce&#65288;BTL&#65289;&#65288;Bradley&#21644;Terry&#65292;1952&#65289;&#27169;&#22411;&#20197;&#21450;&#23545;&#20854;&#26576;&#20123;&#25512;&#24191;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#19968;&#31181;&#21487;&#35777;&#26126;&#22312;&#39640;&#27010;&#29575;&#19979;&#24674;&#22797;{\epsilon}-&#26368;&#20248;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#27599;&#20010;&#27169;&#22411;&#21709;&#24212;&#22810;&#36798;O(n)&#25200;&#21160;&#30340;&#25104;&#23545;&#27604;&#36739;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37096;&#20998;&#35266;&#23519;&#35774;&#32622;&#19979;&#30340;&#20581;&#22766;&#24674;&#22797;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02745v1 Announce Type: new  Abstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#24341;&#23548;&#31232;&#26377;&#29289;&#20307;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#21644;&#22312;&#32447;&#22522;&#20110;&#32858;&#31867;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#26679;&#26412;&#26292;&#38706;&#29575;&#65292;&#23454;&#29616;&#20102;&#20174;2%&#21040;30%&#30340;&#27491;&#26679;&#26412;&#37319;&#26679;&#29575;&#22686;&#21152;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#26426;&#22120;&#23398;&#20064;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.02736</link><description>&lt;p&gt;
&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#24341;&#23548;&#31232;&#26377;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#24341;&#23548;&#31232;&#26377;&#29289;&#20307;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#21644;&#22312;&#32447;&#22522;&#20110;&#32858;&#31867;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#26679;&#26412;&#26292;&#38706;&#29575;&#65292;&#23454;&#29616;&#20102;&#20174;2%&#21040;30%&#30340;&#27491;&#26679;&#26412;&#37319;&#26679;&#29575;&#22686;&#21152;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#26426;&#22120;&#23398;&#20064;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#26377;&#29289;&#20307;&#26816;&#27979;&#26159;&#24212;&#29992;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#28982;&#32780;&#30001;&#20110;&#22823;&#37327;&#30340;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#25110;&#33322;&#31354;&#22270;&#20687;&#20197;&#21450;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#30340;&#27491;&#26679;&#26412;&#65292;&#36825;&#24448;&#24448;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#19968;&#31232;&#26377;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#30340;&#24341;&#23548;&#38382;&#39064;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20551;&#35774;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#65292;&#20063;&#27809;&#26377;&#26377;&#20851;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#31354;&#38388;&#20808;&#39564;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37319;&#26679;&#36148;&#29255;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#26292;&#38706;&#27491;&#26679;&#26412;&#32473;&#20154;&#31867;&#26631;&#27880;&#32773;&#26041;&#38754;&#27604;&#38543;&#26426;&#37319;&#26679;&#35201;&#39640;&#25928;&#24471;&#22810;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#32943;&#23612;&#20122;&#21644;&#22374;&#26705;&#23612;&#20122;&#30340;Serengeti Mara&#22320;&#21306;&#30340;boma&#65288;&#29287;&#32676;&#21160;&#29289;&#30340;&#23567;&#22260;&#22330;&#65289;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#27979;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#23558;&#27491;&#26679;&#26412;&#37319;&#26679;&#29575;&#20174;2%&#65288;&#38543;&#26426;&#65289;&#25552;&#39640;&#21040;30%&#12290;&#36825;&#19968;&#36827;&#23637;&#20351;&#24471;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#27491;&#26679;&#26412;&#26631;&#35760;&#65292;&#20063;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02736v1 Announce Type: cross  Abstract: Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling. We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%. This advancement enables effective machine learning mapping even with minima
&lt;/p&gt;</description></item><item><title>HARGPT&#30740;&#31350;&#34920;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#25552;&#31034;&#29702;&#35299;&#21407;&#22987;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02727</link><description>&lt;p&gt;
HARGPT&#65306;LLMs&#26159;&#21542;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02727
&lt;/p&gt;
&lt;p&gt;
HARGPT&#30740;&#31350;&#34920;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#25552;&#31034;&#29702;&#35299;&#21407;&#22987;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19982;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#26080;&#32541;&#38598;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35299;&#37322;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#23384;&#22312;&#25345;&#32493;&#30340;&#20105;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#26469;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#38646;&#26679;&#26412;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;HARGPT&#65292;&#36890;&#36807;&#23637;&#31034;LLMs&#21487;&#20197;&#29702;&#35299;&#21407;&#22987;IMU&#25968;&#25454;&#24182;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#25191;&#34892;HAR&#20219;&#21153;&#65292;&#20165;&#38656;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#32943;&#23450;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;HARGPT&#23558;&#21407;&#22987;IMU&#25968;&#25454;&#36755;&#20837;LLMs&#65292;&#24182;&#21033;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#31574;&#30053;&#36827;&#34892;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;GPT4&#19978;&#23545;HARGPT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#31867;&#38388;&#30456;&#20284;&#24615;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#30340;&#21508;&#31181;&#22522;&#32447;&#12290;&#26174;&#33879;&#22320;&#65292;LLMs&#25104;&#21151;&#22320;&#20174;&#21407;&#22987;IMU&#25968;&#25454;&#20013;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#65292;&#24182;&#22987;&#32456;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02727v1 Announce Type: cross  Abstract: There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR). Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and think step-by-step strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#29983;&#25104;AI&#24037;&#20855;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#20197;&#21450;&#23545;&#38754;&#37096;&#34920;&#24773;&#21644;&#22806;&#34920;&#30340;&#24494;&#22937;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.02726</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Bias in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#29983;&#25104;AI&#24037;&#20855;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#20197;&#21450;&#23545;&#38754;&#37096;&#34920;&#24773;&#21644;&#22806;&#34920;&#30340;&#24494;&#22937;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#65288;Midjourney&#65292;Stable Diffusion&#21644;DALLE 2&#65289;&#29983;&#25104;&#30340;&#20195;&#34920;&#21508;&#31181;&#32844;&#19994;&#30340;&#22270;&#20687;&#65292;&#20197;&#35843;&#26597;AI&#29983;&#25104;&#22120;&#20013;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;AI&#29983;&#25104;&#22120;&#23384;&#22312;&#30340;&#20004;&#20010;&#20027;&#35201;&#20851;&#27880;&#39046;&#22495;&#65292;&#21253;&#25324;&#65288;1&#65289;&#31995;&#32479;&#24615;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23545;&#38754;&#37096;&#34920;&#24773;&#21644;&#22806;&#34920;&#30340;&#24494;&#22937;&#20559;&#35265;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19977;&#31181;AI&#29983;&#25104;&#22120;&#37117;&#23384;&#22312;&#23545;&#22899;&#24615;&#21644;&#38750;&#35028;&#32654;&#22269;&#20154;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#20998;&#26512;&#20013;&#25581;&#31034;&#30340;&#26126;&#26174;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#19982;&#21171;&#21160;&#21147;&#32479;&#35745;&#25968;&#25454;&#25110;Google&#22270;&#20687;&#30456;&#27604;&#65292;&#29978;&#33267;&#26356;&#21152;&#26174;&#33879;&#65292;&#21152;&#21095;&#20102;&#31038;&#20250;&#20013;&#25105;&#20204;&#27491;&#22312;&#31215;&#26497;&#21162;&#21147;&#32416;&#27491;&#30340;&#26377;&#23475;&#20559;&#35265;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#34920;&#29616;&#24773;&#32490;&#21644;&#22806;&#34920;&#26041;&#38754;&#26356;&#21152;&#24494;&#22937;&#30340;&#20559;&#35265;&#12290;&#20363;&#22914;&#65292;&#22899;&#24615;&#34987;&#25551;&#32472;&#20026;&#24180;&#36731;&#19988;&#26356;&#22810;&#22320;&#24494;&#31505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02726v1 Announce Type: cross  Abstract: This study analyzed images generated by three popular generative artificial intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators. Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances. Firstly, we found that all three AI generators exhibited bias against women and African Americans. Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society. Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances. For example, women were depicted as younger with more smi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26368;&#23567;&#39044;&#31639;&#25299;&#25169;&#25915;&#20987;&#65292;&#21517;&#20026;MiBTack&#65292;&#26088;&#22312;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#23545;&#27599;&#20010;&#33410;&#28857;&#25104;&#21151;&#25915;&#20987;&#25152;&#38656;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.02723</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#25299;&#25169;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Minimum Topology Attacks for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02723
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26368;&#23567;&#39044;&#31639;&#25299;&#25169;&#25915;&#20987;&#65292;&#21517;&#20026;MiBTack&#65292;&#26088;&#22312;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#23545;&#27599;&#20010;&#33410;&#28857;&#25104;&#21151;&#25915;&#20987;&#25152;&#38656;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#27969;&#34892;&#65292;&#23427;&#20204;&#23545;&#25299;&#25169;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#20043;&#21069;&#25552;&#20986;&#20102;&#35768;&#22810;&#25915;&#20987;&#26041;&#27861;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#39044;&#31639;&#25915;&#20987;&#19978;&#65292;&#30446;&#30340;&#26159;&#22312;&#20026;&#30446;&#26631;&#33410;&#28857;&#25214;&#21040;&#22266;&#23450;&#39044;&#31639;&#20869;&#26368;&#20855;&#25932;&#23545;&#24615;&#30340;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#27599;&#20010;&#33410;&#28857;&#30340;&#31283;&#20581;&#24615;&#19981;&#21516;&#65292;&#22266;&#23450;&#39044;&#31639;&#20250;&#36896;&#25104;&#19968;&#20010;&#22256;&#22659;&#65292;&#21363;&#22312;&#39044;&#31639;&#30456;&#23545;&#36739;&#23567;&#26102;&#25214;&#19981;&#21040;&#25104;&#21151;&#25200;&#21160;&#65292;&#32780;&#22914;&#26524;&#39044;&#31639;&#22826;&#22823;&#65292;&#23548;&#33268;&#22810;&#20313;&#30340;&#25200;&#21160;&#23558;&#20260;&#23475;&#19981;&#21487;&#35265;&#24615;&#12290;&#20026;&#20102;&#31361;&#30772;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25299;&#25169;&#25915;&#20987;&#65292;&#31216;&#20026;&#26368;&#23567;&#39044;&#31639;&#25299;&#25169;&#25915;&#20987;&#65292;&#26088;&#22312;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#23545;&#27599;&#20010;&#33410;&#28857;&#25104;&#21151;&#25915;&#20987;&#25152;&#38656;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#27169;&#22411;&#65292;&#21517;&#20026;MiBTack&#65292;&#22522;&#20110;&#21160;&#24577;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02723v1 Announce Type: new  Abstract: With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#22686;&#24378;&#21518;&#21407;&#26377;&#20551;&#35774;&#19981;&#20877;&#25104;&#31435;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02719</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Subgraph Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#22686;&#24378;&#21518;&#21407;&#26377;&#20551;&#35774;&#19981;&#20877;&#25104;&#31435;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23545;&#27604;&#20004;&#20010;&#22686;&#24378;&#22270;&#26469;&#23398;&#20064;&#27599;&#20010;&#22270;&#30340;&#34920;&#31034;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#31616;&#21333;&#22320;&#20551;&#35774;&#19968;&#20010;&#22270;&#21450;&#20854;&#22686;&#24378;&#22270;&#20026;&#27491;&#23545;&#65292;&#21542;&#21017;&#20026;&#36127;&#23545;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22270;&#32467;&#26500;&#36890;&#24120;&#22797;&#26434;&#19988;&#22810;&#23610;&#24230;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;&#22312;&#22270;&#22686;&#24378;&#21518;&#65292;&#20808;&#21069;&#30340;&#20551;&#35774;&#26159;&#21542;&#20173;&#28982;&#25104;&#31435;&#65311;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#19981;&#19968;&#33268;&#20110;&#21407;&#22987;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#20004;&#20010;&#22686;&#24378;&#22270;&#26159;&#27491;&#23545;&#36824;&#26159;&#36127;&#23545;&#19982;&#22810;&#23610;&#24230;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#29983;&#25104;&#20840;&#23616;&#21644;&#23616;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02719v1 Announce Type: new  Abstract: Graph-level contrastive learning, aiming to learn the representations for each graph by contrasting two augmented graphs, has attracted considerable attention. Previous studies usually simply assume that a graph and its augmented graph as a positive pair, otherwise as a negative pair. However, it is well known that graph structure is always complex and multi-scale, which gives rise to a fundamental question: after graph augmentation, will the previous assumption still hold in reality? By an experimental analysis, we discover the semantic information of an augmented graph structure may be not consistent as original graph structure, and whether two augmented graphs are positive or negative pairs is highly related with the multi-scale structures. Based on this finding, we propose a multi-scale subgraph contrastive learning method which is able to characterize the fine-grained semantic information. Specifically, we generate global and local 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.02715</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#35270;&#37326;&#65306;&#36234;&#21335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#65292;&#20294;&#30446;&#21069;&#24320;&#28304;&#30340;LLMs&#22312;&#22788;&#29702;&#36234;&#21335;&#35821;&#26041;&#38754;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#36825;&#19968;&#25361;&#25112;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#36234;&#21335;&#35821;LLM&#35780;&#20272;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#19987;&#38376;&#20026;&#36234;&#21335;&#35821;&#36827;&#34892;&#20102;LLM&#30340;&#24494;&#35843;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#28085;&#30422;10&#20010;&#24120;&#35265;&#20219;&#21153;&#21644;31&#20010;&#25351;&#26631;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#22312;&#36234;&#21335;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#22810;&#30340;&#20559;&#35265;&#21644;&#26410;&#26657;&#20934;&#30340;&#36755;&#20986;&#65292;&#32780;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#28155;&#21152;&#33258;&#36866;&#24212;&#29305;&#24615;&#25913;&#36827;DareFightingICE&#28216;&#25103;&#29609;&#27861;&#30340;&#32972;&#26223;&#38899;&#20048;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;Blind DL AI&#22312;&#20351;&#29992;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#26102;&#27604;&#19981;&#20351;&#29992;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.02701</link><description>&lt;p&gt;
&#25913;&#36827;&#28216;&#25103;&#29609;&#27861;&#30340;&#23545;&#25112;&#28216;&#25103;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Fighting Game Adaptive Background Music for Improved Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#28155;&#21152;&#33258;&#36866;&#24212;&#29305;&#24615;&#25913;&#36827;DareFightingICE&#28216;&#25103;&#29609;&#27861;&#30340;&#32972;&#26223;&#38899;&#20048;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;Blind DL AI&#22312;&#20351;&#29992;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#26102;&#27604;&#19981;&#20351;&#29992;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33258;&#36866;&#24212;&#29305;&#24615;&#26469;&#22686;&#24378;DareFightingICE&#20013;&#32972;&#26223;&#38899;&#20048;&#65288;BGM&#65289;&#30340;&#24037;&#20316;&#12290;&#33258;&#36866;&#24212;BGM&#30001;&#19977;&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#20048;&#22120;&#32452;&#25104;&#65292;&#25773;&#25918;2022&#24180;DareFightingICE&#27604;&#36187;&#20013;&#33719;&#32988;&#32773;&#38899;&#25928;&#35774;&#35745;&#30340;BGM&#12290;BGM&#36890;&#36807;&#25913;&#21464;&#27599;&#20010;&#31867;&#21035;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36827;&#34892;&#35843;&#25972;&#12290;&#27599;&#20010;&#31867;&#21035;&#19982;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#30456;&#36830;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21482;&#20351;&#29992;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;AI&#20195;&#29702;&#65288;Blind DL AI&#65289;&#26469;&#36816;&#34892;&#23454;&#39564;&#20197;&#35780;&#20272;&#33258;&#36866;&#24212;BGM&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27809;&#26377;&#33258;&#36866;&#24212;BGM&#26102;&#30456;&#27604;&#65292;Blind DL AI&#22312;&#29609;&#28216;&#25103;&#26102;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02701v1 Announce Type: cross  Abstract: This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features. The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition. The BGM adapts by changing the volume of each category of instruments. Each category is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;</title><link>https://arxiv.org/abs/2403.02688</link><description>&lt;p&gt;
DOCTOR: &#38024;&#23545;&#26102;&#38388;&#28418;&#31227;&#28909;&#21464;&#21270;&#30340;&#21160;&#24577;&#33455;&#29255;&#30699;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02688
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Photonic computing&#20316;&#20026;&#21152;&#36895;&#35745;&#31639;&#23494;&#38598;&#22411;&#20154;&#24037;&#26234;&#33021;(AI)&#24037;&#20316;&#36127;&#36733;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#12289;&#24310;&#36831;&#25935;&#24863;&#30340;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#36895;&#24230;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#30340;&#37096;&#32626;&#36935;&#21040;&#20102;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#30001;&#20110;&#30828;&#20214;&#22122;&#22768;&#21644;&#29615;&#22659;&#21464;&#21270;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#33073;&#26426;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#21644;&#29255;&#19978;&#35757;&#32451;&#26469;&#22686;&#24378;&#23545;&#20855;&#26377;&#36866;&#24230;&#12289;&#38745;&#24577;&#22122;&#22768;&#30340;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#21464;&#21270;&#23481;&#24525;&#24230;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#23548;&#33268;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#38656;&#35201;&#23454;&#26102;&#12289;&#21407;&#20301;&#26657;&#20934;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;&#65292;&#31216;&#20026;DOCTOR&#65292;&#25552;&#20379;&#36866;&#24212;&#24615;&#30340;&#12289;&#21407;&#20301;&#30340;&#20934;&#30830;&#24230;&#24674;&#22797;&#65292;&#38024;&#23545;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02688v1 Announce Type: cross  Abstract: Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22686;&#24378;&#29256;&#30340;DareFightingICE&#24179;&#21488;&#65292;&#20998;&#20026;&#22768;&#38899;&#35774;&#35745;&#27604;&#36187;&#21644;AI&#27604;&#36187;&#65292;&#36890;&#36807;&#25913;&#36827;&#38899;&#39057;&#31995;&#32479;&#21644;&#21457;&#36865;&#38899;&#39057;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#24179;&#21488;&#26356;&#23481;&#26131;&#20026;&#35270;&#38556;&#29609;&#23478;&#28155;&#21152;&#26032;&#21151;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#22768;&#38899;&#35774;&#35745;&#31454;&#36187;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02687</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;DareFightingICE&#31454;&#36187;&#65306;&#22768;&#38899;&#35774;&#35745;&#21644;AI&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Enhanced DareFightingICE Competitions: Sound Design and AI Competitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22686;&#24378;&#29256;&#30340;DareFightingICE&#24179;&#21488;&#65292;&#20998;&#20026;&#22768;&#38899;&#35774;&#35745;&#27604;&#36187;&#21644;AI&#27604;&#36187;&#65292;&#36890;&#36807;&#25913;&#36827;&#38899;&#39057;&#31995;&#32479;&#21644;&#21457;&#36865;&#38899;&#39057;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#24179;&#21488;&#26356;&#23481;&#26131;&#20026;&#35270;&#38556;&#29609;&#23478;&#28155;&#21152;&#26032;&#21151;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#22768;&#38899;&#35774;&#35745;&#31454;&#36187;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25913;&#36827;&#29256;DareFightingICE&#24179;&#21488;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#35270;&#38556;&#29609;&#23478;&#65288;VIPs&#65289;&#30340;&#26684;&#26007;&#28216;&#25103;&#24179;&#21488;&#65292;&#22522;&#20110;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#23427;&#23558;DareFightingICE&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#27604;&#36187;&#65292;&#20998;&#21035;&#26159;DareFightingICE Sound Design Competition&#21644;DareFightingICE AI Competition&#65292;&#23558;&#22312;2024&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#65288;CoG&#65289;&#19978;&#20030;&#34892;&#65292;&#20351;&#29992;&#19968;&#20010;&#26032;&#24179;&#21488;&#12290;&#36825;&#20010;&#26032;&#24179;&#21488;&#26159;&#26087;DareFightingICE&#24179;&#21488;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#25317;&#26377;&#26356;&#22909;&#30340;&#38899;&#39057;&#31995;&#32479;&#20197;&#20256;&#36798;3D&#22768;&#38899;&#65292;&#36824;&#26377;&#26356;&#22909;&#30340;&#26041;&#27861;&#23558;&#38899;&#39057;&#25968;&#25454;&#21457;&#36865;&#32473;AI&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#31181;&#22686;&#24378;&#21644;&#21033;&#29992;Unity&#65292;&#26032;&#30340;DareFightingICE&#24179;&#21488;&#22312;&#20026;VIPs&#21644;&#26410;&#26469;&#38899;&#39057;&#30740;&#31350;&#28155;&#21152;&#26032;&#21151;&#33021;&#26041;&#38754;&#26356;&#21152;&#26131;&#20110;&#35775;&#38382;&#12290;&#26412;&#25991;&#36824;&#25913;&#36827;&#20102;&#35780;&#20272;&#22768;&#38899;&#35774;&#35745;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36825;&#23558;&#30830;&#20445;VIPs&#33719;&#24471;&#26356;&#22909;&#30340;&#22768;&#38899;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02687v1 Announce Type: cross  Abstract: This paper presents a new and improved DareFightingICE platform, a fighting game platform with a focus on visually impaired players (VIPs), in the Unity game engine. It also introduces the separation of the DareFightingICE Competition into two standalone competitions called DareFightingICE Sound Design Competition and DareFightingICE AI Competition--at the 2024 IEEE Conference on Games (CoG)--in which a new platform will be used. This new platform is an enhanced version of the old DareFightingICE platform, having a better audio system to convey 3D sound and a better way to send audio data to AI agents. With this enhancement and by utilizing Unity, the new DareFightingICE platform is more accessible in terms of adding new features for VIPs and future audio research. This paper also improves the evaluation method for evaluating sound designs in the Sound Design Competition which will ensure a better sound design for VIPs as this competit
&lt;/p&gt;</description></item><item><title>&#23558;AI/ML&#24037;&#20855;&#24212;&#29992;&#20110;MIMO&#65292;&#23454;&#29616;&#23545;&#20110;NextG&#34562;&#31389;&#32593;&#32476;&#20013;&#26080;&#32447;&#29615;&#22659;&#21160;&#24577;&#24615;&#30340;&#22312;&#32447;&#23454;&#26102;&#23398;&#20064;&#21644;&#36866;&#24212;&#24615;&#35843;&#33410;&#12290;</title><link>https://arxiv.org/abs/2403.02651</link><description>&lt;p&gt;
&#20197;&#26080;&#32447;&#36895;&#24230;&#23398;&#20064;&#65306;AI-&#21551;&#29992;MIMO&#30340;&#22312;&#32447;&#23454;&#26102;&#23398;&#20064;&#22312;NextG&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning at the Speed of Wireless: Online Real-Time Learning for AI-Enabled MIMO in NextG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02651
&lt;/p&gt;
&lt;p&gt;
&#23558;AI/ML&#24037;&#20855;&#24212;&#29992;&#20110;MIMO&#65292;&#23454;&#29616;&#23545;&#20110;NextG&#34562;&#31389;&#32593;&#32476;&#20013;&#26080;&#32447;&#29615;&#22659;&#21160;&#24577;&#24615;&#30340;&#22312;&#32447;&#23454;&#26102;&#23398;&#20064;&#21644;&#36866;&#24212;&#24615;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#19982;&#31354;&#20013;&#25509;&#21475;&#30340;&#25972;&#21512;&#34987;&#35270;&#20026;&#19979;&#19968;&#20195;&#65288;NextG&#65289;&#34562;&#31389;&#32593;&#32476;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#22312;&#31354;&#20013;&#25509;&#21475;&#65292;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#21450;&#20854;&#21464;&#31181;&#22914;&#22810;&#29992;&#25143;MIMO&#65288;MU-MIMO&#65289;&#21644;&#22823;&#35268;&#27169;/&#20840;&#32500; MIMO &#24050;&#25104;&#20026;&#36830;&#32493;&#21457;&#23637;&#30340;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25512;&#21160;&#22240;&#32032;&#65292;&#38754;&#20020;&#26085;&#30410;&#22797;&#26434;&#30340;&#35774;&#35745;&#25361;&#25112;&#12290;&#24320;&#22987;&#31215;&#26497;&#35843;&#30740;&#21033;&#29992;AI/ML&#24037;&#20855;&#26469;&#35299;&#20915;MIMO&#25361;&#25112;&#65292;&#25104;&#20026;&#26397;&#30528;AI&#21551;&#29992;NextG&#31354;&#20013;&#25509;&#21475;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290; &#22312;NextG&#31354;&#20013;&#25509;&#21475;&#65292;&#22522;&#30784;&#26080;&#32447;&#29615;&#22659;&#23558;&#26497;&#20026;&#21160;&#24577;&#65292;&#30001;MIMO&#25805;&#20316;&#65288;&#22914;MU-MIMO&#35843;&#24230;&#21644;&#31209;/&#38142;&#36335;&#35843;&#25972;&#65289;&#22312;&#20122;&#27627;&#31186;&#32423;&#21035;&#25191;&#34892;&#25805;&#20316;&#36866;&#24212;&#12290;&#37492;&#20110;&#26497;&#20854;&#24222;&#22823;&#30340;&#25805;&#20316;&#36866;&#24212;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32447;&#23454;&#26102;AI / ML-&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02651v1 Announce Type: cross  Abstract: Integration of artificial intelligence (AI) and machine learning (ML) into the air interface has been envisioned as a key technology for next-generation (NextG) cellular networks. At the air interface, multiple-input multiple-output (MIMO) and its variants such as multi-user MIMO (MU-MIMO) and massive/full-dimension MIMO have been key enablers across successive generations of cellular networks with evolving complexity and design challenges. Initiating active investigation into leveraging AI/ML tools to address these challenges for MIMO becomes a critical step towards an AI-enabled NextG air interface. At the NextG air interface, the underlying wireless environment will be extremely dynamic with operation adaptations performed on a sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and rank/link adaptation. Given the enormously large number of operation adaptation possibilities, we contend that online real-time AI/ML-ba
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>FinReport&#26159;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#37329;&#34701;&#26032;&#38395;&#20844;&#21578;&#21644;&#22810;&#22240;&#32032;&#27169;&#22411;&#26469;&#29983;&#25104;&#19987;&#19994;&#30340;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;&#25253;&#21578;&#65292;&#26088;&#22312;&#24110;&#21161;&#26222;&#36890;&#25237;&#36164;&#32773;&#25910;&#38598;&#20449;&#24687;&#12289;&#20998;&#26512;&#25968;&#25454;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2403.02647</link><description>&lt;p&gt;
FinReport: &#36890;&#36807;&#26032;&#38395;&#22240;&#32032;&#20998;&#26512;&#27169;&#22411;&#35299;&#37322;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02647
&lt;/p&gt;
&lt;p&gt;
FinReport&#26159;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#37329;&#34701;&#26032;&#38395;&#20844;&#21578;&#21644;&#22810;&#22240;&#32032;&#27169;&#22411;&#26469;&#29983;&#25104;&#19987;&#19994;&#30340;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;&#25253;&#21578;&#65292;&#26088;&#22312;&#24110;&#21161;&#26222;&#36890;&#25237;&#36164;&#32773;&#25910;&#38598;&#20449;&#24687;&#12289;&#20998;&#26512;&#25968;&#25454;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02647v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30001;&#20110;&#23454;&#38469;&#24773;&#20917;&#19979;&#25237;&#36164;&#32773;&#30340;&#38656;&#27714;&#65292;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;&#36825;&#19968;&#20219;&#21153;&#21463;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#37329;&#34701;&#26426;&#26500;&#30456;&#27604;&#65292;&#26222;&#36890;&#25237;&#36164;&#32773;&#24456;&#38590;&#25366;&#25496;&#22240;&#32032;&#24182;&#20998;&#26512;&#26032;&#38395;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#24418;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#65292;&#20294;&#20173;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#37329;&#34701;&#30693;&#35782;&#25552;&#20986;&#21512;&#29702;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#65292;&#25105;&#20204;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#65292;&#21517;&#20026;FinReport&#65292;&#20197;&#20415;&#26222;&#36890;&#25237;&#36164;&#32773;&#25910;&#38598;&#20449;&#24687;&#12289;&#20998;&#26512;&#20449;&#24687;&#65292;&#24182;&#22312;&#24635;&#32467;&#21518;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02647v1 Announce Type: cross  Abstract: The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing.   Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news infor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#30340;&#26032;&#26041;&#27861;&#26469;&#21152;&#36895;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#24179;&#22343;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;A-PPS&#65289;&#12289;&#22870;&#21169;&#21487;&#20280;&#32553;&#24615;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;RS-PPS&#65289;&#21644;&#37096;&#20998;&#20010;&#24615;&#21270;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;PP-PPS&#65289;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.02635</link><description>&lt;p&gt;
PPS-QMIX: &#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#29992;&#20110;&#21152;&#36895;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#30340;&#26032;&#26041;&#27861;&#26469;&#21152;&#36895;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#24179;&#22343;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;A-PPS&#65289;&#12289;&#22870;&#21169;&#21487;&#20280;&#32553;&#24615;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;RS-PPS&#65289;&#21644;&#37096;&#20998;&#20010;&#24615;&#21270;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;PP-PPS&#65289;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#27599;&#20010;&#26234;&#20307;&#30340;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;&#12290;&#19968;&#20010;&#32570;&#28857;&#26159;MARL&#20013;&#27599;&#20010;&#26234;&#20307;&#30340;&#31574;&#30053;&#26159;&#29420;&#31435;&#30340;&#65292;&#23454;&#38469;&#19978;&#26159;&#21512;&#20316;&#30340;&#12290;&#22240;&#27492;&#65292;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24403;&#21069;&#30740;&#31350;&#21033;&#29992;&#36328;&#22810;&#26234;&#20307;&#30340;&#20013;&#24515;&#21270;&#21151;&#33021;&#65288;CF&#65289;&#26469;&#23398;&#20064;&#27599;&#20010;&#26234;&#20307;&#22242;&#38431;&#22870;&#21169;&#30340;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;CF&#30340;&#26041;&#27861;&#20250;&#22312;&#20540;&#32593;&#32476;&#20272;&#35745;&#20013;&#24341;&#20837;&#20854;&#20182;&#26234;&#20307;&#30340;&#32852;&#21512;&#35823;&#24046;&#12290;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#24179;&#22343;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;A-PPS&#65289;&#12289;&#22870;&#21169;&#21487;&#20280;&#32553;&#24615;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;RS-PPS&#65289;&#21644;&#37096;&#20998;&#20010;&#24615;&#21270;&#21608;&#26399;&#21442;&#25968;&#20849;&#20139;&#65288;PP-PPS&#65289;&#26426;&#21046;&#26469;&#21152;&#36895;MARL&#35757;&#32451;&#12290;&#26234;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21608;&#26399;&#24615;&#22320;&#20849;&#20139;Q&#20540;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02635v1 Announce Type: new  Abstract: Training for multi-agent reinforcement learning(MARL) is a time-consuming process caused by distribution shift of each agent. One drawback is that strategy of each agent in MARL is independent but actually in cooperation. Thus, a vertical issue in multi-agent reinforcement learning is how to efficiently accelerate training process. To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent. However, CF based methods introduce joint error from other agents in estimation of value network. In so doing, inspired by federated learning, we propose three simple novel approaches called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL. Agents share Q-value network periodically during the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;</title><link>https://arxiv.org/abs/2403.02624</link><description>&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#19987;&#27880;&#20110;&#21457;&#23637;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#26469;&#33258;&#30701;&#26399;&#21644;&#38271;&#26399;&#25928;&#26524;&#30340;&#24635;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290; &#20363;&#22914;&#65292;&#33647;&#29289;&#21058;&#37327;&#30340;&#22686;&#21152;&#21487;&#33021;&#20250;&#25552;&#39640;&#24739;&#32773;&#24247;&#22797;&#36895;&#24230;&#65288;&#30701;&#26399;&#65289;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#38271;&#26399;&#21103;&#20316;&#29992;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#26377;&#20851;&#30701;&#26399;&#25110;&#38271;&#26399;&#25928;&#24212;&#25110;&#20004;&#32773;&#30340;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20197;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#20256;&#32479;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30452;&#25509;&#20272;&#35745;&#22810;&#20010;&#30446;&#26631;&#26102;&#65292;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#20248;&#21270;&#26041;&#21521;&#20063;&#21487;&#33021;&#21457;&#29983;&#20914;&#31361;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24085;&#32047;&#25176;&#26377;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#65288;POE&#65289;&#21644;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65288;POPL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
&lt;/p&gt;</description></item><item><title>&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02622</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
World Models for Autonomous Driving: An Initial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02622
&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#35780;&#20272;&#20854;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#20851;&#38190;&#22320;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#33021;&#22815;&#32508;&#21512;&#21644;&#35299;&#37322;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#28508;&#22312;&#30340;&#26410;&#26469;&#24773;&#26223;&#24182;&#24357;&#34917;&#20449;&#24687;&#32570;&#21475;&#12290;&#26412;&#25991;&#23545;&#33258;&#20027;&#39550;&#39542;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21457;&#23637;&#36827;&#34892;&#20102;&#21021;&#27493;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#24378;&#35843;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#22522;&#30784;&#21442;&#32771;&#65292;&#20415;&#20110;&#24555;&#36895;&#33719;&#24471;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#26041;&#27861;&#65288;MAD-Transformer&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#29366;&#24577;&#30697;&#38453;&#21644;&#31354;&#38388;&#29366;&#24577;&#30697;&#38453;&#26469;&#25581;&#31034;&#24037;&#19994;CPS&#24037;&#20316;&#29366;&#24577;&#30340;&#26102;&#31354;&#20851;&#32852;&#20851;&#31995;&#21644;&#28436;&#21464;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.02616</link><description>&lt;p&gt;
&#24037;&#19994;&#29289;&#29702;&#31995;&#32479;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#30340;&#26080;&#30417;&#30563;&#26102;&#31354;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02616
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#26041;&#27861;&#65288;MAD-Transformer&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#29366;&#24577;&#30697;&#38453;&#21644;&#31354;&#38388;&#29366;&#24577;&#30697;&#38453;&#26469;&#25581;&#31034;&#24037;&#19994;CPS&#24037;&#20316;&#29366;&#24577;&#30340;&#26102;&#31354;&#20851;&#32852;&#20851;&#31995;&#21644;&#28436;&#21464;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#26816;&#27979;&#21644;&#35786;&#26029;&#24322;&#24120;&#34892;&#20026;&#65292;&#22914;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#30340;&#31283;&#23450;&#26377;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24456;&#23569;&#20851;&#27880;&#31995;&#32479;&#24037;&#20316;&#29366;&#24577;&#20043;&#38388;&#30340;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#38590;&#20197;&#35299;&#37322;&#24322;&#24120;&#20449;&#21495;&#30340;&#28436;&#21464;&#26426;&#21046;&#12290;&#20026;&#20102;&#25581;&#31034;&#24037;&#19994;CPS&#24037;&#20316;&#29366;&#24577;&#30340;&#26102;&#31354;&#20851;&#32852;&#20851;&#31995;&#21644;&#28436;&#21464;&#26426;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#33258;&#36866;&#24212;&#24322;&#24120;&#35786;&#26029;&#26041;&#27861;&#65288;&#21363;MAD-Transformer&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35786;&#26029;MTS&#20013;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02616v1 Announce Type: cross  Abstract: Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS). However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals. To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension. Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation rel
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#30340;&#30740;&#31350;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#26412;&#25991;&#36890;&#36807;&#21021;&#27493;&#33539;&#22260;&#23457;&#26597;&#20102;76&#31687;&#20851;&#20110;LLMs&#21644;&#35270;&#39057;&#28216;&#25103;&#30340;&#35770;&#25991;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#35780;&#35770;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.02613</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#39057;&#28216;&#25103;&#65306;&#21021;&#27493;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Video Games: A Preliminary Scoping Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02613
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#30340;&#30740;&#31350;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#26412;&#25991;&#36890;&#36807;&#21021;&#27493;&#33539;&#22260;&#23457;&#26597;&#20102;76&#31687;&#20851;&#20110;LLMs&#21644;&#35270;&#39057;&#28216;&#25103;&#30340;&#35770;&#25991;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#35780;&#35770;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#39057;&#28216;&#25103;&#30340;&#35774;&#35745;&#12289;&#24320;&#21457;&#21644;&#30740;&#31350;&#20013;&#20855;&#26377;&#26377;&#36259;&#30340;&#28508;&#21147;&#12290; &#26412;&#25991;&#22522;&#20110;&#28216;&#25103;&#20013;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20960;&#21313;&#24180;&#21069;&#30340;&#30740;&#31350;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21152;&#24555;&#20102;&#23545;LLMs&#22312;&#28216;&#25103;&#20013;&#30340;&#21147;&#37327;&#21644;&#28508;&#21147;&#36827;&#34892;&#30740;&#31350;&#12290; &#37492;&#20110;&#26368;&#36817;LLM&#30456;&#20851;&#30740;&#31350;&#22312;&#28216;&#25103;&#39046;&#22495;&#30340;&#28608;&#22686;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30456;&#20851;&#30740;&#31350;&#21487;&#20379;&#35843;&#26597;&#12290; &#20026;&#20102;&#25429;&#25417;LLM&#22312;&#28216;&#25103;&#20013;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#22312;&#26410;&#26469;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#36804;&#20170;&#20026;&#27490;&#24050;&#21457;&#34920;&#30340;&#30456;&#20851;&#35770;&#25991;&#30340;&#21021;&#27493;&#33539;&#22260;&#23457;&#26597;&#12290; &#26412;&#25991;&#23457;&#26597;&#20102;2022&#24180;&#33267;2024&#24180;&#21021;&#21457;&#34920;&#30340;76&#31687;&#26377;&#20851;LLMs&#21644;&#35270;&#39057;&#28216;&#25103;&#30340;&#35770;&#25991;&#65292;&#37325;&#28857;&#20851;&#27880;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#12289;&#28216;&#25103;&#24320;&#21457;&#12289;&#21465;&#20107;&#20197;&#21450;&#28216;&#25103;&#30740;&#31350;&#21644;&#35780;&#35770;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290; &#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#21021;&#27493;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#20026;&#26410;&#26469;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#21644;&#35780;&#35770;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02613v1 Announce Type: cross  Abstract: Large language models (LLMs) hold interesting potential for the design, development, and research of video games. Building on the decades of prior research on generative AI in games, many researchers have sped to investigate the power and potential of LLMs for games. Given the recent spike in LLM-related research in games, there is already a wealth of relevant research to survey. In order to capture a snapshot of the state of LLM research in games, and to help lay the foundation for future work, we carried out an initial scoping review of relevant papers published so far. In this paper, we review 76 papers published between 2022 to early 2024 on LLMs and video games, with key focus areas in game AI, game development, narrative, and game research and reviews. Our paper provides an early state of the field and lays the groundwork for future research and reviews on this topic.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#21644;&#29305;&#24449;&#19981;&#36275;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02611</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26174;&#24494;&#38236;&#28966;&#22806;&#27169;&#31946;&#21435;&#38500;&#26694;&#26550;: &#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#21644;&#29305;&#24449;&#19981;&#36275;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#28966;&#27169;&#31946;&#26159;&#26174;&#24494;&#38236;&#25104;&#20687;&#20013;&#30340;&#25345;&#32493;&#38382;&#39064;&#65292;&#23545;&#30149;&#29702;&#35299;&#37322;&#21644;&#32454;&#32990;&#26174;&#24494;&#38236;&#21644;&#26174;&#24494;&#25163;&#26415;&#20013;&#30340;&#21307;&#30103;&#24178;&#39044;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#65288;MPT&#65289;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65288;EFCR&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#20004;&#20010;&#31361;&#20986;&#25361;&#25112;&#65306;&#36739;&#38271;&#30340;&#27880;&#24847;&#21147;&#36328;&#24230;&#21644;&#29305;&#24449;&#19981;&#36275;&#12290;MPT&#22312;&#27599;&#20010;&#32593;&#32476;&#38454;&#27573;&#20351;&#29992;&#26174;&#24335;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#38598;&#25104;&#20102;&#36328;&#23610;&#24230;&#31383;&#21475;&#27880;&#24847;&#21147;&#65288;CSWA&#65289;&#12289;&#20869;&#23610;&#24230;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;ISCA&#65289;&#21644;&#29305;&#24449;&#22686;&#24378;&#21069;&#21521;&#32593;&#32476;&#65288;FEFN&#65289;&#65292;&#20197;&#25429;&#33719;&#38271;&#36317;&#31163;&#36328;&#23610;&#24230;&#31354;&#38388;&#20132;&#20114;&#21644;&#20840;&#23616;&#36890;&#36947;&#19978;&#19979;&#25991;&#12290;EFCR&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#39057;&#27573;&#30340;&#28508;&#22312;&#21435;&#27169;&#31946;&#20449;&#21495;&#26469;&#35299;&#20915;&#29305;&#24449;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23427;&#36824;&#20351;&#21435;&#27169;&#31946;&#30693;&#35782;&#20256;&#36755;&#65292;&#20174;&#39069;&#22806;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02611v1 Announce Type: cross  Abstract: Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;2024&#24180;IEEE&#28216;&#25103;&#22823;&#20250;&#19978;&#31532;&#20108;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#24341;&#20837;&#22810;&#26679;&#24615;&#20316;&#20026;&#26032;&#30340;&#25351;&#26631;&#65292;&#20801;&#35768;&#25552;&#20132;Python&#31243;&#24207;&#20197;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;&#20808;&#36827;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02610</link><description>&lt;p&gt;
ChatGPT4PCG 2&#27604;&#36187;&#65306;&#20026;&#31185;&#23398;&#40479;&#32423;&#21035;&#29983;&#25104;&#35774;&#35745;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2024&#24180;IEEE&#28216;&#25103;&#22823;&#20250;&#19978;&#31532;&#20108;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#24341;&#20837;&#22810;&#26679;&#24615;&#20316;&#20026;&#26032;&#30340;&#25351;&#26631;&#65292;&#20801;&#35768;&#25552;&#20132;Python&#31243;&#24207;&#20197;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;&#20808;&#36827;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2024&#24180;IEEE&#28216;&#25103;&#22823;&#20250;&#19978;&#31532;&#20108;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#22312;&#36825;&#19968;&#23626;&#27604;&#36187;&#20013;&#65292;&#25105;&#20204;&#27839;&#34989;&#31532;&#19968;&#23626;&#30340;&#22522;&#30784;&#19978;&#20570;&#20986;&#20102;&#20960;&#39033;&#25913;&#36827;&#21644;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20801;&#35768;&#21442;&#19982;&#32773;&#20197;&#26356;&#28789;&#27963;&#30340;&#26684;&#24335;&#25552;&#20132;&#20316;&#21697;&#65292;&#24182;&#23545;&#35780;&#20272;&#27969;&#31243;&#36827;&#34892;&#20102;&#20960;&#39033;&#25913;&#36827;&#12290;&#24310;&#32493;&#31532;&#19968;&#23626;&#27604;&#36187;&#30340;&#31934;&#31070;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#21644;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#65288;PE&#65289;&#22312;&#31243;&#24207;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#31532;&#19968;&#27425;&#27604;&#36187;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20063;&#21463;&#21040;&#20102;&#21508;&#31181;&#38480;&#21046;&#30340;&#38459;&#30861;&#65307;&#25105;&#20204;&#26088;&#22312;&#22312;&#36825;&#27425;&#27604;&#36187;&#20013;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26679;&#24615;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#25351;&#26631;&#65292;&#20197;&#38459;&#27490;&#20135;&#29983;&#37325;&#22797;&#32467;&#26500;&#30340;&#20316;&#21697;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#28789;&#27963;&#22320;&#23454;&#29616;&#20808;&#36827;&#30340;PE&#26041;&#27861;&#65292;&#25105;&#20204;&#20801;&#35768;&#25552;&#20132;Python&#31243;&#24207;&#32780;&#19981;&#26159;&#25552;&#31034;&#25991;&#26412;&#25991;&#20214;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#38656;&#35201;&#25511;&#21046;&#27969;&#65292;&#21253;&#25324;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02610v1 Announce Type: new  Abstract: This paper presents the second ChatGPT4PCG competition at the 2024 IEEE Conference on Games. In this edition of the competition, we follow the first edition, but make several improvements and changes. We introduce a new evaluation metric along with allowing a more flexible format for participants' submissions and making several improvements to the evaluation pipeline. Continuing from the first edition, we aim to foster and explore the realm of prompt engineering (PE) for procedural content generation (PCG). While the first competition saw success, it was hindered by various limitations; we aim to mitigate these limitations in this edition. We introduce diversity as a new metric to discourage submissions aimed at producing repetitive structures. Furthermore, we allow submission of a Python program instead of a prompt text file for greater flexibility in implementing advanced PE approaches, which may require control flow, including conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20986;&#20215;&#36974;&#34109;&#21040;&#22810;&#27133;&#20301;&#23637;&#31034;&#24191;&#21578;&#20013;&#65292;&#36890;&#36807;MEBS&#26041;&#27861;&#23454;&#29616;&#20102;&#31454;&#20215;&#20215;&#26684;&#35843;&#25972;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#24615;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#27611;&#21033;&#37327;&#30340;7.01%&#22686;&#38271;&#21644;&#25237;&#36164;&#22238;&#25253;&#29575;&#30340;7.42%&#22686;&#38271;</title><link>https://arxiv.org/abs/2403.02607</link><description>&lt;p&gt;
MEBS: &#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#20986;&#20215;&#36974;&#34109;&#29992;&#20110;&#22810;&#27133;&#20301;&#23637;&#31034;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
MEBS: Multi-task End-to-end Bid Shading for Multi-slot Display Advertising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20986;&#20215;&#36974;&#34109;&#21040;&#22810;&#27133;&#20301;&#23637;&#31034;&#24191;&#21578;&#20013;&#65292;&#36890;&#36807;MEBS&#26041;&#27861;&#23454;&#29616;&#20102;&#31454;&#20215;&#20215;&#26684;&#35843;&#25972;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#24615;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#27611;&#21033;&#37327;&#30340;7.01%&#22686;&#38271;&#21644;&#25237;&#36164;&#22238;&#25253;&#29575;&#30340;7.42%&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02607v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#22312;&#32447;&#31454;&#20215;&#21644;&#25293;&#21334;&#26159;&#22312;&#32447;&#24191;&#21578;&#34892;&#19994;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20256;&#32479;&#19978;&#65292;&#21482;&#26377;&#19968;&#20010;&#24191;&#21578;&#27133;&#20301;&#29992;&#20110;&#24191;&#21578;&#23637;&#31034;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27492;&#12290;&#22914;&#20170;&#65292;&#22810;&#27133;&#20301;&#23637;&#31034;&#24191;&#21578;&#36880;&#28176;&#27969;&#34892;&#36215;&#26469;&#65292;&#21487;&#20197;&#22312;&#21015;&#34920;&#20013;&#23637;&#31034;&#35768;&#22810;&#24191;&#21578;&#65292;&#24182;&#25972;&#20307;&#23637;&#31034;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#22810;&#27133;&#20301;&#23637;&#31034;&#24191;&#21578;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#21578;&#20027;&#26377;&#21160;&#26426;&#35843;&#25972;&#31454;&#26631;&#20215;&#26684;&#65292;&#20197;&#36194;&#24471;&#26368;&#32463;&#27982;&#30340;&#24191;&#21578;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20986;&#20215;&#36974;&#34109;&#21040;&#22810;&#27133;&#20301;&#23637;&#31034;&#24191;&#21578;&#20013;&#65292;&#36890;&#36807;&#19968;&#31181;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#20986;&#20215;&#36974;&#34109;&#65288;MEBS&#65289;&#26041;&#27861;&#36827;&#34892;&#31454;&#20215;&#20215;&#26684;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#33719;&#24471;&#20102;&#27611;&#21033;&#37327;&#30340;7.01%&#22686;&#38271;&#65292;&#25237;&#36164;&#22238;&#25253;&#29575;&#30340;7.42%&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02607v1 Announce Type: cross  Abstract: Online bidding and auction are crucial aspects of the online advertising industry. Conventionally, there is only one slot for ad display and most current studies focus on it. Nowadays, multi-slot display advertising is gradually becoming popular where many ads could be displayed in a list and shown as a whole to users. However, multi-slot display advertising leads to different cost-effectiveness. Advertisers have the incentive to adjust bid prices so as to win the most economical ad positions. In this study, we introduce bid shading into multi-slot display advertising for bid price adjustment with a Multi-task End-to-end Bid Shading(MEBS) method. We prove the optimality of our method theoretically and examine its performance experimentally. Through extensive offline and online experiments, we demonstrate the effectiveness and efficiency of our method, and we obtain a 7.01% lift in Gross Merchandise Volume, a 7.42% lift in Return on Inv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#20801;&#35768;&#27599;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#21644;&#27599;&#20010;&#36845;&#20195;&#20013;&#30340;&#21333;&#20010;&#32452;&#21512;&#65292;&#21516;&#26102;&#23558;&#19981;&#31934;&#30830;&#21644;&#31934;&#30830;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#30340;&#32447;&#24615;&#25910;&#25947;&#21644;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02589</link><description>&lt;p&gt;
MUSIC: &#20855;&#26377;&#19981;&#31934;&#30830;&#21644;&#31934;&#30830;&#26041;&#27861;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#21152;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
MUSIC: Accelerated Convergence for Distributed Optimization With Inexact and Exact Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#20801;&#35768;&#27599;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#21644;&#27599;&#20010;&#36845;&#20195;&#20013;&#30340;&#21333;&#20010;&#32452;&#21512;&#65292;&#21516;&#26102;&#23558;&#19981;&#31934;&#30830;&#21644;&#31934;&#30830;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#30340;&#32447;&#24615;&#25910;&#25947;&#21644;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#22411;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#32593;&#32476;&#21270;&#20195;&#29702;&#31995;&#32479;&#19978;&#30340;&#26368;&#23567;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#37325;&#35201;&#24037;&#20855;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27599;&#27425;&#36845;&#20195;&#21482;&#33021;&#23454;&#29616;&#19968;&#20010;&#26799;&#24230;&#26356;&#26032;&#65292;&#38590;&#20197;&#23454;&#29616;&#25910;&#25947;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#20801;&#35768;&#27599;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#21644;&#27599;&#20010;&#36845;&#20195;&#20013;&#30340;&#21333;&#20010;&#32452;&#21512;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#19981;&#31934;&#30830;&#21644;&#31934;&#30830;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#35013;&#22791;&#21040;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22240;&#27492;&#24320;&#21457;&#20986;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;&#34920;&#29616;&#20986;&#21152;&#36895;&#30340;&#32447;&#24615;&#25910;&#25947;&#21644;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20005;&#26684;&#25910;&#25947;&#20998;&#26512;&#25581;&#31034;&#20102;&#30001;&#19981;&#31934;&#30830;&#31574;&#30053;&#24341;&#36215;&#30340;&#31283;&#24577;&#35823;&#24046;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21160;&#26426;&#21644;&#20998;&#26512;&#65292;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02589v1 Announce Type: cross  Abstract: Gradient-type distributed optimization methods have blossomed into one of the most important tools for solving a minimization learning task over a networked agent system. However, only one gradient update per iteration is difficult to achieve a substantive acceleration of convergence. In this paper, we propose an accelerated framework named as MUSIC allowing each agent to perform multiple local updates and a single combination in each iteration. More importantly, we equip inexact and exact distributed optimization methods into this framework, thereby developing two new algorithms that exhibit accelerated linear convergence and high communication efficiency. Our rigorous convergence analysis reveals the sources of steady-state errors arising from inexact policies and offers effective solutions. Numerical results based on synthetic and real datasets demonstrate both our theoretical motivations and analysis, as well as performance advanta
&lt;/p&gt;</description></item><item><title>ChatCite&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#36827;&#34892;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;&#65292;&#21033;&#29992;&#21453;&#24605;&#36880;&#27493;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.02574</link><description>&lt;p&gt;
ChatCite&#65306;LLM&#20195;&#29702;&#19982;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#29992;&#20110;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02574
&lt;/p&gt;
&lt;p&gt;
ChatCite&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#36827;&#34892;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;&#65292;&#21033;&#29992;&#21453;&#24605;&#36880;&#27493;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02574v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#25688;&#35201;&#65306;&#25991;&#29486;&#32508;&#36848;&#26159;&#30740;&#31350;&#36807;&#31243;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#27493;&#12290;&#23427;&#26377;&#21161;&#20110;&#29702;&#35299;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#22312;&#36827;&#34892;&#20197;&#24448;&#20316;&#21697;&#27604;&#36739;&#20998;&#26512;&#26102;&#20102;&#35299;&#24403;&#21069;&#30740;&#31350;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#24635;&#32467;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#20808;&#21069;&#22522;&#20110;LLM&#30340;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23436;&#25972;&#36807;&#31243;&#65292;&#21253;&#25324;&#25991;&#29486;&#26816;&#32034;&#12289;&#31579;&#36873;&#21644;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24635;&#32467;&#27493;&#39588;&#65292;&#31616;&#21333;&#30340;CoT&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#25552;&#20379;&#24191;&#27867;&#27604;&#36739;&#24635;&#32467;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#19987;&#27880;&#20110;&#29420;&#31435;&#25991;&#29486;&#24635;&#32467;&#27493;&#39588;&#24182;&#24341;&#20837;ChatCite&#65292;&#19968;&#20010;&#20855;&#26377;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#29992;&#20110;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;&#30340;LLM&#20195;&#29702;&#12290;&#35813;&#20195;&#29702;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24037;&#20316;&#27969;&#65292;&#39318;&#20808;&#20174;&#30456;&#20851;&#25991;&#29486;&#20013;&#25552;&#21462;&#20851;&#38190;&#35201;&#32032;&#65292;&#28982;&#21518;&#20351;&#29992;&#21453;&#24605;&#36880;&#27493;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02574v1 Announce Type: cross  Abstract: The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better ev
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02567</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Eliciting Better Multilingual Structured Reasoning from LLMs through Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02567
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#22312;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#25110;&#31616;&#21333;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;xSTREET&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20845;&#31181;&#35821;&#35328;&#30340;&#22235;&#20010;&#20219;&#21153;&#12290;xSTREET&#26292;&#38706;&#20102;&#22522;&#26412;LLM&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#25512;&#29702;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24314;&#31435;&#22312;LLM&#22312;&#20195;&#30721;&#19978;&#35757;&#32451;&#26356;&#22909;&#30340;&#25512;&#29702;&#36825;&#19968;&#35266;&#28857;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#22312;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#20195;&#30721;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#22810;&#35821;&#35328;&#27880;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#31243;&#24207;&#20195;&#30721;&#19981;&#21464;&#12290;&#20854;&#27425;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#21253;&#21547;&#36880;&#27493;&#20195;&#30721;&#21407;&#35821;&#30340;&#25552;&#31034;&#32467;&#26500;&#26469;&#24357;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#23548;&#20986;&#26032;&#20107;&#23454;&#24182;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;xSTREET&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31185;&#23398;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02567v1 Announce Type: cross  Abstract: Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks. We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reaso
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#23545;&#25968;&#25454;&#20998;&#26512;&#36825;&#19968;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.02528</link><description>&lt;p&gt;
DACO: &#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#23454;&#29616;&#24212;&#29992;&#39537;&#21160;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#23545;&#25968;&#25454;&#20998;&#26512;&#36825;&#19968;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#26512;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20998;&#26512;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28145;&#20837;&#30740;&#31350;&#21644;&#32467;&#35770;&#24615;&#35265;&#35299;&#65292;&#20840;&#38754;&#22238;&#31572;&#32473;&#23450;&#29992;&#25143;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26597;&#35810;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#26032;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#65292;&#28608;&#21457;&#26410;&#26469;&#23545;&#36825;&#19968;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26410;&#20805;&#20998;&#25366;&#25496;&#30340;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21644;&#22810;&#36718;&#25552;&#31034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;440&#20010;&#26469;&#33258;&#30495;&#23454;&#22330;&#26223;&#30340;&#25968;&#25454;&#24211;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#65292;&#32422;2k&#20010;&#26597;&#35810;-&#31572;&#26696;&#23545;&#21487;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#24369;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#20154;&#24037;&#31934;&#32454;&#35843;&#25972;&#30340;&#26631;&#27880;&#30340;&#32039;&#20945;&#20294;&#39640;&#36136;&#37327;&#27979;&#35797;&#38598;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02528v1 Announce Type: cross  Abstract: Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#20123;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02523</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#20197;S&amp;P500&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Transformer for Times Series: an Application to the S&amp;P500
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#20123;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#20004;&#31181;&#20856;&#22411;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#38598;&#26500;&#36896;&#65306;&#19968;&#31181;&#26159;&#22343;&#20540;&#22238;&#24402;&#30340;&#21512;&#25104;Ornstein-Uhlenbeck&#36807;&#31243;&#65292;&#21478;&#19968;&#31181;&#26159;&#30495;&#23454;&#30340;S&amp;P500&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25552;&#20986;&#30340;Transformer&#26550;&#26500;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#19968;&#20123;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#39044;&#27979;&#19979;&#19968;&#27493;&#30340;&#36208;&#21183;&#65292;&#32780;&#23545;&#20110;S&amp;P500&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20123;&#19982;&#20108;&#27425;&#21464;&#21160;&#21644;&#27874;&#21160;&#29575;&#39044;&#27979;&#30456;&#20851;&#30340;&#26377;&#36259;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02523v1 Announce Type: new  Abstract: The transformer models have been extensively used with good results in a wide area of machine learning applications including Large Language Models and image generation. Here, we inquire on the applicability of this approach to financial time series. We first describe the dataset construction for two prototypical situations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand and real S&amp;P500 data on the other hand. Then, we present in detail the proposed Transformer architecture and finally we discuss some encouraging results. For the synthetic data we predict rather accurately the next move, and for the S&amp;P500 we get some interesting results related to quadratic variation and volatility prediction.
&lt;/p&gt;</description></item><item><title>HeAR&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#22312;33&#20010;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#26377;&#26395;&#25512;&#21160;&#20581;&#24247;&#22768;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.02522</link><description>&lt;p&gt;
&#20581;&#24247;&#22768;&#23398;&#34920;&#31034;&#65306;HeAR
&lt;/p&gt;
&lt;p&gt;
HeAR -- Health Acoustic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02522
&lt;/p&gt;
&lt;p&gt;
HeAR&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#22312;33&#20010;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#26377;&#26395;&#25512;&#21160;&#20581;&#24247;&#22768;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#22768;&#23398;&#22768;&#38899;&#65292;&#22914;&#21683;&#22013;&#21644;&#21628;&#21560;&#22768;&#65292;&#24050;&#30693;&#21253;&#21547;&#26377;&#29992;&#30340;&#20581;&#24247;&#20449;&#21495;&#65292;&#20855;&#26377;&#30417;&#27979;&#20581;&#24247;&#21644;&#30142;&#30149;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#29616;&#26377;&#30340;&#20581;&#24247;&#22768;&#23398;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#21482;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#29421;&#31364;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#36825;&#21463;&#21040;&#25968;&#25454;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#23545;&#20854;&#20182;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HeAR&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20280;&#32553;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;3.13&#20159;&#20010;&#20004;&#31186;&#38271;&#38899;&#39057;&#21098;&#36753;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#12290;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#65292;&#25105;&#20204;&#23558;HeAR&#30830;&#31435;&#20026;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;33&#20010;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#20581;&#24247;&#38899;&#39057;&#23884;&#20837;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#21551;&#29992;&#21644;&#21152;&#36895;&#36827;&#19968;&#27493;&#30340;&#20581;&#24247;&#22768;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02522v1 Announce Type: cross  Abstract: Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and disease, yet are underexplored in the medical machine learning community. The existing deep learning systems for health acoustics are often narrowly trained and evaluated on a single task, which is limited by data and may hinder generalization to other tasks. To mitigate these gaps, we develop HeAR, a scalable self-supervised learning-based deep learning system using masked autoencoders trained on a large dataset of 313 million two-second long audio clips. Through linear probes, we establish HeAR as a state-of-the-art health audio embedding model on a benchmark of 33 health acoustic tasks across 6 datasets. By introducing this work, we hope to enable and accelerate further health acoustics research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.02514</link><description>&lt;p&gt;
&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#26426;&#22120;&#20154;&#35774;&#23450;&#30446;&#30340;&#65306;&#19968;&#20010;&#35745;&#31639;&#20998;&#31867;&#12289;&#23450;&#20041;&#21644;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#20027;&#24320;&#25918;&#24335;&#23398;&#20064;(OEL)&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#30452;&#25509;&#20132;&#20114;&#32047;&#31215;&#33719;&#21462;&#26032;&#25216;&#33021;&#21644;&#30693;&#35782;&#65292;&#20363;&#22914;&#20381;&#38752;&#20869;&#22312;&#21160;&#26426;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#30446;&#26631;&#30340;&#25351;&#23548;&#12290;OEL&#26426;&#22120;&#20154;&#23545;&#24212;&#29992;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#20027;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23436;&#25104;&#23545;&#20154;&#31867;&#29992;&#25143;&#26377;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;OEL&#26426;&#22120;&#20154;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#36825;&#21487;&#33021;&#23548;&#33268;&#33719;&#21462;&#30340;&#30693;&#35782;&#23545;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#24182;&#19981;&#37027;&#20040;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22260;&#32469;&#8220;&#30446;&#30340;&#8221;&#36825;&#19968;&#26032;&#27010;&#24565;&#23637;&#24320;&#12290;&#30446;&#30340;&#34920;&#31034;&#35774;&#35745;&#32773;&#21644;/&#25110;&#29992;&#25143;&#24076;&#26395;&#26426;&#22120;&#20154;&#20174;&#20013;&#33719;&#24471;&#20160;&#20040;&#12290;&#26426;&#22120;&#20154;&#24212;&#20351;&#29992;&#30446;&#30340;&#30340;&#20869;&#37096;&#34920;&#24449;&#65292;&#36825;&#37324;&#31216;&#20026;&#8220;&#24895;&#26395;&#8221;&#65292;&#26469;&#23558;&#20854;&#24320;&#25918;&#24335;&#25506;&#32034;&#38598;&#20013;&#20110;&#33719;&#21462;&#19982;&#20854;&#23436;&#25104;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#21457;&#23637;&#19968;&#20010;&#20849;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 Announce Type: cross  Abstract: Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a co
&lt;/p&gt;</description></item><item><title>SPUQ&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#29616;&#35937;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;</title><link>https://arxiv.org/abs/2403.02509</link><description>&lt;p&gt;
SPUQ&#65306;&#22522;&#20110;&#25200;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02509
&lt;/p&gt;
&lt;p&gt;
SPUQ&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#29616;&#35937;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25552;&#20379;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20570;&#20986;&#33258;&#20449;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#31361;&#26174;&#20102;&#22312;LLMs&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#29616;&#35937;&#24615;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#23545;&#21253;&#25324;&#35748;&#30693;&#24615;&#22312;&#20869;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#20840;&#35889;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;&#36825;&#19968;&#24046;&#36317;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;UQ&#26041;&#27861;&#65292;&#21363;&#36866;&#29992;&#20110;UQ&#30340;&#25200;&#21160;&#37319;&#26679;&#65288;SPUQ&#65289;&#65292;&#26088;&#22312;&#24212;&#23545;&#29616;&#35937;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20026;LLM&#36755;&#20837;&#29983;&#25104;&#19968;&#32452;&#25200;&#21160;&#65292;&#23545;&#27599;&#20010;&#25200;&#21160;&#36827;&#34892;&#36755;&#20986;&#37319;&#26679;&#65292;&#24182;&#32467;&#21512;&#19968;&#20010;&#32858;&#21512;&#27169;&#22359;&#65292;&#35813;&#32858;&#21512;&#27169;&#22359;&#27010;&#25324;&#20102;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25200;&#21160;&#21644;&#32858;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02509v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggrega
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.02504</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02504
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21306;&#21035;&#20110;&#20247;&#65292;&#23637;&#31034;&#20102;&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#12290;&#36825;&#31181;&#25928;&#29575;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#27880;&#37322;&#26679;&#26412;&#30340;&#25968;&#37327;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#25945;&#31243;&#20840;&#38754;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#26696;&#20363;&#32451;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#24378;&#35843;&#20854;&#39640;&#25928;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#65292;&#35813;&#25945;&#31243;&#26088;&#22312;&#40723;&#21169;&#26356;&#24191;&#27867;&#22320;&#37319;&#32435;&#36825;&#31181;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v1 Announce Type: cross  Abstract: The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02502</link><description>&lt;p&gt;
&#35797;&#38169;&#27861;&#65306;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;ETO&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;LLM&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#35757;&#32451;&#25104;&#21151;&#19987;&#23478;&#36712;&#36857;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20174;&#20854;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#36825;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#23436;&#25104;&#25351;&#23450;&#20219;&#21153;&#65292;&#25910;&#38598;&#22833;&#36133;&#36712;&#36857;&#20197;&#21019;&#24314;&#23545;&#27604;&#36712;&#36857;&#23545;&#12290;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#29702;&#21033;&#29992;&#36825;&#20123;&#36712;&#36857;&#20559;&#22909;&#23545;&#26356;&#26032;&#20854;&#31574;&#30053;&#65292;&#20351;&#29992;&#31867;&#20284;DPO&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#25506;&#32034;&#21644;&#35757;&#32451;&#30340;&#36845;&#20195;&#24490;&#29615;&#20419;&#36827;&#20102;&#20195;&#29702;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;SSL-ConvSAC&#26041;&#27861;&#65292;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#24773;&#22659;&#35838;&#31243;&#30340;&#26041;&#27861;&#35299;&#20915;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#22312;&#22403;&#22334;&#26742;&#25361;&#36873;&#20219;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.02495</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#20013;&#30340;&#20266;&#26631;&#31614;&#21644;&#24773;&#22659;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;SSL-ConvSAC&#26041;&#27861;&#65292;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#24773;&#22659;&#35838;&#31243;&#30340;&#26041;&#27861;&#35299;&#20915;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#22312;&#22403;&#22334;&#26742;&#25361;&#36873;&#20219;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#25235;&#21462;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#24573;&#35270;&#20102;&#23454;&#26102;&#36866;&#24212;&#26032;&#39062;&#25361;&#36873;&#22330;&#26223;&#20013;&#21457;&#29983;&#30340;&#21160;&#24577;&#25235;&#21462;&#23398;&#20064;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#26041;&#27861;SSL-ConvSAC&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#24102;&#26377;&#22870;&#21169;&#21453;&#39304;&#30340;&#20687;&#32032;&#35270;&#20026;&#26631;&#35760;&#25968;&#25454;&#65292;&#20854;&#20182;&#20687;&#32032;&#35270;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#22686;&#24378;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#35838;&#31243;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#20272;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#20855;&#26377;&#21560;&#30424;&#22841;&#20855;&#30340;&#29289;&#29702;7&#33258;&#30001;&#24230;Franka Emika&#26426;&#26800;&#33218;&#25913;&#36827;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02495v1 Announce Type: cross  Abstract: The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method. We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction grippe
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#22120;&#26041;&#27861;&#22312;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#38754;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32534;&#30721;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#32534;&#30721;&#65292;&#25193;&#23637;&#20102;NAS&#39044;&#27979;&#22120;&#21040;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.02484</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#32534;&#30721;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Encodings for Prediction-based Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02484
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22120;&#26041;&#27861;&#22312;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#38754;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#32534;&#30721;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#32534;&#30721;&#65292;&#25193;&#23637;&#20102;NAS&#39044;&#27979;&#22120;&#21040;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22120;&#26041;&#27861;&#22823;&#22823;&#22686;&#24378;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20248;&#21270;&#30340;&#25928;&#26524;&#12290;&#36825;&#20123;&#39044;&#27979;&#22120;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;&#26412;&#25991;&#23545;&#19977;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#31070;&#32463;&#32534;&#30721;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65306;&#32467;&#26500;&#22411;&#12289;&#23398;&#20064;&#22411;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#32534;&#30721;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#32479;&#19968;&#32534;&#30721;&#8221;&#65292;&#23558;NAS&#39044;&#27979;&#22120;&#25193;&#23637;&#21040;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26469;&#33258;&#20110;&#22312;NASBench-101&#65288;NB101&#65289;&#12289;NB201&#12289;NB301&#12289;&#32593;&#32476;&#35774;&#35745;&#31354;&#38388;&#65288;NDS&#65289;&#21644;TransNASBench-101&#31561;NAS&#31354;&#38388;&#19978;&#36827;&#34892;&#30340;&#36229;&#36807;150&#19975;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23454;&#39564;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02484v1 Announce Type: cross  Abstract: Predictor-based methods have substantially enhanced Neural Architecture Search (NAS) optimization. The efficacy of these predictors is largely influenced by the method of encoding neural network architectures. While traditional encodings used an adjacency matrix describing the graph structure of a neural network, novel encodings embrace a variety of approaches from unsupervised pretraining of latent representations to vectors of zero-cost proxies. In this paper, we categorize and investigate neural encodings from three main types: structural, learned, and score-based. Furthermore, we extend these encodings and introduce \textit{unified encodings}, that extend NAS predictors to multiple search spaces. Our analysis draws from experiments conducted on over 1.5 million neural network architectures on NAS spaces such as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and TransNASBench-101. Building on our study, we present 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;BDD&#31232;&#30095;&#21270;&#22120;MORBDD&#65292;&#36890;&#36807;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#28040;&#38500;&#19981;&#22826;&#21487;&#33021;&#36129;&#29486;&#20110;&#24085;&#32047;&#25176;&#35299;&#30340;BDD&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02482</link><description>&lt;p&gt;
MORBDD&#65306;&#36890;&#36807;&#23398;&#20064;&#31232;&#30095;&#21270;&#23454;&#29616;&#30340;&#22810;&#30446;&#26631;&#32422;&#26463;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;
&lt;/p&gt;
&lt;p&gt;
MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;BDD&#31232;&#30095;&#21270;&#22120;MORBDD&#65292;&#36890;&#36807;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#28040;&#38500;&#19981;&#22826;&#21487;&#33021;&#36129;&#29486;&#20110;&#24085;&#32047;&#25176;&#35299;&#30340;BDD&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#20934;&#20915;&#31574;&#20013;&#65292;&#29992;&#25143;&#23547;&#27714;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#25903;&#37197;&#35299;&#38598;&#65292;&#21363;&#25152;&#35859;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#19968;&#31181;&#31934;&#30830;&#30340;&#22810;&#30446;&#26631;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26041;&#27861;&#24341;&#20837;&#21040;&#21551;&#21457;&#24335;&#39046;&#22495;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#65288;BDDs&#65289;&#65292;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#20195;&#34920;&#38382;&#39064;&#25152;&#26377;&#21487;&#34892;&#35299;&#30340;&#22270;&#24418;&#65292;&#28982;&#21518;&#36941;&#21382;&#35813;&#22270;&#20197;&#25552;&#21462;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#30001;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#21487;&#33021;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;BDD&#19978;&#30340;&#26522;&#20030;&#21487;&#33021;&#32791;&#26102;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23558;&#24050;&#34987;&#35777;&#26126;&#23545;&#21333;&#30446;&#26631;&#38382;&#39064;&#26377;&#25928;&#30340;&#21463;&#38480;BDDs&#35843;&#25972;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;ML&#30340;BDD&#31232;&#30095;&#21270;&#22120;MORBDD&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#20197;&#28040;&#38500;&#19981;&#22826;&#21487;&#33021;&#36129;&#29486;&#20110;&#24085;&#32047;&#25176;&#35299;&#30340;BDD&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02482v1 Announce Type: new  Abstract: In multicriteria decision-making, a user seeks a set of non-dominated solutions to a (constrained) multiobjective optimization problem, the so-called Pareto frontier. In this work, we seek to bring a state-of-the-art method for exact multiobjective integer linear programming into the heuristic realm. We focus on binary decision diagrams (BDDs) which first construct a graph that represents all feasible solutions to the problem and then traverse the graph to extract the Pareto frontier. Because the Pareto frontier may be exponentially large, enumerating it over the BDD can be time-consuming. We explore how restricted BDDs, which have already been shown to be effective as heuristics for single-objective problems, can be adapted to multiobjective optimization through the use of machine learning (ML). MORBDD, our ML-based BDD sparsifier, first trains a binary classifier to eliminate BDD nodes that are unlikely to contribute to Pareto solution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#28216;&#25103;&#35774;&#35745;&#20013;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#23545;&#28216;&#25103;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.02454</link><description>&lt;p&gt;
&#22696;&#36857;&#25928;&#24212;&#65306;&#20197;ChatGPT&#20026;&#20849;&#21019;&#28216;&#25103;&#35774;&#35745;&#24072;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#28216;&#25103;&#35774;&#35745;&#20013;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#23545;&#28216;&#25103;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#20316;&#20026;&#26377;&#25928;&#30340;&#39640;&#23618;&#27425;&#21019;&#24847;&#21512;&#20316;&#32773;&#21644;&#28216;&#25103;&#35774;&#35745;&#30340;&#8220;&#28789;&#24863;&#27849;&#28304;&#8221;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#20223;&#33402;&#26415;&#23478;&#20351;&#29992;&#30340;&#32451;&#20064;&#65292;&#21363;&#35266;&#30475;&#26080;&#23450;&#24418;&#30340;&#22696;&#36857;&#26001;&#28857;&#20197;&#33719;&#24471;&#21019;&#24847;&#28789;&#24863;&#65292;&#26469;&#26500;&#24314;&#36825;&#39033;&#30740;&#31350;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30456;&#23545;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#24847;&#22270;&#23454;&#26045;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#25913;&#21892;&#12289;&#38459;&#30861;&#25110;&#25552;&#20379;&#21478;&#19968;&#31181;&#28216;&#25103;&#36136;&#37327;&#12290;LLMs&#20316;&#20026;&#28216;&#25103;&#35774;&#35745;&#24072;&#30340;&#33021;&#21147;&#36890;&#36807;&#23558;&#20854;&#32622;&#20110;&#20915;&#31574;&#36807;&#31243;&#30340;&#21069;&#27839;&#26469;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#22312;3&#20010;&#19981;&#21516;&#30340;&#27969;&#27966;&#20013;&#35774;&#35745;&#20102;&#19977;&#27454;&#21407;&#22411;&#28216;&#25103;&#65306;&#65288;1&#65289;&#26497;&#31616;&#22522;&#30784;&#28216;&#25103;&#65292;&#65288;2&#65289;&#30001;&#20154;&#31867;&#28216;&#25103;&#35774;&#35745;&#24072;&#28155;&#21152;&#21151;&#33021;&#21644;&#28216;&#25103;&#24863;&#20803;&#32032;&#30340;&#28216;&#25103;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20174;ChatGPT&#25552;&#31034;&#30340;&#36755;&#20986;&#30452;&#25509;&#23454;&#29616;&#21151;&#33021;&#21644;&#24863;&#35273;&#20803;&#32032;&#30340;&#28216;&#25103;&#12290;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#30450;&#30446;&#35780;&#20272;&#28216;&#25103;&#30340;&#36136;&#37327;&#21644;&#23427;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02454v1 Announce Type: new  Abstract: This paper studies how large language models (LLMs) can act as effective, high-level creative collaborators and ``muses'' for game design. We model the design of this study after the exercises artists use by looking at amorphous ink splotches for creative inspiration. Our goal is to determine whether AI-assistance can improve, hinder, or provide an alternative quality to games when compared to the creative intents implemented by human designers. The capabilities of LLMs as game designers are stress tested by placing it at the forefront of the decision making process. Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from prompted outputs of the LLM, ChatGPT. A user study was conducted and participants were asked to blindly evaluate the quality and their
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31934;&#30830;&#20998;&#21106;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#30340;&#35299;&#21078;&#32422;&#26463;&#32420;&#32500;&#26463;&#36861;&#36394;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#20998;&#21106;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#32974;&#20799;&#22823;&#33041;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02444</link><description>&lt;p&gt;
&#32974;&#20799;&#22823;&#33041;&#35299;&#21078;&#32422;&#26463;&#19979;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Anatomically Constrained Tractography of the Fetal Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31934;&#30830;&#20998;&#21106;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#30340;&#35299;&#21078;&#32422;&#26463;&#32420;&#32500;&#26463;&#36861;&#36394;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#20998;&#21106;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#23545;&#32974;&#20799;&#22823;&#33041;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25958;&#36335;:2403.02444v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36807;&#25688;&#35201;&#65306;&#25193;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#30740;&#31350;&#23376;&#23467;&#20869;&#30340;&#32974;&#20799;&#22823;&#33041;&#12290; dMRI&#20351;&#24471;&#27969;&#32447;&#36861;&#36394;&#25104;&#20026;&#21487;&#33021;&#30340;&#37325;&#35201;&#35745;&#31639;&#65292;&#20854;&#20855;&#26377;&#29420;&#29305;&#30340;&#24212;&#29992;&#65292;&#22914;&#23545;&#33041;&#30333;&#36136;&#36827;&#34892;&#32420;&#32500;&#26463;&#29305;&#24322;&#24615;&#20998;&#26512;&#21644;&#32467;&#26500;&#36830;&#25509;&#24615;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32974;&#20799;dMRI&#25968;&#25454;&#36136;&#37327;&#36739;&#20302;&#19988;&#32420;&#32500;&#26463;&#36861;&#36394;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#39640;&#24230;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#23427;&#20204;&#29983;&#25104;&#35768;&#22810;&#34394;&#20551;&#30340;&#27969;&#32447;&#65292;&#21516;&#26102;&#26410;&#33021;&#37325;&#26500;&#26500;&#25104;&#20027;&#35201;&#30333;&#36136;&#32420;&#32500;&#26463;&#30340;&#27969;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#22312;dMRI&#31354;&#38388;&#20013;&#30452;&#25509;&#23545;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#36827;&#34892;&#20934;&#30830;&#20998;&#21106;&#30340;&#35299;&#21078;&#32422;&#26463;&#32420;&#32500;&#26463;&#36861;&#36394;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#33258;&#21160;&#35745;&#31639;&#20998;&#21106;&#12290;&#23545;&#29420;&#31435;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20998;&#21106;&#32974;&#20799;&#22823;&#33041;&#32452;&#32455;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;tra
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02444v1 Announce Type: cross  Abstract: Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to study the fetal brain in utero. An important computation enabled by dMRI is streamline tractography, which has unique applications such as tract-specific analysis of the brain white matter and structural connectivity assessment. However, due to the low fetal dMRI data quality and the challenging nature of tractography, existing methods tend to produce highly inaccurate results. They generate many false streamlines while failing to reconstruct streamlines that constitute the major white matter tracts. In this paper, we advocate for anatomically constrained tractography based on an accurate segmentation of the fetal brain tissue directly in the dMRI space. We develop a deep learning method to compute the segmentation automatically. Experiments on independent test data show that this method can accurately segment the fetal brain tissue and drastically improve tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26681;&#26412;&#19978;&#35299;&#20915;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#39044;&#27979;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2403.02439</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021; (XAI) &#20998;&#26512;&#39044;&#27979;&#24322;&#24120;&#26681;&#26412;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Root Causing Prediction Anomalies Using Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26681;&#26412;&#19978;&#35299;&#20915;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#39044;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#26681;&#22240;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#19981;&#26029;&#20174;&#29992;&#25143;&#21442;&#19982;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#21333;&#20010;&#29305;&#24449;&#25439;&#22351;&#21487;&#33021;&#23548;&#33268;&#32423;&#32852;&#29305;&#24449;&#12289;&#26631;&#31614;&#21644;&#27010;&#24565;&#28418;&#31227;&#12290;&#25105;&#20204;&#24050;&#25104;&#21151;&#23558;&#36825;&#19968;&#25216;&#26415;&#24212;&#29992;&#20110;&#25552;&#39640;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#24615;&#33021;&#19979;&#38477;&#34920;&#29616;&#20026;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02439v1 Announce Type: cross  Abstract: This paper presents a novel application of explainable AI (XAI) for root-causing performance degradation in machine learning models that learn continuously from user engagement data. In such systems a single feature corruption can cause cascading feature, label and concept drifts. We have successfully applied this technique to improve the reliability of models used in personalized advertising. Performance degradation in such systems manifest as prediction anomalies in the models. These models are typically trained continuously using features that are produced by hundreds of real time data processing pipelines or derived from other upstream models. A failure in any of these pipelines or an instability in any of the upstream models can cause feature corruption, causing the model's predicted output to deviate from the actual output and the training data to become corrupted. The causal relationship between the features and the predicted ou
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.02437</link><description>&lt;p&gt;
SoK: &#32852;&#37030;&#21453;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
SoK: Challenges and Opportunities in Federated Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02437
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20110;2017&#24180;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#19981;&#20449;&#20219;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#21508;&#26041;&#26126;&#30830;&#20849;&#20139;&#20854;&#25968;&#25454;&#12290;&#36825;&#20801;&#35768;&#22312;&#23562;&#37325;GDPR&#21644;CPRA&#31561;&#38544;&#31169;&#35268;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#38544;&#31169;&#35201;&#27714;&#21487;&#33021;&#35201;&#27714;&#27169;&#22411;&#25152;&#26377;&#32773;&#33021;&#22815;&#8220;&#36951;&#24536;&#8221;&#19968;&#20123;&#24050;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#24403;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#25191;&#27861;&#26426;&#26500;&#35201;&#27714;&#26102;&#12290;&#36825;&#20652;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#30340;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;&#35768;&#22810;&#20026;&#38598;&#20013;&#24335;&#29615;&#22659;&#24320;&#21457;&#30340;&#21453;&#23398;&#20064;&#25216;&#26415;&#24182;&#19981;&#23481;&#26131;&#24212;&#29992;&#65281;&#36825;&#26159;&#30001;&#20110;FL&#20013;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#20043;&#38388;&#30340;&#29420;&#29305;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#20114;&#21160;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#24322;&#26500;&#24615;&#21644;&#26377;&#38480;&#21487;&#35775;&#38382;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24037;&#20316;&#32858;&#28966;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;FL&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02437v1 Announce Type: cross  Abstract: Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK pape
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#20943;&#23569;&#26435;&#37325;&#25968;&#37327;&#24182;&#38450;&#27490;&#31934;&#24230;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#21644;&#20869;&#23384;&#32422;&#26463;&#30340;&#23454;&#26102;&#31995;&#32479;&#20013;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02429</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards efficient deep autoencoders for multivariate time series anomaly detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#20943;&#23569;&#26435;&#37325;&#25968;&#37327;&#24182;&#38450;&#27490;&#31934;&#24230;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#21644;&#20869;&#23384;&#32422;&#26463;&#30340;&#23454;&#26102;&#31995;&#32479;&#20013;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24037;&#19994;&#21644;&#30740;&#31350;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#20801;&#35768;&#38450;&#27490;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#32570;&#38519;&#21644;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#20854;&#23545;&#22797;&#26434;&#22810;&#21464;&#37327;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#21644;&#31283;&#20581;&#20998;&#26512;&#30340;&#29305;&#28857;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#33021;&#22815;&#21450;&#26102;&#25552;&#21462;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#23454;&#26102;&#38656;&#27714;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#27169;&#22411;&#32553;&#20943;&#23545;&#20110;&#22312;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#21644;&#20869;&#23384;&#32422;&#26463;&#30340;&#23454;&#26102;&#31995;&#32479;&#20013;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#28041;&#21450;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#26032;&#21387;&#32553;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20462;&#21098;&#20943;&#23569;&#26435;&#37325;&#25968;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#24555;&#36895;&#25628;&#32034;&#36807;&#31243;&#38450;&#27490;&#31934;&#24230;&#30340;&#28798;&#38590;&#24615;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02429v1 Announce Type: cross  Abstract: Multivariate time series anomaly detection is a crucial problem in many industrial and research applications. Timely detection of anomalies allows, for instance, to prevent defects in manufacturing processes and failures in cyberphysical systems. Deep learning methods are preferred among others for their accuracy and robustness for the analysis of complex multivariate data. However, a key aspect is being able to extract predictions in a timely manner, to accommodate real-time requirements in different applications. In the case of deep learning models, model reduction is extremely important to achieve optimal results in real-time systems with limited time and memory constraints. In this paper, we address this issue by proposing a novel compression method for deep autoencoders that involves three key factors. First, pruning reduces the number of weights, while preventing catastrophic drops in accuracy by means of a fast search process th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02419</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#26356;&#22810;LLM&#35843;&#29992;&#21527;&#65311;&#36208;&#21521;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#26159;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#29992;&#24182;&#27719;&#24635;&#23427;&#20204;&#30340;&#21709;&#24212;&#30340;&#22797;&#21512;&#31995;&#32479;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#24433;&#21709; -- &#20363;&#22914;&#65292;&#24403;&#35201;&#27714;LLM&#22810;&#27425;&#22238;&#31572;&#27599;&#20010;&#38382;&#39064;&#24182;&#21462;&#24471;&#20849;&#35782;&#26102; -- &#23545;&#20110;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLM&#35843;&#29992;&#27425;&#25968;&#22914;&#20309;&#24433;&#21709;&#19968;&#20010;&#23618;&#32423;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021; -- &#36825;&#26159;&#26368;&#31616;&#21333;&#30340;&#22797;&#21512;&#31995;&#32479;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32858;&#21512;LLM&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38750;&#21333;&#35843;&#24615;&#26159;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;OTClean&#26694;&#26550;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#32422;&#26463;&#19979;&#25968;&#25454;&#28165;&#27927;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21463;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#31639;&#27861;&#21551;&#21457;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02372</link><description>&lt;p&gt;
OTClean&#65306;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#36829;&#35268;&#25968;&#25454;&#28165;&#27927;
&lt;/p&gt;
&lt;p&gt;
OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02372
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;OTClean&#26694;&#26550;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#32422;&#26463;&#19979;&#25968;&#25454;&#28165;&#27927;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21463;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#31639;&#27861;&#21551;&#21457;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#32422;&#26463;&#23545;&#20110;&#20844;&#24179;&#21644;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OTClean&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#22312;CI&#32422;&#26463;&#19979;&#36827;&#34892;&#25968;&#25454;&#20462;&#22797;&#12290;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#30830;&#20445;&#23545;&#25968;&#25454;&#25928;&#29992;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#28041;&#21450;CI&#30340;&#25968;&#25454;&#20462;&#22797;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#27425;&#32422;&#26463;&#32447;&#24615;&#35268;&#21010;&#65288;QCLP&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#20132;&#26367;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65288;&#22914;Wasserstein&#36317;&#31163;&#65289;&#25152;&#28041;&#21450;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#19968;&#20010;&#21463;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#31639;&#27861;&#21551;&#21457;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02372v1 Announce Type: cross  Abstract: Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models. In this paper, we introduce \sys, a framework that harnesses optimal transport theory for data repair under CI constraints. Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility. We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution. However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance. To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which e
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02371</link><description>&lt;p&gt;
NeuroVoz&#65306;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#30340;&#36827;&#23637;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#12289;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#32570;&#20047;&#30340;&#38459;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#30340;&#35828;&#35805;&#32773;&#65292;&#21253;&#25324;55&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;53&#21517;&#34987;&#35786;&#26029;&#24739;&#26377;PD&#30340;&#20010;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20010;&#20307;&#37117;&#22312;&#33647;&#29289;&#27835;&#30103;&#19979;&#65292;&#24182;&#19988;&#22312;&#33647;&#29289;&#20248;&#21270;&#29366;&#24577;&#19979;&#36827;&#34892;&#35760;&#24405;&#12290; &#36825;&#19968;&#29420;&#29305;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#21253;&#25324;&#25345;&#32493;&#21457;&#38899;&#20116;&#20010;&#35199;&#29677;&#29273;&#20803;&#38899;&#12289;&#21457;&#38899;&#27979;&#35797;&#12289;16&#20010;&#21548;&#21518;&#37325;&#22797;&#30340;&#35805;&#35821;&#20197;&#21450;&#33258;&#30001;&#29420;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#36716;&#24405;&#21548;&#21518;&#37325;&#22797;&#20219;&#21153;&#24378;&#35843;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21033;&#29992;Whisper&#36827;&#34892;&#33258;&#21160;&#29420;&#30333;&#36716;&#24405;&#65292;&#20351;&#20854;&#25104;&#20026;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;adaptMLLM&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#31616;&#21270;&#20102;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02370</link><description>&lt;p&gt;
adaptMLLM&#65306;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#29992;&#38598;&#25104;LLM&#28216;&#20048;&#22330;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;adaptMLLM&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#31616;&#21270;&#20102;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Multilingual Language Models (MLLMs)&#21644;Large Language Models&#30340;&#20986;&#29616;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#39046;&#22495;&#24341;&#21457;&#20102;&#21019;&#26032;&#12290;&#23613;&#31649;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#20196;&#20154;&#20852;&#22859;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#36755;&#20986;&#30340;&#24433;&#21709;&#30456;&#23545;&#36739;&#23569;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#23578;&#26410;&#25512;&#20986;&#19968;&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#19987;&#38376;&#29992;&#20110;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#31649;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23436;&#25972;MT&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;adaptMLLM&#26469;&#35299;&#20915;&#36825;&#20123;&#19981;&#24179;&#34913;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#31616;&#21270;&#20102;&#29992;&#20110;MT&#30340;MLLM&#24494;&#35843;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;&#36825;&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#19987;&#38376;&#20026;&#20174;&#20107;MT&#30340;&#24320;&#21457;&#20154;&#21592;&#12289;&#32763;&#35793;&#20154;&#21592;&#21644;&#29992;&#25143;&#37327;&#36523;&#23450;&#21046;&#12290;&#30452;&#35266;&#30340;&#30028;&#38754;&#20801;&#35768;&#36731;&#26494;&#22320;&#33258;&#23450;&#20041;&#36229;&#21442;&#25968;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#19968;&#31995;&#21015;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#30340;&#25351;&#26631;&#65292;&#24182;&#20855;&#26377;&#37096;&#32626;&#27169;&#22411;&#20316;&#20026;&#32763;&#35793;&#26381;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02370v1 Announce Type: cross  Abstract: The advent of Multilingual Language Models (MLLMs) and Large Language Models has spawned innovation in many areas of natural language processing. Despite the exciting potential of this technology, its impact on developing high-quality Machine Translation (MT) outputs for low-resource languages remains relatively under-explored. Furthermore, an open-source application, dedicated to both fine-tuning MLLMs and managing the complete MT workflow for low-resources languages, remains unavailable. We aim to address these imbalances through the development of adaptMLLM, which streamlines all processes involved in the fine-tuning of MLLMs for MT. This open-source application is tailored for developers, translators, and users who are engaged in MT. An intuitive interface allows for easy customisation of hyperparameters, and the application offers a range of metrics for model evaluation and the capability to deploy models as a translation service 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#29305;&#24449;&#37325;&#35201;&#24615;&#26816;&#27979;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;4.0&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02368</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24037;&#19994;4.0&#24212;&#29992;&#20013;&#39044;&#27979;&#20248;&#21270;&#30340;&#26032;&#22411;&#28151;&#21512;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Hybrid Feature Importance and Feature Interaction Detection Framework for Predictive Optimization in Industry 4.0 Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#29305;&#24449;&#37325;&#35201;&#24615;&#26816;&#27979;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;4.0&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#20013;&#25552;&#20379;&#22522;&#20110;&#25968;&#25454;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#25152;&#36798;&#21040;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19981;&#36275;&#20197;&#20445;&#35777;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#23454;&#26045;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#24182;&#38750;&#25152;&#26377;&#29305;&#24449;&#37117;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#39044;&#27979;&#20998;&#26512;&#30452;&#25509;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#31934;&#24515;&#36873;&#25321;&#29305;&#24449;&#30340;&#32467;&#21512;&#26377;&#28508;&#21147;&#23545;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#31215;&#26497;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#26816;&#27979;&#22120;&#8212;&#8212;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65288;LIME&#65289;&#21644;&#29305;&#24449;&#20132;&#20114;&#26816;&#27979;&#22120;&#8212;&#8212;&#31070;&#32463;&#20132;&#20114;&#26816;&#27979;&#65288;NID&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#65292;&#24182;&#23545;&#20132;&#20114;&#20316;&#29992;&#36827;&#34892;&#32534;&#30721;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02368v1 Announce Type: cross  Abstract: Advanced machine learning algorithms are increasingly utilized to provide data-based prediction and decision-making support in Industry 4.0. However, the prediction accuracy achieved by the existing models is insufficient to warrant practical implementation in real-world applications. This is because not all features present in real-world datasets possess a direct relevance to the predictive analysis being conducted. Consequently, the careful incorporation of select features has the potential to yield a substantial positive impact on the outcome. To address the research gap, this paper proposes a novel hybrid framework that combines the feature importance detector - local interpretable model-agnostic explanations (LIME) and the feature interaction detector - neural interaction detection (NID), to improve prediction accuracy. By applying the proposed framework, unnecessary features can be eliminated, and interactions are encoded to gene
&lt;/p&gt;</description></item><item><title>adaptNMT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#29615;&#22659;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#27969;&#31243;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26032;&#25163;&#29992;&#25143;&#65292;&#24182;&#25552;&#20379;&#22270;&#24418;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#31561;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02367</link><description>&lt;p&gt;
adaptNMT&#65306;&#19968;&#31181;&#38754;&#21521;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24320;&#28304;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;&#24320;&#21457;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02367
&lt;/p&gt;
&lt;p&gt;
adaptNMT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#29615;&#22659;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#27969;&#31243;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26032;&#25163;&#29992;&#25143;&#65292;&#24182;&#25552;&#20379;&#22270;&#24418;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.02367v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;adaptNMT&#31616;&#21270;&#20102;RNN&#21644;Transformer&#31070;&#32463;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#28041;&#21450;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;&#20316;&#20026;&#19968;&#27454;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#26088;&#22312;&#38754;&#21521;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#29992;&#25143;&#12290;&#35813;&#24212;&#29992;&#26159;&#24314;&#31435;&#22312;&#24191;&#27867;&#37319;&#29992;&#30340;OpenNMT&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#30340;&#65292;&#23545;&#20110;&#26032;&#36827;&#20837;&#35813;&#39046;&#22495;&#30340;&#29992;&#25143;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#24320;&#21457;&#29615;&#22659;&#30340;&#35774;&#32622;&#20197;&#21450;&#21019;&#24314;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#20998;&#21106;&#34987;&#22823;&#22823;&#31616;&#21270;&#12290;&#24212;&#29992;&#31243;&#24207;&#20869;&#32622;&#22270;&#24418;&#23637;&#31034;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#36827;&#24230;&#65292;&#24182;&#20351;&#29992;SentencePiece&#21019;&#24314;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#12290;&#36890;&#36807;&#30452;&#35266;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#20415;&#25463;&#22320;&#23450;&#21046;&#36229;&#21442;&#25968;&#65292;&#23454;&#26045;&#20102;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;&#30001;adaptNMT&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20316;&#20026;&#32763;&#35793;&#26381;&#21153;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02367v1 Announce Type: cross  Abstract: adaptNMT streamlines all processes involved in the development and deployment of RNN and Transformer neural translation models. As an open-source application, it is designed for both technical and non-technical users who work in the field of machine translation. Built upon the widely-adopted OpenNMT ecosystem, the application is particularly useful for new entrants to the field since the setup of the development environment and creation of train, validation and test splits is greatly simplified. Graphing, embedded within the application, illustrates the progress of model training, and SentencePiece is used for creating subword segmentation models. Hyperparameter customization is facilitated through an intuitive user interface, and a single-click model development approach has been implemented. Models developed by adaptNMT can be evaluated using a range of metrics, and deployed as a translation service within the application. To support
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36229;&#21442;&#25968;&#35774;&#32622;&#23545;&#20302;&#36164;&#28304;&#33521;-&#29233;&#21464;&#21387;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#22312;16k BPE&#23376;&#35789;&#27169;&#22411;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;RNN&#27169;&#22411;&#25552;&#39640;&#20102;7.8&#20010;BLEU&#20998;&#25968;&#65292;&#24182;&#22312;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02366</link><description>&lt;p&gt;
&#20154;&#31867;&#35780;&#20272;&#33521;&#29233;&#21464;&#21387;&#22120;&#22522;&#20110;NMT&#30340;&#32763;&#35793;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Human Evaluation of English--Irish Transformer-Based NMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36229;&#21442;&#25968;&#35774;&#32622;&#23545;&#20302;&#36164;&#28304;&#33521;-&#29233;&#21464;&#21387;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#22312;16k BPE&#23376;&#35789;&#27169;&#22411;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;RNN&#27169;&#22411;&#25552;&#39640;&#20102;7.8&#20010;BLEU&#20998;&#25968;&#65292;&#24182;&#22312;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#22914;&#20309;&#24433;&#21709;&#20302;&#36164;&#28304;&#33521;&#35821;-&#29233;&#23572;&#20848;&#25991;&#23545;&#30340;&#21464;&#21387;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36136;&#37327;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#12290;&#20351;&#29992;Byte Pair Encoding&#65288;BPE&#65289;&#21644;unigram&#26041;&#27861;&#30340;SentencePiece&#27169;&#22411;&#21463;&#21040;&#20102;&#35780;&#20215;&#12290;&#27169;&#22411;&#26550;&#26500;&#30340;&#21464;&#21270;&#21253;&#25324;&#20462;&#25913;&#23618;&#25968;&#65292;&#35780;&#20272;&#27880;&#24847;&#21147;&#30340;&#26368;&#20339;&#22836;&#25968;&#20197;&#21450;&#27979;&#35797;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#22312;&#19968;&#20010;&#20248;&#21270;&#20102;&#30340;16k BPE&#23376;&#35789;&#27169;&#22411;&#19979;&#65292;Transformer&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#24471;&#26368;&#22909;&#12290;&#19982;&#22522;&#20934;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#27169;&#22411;&#30456;&#27604;&#65292;Transformer&#20248;&#21270;&#27169;&#22411;&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;7.8&#20010;&#28857;&#12290;&#19982;&#35895;&#27468;&#32763;&#35793;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32763;&#35793;&#24341;&#25806;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#23450;&#37327;&#32454;&#31890;&#24230;&#30340;&#25163;&#21160;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02366v1 Announce Type: cross  Abstract: In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English--Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimized model with a 16k BPE subword model. Compared with a baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#31614;&#20462;&#22797;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02363</link><description>&lt;p&gt;
&#35299;&#20915;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65306;&#32771;&#34385;&#26631;&#31614;&#31232;&#26377;&#24615;&#30340;&#20004;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution with Label Refurbishment Considering Label Rarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02363
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#31614;&#20462;&#22797;&#21644;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#34920;&#29616;&#20986;&#22024;&#26434;&#26631;&#31614;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#22914;&#38271;&#23614;&#20998;&#24067;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21306;&#20998;&#22024;&#26434;&#21644;&#24178;&#20928;&#26679;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20381;&#36182;&#20110;&#22522;&#20110;&#22024;&#26434;&#38271;&#23614;&#25968;&#25454;&#30340;&#39044;&#27979;&#20449;&#24687;&#20250;&#24341;&#20837;&#28508;&#22312;&#38169;&#35823;&#12290;&#20026;&#20102;&#20811;&#26381;&#20808;&#21069;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;&#36719;&#26631;&#31614;&#20462;&#22797;&#19982;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#22312;&#31283;&#20581;&#30340;&#36719;&#26631;&#31614;&#20462;&#22797;&#30340;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#33719;&#24471;&#26080;&#20559;&#29305;&#24449;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;BAlanced Noise-tolerant Cross-entropy (BANC) &#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#21021;&#27493;&#39044;&#27979;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#20462;&#22797;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20026;&#22810;&#19987;&#23478;&#38598;&#25104;&#23398;&#20064;&#33719;&#21462;&#36719;&#26631;&#31614;&#65292;&#20026;&#38271;&#23614;&#22024;&#26434;&#26631;&#31614;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02363v1 Announce Type: cross  Abstract: Real-world datasets commonly exhibit noisy labels and class imbalance, such as long-tailed distributions. While previous research addresses this issue by differentiating noisy and clean samples, reliance on information from predictions based on noisy long-tailed data introduces potential errors. To overcome the limitations of prior works, we introduce an effective two-stage approach by combining soft-label refurbishing with multi-expert ensemble learning. In the first stage of robust soft label refurbishing, we acquire unbiased features through contrastive learning, making preliminary predictions using a classifier trained with a carefully designed BAlanced Noise-tolerant Cross-entropy (BANC) loss. In the second stage, our label refurbishment method is applied to obtain soft labels for multi-expert ensemble learning, providing a principled solution to the long-tail noisy label problem. Experiments conducted across multiple benchmarks v
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#20113;&#36793;&#27169;&#22411;&#35299;&#32806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#23450;&#21046;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#25429;&#33719;&#20849;&#20139;&#34920;&#31034;&#30340;&#20027;&#20307;&#21644;&#22788;&#29702;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#20010;&#24615;&#21270;&#22836;&#37096;&#65292;&#20197;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02360</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#23450;&#21046;&#26550;&#26500;&#30740;&#31350;: &#23545;&#27604;&#20113;&#36793;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Customized Architecture for Heterogeneous Federated Learning with Contrastive Cloud-Edge Model Decoupling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02360
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#20113;&#36793;&#27169;&#22411;&#35299;&#32806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#23450;&#21046;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#25429;&#33719;&#20849;&#20139;&#34920;&#31034;&#30340;&#20027;&#20307;&#21644;&#22788;&#29702;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#20010;&#24615;&#21270;&#22836;&#37096;&#65292;&#20197;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#32593;&#32476;&#36793;&#32536;&#23458;&#25143;&#31471;&#20043;&#38388;&#20840;&#23616;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20013;&#22830;&#25968;&#25454;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#26500;&#24615;&#20351;&#24471;&#27169;&#22411;&#20542;&#21521;&#20110;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#36825;&#20123;&#26497;&#23567;&#20540;&#21487;&#33021;&#36828;&#31163;&#20840;&#23616;&#26368;&#20248;&#12290;&#36825;&#31181;&#24322;&#26500;&#24615;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#19988;&#36890;&#20449;&#24320;&#38144;&#24040;&#22823;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCMD&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#36866;&#24212;&#20113;&#36793;&#25903;&#25345;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#35299;&#32806;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#29992;&#20110;&#33719;&#21462;&#20113;&#31471;&#20849;&#20139;&#34920;&#31034;&#30340;&#20027;&#20307;&#21644;&#29992;&#20110;&#36801;&#31227;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#20010;&#24615;&#21270;&#22836;&#37096;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#36873;&#25321;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#23618;&#20316;&#20026;&#20010;&#24615;&#21270;&#22836;&#37096;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23558;&#26368;&#21518;&#19968;&#23618;&#21018;&#24615;&#20998;&#37197;&#20026;&#20010;&#24615;&#21270;&#22836;&#37096;&#21487;&#33021;&#26080;&#27861;&#26368;&#22823;&#21270;&#25913;&#21892;&#24322;&#26500;&#24615;&#25968;&#25454;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#36873;&#25321;&#20010;&#24615;&#21270;&#22836;&#37096;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02360v1 Announce Type: cross  Abstract: Federated learning, as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel federated learning framework called FedCMD, a model decoupling tailored to the Cloud-edge supported federated learning that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized he
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#21033;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22235;&#20803;&#25968;&#34920;&#31034;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65292;&#30528;&#37325;&#25429;&#25417;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#24182;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.02355</link><description>&lt;p&gt;
&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#21033;&#29992;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#21033;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22235;&#20803;&#25968;&#34920;&#31034;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65292;&#30528;&#37325;&#25429;&#25417;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#24182;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;TKGC&#65289;&#26088;&#22312;&#22312;&#32473;&#23450;&#29305;&#23450;&#26102;&#38388;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#20013;&#22635;&#34917;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22235;&#20803;&#25968;&#34920;&#31034;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;TKGC&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#25429;&#25417;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#30340;&#26059;&#36716;&#21644;&#21608;&#26399;&#24615;&#26102;&#38388;&#24179;&#31227;&#26469;&#24314;&#27169;&#26102;&#38388;&#25935;&#24863;&#20851;&#31995;&#65292;&#26377;&#25928;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24314;&#27169;&#23545;&#31216;&#12289;&#38750;&#23545;&#31216;&#12289;&#36870;&#21521;&#12289;&#32452;&#21512;&#21644;&#28436;&#21464;&#30340;&#20851;&#31995;&#27169;&#24335;&#12290;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#32508;&#21512;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02355v1 Announce Type: cross  Abstract: Temporal knowledge graph completion (TKGC) aims to fill in missing facts within a given temporal knowledge graph at a specific time. Existing methods, operating in real or complex spaces, have demonstrated promising performance in this task. This paper advances beyond conventional approaches by introducing more expressive quaternion representations for TKGC within hypercomplex space. Unlike existing quaternion-based methods, our study focuses on capturing time-sensitive relations rather than time-aware entities. Specifically, we model time-sensitive relations through time-aware rotation and periodic time translation, effectively capturing complex temporal variability. Furthermore, we theoretically demonstrate our method's capability to model symmetric, asymmetric, inverse, compositional, and evolutionary relation patterns. Comprehensive experiments on public datasets validate that our proposed approach achieves state-of-the-art perform
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02354</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Field Neural Networks for Air Quality Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#26377;&#38480;&#35266;&#27979;&#31449;&#30340;&#21382;&#21490;&#25968;&#25454;&#25512;&#26029;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#12290;&#32771;&#34385;&#21040;&#35266;&#27979;&#31449;&#39640;&#26114;&#30340;&#32500;&#25252;&#25104;&#26412;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#33391;&#22909;&#30340;&#25512;&#26029;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#33410;&#32422;&#25104;&#26412;&#24182;&#32454;&#21270;&#25968;&#25454;&#31890;&#24230;&#12290;&#23613;&#31649;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#29616;&#23454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#31163;&#25955;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#38480;&#21046;&#20102;&#28508;&#21147;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#21363;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#65292;&#21450;&#20854;&#23545;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#37329;&#23383;&#22612;&#25512;&#26029;&#65292;&#23558;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#31354;&#35266;&#28857;&#65292;&#22330;&#21644;&#22270;&#65292;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20013;&#22269;&#22823;&#38470;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;ATP&#65292;&#36890;&#36807;&#20851;&#27880;&#39030;&#32423;&#20027;&#35201;&#38190;&#32780;&#38750;&#27599;&#20010;&#26631;&#35760;&#65292;&#20197;&#23454;&#29616;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#24555;&#36895;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#22312;&#38477;&#20302;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25429;&#25417;&#36755;&#20837;&#24207;&#21015;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.02352</link><description>&lt;p&gt;
ATP: &#36890;&#36807;&#23545;&#39030;&#32423;&#20027;&#35201;&#38190;&#36827;&#34892;&#20851;&#27880;&#23454;&#29616;&#24555;&#36895;LLM&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02352
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;ATP&#65292;&#36890;&#36807;&#20851;&#27880;&#39030;&#32423;&#20027;&#35201;&#38190;&#32780;&#38750;&#27599;&#20010;&#26631;&#35760;&#65292;&#20197;&#23454;&#29616;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#24555;&#36895;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#22312;&#38477;&#20302;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25429;&#25417;&#36755;&#20837;&#24207;&#21015;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#26426;&#21046; ATP&#65292;&#35813;&#26426;&#21046;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#39030;&#32423;&#20027;&#35201;&#38190;&#19978;&#65292;&#32780;&#19981;&#26159;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#35760;&#19978;&#12290;&#29305;&#21035;&#22320;&#65292;ATP&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#30340;&#39537;&#21160;&#65292;&#21363;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#21363;&#36755;&#20837;&#24207;&#21015;&#21487;&#20197;&#30001;&#23569;&#37327;&#20027;&#35201;&#22522;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;ATP&#23558;&#36755;&#20837;&#36716;&#25442;&#20026;&#27491;&#20132;&#31354;&#38388;&#65292;&#24182;&#20165;&#22312;&#39030;&#32423;&#20027;&#35201;&#22522;&#19978;&#35745;&#31639;&#27880;&#24847;&#21147;&#12290;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#35266;&#23519;&#21040;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;ATP&#33021;&#22815;&#20165;&#36890;&#36807;&#23569;&#37327;&#20027;&#35201;&#22522;&#25429;&#25417;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#27880;&#24847;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#38477;&#20302;&#21040;&#32447;&#24615;&#65292;&#32780;&#19981;&#20250;&#24341;&#36215;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;ATP&#36827;&#19968;&#27493;&#20026;&#20855;&#26377;&#20302;&#31209;&#36755;&#20837;&#30340;&#20854;&#20182;&#32447;&#24615;&#23618;&#20943;&#23569;&#20102;&#22797;&#26434;&#24230;&#65292;&#19982;&#20165;&#21333;&#32431;&#36827;&#34892;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02352v1 Announce Type: cross  Abstract: We propose a new attention mechanism with linear complexity, ATP, that fixates \textbf{A}ttention on \textbf{T}op \textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \emph{quadratic} to \emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26368;&#22823;&#34917;&#20607;&#38382;&#39064;&#21644;&#24809;&#32602;&#20248;&#21183;&#38382;&#39064;&#65292;&#22312;&#20449;&#24687;&#20256;&#25773;&#20013;&#35774;&#35745;&#20102;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#24182;&#26368;&#22823;&#21270;&#21487;&#20449;&#20449;&#24687;&#20256;&#25773;&#65292;&#20197;&#37325;&#26032;&#35780;&#20272;&#26032;&#38395;&#25552;&#20379;&#32773;&#30340;&#28608;&#21169;&#25514;&#26045;&#23545;&#20449;&#24687;&#24066;&#22330;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02342</link><description>&lt;p&gt;
&#32416;&#32544;: &#22312;&#20107;&#23454;&#26680;&#23454;&#20013;&#24179;&#34913;&#24809;&#32602;&#21644;&#34917;&#20607;&#30340;&#26368;&#22823;&#34917;&#20607;&#38382;&#39064;&#37325;&#22797;&#22256;&#22659;&#21338;&#24328;&#20998;&#26512;&#65292;&#34394;&#20551;&#26032;&#38395;&#19982;&#24369;&#27779;&#21202;&#26031;&#23450;&#24459;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Entanglement: Balancing Punishment and Compensation, Repeated Dilemma Game-Theoretic Analysis of Maximum Compensation Problem for Bypass and Least Cost Paths in Fact-Checking, Case of Fake News with Weak Wallace's Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02342
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26368;&#22823;&#34917;&#20607;&#38382;&#39064;&#21644;&#24809;&#32602;&#20248;&#21183;&#38382;&#39064;&#65292;&#22312;&#20449;&#24687;&#20256;&#25773;&#20013;&#35774;&#35745;&#20102;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#24182;&#26368;&#22823;&#21270;&#21487;&#20449;&#20449;&#24687;&#20256;&#25773;&#65292;&#20197;&#37325;&#26032;&#35780;&#20272;&#26032;&#38395;&#25552;&#20379;&#32773;&#30340;&#28608;&#21169;&#25514;&#26045;&#23545;&#20449;&#24687;&#24066;&#22330;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30740;&#31350;&#31508;&#35760;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19982;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#21644;&#26377;&#25928;&#20107;&#23454;&#26680;&#23454;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#37325;&#28857;&#25918;&#22312;&#26368;&#23567;&#25104;&#26412;&#36335;&#24452;&#38382;&#39064;&#19978;&#65292;&#35752;&#35770;&#22260;&#32469;&#30528;&#20351;&#29992;Metzler&#20989;&#25968;&#21644;Metzler&#30697;&#38453;&#26469;&#27169;&#25311;&#26032;&#38395;&#25552;&#20379;&#32773;&#20043;&#38388;&#20449;&#24687;&#20256;&#25773;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#34394;&#20551;&#26032;&#38395;&#30340;&#20256;&#25773;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#20449;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#36890;&#36807;&#24809;&#32602;&#20248;&#21183;&#38382;&#39064;&#21644;&#26368;&#22823;&#34917;&#20607;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#37325;&#26032;&#35780;&#20272;&#26032;&#38395;&#25552;&#20379;&#32773;&#28608;&#21169;&#25514;&#26045;&#30340;&#36335;&#24452;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#23545;&#20449;&#24687;&#24066;&#22330;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#32416;&#32544;&#27010;&#24565;&#24212;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#32972;&#26223;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26032;&#38395;&#25552;&#20379;&#32773;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02342v1 Announce Type: cross  Abstract: This research note is organized with respect to a novel approach to solving problems related to the spread of fake news and effective fact-checking. Focusing on the least-cost routing problem, the discussion is organized with respect to the use of Metzler functions and Metzler matrices to model the dynamics of information propagation among news providers. With this approach, we designed a strategy to minimize the spread of fake news, which is detrimental to informational health, while at the same time maximizing the spread of credible information. In particular, through the punitive dominance problem and the maximum compensation problem, we developed and examined a path to reassess the incentives of news providers to act and to analyze their impact on the equilibrium of the information market. By applying the concept of entanglement to the context of information propagation, we shed light on the complexity of interactions among news pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02241</link><description>&lt;p&gt;
&#31070;&#32463;&#32418;&#31227;&#65306;&#38543;&#26426;&#32593;&#32476;&#24182;&#38750;&#38543;&#26426;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Redshift: Random Networks are not Random Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#19981;&#23436;&#25972;&#12290;&#30446;&#21069;&#30340;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#38544;&#21547;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#35299;&#37322;&#26799;&#24230;&#33258;&#30001;&#26041;&#27861;&#20013;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20063;&#26080;&#27861;&#35299;&#37322;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#26410;&#32463;&#35757;&#32451;&#32593;&#32476;&#30340;&#31616;&#21333;&#20559;&#35265;&#12290;&#26412;&#25991;&#23547;&#25214;NNs&#20013;&#30340;&#20854;&#20182;&#27867;&#21270;&#28304;&#12290;&#20026;&#20102;&#29420;&#31435;&#20110;GD&#29702;&#35299;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30740;&#31350;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;MLPs&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65306;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#20250;&#20135;&#29983;&#19968;&#20010;&#38750;&#24120;&#20559;&#21521;&#20110;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#20998;&#24067;&#12290;&#20294;&#19982;&#24120;&#35268;&#26234;&#24935;&#19981;&#21516;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#12290;&#36825;&#19968;&#29305;&#24615;&#21462;&#20915;&#20110;&#32452;&#20214;&#65292;&#22914;ReLU&#12289;&#27531;&#24046;&#36830;&#25509;&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;&#21487;&#21033;&#29992;&#26367;&#20195;&#20307;&#31995;&#32467;&#26500;&#26500;&#24314;&#20559;&#21521;&#20110;&#20219;&#20309;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20559;&#35265;&#12290;Transformers&#20063;&#20855;&#26377;&#36825;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#23454;&#29616;&#20197;&#31243;&#24207;&#23450;&#20041;&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#30340;&#39640;&#32423;&#26694;&#26550;&#65292;&#20026;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#22810;&#27493;&#30693;&#35782;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.02164</link><description>&lt;p&gt;
&#35748;&#30693;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999; - &#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#19979;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cognition is All You Need - The Next Layer of AI Above Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02164
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#23454;&#29616;&#20197;&#31243;&#24207;&#23450;&#20041;&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#30340;&#39640;&#32423;&#26694;&#26550;&#65292;&#20026;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#22810;&#27493;&#30693;&#35782;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#30740;&#31350;&#65292;&#27604;&#22914;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#30693;&#35782;&#24037;&#20316;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#19982;&#25512;&#29702;&#21644;&#22810;&#27493;&#38382;&#39064;&#35299;&#20915;&#30456;&#20851;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#21644;&#20043;&#22806;&#23454;&#29616;&#20197;&#31243;&#24207;&#23450;&#20041;&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#30340;&#39640;&#32423;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#30340;&#21452;&#23618;&#21151;&#33021;&#26550;&#26500;&#65292;&#20316;&#20026;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#22810;&#27493;&#30693;&#35782;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02164v1 Announce Type: new  Abstract: Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving. Specifically, while existing chatbots simulate shallow reasoning and understanding they are prone to errors as problem complexity increases. The failure of these systems to address complex knowledge work is due to the fact that they do not perform any actual cognition. In this position paper, we present Cognitive AI, a higher-level framework for implementing programmatically defined neuro-symbolic cognition above and outside of large language models. Specifically, we propose a dual-layer functional architecture for Cognitive AI that serves as a roadmap for AI systems that can perform complex multi-step knowledge work. We propose that Cognitive AI is a necessary precursor for the evolution of higher forms of AI, suc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.01528</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01528
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;BL&#65289;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#28304;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#20016;&#23500;&#22810;&#38754;&#25551;&#36848;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#35745;&#31639;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#34920;&#36798;&#30340;&#24494;&#22937;&#21465;&#36848;&#19982;&#36890;&#36807;&#21508;&#31181;&#20998;&#23376;&#24314;&#27169;&#25216;&#26415;&#25551;&#36848;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#32454;&#33410;&#34701;&#21512;&#65292;&#25171;&#24320;&#20102;&#20840;&#38754;&#34920;&#24449;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#30340;&#26032;&#36884;&#24452;&#12290;&#36890;&#36807;&#23558;&#22260;&#32469;&#29983;&#29289;&#20998;&#23376;&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#25968;&#25454;&#32435;&#20837;&#24314;&#27169;&#20013;&#65292;BL&#26088;&#22312;&#25429;&#25417;&#21253;&#21547;&#35821;&#35328;&#20256;&#36798;&#30340;&#31526;&#21495;&#29305;&#24615;&#20197;&#21450;&#25968;&#37327;&#21270;&#32467;&#26500;&#29305;&#24449;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01528v1 Announce Type: cross  Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this r
&lt;/p&gt;</description></item><item><title>LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01131</link><description>&lt;p&gt;
LLaMoCo&#65306;&#29992;&#20110;&#20248;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01131
&lt;/p&gt;
&lt;p&gt;
LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#26041;&#27861;&#21253;&#25324;&#20174;LLMs&#36845;&#20195;&#22320;&#23547;&#25214;&#19979;&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#30452;&#25509;&#25552;&#31034;LLMs&#20197;&#33719;&#21462;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#20302;&#12289;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#24230;&#39640;&#20197;&#21450;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMoCo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#28165;&#26224;&#25551;&#36848;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#26377;&#25928;&#20248;&#21270;&#20195;&#30721;&#30340;&#20840;&#38754;&#25351;&#20196;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#20043;&#21069;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28909;&#36523;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26399;&#38388;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;LLaMoCo&#31934;&#35843;&#30340;CodeGen&#65288;350M&#65289;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01053</link><description>&lt;p&gt;
&#36879;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#21457;&#29616;&#26032;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;&#20197;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#29305;&#24615;&#65292;&#23545;&#31185;&#23398;&#21457;&#29616;&#30340;&#22522;&#26412;&#23454;&#36341;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#25913;&#21464;&#12290;&#38543;&#30528;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#33258;&#21160;&#25506;&#32034;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#35265;&#35299;&#65292;&#21457;&#29616;&#26032;&#30340;&#34920;&#22411;&#31867;&#21035;&#21644;&#27010;&#24565;&#23558;&#20250;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#32047;&#31215;&#25968;&#25454;&#20013;&#23384;&#22312;&#33509;&#24178;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#26032;&#31867;&#21457;&#29616;&#30340;&#36827;&#23637;&#12290;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#20276;&#38543;&#30528;&#19981;&#21516;&#31867;&#21035;&#32452;&#20043;&#38388;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#26412;&#36136;&#19978;&#23548;&#33268;&#27169;&#31946;&#21644;&#20559;&#20506;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#23454;&#20363;&#23884;&#20837;&#30340;&#36817;&#20284;&#21518;&#39564;&#21442;&#25968;&#21270;&#20026;&#36793;&#38469; von Mises-Fisher &#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#23884;&#20837;&#26041;&#26696;&#30340;&#27169;&#31946;&#24615;&#19982;&#20559;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 Announce Type: cross  Abstract: Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the int
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20849;&#21516;&#21407;&#22240;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#65292;&#35299;&#20915;&#20102;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#25512;&#24191;&#20102;&#24726;&#35770;&#65292;&#24182;&#34920;&#26126;&#22312;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30340;&#20851;&#32852;&#26041;&#21521;&#19982;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;</title><link>https://arxiv.org/abs/2403.00957</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#22240;&#21407;&#21017;&#35299;&#20915;&#36763;&#26222;&#26862;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Resolution of Simpson's paradox via the common cause principle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00957
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20849;&#21516;&#21407;&#22240;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#65292;&#35299;&#20915;&#20102;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#25512;&#24191;&#20102;&#24726;&#35770;&#65292;&#24182;&#34920;&#26126;&#22312;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30340;&#20851;&#32852;&#26041;&#21521;&#19982;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36763;&#26222;&#26862;&#24726;&#35770;&#26159;&#24314;&#31435;&#20004;&#20010;&#20107;&#20214;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#27010;&#29575;&#20851;&#32852;&#26102;&#30340;&#38556;&#30861;&#65292;&#32473;&#23450;&#31532;&#19977;&#20010;&#65288;&#28508;&#22312;&#30340;&#65289;&#38543;&#26426;&#21464;&#37327;$B$&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24773;&#26223;&#26159;&#38543;&#26426;&#21464;&#37327;$A$&#65288;&#27719;&#24635;&#20102;$a_1$&#12289;$a_2$&#21450;&#20854;&#34917;&#38598;&#65289;&#21644;$B$&#26377;&#19968;&#20010;&#21487;&#33021;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;$C$&#12290;&#25110;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#20551;&#35774;$C$&#23558;$A$&#20174;$B$&#20013;&#31579;&#36873;&#20986;&#21435;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#27491;&#30830;&#30340;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#20851;&#32852;&#24212;&#35813;&#36890;&#36807;&#23545;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#26469;&#23450;&#20041;&#12290;&#36825;&#19968;&#35774;&#32622;&#23558;&#21407;&#22987;&#36763;&#26222;&#26862;&#24726;&#35770;&#25512;&#24191;&#20102;&#12290;&#29616;&#22312;&#23427;&#30340;&#20004;&#20010;&#30456;&#20114;&#30683;&#30462;&#30340;&#36873;&#39033;&#31616;&#21333;&#22320;&#25351;&#30340;&#26159;&#20004;&#20010;&#29305;&#23450;&#19988;&#19981;&#21516;&#30340;&#21407;&#22240;$C$&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;$B$&#21644;$C$&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;$A$&#26159;&#22235;&#36827;&#21046;&#30340;&#65288;&#23545;&#20110;&#26377;&#25928;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#26469;&#35828;&#26159;&#26368;&#23567;&#19988;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#65289;&#65292;&#22312;&#20219;&#20309;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#23558;&#24314;&#31435;&#19982;&#22312;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;&#30340;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#20851;&#32852;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00957v1 Announce Type: cross  Abstract: Simpson's paradox is an obstacle to establishing a probabilistic association between two events $a_1$ and $a_2$, given the third (lurking) random variable $B$. We focus on scenarios when the random variables $A$ (which combines $a_1$, $a_2$, and their complements) and $B$ have a common cause $C$ that need not be observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For such cases, the correct association between $a_1$ and $a_2$ is to be defined via conditioning over $C$. This set-up generalizes the original Simpson's paradox. Now its two contradicting options simply refer to two particular and different causes $C$. We show that if $B$ and $C$ are binary and $A$ is quaternary (the minimal and the most widespread situation for valid Simpson's paradox), the conditioning over any binary common cause $C$ establishes the same direction of the association between $a_1$ and $a_2$ as the conditioning over $B$ in the original
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#26410;&#30693;Android&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#26469;&#25913;&#36827;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00890</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25913;&#36827;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#26410;&#30693;Android&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#26469;&#25913;&#36827;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;GAN&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;Android&#24694;&#24847;&#36719;&#20214;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;Android&#24212;&#29992;&#30340;&#23384;&#20648;&#38656;&#27714;&#30456;&#24403;&#22823;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GAN&#26469;&#21512;&#25104;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21019;&#24314;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;GAN&#27169;&#22411;&#29983;&#25104;&#30001;&#36924;&#30495;&#30340;&#21512;&#25104;&#28784;&#24230;&#22270;&#20687;&#32452;&#25104;&#30340;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26088;&#22312;&#35782;&#21035;&#20197;&#21069;&#30475;&#19981;&#21040;&#30340;Android&#24694;&#24847;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#24403;CNN&#22312;&#30495;&#23454;&#22270;&#20687;&#19982;GAN&#29983;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#24615;&#33021;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00890v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have demonstrated their versatility across various applications, including data augmentation and malware detection. This research explores the effectiveness of utilizing GAN-generated data to train a model for the detection of Android malware. Given the considerable storage requirements of Android applications, the study proposes a method to synthetically represent data using GANs, thereby reducing storage demands. The proposed methodology involves creating image representations of features extracted from an existing dataset. A GAN model is then employed to generate a more extensive dataset consisting of realistic synthetic grayscale images. Subsequently, this synthetic dataset is utilized to train a Convolutional Neural Network (CNN) designed to identify previously unseen Android malware applications. The study includes a comparative analysis of the CNN's performance when trained on real images v
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;</title><link>https://arxiv.org/abs/2403.00884</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#36827;&#34892;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#30340;&#21463;&#25511;&#35789;&#27719;&#21015;&#26631;&#39064;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#26816;&#32034;&#31995;&#32479;&#20027;&#35201;&#22312;&#20803;&#25968;&#25454;&#20449;&#24687;&#32780;&#38750;&#25968;&#25454;&#20540;&#19978;&#24314;&#31435;&#32034;&#24341;&#12290;&#22240;&#27492;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#21644;&#39640;&#36136;&#37327;&#30340;&#20803;&#25968;&#25454;&#65292;&#36825;&#20123;&#36807;&#31243;&#34987;&#35748;&#20026;&#26159;&#32791;&#26102;&#19988;&#38590;&#20197;&#33258;&#21160;&#21270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65306;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22522;&#20110;&#21463;&#25511;&#35789;&#27719;&#30340;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#38388;&#23545;&#40784;&#20197;&#21450;&#20154;&#26426;&#23545;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#25968;&#25454;&#38598;&#25551;&#36848;&#65289;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;LLM&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00884v1 Announce Type: cross  Abstract: Traditional dataset retrieval systems index on metadata information rather than on the data values. Thus relying primarily on manual annotations and high-quality metadata, processes known to be labour-intensive and challenging to automate. We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to classify column headers based on domain-specific topics from a controlled vocabulary. We evaluate our approach by assessing the internal consistency of the LLMs, the inter-machine alignment, and the human-machine agreement for the topic classification task. Additionally, we investigate the impact of contextual information (i.e. dataset description) on the classification outcomes. Our results suggest that ChatGPT and GoogleGemini outperform GoogleBard for internal consistency as well as LLM-hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00801</link><description>&lt;p&gt;
&#33258;&#20027;&#26816;&#32034;&#65306;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Retrieval: Building an Information Retrieval System with One Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00801
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#25913;&#21464;&#20102;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#22312;&#20154;&#31867;&#33719;&#21462;&#20449;&#24687;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#29616;&#26377;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20855;&#26377;&#23396;&#31435;&#30340;&#26550;&#26500;&#21644;&#26377;&#38480;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#30452;&#25509;&#21521;&#20154;&#31867;&#25552;&#20379;&#20449;&#24687;&#36716;&#21464;&#20026;&#38388;&#25509;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38656;&#30340;&#33021;&#21147;&#21040;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#33258;&#20027;&#26816;&#32034;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32034;&#24341;&#26550;&#26500;&#23558;&#35201;&#26816;&#32034;&#30340;&#35821;&#26009;&#20869;&#21270;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#25972;&#20010;&#26816;&#32034;&#36807;&#31243;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26723;&#29983;&#25104;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31471;&#21040;&#31471;&#25191;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00801v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24322;&#24120;&#24615;&#21644;&#21487;&#36777;&#35782;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#24418;&#24335;&#21270;&#20307;&#31995;&#24182;&#25581;&#31034;&#20854;&#26412;&#20307;&#35770;&#25215;&#35834;&#65292;&#24212;&#29992;&#26694;&#26550;&#27604;&#36739;&#20102;&#22235;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20174;&#26412;&#20307;&#35770;&#35282;&#24230;&#21487;&#33021;&#21457;&#29983;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.00685</link><description>&lt;p&gt;
&#35748;&#35782;&#20320;&#30340;&#24322;&#24120;&#65306;&#36208;&#21521;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#24322;&#24120;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24322;&#24120;&#24615;&#21644;&#21487;&#36777;&#35782;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#24418;&#24335;&#21270;&#20307;&#31995;&#24182;&#25581;&#31034;&#20854;&#26412;&#20307;&#35770;&#25215;&#35834;&#65292;&#24212;&#29992;&#26694;&#26550;&#27604;&#36739;&#20102;&#22235;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20174;&#26412;&#20307;&#35770;&#35282;&#24230;&#21487;&#33021;&#21457;&#29983;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36777;&#39539;&#25512;&#29702;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65292;&#20854;&#20013;&#19968;&#20123;&#27010;&#25324;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#25928;&#65292;&#20063;&#23601;&#26159;&#36890;&#24120;&#24773;&#20917;&#19979;&#30340;&#19968;&#33324;&#24615;&#32467;&#35770;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#24418;&#24335;&#21270;&#20307;&#31995;&#26469;&#27169;&#25311;&#36825;&#31181;&#25512;&#29702;&#65292;&#36825;&#26159;&#26222;&#36890;&#24120;&#35782;&#32972;&#26223;&#19979;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#27169;&#22411;&#32773;&#26469;&#35828;&#65292;&#20174;&#26412;&#20307;&#35770;&#30340;&#35282;&#24230;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#39046;&#22495;&#30340;&#20307;&#31995;&#24182;&#19981;&#23481;&#26131;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24322;&#24120;&#24615;&#21644;&#21487;&#36777;&#35782;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#20415;&#27604;&#36739;&#24418;&#24335;&#21270;&#20307;&#31995;&#24182;&#25581;&#31034;&#20854;&#26412;&#20307;&#35770;&#25215;&#35834;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#35813;&#26694;&#26550;&#26469;&#27604;&#36739;&#22235;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20174;&#26412;&#20307;&#35770;&#35282;&#24230;&#21487;&#33021;&#21457;&#29983;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00685v1 Announce Type: new  Abstract: Defeasible reasoning is a kind of reasoning where some generalisations may not be valid in all circumstances, that is general conclusions may fail in some cases. Various formalisms have been developed to model this kind of reasoning, which is characteristic of common-sense contexts. However, it is not easy for a modeller to choose among these systems the one that better fits its domain from an ontological point of view. In this paper we first propose a framework based on the notions of exceptionality and defeasibility in order to be able to compare formalisms and reveal their ontological commitments. Then, we apply this framework to compare four systems, showing the differences that may occur from an ontological perspective.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Metamorpheus&#65292;&#19968;&#31181;&#24773;&#24863;&#25509;&#21475;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#35270;&#35273;&#21465;&#20107;&#26469;&#21442;&#19982;&#29992;&#25143;&#22312;&#26790;&#22659;&#20013;&#30340;&#24773;&#24863;&#32463;&#21382;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#21644;&#25991;&#26412;&#25551;&#32472;&#65292;&#20419;&#20351;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>https://arxiv.org/abs/2403.00632</link><description>&lt;p&gt;
&#24418;&#24577;&#21464;&#24322;&#65306;&#36890;&#36807;&#38544;&#21947;&#35270;&#35273;&#21465;&#20107;&#36827;&#34892;&#20114;&#21160;&#12289;&#24773;&#24863;&#21644;&#21019;&#36896;&#24615;&#26790;&#22659;&#21465;&#36848;
&lt;/p&gt;
&lt;p&gt;
Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Metamorpheus&#65292;&#19968;&#31181;&#24773;&#24863;&#25509;&#21475;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#35270;&#35273;&#21465;&#20107;&#26469;&#21442;&#19982;&#29992;&#25143;&#22312;&#26790;&#22659;&#20013;&#30340;&#24773;&#24863;&#32463;&#21382;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#21644;&#25991;&#26412;&#25551;&#32472;&#65292;&#20419;&#20351;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#24773;&#24863;&#22522;&#26412;&#19978;&#26159;&#30001;&#29983;&#27963;&#32463;&#39564;&#22609;&#36896;&#30340;&#65292;&#25105;&#20204;&#20174;&#20013;&#26500;&#24314;&#20010;&#24615;&#21270;&#24847;&#20041;&#12290;&#21442;&#19982;&#36825;&#31181;&#24847;&#20041;&#22609;&#36896;&#36807;&#31243;&#24050;&#32463;&#34987;&#20316;&#20026;&#21508;&#31181;&#24515;&#29702;&#27835;&#30103;&#20013;&#30340;&#19968;&#31181;&#24178;&#39044;&#26469;&#20419;&#36827;&#20581;&#24247;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22238;&#24518;&#21644;&#21465;&#36848;&#29983;&#27963;&#32463;&#39564;&#20173;&#28982;&#22312;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#40092;&#26377;&#25506;&#35752;&#12290;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#31561;&#25216;&#26415;&#26469;&#20419;&#36827;&#24847;&#20041;&#22609;&#36896;&#36807;&#31243;&#65292;&#26368;&#32456;&#25903;&#25345;&#24773;&#24863;&#27491;&#24565;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;Metamorpheus&#65292;&#19968;&#31181;&#24773;&#24863;&#25509;&#21475;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#35270;&#35273;&#21465;&#20107;&#26469;&#21442;&#19982;&#29992;&#25143;&#22312;&#26790;&#22659;&#20013;&#30340;&#24773;&#24863;&#32463;&#21382;&#12290;Metamorpheus&#26681;&#25454;&#26790;&#22659;&#30340;&#24773;&#24863;&#24359;&#32447;&#25490;&#21015;&#25925;&#20107;&#24773;&#33410;&#65292;&#24182;&#36890;&#36807;&#21019;&#36896;&#38544;&#21947;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#32472;&#26469;&#20419;&#20351;&#33258;&#25105;&#21453;&#24605;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#38544;&#21947;&#24314;&#35758;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#21644;&#25991;&#26412;&#25551;&#32472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00632v1 Announce Type: cross  Abstract: Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.00030</link><description>&lt;p&gt;
GraphPub: &#20855;&#26377;&#39640;&#21487;&#29992;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphPub: Generation of Differential Privacy Graph with High Availability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;GNN&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#19978;&#28216;&#25968;&#25454;&#25152;&#26377;&#32773;&#21457;&#24067;&#22270;&#25968;&#25454;&#26102;&#65292;&#24448;&#24448;&#20250;&#23384;&#22312;&#35768;&#22810;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#21253;&#21547;&#20687;&#20010;&#20154;&#30340;&#26379;&#21451;&#21015;&#34920;&#31561;&#25935;&#24863;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22270;&#25968;&#25454;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;DP&#24212;&#29992;&#22312;&#22270;&#19978;&#24448;&#24448;&#20250;&#24433;&#21709;GNN&#27169;&#22411;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#65292;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;GraphPub&#65292;&#21487;&#20197;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#25105;&#20204;&#25628;&#32034;&#19968;&#20123;&#23545;&#33410;&#28857;&#29305;&#24449;&#32858;&#21512;&#27809;&#26377;&#22826;&#22823;&#36127;&#38754;&#24433;&#21709;&#30340;&#34394;&#20551;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00030v1 Announce Type: cross  Abstract: In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.19170</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21319;&#32599;&#39532;&#23612;&#20122;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Legal Judgement Prediction in Romanian with Long Text Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#26032;&#25104;&#26524;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#27861;&#24459;NLP&#39046;&#22495;&#20063;&#38543;&#20043;&#21457;&#23637;&#36805;&#29467;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#12290;&#30001;&#20110;&#20854;&#19987;&#19994;&#35789;&#27719;&#12289;&#38271;&#25991;&#26723;&#31561;&#29305;&#28857;&#65292;&#27861;&#24459;NLP&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#21644;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#30340;&#26368;&#32456;&#35009;&#20915;&#30340;&#26041;&#27861;&#65292;&#21363;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22914;&#20309;&#25193;&#23637;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#12290;&#22312;&#26469;&#33258;&#20004;&#20010;&#26469;&#28304;&#12289;&#35268;&#27169;&#21644;&#25991;&#26723;&#38271;&#24230;&#26174;&#33879;&#19981;&#21516;&#26102;&#30340;4&#20010;&#32599;&#39532;&#23612;&#20122;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#19987;&#38376;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19170v1 Announce Type: cross  Abstract: In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;</title><link>https://arxiv.org/abs/2402.18372</link><description>&lt;p&gt;
FedUV: &#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#22343;&#21248;&#24615;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
FedUV: Uniformity and Variance for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20250;&#38543;&#30528;&#24322;&#26500;&#20998;&#24067;&#30340;&#25968;&#25454;&#32780;&#19979;&#38477;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26159;&#30001;&#20110;&#32593;&#32476;&#30340;&#26368;&#32456;&#23618;&#26368;&#23481;&#26131;&#20986;&#29616;&#23616;&#37096;&#20559;&#24046;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23558;&#26368;&#32456;&#23618;&#20923;&#32467;&#20026;&#27491;&#20132;&#20998;&#31867;&#22120;&#21487;&#20197;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26435;&#37325;&#24212;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#30740;&#31350;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36825;&#26159;&#21463;&#21040;&#20923;&#32467;&#26435;&#37325;&#23548;&#33268;&#22855;&#24322;&#20540;&#24658;&#23450;&#30340;&#35266;&#23519;&#21551;&#21457;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#35757;&#32451;&#26102;&#23384;&#22312;&#24046;&#24322;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20004;&#31181;&#23616;&#37096;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#25345;&#32493;&#27169;&#25311;IID&#35774;&#32622;&#65306;&#65288;1&#65289;&#20998;&#31867;&#22120;&#30340;&#32500;&#24230;&#27010;&#29575;&#20998;&#24067;&#26041;&#24046;&#21644;&#65288;2&#65289;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#36229;&#29699;&#22343;&#21248;&#24615;&#12290;&#36825;&#20123;&#27491;&#21017;&#21270;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#34920;&#29616;&#24471;&#22909;&#20687;&#22312;IID&#35774;&#32622;&#20013;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18372v1 Announce Type: cross  Abstract: Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550; BiVRec&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#32852;&#21512;&#35757;&#32451; ID &#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#65292;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17334</link><description>&lt;p&gt;
BiVRec: &#21452;&#21521;&#22522;&#20110;&#35270;&#22270;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
BiVRec: Bidirectional View-based Multimodal Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17334
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550; BiVRec&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#32852;&#21512;&#35757;&#32451; ID &#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#65292;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#20837;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#36817;&#26469;&#24341;&#36215;&#20102;&#30740;&#31350;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#20027;&#27969;&#33539;&#24335;&#26159;ID&#20027;&#23548;&#25512;&#33616;&#65292;&#21363;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22312;&#21487;&#36716;&#31227;&#24615;&#21644;&#20449;&#24687;&#20405;&#20837;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21478;&#19968;&#31181;&#33539;&#24335;&#20986;&#29616;&#20102;&#65292;&#21363;&#30452;&#25509;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#25512;&#33616;&#65292;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#24573;&#30053;&#20102;&#29992;&#25143;ID&#20449;&#24687;&#65292;&#23548;&#33268;&#20449;&#24687;&#21033;&#29992;&#29575;&#20302;&#21644;&#35757;&#32451;&#25104;&#26412;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;BiVRec&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;ID&#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#20013;&#30340;&#25512;&#33616;&#20219;&#21153;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17334v1 Announce Type: cross  Abstract: The integration of multimodal information into sequential recommender systems has attracted significant attention in recent research. In the initial stages of multimodal sequential recommendation models, the mainstream paradigm was ID-dominant recommendations, wherein multimodal information was fused as side information. However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein multimodal features were employed directly for recommendation, enabling recommendation across datasets. Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs. To this end, we propose an innovative framework, BivRec, that jointly trains the recommendation tasks in both ID and multimodal views, leveraging their synergistic relationship to enhance recommendation performance bidirectionally. To tackle the information heterogeneity issue, we fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#12289;&#35774;&#35745;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#26469;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#19988;&#20511;&#21161;&#39640;&#36136;&#37327;&#30721;&#20070;&#24110;&#21161;Transformer&#65292;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16868</link><description>&lt;p&gt;
&#30001;Transformer&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#36890;&#20449;&#30340;&#30721;&#20070;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#12289;&#35774;&#35745;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#26469;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#19988;&#20511;&#21161;&#39640;&#36136;&#37327;&#30721;&#20070;&#24110;&#21161;Transformer&#65292;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30721;&#20070;&#30340;&#29983;&#25104;&#24335;&#35821;&#20041;&#36890;&#20449;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#24403;&#30721;&#20070;&#22312;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#20043;&#38388;&#20849;&#20139;&#26102;&#65292;&#21482;&#38656;&#35201;&#20256;&#36755;&#32034;&#24341;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30721;&#21521;&#37327;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26410;&#24517;&#19982;&#23545;&#24212;&#30721;&#32034;&#24341;&#30340;&#36317;&#31163;&#30456;&#20851;&#65292;&#30721;&#20070;&#21551;&#29992;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#20449;&#36947;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#20854;&#20013;&#39318;&#20808;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#65292;&#26681;&#25454;&#30721;&#20070;&#24341;&#23548;&#20197;&#28040;&#38500;&#20449;&#36947;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30721;&#20070;&#23545;Transformer&#30340;&#36741;&#21161;&#65292;&#25509;&#25910;&#31471;&#29983;&#25104;&#30340;&#22270;&#20687;&#25928;&#26524;&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16868v1 Announce Type: cross  Abstract: Codebook-based generative semantic communication attracts increasing attention, since only indices are required to be transmitted when the codebook is shared between transmitter and receiver. However, due to the fact that the semantic relations among code vectors are not necessarily related to the distance of the corresponding code indices, the performance of the codebook-enabled semantic communication system is susceptible to the channel noise. Thus, how to improve the system robustness against the noise requires careful design. This paper proposes a robust codebook-assisted image semantic communication system, where semantic codec and codebook are first jointly constructed, and then vector-to-index transformer is designed guided by the codebook to eliminate the effects of channel noise, and achieve image generation. Thanks to the assistance of the high-quality codebook to the Transformer, the generated images at the receiver outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.14160</link><description>&lt;p&gt;
&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;&#65306;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#21152;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#21152;&#36895;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#33609;&#31295;&#20196;&#29260;&#24207;&#21015;&#65292;&#35813;&#24207;&#21015;&#36827;&#19968;&#27493;&#30001;&#30446;&#26631;LLM&#24182;&#34892;&#39564;&#35777;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#33609;&#31295;&#20196;&#29260;&#26641;&#25512;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#21333;&#24207;&#21015;&#25512;&#27979;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26641;&#30340;&#27599;&#20010;&#32423;&#21035;&#29420;&#31435;&#29983;&#25104;&#20196;&#29260;&#65292;&#27809;&#26377;&#21033;&#29992;&#25972;&#20010;&#26641;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#24050;&#32463;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#22312;&#22266;&#23450;&#30446;&#26631;&#35745;&#31639;&#36164;&#28304;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#23427;&#23545;&#19981;&#37325;&#22797;&#25277;&#26679;&#30340;&#33609;&#31295;&#20196;&#29260;&#36827;&#34892;&#26368;&#22823;&#21270;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#23454;&#29616;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14160v1 Announce Type: cross  Abstract: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the dive
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13852</link><description>&lt;p&gt;
&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural Control System for Continuous Glucose Monitoring and Maintenance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#33889;&#33796;&#31958;&#27700;&#24179;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#21033;&#29992;&#24494;&#20998;&#39044;&#27979;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21463;&#21040;&#22797;&#26434;&#31070;&#32463;&#31574;&#30053;&#21644;&#21487;&#21306;&#20998;&#24314;&#27169;&#30340;&#25351;&#23548;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#12290;&#36825;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#26368;&#22823;&#21270;&#25928;&#29575;&#65292;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#21644;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#32463;&#39564;&#21457;&#29616;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12721</link><description>&lt;p&gt;
PAC-FNO&#65306;&#24182;&#34892;&#32467;&#26500;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35782;&#21035;&#20302;&#36136;&#37327;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26159;&#22312;&#29305;&#23450;&#22270;&#20687;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#37096;&#32626;&#23427;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#25512;&#29702;&#20013;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#19982;&#35757;&#32451;&#38598;&#20013;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21644;/&#25110;&#21463;&#21040;&#33258;&#28982;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22825;&#27668;&#21464;&#21270;&#12289;&#22122;&#22768;&#31867;&#22411;&#21644;&#21387;&#32553;&#20266;&#24433;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#20026;&#19981;&#21516;&#20998;&#36776;&#29575;&#25110;&#36755;&#20837;&#21464;&#21270;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#19981;&#21487;&#25193;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#24182;&#34892;&#32467;&#26500;&#21644;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;PAC-FNO&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;PAC-FNO&#22312;&#39057;&#22495;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20869;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#30340;&#20462;&#25913;&#35757;&#32451;PAC-FNO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12721v1 Announce Type: cross  Abstract: A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the orig
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.11000</link><description>&lt;p&gt;
ASGEA&#65306;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26088;&#22312;&#35782;&#21035;&#20195;&#34920;&#30456;&#21516;&#29616;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#19981;&#21516;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#12290;&#26368;&#36817;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#26041;&#27861;&#22312;EA&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#38754;&#20020;&#30528;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23884;&#20837;&#36317;&#31163;&#65292;&#24182;&#24573;&#35270;&#20102;&#19968;&#23545;&#23545;&#40784;&#23454;&#20307;&#32972;&#21518;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Align-Subgraph&#23454;&#20307;&#23545;&#40784;&#65288;ASGEA&#65289;&#26694;&#26550;&#26469;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;ASGEA&#20351;&#29992;&#38170;&#38142;&#25509;&#20316;&#20026;&#26725;&#26753;&#26469;&#26500;&#24314;Align-Subgraphs&#65292;&#24182;&#27839;&#30528;&#36328;&#30693;&#35782;&#22270;&#30340;&#36335;&#24452;&#20256;&#25773;&#65292;&#36825;&#20351;&#20854;&#21306;&#21035;&#20110;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#30693;&#35782;&#22270;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33410;&#28857;&#32423;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#38170;&#28857;&#26469;&#22686;&#24378;Align-Subgraph&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09099</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#25506;&#32034;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#24448;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#65292;&#20851;&#20110;&#20986;&#29616;&#29616;&#35937;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21151;&#33021;&#33021;&#21147;&#22914;&#20309;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#32780;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36229;&#36234;&#20102;&#36825;&#19968;&#20256;&#32479;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#26356;&#21152;&#20851;&#27880;&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#21152;&#28145;&#25105;&#20204;&#23545;LLMs&#20869;&#37096;&#20986;&#29616;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#24341;&#20837;&#8220;&#33258;&#32452;&#32455;&#8221;&#21644;&#8220;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#21160;&#24577;&#28436;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#8220;&#20986;&#29616;&#29616;&#35937;&#8221;&#65292;&#36825;&#31181;&#29616;&#35937;&#21453;&#26144;&#20102;&#33258;&#28982;&#31995;&#32479;&#20013;&#31616;&#21333;&#30340;&#24494;&#35266;&#30456;&#20114;&#20316;&#29992;&#22914;&#20309;&#23548;&#33268;&#22797;&#26434;&#30340;&#23439;&#35266;&#34892;&#20026;&#12290;&#20026;&#20102;&#23450;&#37327;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#19981;&#26029;&#28436;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#65288;NeuroMFA&#65289;&#12290;&#21033;&#29992;NeuroMFA&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09099v1 Announce Type: new Abstract: Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of "self-organization" and "multifractal analysis," we explore how neuron interactions dynamically evolve during training, leading to "emergence," mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.04412</link><description>&lt;p&gt;
VampPrior&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The VampPrior Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;DLVMs&#65289;&#30340;&#32858;&#31867;&#20808;&#39564;&#38656;&#35201;&#39044;&#20808;&#23450;&#20041;&#32858;&#31867;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36739;&#24046;&#30340;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21516;&#26102;&#25191;&#34892;&#38598;&#25104;&#21644;&#32858;&#31867;&#30340;&#26041;&#24335;&#26497;&#22823;&#22320;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;scRNA-seq&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;VampPrior&#65288;Tomczak&#21644;Welling&#65292;2018&#65289;&#35843;&#25972;&#20026;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#24471;&#21040;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#65292;&#20132;&#26367;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#21644;&#32463;&#39564;&#36125;&#21494;&#26031;&#65292;&#20197;&#28165;&#26970;&#22320;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#23558;VMM&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;scRNA-seq&#38598;&#25104;&#26041;&#27861;scVI&#65288;Lopez&#31561;&#65292;2018&#65289;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak &amp; Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
&lt;/p&gt;</description></item><item><title>LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16044</link><description>&lt;p&gt;
LLMLight: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLMLight: Large Language Models as Traffic Signal Control Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16044
&lt;/p&gt;
&lt;p&gt;
LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#20248;&#21270;&#36947;&#36335;&#32593;&#32476;&#25928;&#29575;&#21644;&#20943;&#23569;&#25317;&#22581;&#12290;&#20256;&#32479;&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#20132;&#36890;&#24037;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#24448;&#24448;&#22312;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#20013;&#23384;&#22312;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLMLight&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;TSC&#20915;&#31574;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21521;LLM&#25552;&#20379;&#35814;&#32454;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#20917;&#35828;&#26126;&#20316;&#20026;&#25351;&#23548;&#65292;&#20511;&#21161;LLM&#30340;&#20808;&#36827;&#27867;&#21270;&#33021;&#21147;&#65292;LLMLight&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;LightGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;TSC&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#39592;&#24178;LLM&#12290;&#36890;&#36807;&#23398;&#20064;&#32454;&#24494;&#30340;&#20132;&#36890;&#27169;&#24335;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;LightGPT&#22312;&#32463;&#27982;&#25104;&#26412;&#26041;&#38754;&#25552;&#21319;&#20102;LLMLight&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;LLMLight&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#19978;&#21457;&#29616;&#20102;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2312.12450</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36827;&#34892;&#32534;&#36753;&#65311;&#35780;&#20272;&#20854;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20195;&#30721;&#32534;&#36753;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#19978;&#21457;&#29616;&#20102;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#21508;&#31181;&#20195;&#30721;&#21512;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#21512;&#25104;&#20195;&#30721;&#65292;&#20174;&#20195;&#30721;&#20013;&#21512;&#25104;&#27979;&#35797;&#65292;&#20197;&#21450;&#20174;&#20195;&#30721;&#20013;&#21512;&#25104;&#35299;&#37322;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#25351;&#20196;&#24335;&#20195;&#30721;&#32534;&#36753;&#30340;&#34892;&#20026;&#30740;&#31350;&#19981;&#36275;&#12290;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25353;&#29031;&#25552;&#20379;&#30340;&#25552;&#31034;&#26356;&#26032;&#19968;&#22359;&#20195;&#30721;&#12290;&#32534;&#36753;&#25351;&#20196;&#21487;&#33021;&#35201;&#27714;&#28155;&#21152;&#25110;&#21024;&#38500;&#21151;&#33021;&#65292;&#25551;&#36848;&#38169;&#35823;&#24182;&#35201;&#27714;&#20462;&#22797;&#65292;&#35201;&#27714;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#32773;&#20854;&#20182;&#24120;&#35265;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#22522;&#20934;&#65292;&#24182;&#29992;&#23427;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#21644;&#23553;&#38381;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#26159;GPT-3.5-Turbo&#20063;&#27604;&#26368;&#22909;&#30340;&#24320;&#25918;&#27169;&#22411;&#22312;&#32534;&#36753;&#20195;&#30721;&#26041;&#38754;&#22909;&#20102;8.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12450v4 Announce Type: replace-cross  Abstract: A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks.   We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing cod
&lt;/p&gt;</description></item><item><title>RDR&#26041;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#19977;&#20010;&#30446;&#26631;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#27169;&#22411;&#25805;&#32437;NLU&#22522;&#20934;&#27979;&#35797;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.09932</link><description>&lt;p&gt;
RDR&#65306;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09932
&lt;/p&gt;
&lt;p&gt;
RDR&#26041;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#19977;&#20010;&#30446;&#26631;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#27169;&#22411;&#25805;&#32437;NLU&#22522;&#20934;&#27979;&#35797;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#65292;&#32780;&#36825;&#20123;&#19978;&#19979;&#25991;&#24182;&#19981;&#20165;&#20165;&#23384;&#22312;&#20110;&#36755;&#20837;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24050;&#32463;&#26126;&#26174;&#22320;&#35777;&#26126;&#20102;NLU&#22522;&#20934;&#27979;&#35797;&#23545;&#31070;&#32463;&#27169;&#22411;&#30340;&#25805;&#32437;&#25935;&#24863;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#32534;&#30721;&#30340;&#22806;&#37096;&#30693;&#35782;&#20013;&#30340;&#32479;&#35745;&#29305;&#24449;&#20154;&#20026;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#65288;RDR&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#20013;&#24341;&#20837;&#19977;&#20010;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22238;&#39038;&#30446;&#26631;&#28041;&#21450;&#20351;&#29992;&#19968;&#20010;&#37322;&#20041;&#27169;&#22411;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;&#37322;&#20041;&#65292;&#20197;&#20415;&#23545;&#20854;&#36827;&#34892;&#24635;&#32467;&#21644;&#27010;&#25324;&#12290;&#20854;&#27425;&#65292;&#23457;&#24910;&#30446;&#26631;&#28041;&#21450;&#21033;&#29992;&#22270;&#23884;&#20837;&#27169;&#22411;&#23545;&#19982;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30456;&#20851;&#30340;&#22806;&#37096;&#22270;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#26368;&#21518;&#65292;&#22238;&#24212;&#30446;&#26631;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09932v2 Announce Type: replace-cross  Abstract: Natural language understanding (NLU) using neural network pipelines often requires additional context that is not solely present in the input data. Through Prior research, it has been evident that NLU benchmarks are susceptible to manipulation by neural models, wherein these models exploit statistical artifacts within the encoded external knowledge to artificially inflate performance metrics for downstream tasks. Our proposed approach, known as the Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by incorporating three distinct objectives within the neural network pipeline. Firstly, the Recap objective involves paraphrasing the input text using a paraphrasing model in order to summarize and encapsulate its essence. Secondly, the Deliberation objective entails encoding external graph information related to entities mentioned in the input text, utilizing a graph embedding model. Finally, the Respond objective e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#21270;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26641;&#29366;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#26159;&#23545;&#20256;&#32479;Gr&#246;bner&#22522;&#30784;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2311.14058</link><description>&lt;p&gt;
&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#26641;&#29366;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identification for Tree-shaped Structural Causal Models in Polynomial Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#21270;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26641;&#29366;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#26159;&#23545;&#20256;&#32479;Gr&#246;bner&#22522;&#30784;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#29992;&#20110;&#34920;&#31034;&#21644;&#20998;&#26512;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#34920;&#31034;&#20026;&#26377;&#21521;&#36793;&#65292;&#28151;&#28102;&#22240;&#32032;&#34920;&#31034;&#20026;&#21452;&#21521;&#36793;&#12290;&#20174;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20013;&#35782;&#21035;&#22240;&#26524;&#21442;&#25968;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#26410;&#35299;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#26377;&#21521;&#20998;&#37327;&#24418;&#25104;&#26641;&#29366;&#30340;SCM&#12290;Van der Zander&#31561;&#20154;&#65288;AISTATS'22&#65292;PLMR 151&#65292;&#31532;6770--6792&#39029;&#65292;2022&#24180;&#65289;&#32473;&#20986;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#35782;&#21035;&#38382;&#39064;&#30340;PSPACE&#31639;&#27861;&#65292;&#36825;&#26159;&#23545;&#19968;&#33324;Gr\"obner&#22522;&#30784;&#26041;&#27861;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#21518;&#32773;&#22312;&#32467;&#26500;&#21442;&#25968;&#25968;&#37327;&#30340;&#25351;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26641;&#29366;SCM&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#32467;&#26500;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#23450;&#20854;&#26159;&#21542;&#26159;&#19968;&#33324;&#21487;&#35782;&#21035;&#30340;&#65292;&#19968;&#33324;2-identifiabl
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14058v2 Announce Type: replace  Abstract: Linear structural causal models (SCMs) are used to express and analyse the relationships between random variables. Direct causal effects are represented as directed edges and confounding factors as bidirected edges. Identifying the causal parameters from correlations between the nodes is an open problem in artificial intelligence. In this paper, we study SCMs whose directed component forms a tree. Van der Zander et al. (AISTATS'22, PLMR 151, pp. 6770--6792, 2022) give a PSPACE-algorithm for the identification problem in this case, which is a significant improvement over the general Gr\"obner basis approach, which has doubly-exponential time complexity in the number of structural parameters. In this work, we present a randomized polynomial-time algorithm, which solves the identification problem for tree-shaped SCMs. For every structural parameter, our algorithms decides whether it is generically identifiable, generically 2-identifiabl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#32534;&#36753;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#27169;&#22411;&#31354;&#38388;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#23545;&#27604;&#65292;&#20026;&#26410;&#26469;&#26356;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.13720</link><description>&lt;p&gt;
&#21487;&#20197;&#36890;&#36807;LLMs&#20462;&#22797;&#25512;&#29702;&#27169;&#22411;&#30340;&#38382;&#39064;&#21527;&#65311;&#26397;&#30528;&#26356;&#21487;&#33021;&#30340;AI&#35268;&#21010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#32534;&#36753;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#27169;&#22411;&#31354;&#38388;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#23545;&#27604;&#65292;&#20026;&#26410;&#26469;&#26356;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#32534;&#36753;&#30340;&#24037;&#20316;&#12290;&#20026;&#27492;&#32852;&#30431;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#35268;&#21010;&#25991;&#29486;&#20013;&#30740;&#31350;&#30340;&#20004;&#31181;&#19981;&#21516;&#21619;&#36947;&#30340;&#27169;&#22411;&#31354;&#38388;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;LLM&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#28436;&#31034;&#20102;LLM&#30340;&#24615;&#33021;&#22914;&#20309;&#19982;&#32452;&#21512;&#25628;&#32034;&#65288;CS&#65289;&#24418;&#25104;&#23545;&#27604; -- &#20256;&#32479;&#19978;&#29992;&#20110;&#35299;&#20915;&#35268;&#21010;&#20013;&#27169;&#22411;&#31354;&#38388;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;LLM&#26082;&#25198;&#28436;&#29420;&#31435;&#30340;&#27169;&#22411;&#31354;&#38388;&#25512;&#29702;&#32773;&#30340;&#35282;&#33394;&#65292;&#20063;&#19982;CS&#26041;&#27861;&#19968;&#36215;&#20316;&#20026;&#20004;&#38454;&#27573;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#32479;&#35745;&#20449;&#21495;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26263;&#31034;LLMs&#22312;&#26410;&#26469;&#26356;&#28145;&#20837;&#22320;&#28041;&#36275;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#25512;&#29702;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13720v2 Announce Type: replace  Abstract: This is the first work to look at the application of large language models (LLMs) for the purpose of model space edits in automated planning tasks. To set the stage for this union, we explore two different flavors of model space problems that have been studied in the AI planning literature and explore the effect of an LLM on those tasks. We empirically demonstrate how the performance of an LLM contrasts with combinatorial search (CS) -- an approach that has been traditionally used to solve model space tasks in planning, both with the LLM in the role of a standalone model space reasoner as well as in the role of a statistical signal in concert with the CS approach as part of a two-stage process. Our experiments show promising results suggesting further forays of LLMs into the exciting world of model space reasoning for planning tasks in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#25193;&#23637;&#23545;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#23548;&#33268;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#25552;&#20379;&#20102;&#21508;&#31181;&#21487;&#33021;&#22312;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#20986;&#29616;&#30340;&#27844;&#28431;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2311.04179</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Leakage in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25193;&#23637;&#23545;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#23548;&#33268;&#20449;&#24687;&#27844;&#28431;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#25552;&#20379;&#20102;&#21508;&#31181;&#21487;&#33021;&#22312;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#20013;&#20986;&#29616;&#30340;&#27844;&#28431;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#39044;&#27979;&#24314;&#27169;&#24037;&#20855;&#65292;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#28304;&#33258;&#20110;&#22312;&#29289;&#29702;&#23398;&#12289;&#24066;&#22330;&#33829;&#38144;&#12289;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#26679;&#26412;&#32423;&#21035;&#30340;&#39044;&#27979;&#30340;&#25215;&#35834;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#26410;&#32463;&#36866;&#24403;&#23454;&#26045;&#21644;&#35780;&#20272;&#65292;ML&#31649;&#36947;&#21487;&#33021;&#21253;&#21547;&#27844;&#28431;&#65292;&#36890;&#24120;&#23548;&#33268;&#36807;&#24230;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#24182;&#19988;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#12290;&#36825;&#21487;&#33021;&#23545;&#36130;&#21153;&#21644;&#31038;&#20250;&#20135;&#29983;&#20005;&#37325;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;ML&#31649;&#36947;&#26102;&#25193;&#23637;&#19982;&#23548;&#33268;&#27844;&#28431;&#30456;&#20851;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#35828;&#26126;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;ML&#31649;&#36947;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#27844;&#28431;&#30340;&#32508;&#21512;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04179v2 Announce Type: replace-cross  Abstract: Machine learning (ML) provides powerful tools for predictive modeling. ML's popularity stems from the promise of sample-level prediction with applications across a variety of fields from physics and marketing to healthcare. However, if not properly implemented and evaluated, ML pipelines may contain leakage typically resulting in overoptimistic performance estimates and failure to generalize to new data. This can have severe negative financial and societal implications. Our aim is to expand understanding associated with causes leading to leakage when designing, implementing, and evaluating ML pipelines. Illustrated by concrete examples, we provide a comprehensive overview and discussion of various types of leakage that may arise in ML pipelines.
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.18794</link><description>&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#38477;&#20302;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18794
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20316;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#34394;&#26500;&#24773;&#20917;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24187;&#35273;&#27700;&#24179;&#19982;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65306;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21709;&#24212;&#20013;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#25552;&#39640;&#19982;&#34394;&#26500;&#27700;&#24179;&#30340;&#38477;&#20302;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#65288;CRR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#30721;&#26102;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#23545;&#21709;&#24212;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#26368;&#39640;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#31572;&#26696;&#12290;&#19982;&#25105;&#20204;&#20851;&#20110;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#30340;&#23450;&#20041;&#30456;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;CRR&#26041;&#27861;&#65306;&#27010;&#29575;CRR&#65288;P-CRR&#65289;&#21644;&#35821;&#20041;CRR&#65288;S-CRR&#65289;&#12290;P-CRR&#20351;&#29992;&#25972;&#20010;&#24207;&#21015;&#30340;&#31639;&#26415;&#24179;&#22343;&#23545;&#21508;&#20010;&#21333;&#29420;&#25277;&#26679;&#30340;&#27169;&#22411;&#21709;&#24212;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18794v2 Announce Type: replace-cross  Abstract: In this work, we propose sequence-level certainty as a common theme over hallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the correlation between the level of hallucination and two types of sequence-level certainty: probabilistic certainty and semantic certainty. Empirical results reveal that a higher level of both types of sequence-level certainty in model responses is correlated with a lower level of hallucination. We further propose Certainty-based Response Ranking (CRR), a decoding-time hallucination mitigation method that ranks response candidates based on their sequence-level certainty and outputs the answer with the highest certainty level. Aligning with our definitions of sequence-level certainty, we design 2 types of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually sampled model responses using the arithmetic mean log-probability of the entire sequen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.14540</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Spatial Understanding of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#30475;&#21040;&#25991;&#26412;&#65292;&#20294;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#34920;&#31034;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#22320;&#38754;&#27010;&#24565;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#34920;&#31034;&#23545;&#19968;&#31181;&#29305;&#21035;&#26174;&#33879;&#30340;&#22522;&#30784;&#30693;&#35782; -- &#31354;&#38388;&#20851;&#31995;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#65292;&#29305;&#21035;&#26159;GPT-3.5-turbo&#12289;GPT-4 &#21644; Llama2 &#31995;&#21015;&#27169;&#22411;&#65292;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#25581;&#31034;&#20102;LLM&#22312;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500; (&#21253;&#25324;&#27491;&#26041;&#24418;&#12289;&#20845;&#36793;&#24418;&#21644;&#19977;&#35282;&#24418;&#26684;&#12289;&#29615;&#21644;&#26641;) &#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22312;&#24191;&#27867;&#30340;&#38169;&#35823;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#38169;&#35823;&#21453;&#26144;&#20102;&#31354;&#38388;&#21644;&#38750;&#31354;&#38388;&#22240;&#32032;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#20284;&#20046;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#31354;&#38388;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24809;&#32602;&#20989;&#25968;&#22312;&#35757;&#32451;&#26102;&#24809;&#32602;&#36829;&#21453;&#29615;&#22659;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#33021;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#33021;&#32791;&#26368;&#23567;&#21270;&#12290;</title><link>https://arxiv.org/abs/2308.10664</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#33021;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10664
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24809;&#32602;&#20989;&#25968;&#22312;&#35757;&#32451;&#26102;&#24809;&#32602;&#36829;&#21453;&#29615;&#22659;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#20013;&#33021;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#33021;&#32791;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36171;&#33021;&#30340;&#26080;&#32447;&#32593;&#32476;&#26032;&#26102;&#20195;&#36808;&#36827;&#65292;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#23545;AI&#30340;&#29615;&#22659;&#24433;&#21709;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#24335;AI&#25216;&#26415;&#24050;&#32463;&#20986;&#29616;&#12290;&#23613;&#31649;&#30446;&#21069;&#22312;FL&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#21162;&#21147;&#65292;&#20294;&#20854;&#29615;&#22659;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;FL&#36807;&#31243;&#30340;&#24635;&#33021;&#32791;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32534;&#25490;&#21442;&#19982;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#24635;&#33021;&#37327;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#30340;&#19968;&#23450;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Soft Actor Critic Deep Reinforcement Learning (DRL)&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#24809;&#32602;&#20989;&#25968;&#65292;&#24809;&#32602;&#36829;&#21453;&#29615;&#22659;&#32422;&#26463;&#30340;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#23433;&#20840;&#30340;RL&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10664v3 Announce Type: replace-cross  Abstract: Progressing towards a new era of Artificial Intelligence (AI) - enabled wireless networks, concerns regarding the environmental impact of AI have been raised both in industry and academia. Federated Learning (FL) has emerged as a key privacy preserving decentralized AI technique. Despite efforts currently being made in FL, its environmental impact is still an open problem. Targeting the minimization of the overall energy consumption of an FL process, we propose the orchestration of computational and communication resources of the involved devices to minimize the total energy required, while guaranteeing a certain performance of the model. To this end, we propose a Soft Actor Critic Deep Reinforcement Learning (DRL) solution, where a penalty function is introduced during training, penalizing the strategies that violate the constraints of the environment, and contributing towards a safe RL process. A device level synchronization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2308.07061</link><description>&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65306;&#35299;&#20915;&#26041;&#26696;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning: Solutions and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#35760;&#20303;&#25935;&#24863;&#12289;&#26410;&#32463;&#25480;&#26435;&#25110;&#24694;&#24847;&#25968;&#25454;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#23433;&#20840;&#28431;&#27934;&#21644;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26426;&#22120;&#36951;&#24536;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#28040;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20998;&#20026;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#24433;&#21709;&#30340;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#22238;&#39038;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#24182;&#23558;&#20854;&#24314;&#31435;&#20026;&#20540;&#24471;&#20449;&#36182;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07061v2 Announce Type: replace-cross  Abstract: Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy breaches, security vulnerabilities, and performance degradation. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of the solutions in machine unlearning. We categorize existing solutions into exact unlearning approaches that remove data influence thoroughly and approximate unlearning approaches that efficiently minimize data influence. By comprehensively reviewing solutions, we identify and discuss their strengths and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning models. This paper provides researchers with a roadmap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Masked Transformers&#24555;&#36895;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#21033;&#29992;Masked training&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#25552;&#21319;&#23545;&#20840;patches&#30340;&#38271;&#31243;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2306.09305</link><description>&lt;p&gt;
&#20351;&#29992;Masked Transformers&#24555;&#36895;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast Training of Diffusion Models with Masked Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.09305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Masked Transformers&#24555;&#36895;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#21033;&#29992;Masked training&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#25552;&#21319;&#23545;&#20840;patches&#30340;&#38271;&#31243;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Masked Transformers&#26469;&#35757;&#32451;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#12290;&#23613;&#31649;Masked Transformers&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#23398;&#20064;&#26041;&#38754;&#30340;&#24212;&#29992;&#21364;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;Masked training&#26174;&#33879;&#38477;&#20302;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#23631;&#34109;&#25193;&#25955;&#36755;&#20837;&#22270;&#20687;&#20013;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;50%&#65289;&#30340;patches&#12290;&#20026;&#20102;&#36827;&#34892;Masked training&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20165;&#22312;&#26410;&#23631;&#34109;patches&#19978;&#36816;&#34892;&#30340;transformer&#32534;&#30721;&#22120;&#21644;&#22312;&#20840;&#37096;patches&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;transformer&#35299;&#30721;&#22120;&#12290;&#20026;&#20102;&#25552;&#21319;&#23545;&#20840;patches&#30340;&#38271;&#31243;&#29702;&#35299;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;&#21363;&#37325;&#26500;&#23631;&#34109;patches&#65292;&#36825;&#26159;&#20026;&#20102;denoising score matching&#30446;&#26631;&#23398;&#20064;&#26410;&#23631;&#34109;patches&#30340;score&#12290;&#25105;&#20204;&#22312;ImageNet-256x256&#21644;ImageNet-512x...&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.09305v2 Announce Type: replace-cross  Abstract: We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#20154;&#20307;&#32908;&#32905;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#27493;&#24577;&#21608;&#26399;&#20013;&#19979;&#21322;&#36523;&#30340;&#31070;&#32463;&#21050;&#28608;&#65292;&#26816;&#27979;&#32908;&#32905;&#32676;&#30340;&#24037;&#20316;&#24773;&#20917;&#65292;&#24182;&#35774;&#35745;&#20102;Boots&#31639;&#27861;&#36827;&#34892;&#20154;&#20307;&#36816;&#21160;&#30340;&#36870;&#21160;&#21147;&#23398;&#65292;&#26368;&#32456;&#24320;&#21457;&#20986;&#29992;&#25143;&#21451;&#22909;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2212.12760</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#30340;&#20154;&#20307;&#32908;&#32905;&#24314;&#27169;&#19982;&#20223;&#30495;&#29992;&#20110;&#20154;&#20307;&#27493;&#24577;&#20998;&#26512;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Agent-based Modeling and Simulation of Human Muscle For Development of Human Gait Analyzer Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.12760
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#20154;&#20307;&#32908;&#32905;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#27493;&#24577;&#21608;&#26399;&#20013;&#19979;&#21322;&#36523;&#30340;&#31070;&#32463;&#21050;&#28608;&#65292;&#26816;&#27979;&#32908;&#32905;&#32676;&#30340;&#24037;&#20316;&#24773;&#20917;&#65292;&#24182;&#35774;&#35745;&#20102;Boots&#31639;&#27861;&#36827;&#34892;&#20154;&#20307;&#36816;&#21160;&#30340;&#36870;&#21160;&#21147;&#23398;&#65292;&#26368;&#32456;&#24320;&#21457;&#20986;&#29992;&#25143;&#21451;&#22909;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20165;&#26377;&#23569;&#37096;&#20998;&#32908;&#32905;&#21463;&#21040;&#36816;&#21160;&#30142;&#30149;&#21644;&#32010;&#20081;&#30340;&#24433;&#21709;&#65292;&#20294;&#21307;&#30103;&#30103;&#27861;&#24182;&#26410;&#21306;&#20998;&#20581;&#24247;&#19982;&#19981;&#20581;&#24247;&#32908;&#32905;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35745;&#31639;&#27493;&#24577;&#21608;&#26399;&#20013;&#19979;&#21322;&#36523;&#30340;&#31070;&#32463;&#21050;&#28608;&#65292;&#24182;&#26816;&#26597;&#26159;&#21542;&#26377;&#20219;&#20309;&#32908;&#32905;&#32676;&#27809;&#26377;&#27491;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20154;&#20307;&#32908;&#32905;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#12290;&#35813;&#20195;&#29702;&#33021;&#22815;&#23558;&#31070;&#32463;&#21050;&#28608;&#36716;&#21270;&#20026;&#32908;&#32905;&#20135;&#29983;&#30340;&#21147;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#23427;&#21487;&#20197;&#22312;&#21253;&#25324;&#21307;&#23398;&#25945;&#32946;&#21644;&#30740;&#31350;&#20197;&#21450;&#20551;&#32930;&#24320;&#21457;&#22312;&#20869;&#30340;&#35768;&#22810;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#20154;&#20307;&#19979;&#21322;&#36523;&#30340;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#35774;&#35745;&#20102;Boots&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#32908;&#32905;&#32676;&#29983;&#25104;&#30340;&#21147;&#26469;&#36827;&#34892;&#20154;&#20307;&#36816;&#21160;&#30340;&#36870;&#21160;&#21147;&#23398;&#12290;&#21033;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#20154;&#20307;&#32908;&#32905;&#27169;&#22411;&#21644;Boots&#31639;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#35745;&#31639;&#31070;&#32463;&#21050;&#28608;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.12760v2 Announce Type: replace  Abstract: Despite the fact that only a small portion of muscles are affected in motion disease and disorders, medical therapies do not distinguish between healthy and unhealthy muscles. In this paper, a method is devised in order to calculate the neural stimuli of the lower body during gait cycle and check if any group of muscles are not acting properly. For this reason, an agent-based model of human muscle is proposed. The agent is able to convert neural stimuli to force generated by the muscle and vice versa. It can be used in many researches including medical education and research and prosthesis development. Then, Boots algorithm is designed based on a biomechanical model of human lower body to do a reverse dynamics of human motion by computing the forces generated by each muscle group. Using the agent-driven model of human muscle and boots algorithm, a user-friendly application is developed which can calculate the number of neural stimuli
&lt;/p&gt;</description></item><item><title>&#22312;&#21160;&#24577;&#22270;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#26426;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#21516;&#26102;&#20851;&#31995;&#21644;&#26102;&#28382;&#20132;&#20114;&#20851;&#31995;&#26469;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#29305;&#24449;&#29983;&#25104;&#36807;&#31243;</title><link>https://arxiv.org/abs/2211.17029</link><description>&lt;p&gt;
&#20174;&#21160;&#24577;&#22270;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Directed Acyclic Graph Structure Learning from Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.17029
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#22270;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#26426;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#21516;&#26102;&#20851;&#31995;&#21644;&#26102;&#28382;&#20132;&#20114;&#20851;&#31995;&#26469;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#29305;&#24449;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#29305;&#24449;&#65288;&#21464;&#37327;&#65289;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32467;&#26500;&#22312;&#25581;&#31034;&#28508;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#25552;&#20379;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#22240;&#26524;&#27934;&#35265;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#32467;&#26500;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#20294;&#21160;&#24577;&#22270;&#19978;&#30340;&#32467;&#26500;&#23398;&#20064;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#22240;&#27492;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26080;&#22788;&#19981;&#22312;&#30340;&#21160;&#24577;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#26426;&#21046;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#21160;&#24577;&#22270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#20272;&#35745;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#21516;&#26102;&#20851;&#31995;&#21644;&#26102;&#28382;&#20132;&#20114;&#20851;&#31995;&#12290;&#36825;&#20004;&#31181;&#20851;&#31995;&#24418;&#25104;&#19968;&#20010;DAG&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#31616;&#27905;&#25551;&#36848;&#29305;&#24449;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#26679;&#30340;DAG&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#24471;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.17029v2 Announce Type: replace-cross  Abstract: Estimating the structure of directed acyclic graphs (DAGs) of features (variables) plays a vital role in revealing the latent data generation process and providing causal insights in various applications. Although there have been many studies on structure learning with various types of data, the structure learning on the dynamic graph has not been explored yet, and thus we study the learning problem of node feature generation mechanism on such ubiquitous dynamic graph data. In a dynamic graph, we propose to simultaneously estimate contemporaneous relationships and time-lagged interaction relationships between the node features. These two kinds of relationships form a DAG, which could effectively characterize the feature generation process in a concise way. To learn such a DAG, we cast the learning problem as a continuous score-based optimization problem, which consists of a differentiable score function to measure the validity 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2211.13715</link><description>&lt;p&gt;
&#30456;&#20449;&#24744;&#30340; $\nabla$: &#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#31185;&#23398;&#20013;&#19968;&#39033;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#35266;&#27979;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#20197;&#21807;&#19968;&#30830;&#23450;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#34429;&#28982;&#36827;&#34892;&#24178;&#39044;&#65288;&#21363;&#23454;&#39564;&#65289;&#21487;&#20197;&#25913;&#21892;&#21487;&#35782;&#21035;&#24615;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#38590;&#20197;&#33719;&#24471;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#22240;&#26524;&#21457;&#29616;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20272;&#35745;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#24178;&#39044;&#30446;&#26631;&#26469;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;&#31616;&#31216;&#20026;GIT&#65292;&#23427;&#8216;&#30456;&#20449;&#8217;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#25552;&#20379;&#24178;&#39044;&#37319;&#38598;&#20989;&#25968;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;GIT&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2111.10657</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#25512;&#24191;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalizing Graph Neural Networks on Out-Of-Distribution Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.10657
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#27809;&#26377;&#32771;&#34385;&#35757;&#32451;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#65292;&#23548;&#33268;GNNs&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35774;&#32622;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19979;&#38477;&#12290;&#36825;&#31181;&#36864;&#21270;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;GNNs&#26159;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#24320;&#21457;&#30340;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#32479;&#35745;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#21363;&#20351;&#36825;&#26159;&#19968;&#31181;&#20266;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20266;&#30456;&#20851;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#23548;&#33268;GNNs&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#28040;&#38500;&#20266;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#23545;&#20110;&#31283;&#23450;&#30340;GNNs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#39318;&#20808;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#34920;&#31034;&#65292;&#28982;&#21518;&#20511;&#21161;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#26469;&#24110;&#21161;&#27169;&#22411;&#33719;&#24471;&#31283;&#23450;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.10657v3 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get ri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24178;&#39044;&#25928;&#24212;&#22806;&#37096;&#26377;&#25928;&#24615;&#30340;&#32452;&#21512;&#23450;&#20041;&#65292;&#25581;&#31034;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#24182;&#37325;&#26032;&#23457;&#35270;&#20102;&#21407;&#22987;&#21453;&#20107;&#23454;&#20844;&#24335;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2108.04376</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#25928;&#24212;&#27010;&#25324;&#65306;&#19968;&#31181;&#32452;&#21512;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Effect Generalization: A Combinatorial Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.04376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24178;&#39044;&#25928;&#24212;&#22806;&#37096;&#26377;&#25928;&#24615;&#30340;&#32452;&#21512;&#23450;&#20041;&#65292;&#25581;&#31034;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#24182;&#37325;&#26032;&#23457;&#35270;&#20102;&#21407;&#22987;&#21453;&#20107;&#23454;&#20844;&#24335;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21453;&#20107;&#23454;&#8221;&#23450;&#20041;&#30340;&#22240;&#26524;&#25928;&#24212;&#24191;&#27867;&#29992;&#20110;&#20559;&#24046;&#21644;&#20934;&#30830;&#24615;&#30340;&#25512;&#23548;&#65292;&#20294;&#24182;&#38750;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24178;&#39044;&#25928;&#24212;&#22806;&#37096;&#26377;&#25928;&#24615;&#65288;EV&#65289;&#30340;&#32452;&#21512;&#23450;&#20041;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#31181;&#25928;&#24212;&#35266;&#23519;&#8220;&#32972;&#26223;&#8221;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#65288;&#21487;&#35266;&#23519;&#21644;&#19981;&#21487;&#35266;&#23519;&#30340;&#65289;&#32972;&#26223;&#38598;&#21512;&#21046;&#23450;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#26465;&#20214;&#12290;&#36825;&#25581;&#31034;&#20102;&#25928;&#24212;&#27010;&#25324;&#30340;&#20004;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#24403;&#25928;&#24212;&#22312;&#25152;&#26377;&#21487;&#20197;&#26522;&#20030;&#30340;&#32972;&#26223;&#19979;&#34987;&#35266;&#23519;&#26102;&#65292;&#25110;&#32773;&#65288;2&#65289;&#24403;&#32972;&#26223;&#21464;&#24471;&#36275;&#22815;&#38543;&#26426;&#26102;&#12290;&#25105;&#20204;&#21033;&#29992;&#30001;&#27492;&#20135;&#29983;&#30340;&#32452;&#21512;&#26694;&#26550;&#37325;&#26032;&#23457;&#35270;&#20102;&#21407;&#22987;&#21453;&#20107;&#23454;&#20844;&#24335;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65306;&#26679;&#26412;&#22806;&#26377;&#25928;&#24615;&#12289;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#25928;&#24212;&#12289;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12289;&#32479;&#35745;&#21151;&#25928;&#20197;&#21450;&#19982;&#24403;&#21069;&#39044;&#27979;&#21644;&#35299;&#37322;&#25216;&#26415;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.04376v4 Announce Type: replace-cross  Abstract: The widely used 'Counterfactual' definition of Causal Effects was derived for unbiasedness and accuracy - and not generalizability. We propose a Combinatorial definition for the External Validity (EV) of intervention effects. We first define the concept of an effect observation 'background'. We then formulate conditions for effect generalization based on their sets of (observable and unobservable) backgrounds. This reveals two limits for effect generalization: (1) when effects are observed under all their enumerable backgrounds, or, (2) when backgrounds have become sufficiently randomized. We use the resulting combinatorial framework to re-examine several issues in the original counterfactual formulation: out-of-sample validity, concurrent estimation of multiple effects, bias-variance tradeoffs, statistical power, and connections to current predictive and explaining techniques.   Methodologically, the definitions also allow us 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#8220;&#35299;&#37322;&#20132;&#20114;&#23398;&#20064;&#8221;(XIL)&#23398;&#20064;&#35774;&#32622;&#65292;&#30740;&#31350;&#32773;&#21487;&#20197;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#20854;&#21033;&#29992;&#28151;&#28102;&#22240;&#32032;&#32780;&#23548;&#33268;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2001.05371</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#35299;&#37322;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20986;&#20110;&#27491;&#30830;&#30340;&#31185;&#23398;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Making deep neural networks right for the right scientific reasons by interacting with their explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.05371
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#8220;&#35299;&#37322;&#20132;&#20114;&#23398;&#20064;&#8221;(XIL)&#23398;&#20064;&#35774;&#32622;&#65292;&#30740;&#31350;&#32773;&#21487;&#20197;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#20854;&#21033;&#29992;&#28151;&#28102;&#22240;&#32032;&#32780;&#23548;&#33268;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#24335;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;&#35299;&#37322;&#20132;&#20114;&#23398;&#20064;&#8221;(XIL)&#30340;&#26032;&#22411;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#26893;&#29289;&#34920;&#22411;&#30740;&#31350;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#22909;&#22788;&#12290;XIL&#23558;&#31185;&#23398;&#23478;&#24341;&#20837;&#35757;&#32451;&#24490;&#29615;&#65292;&#20351;&#22905;&#36890;&#36807;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#21453;&#39304;&#36827;&#34892;&#20132;&#20114;&#24335;&#20462;&#35746;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XIL&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#32874;&#26126;&#27721;&#26031;&#8221;&#26102;&#21051;&#65292;&#24182;&#40723;&#21169;&#65288;&#25110;&#40723;&#21169;&#65292;&#22914;&#26524;&#21512;&#36866;&#30340;&#35805;&#65289;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2001.05371v4 Announce Type: replace-cross  Abstract: Deep neural networks have shown excellent performances in many real-world applications. Unfortunately, they may show "Clever Hans"-like behavior -- making use of confounding factors within datasets -- to achieve high performance. In this work, we introduce the novel learning setting of "explanatory interactive learning" (XIL) and illustrate its benefits on a plant phenotyping research task. XIL adds the scientist into the training loop such that she interactively revises the original model via providing feedback on its explanations. Our experimental results demonstrate that XIL can help avoiding Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust into the underlying model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.16650</link><description>&lt;p&gt;
&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#25918;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#29615;&#22659;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25104;&#21151;&#30340;&#31995;&#32479;&#24212;&#35813;&#36866;&#24403;&#24179;&#34913;&#20445;&#25345;&#24050;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20195;&#29702;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#35201;&#27714;&#12290;&#39318;&#36827;&#20808;&#20986;&#32531;&#20914;&#21306;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#20110;&#27492;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#38480;&#21046;&#65292;&#24182;&#19982;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Atari&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#22686;&#24378;&#21487;&#20197;&#25104;&#21151;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#38750;&#23436;&#20840;&#26080;&#25032;&#21487;&#20987;&#65292;
&lt;/p&gt;
&lt;p&gt;
In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16461</link><description>&lt;p&gt;
&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65306;&#26356;&#24555;&#30340;&#20986;&#29616;&#65292;&#26356;&#24555;&#20048;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16461
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21487;&#35270;&#20026;&#19968;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#65292;&#36890;&#36807;&#31038;&#20250;&#35268;&#33539;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25511;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#30340;&#35268;&#33539;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#32780;&#26159;&#20174;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#20013;&#30340;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#20316;&#20986;&#30340;&#21453;&#24212;&#20197;&#21450;&#23545;&#20182;&#20154;&#21453;&#24212;&#30340;&#22238;&#24212;&#65292;&#20915;&#23450;&#20102;&#31038;&#20250;&#20013;&#20986;&#29616;&#21738;&#20123;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#28385;&#24847;&#25110;&#19981;&#28385;&#24847;&#34892;&#20026;&#30340;&#21453;&#24212;&#35270;&#20026;&#31532;&#19968;&#20010;&#26234;&#33021;&#20307;&#21521;&#31532;&#20108;&#20010;&#26234;&#33021;&#20307;&#30340;&#20132;&#27969;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#27969;&#26159;&#19968;&#31181;&#31038;&#20250;&#26234;&#33021;&#65306;&#36825;&#20123;&#20132;&#27969;&#36890;&#36807;&#25512;&#21160;&#26234;&#33021;&#20307;&#26397;&#30528;&#26576;&#20123;&#34892;&#20026;&#36827;&#34892;&#65292;&#20174;&#32780;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#24809;&#32602;&#21487;&#20197;&#23548;&#33268;&#35268;&#33539;&#30340;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#23485;&#27867;&#30340;&#31038;&#20250;&#26234;&#33021;&#21487;&#33021;&#22312;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;Ne&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>DiffDA&#26159;&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#21516;&#21270;&#65292;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#33021;&#23545;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#12290;</title><link>http://arxiv.org/abs/2401.05932</link><description>&lt;p&gt;
DiffDA:&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffDA: a diffusion model for weather-scale data assimilation. (arXiv:2401.05932v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05932
&lt;/p&gt;
&lt;p&gt;
DiffDA&#26159;&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#21516;&#21270;&#65292;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#33021;&#23545;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#30340;&#25968;&#25454;&#21516;&#21270;&#29983;&#25104;&#21021;&#22987;&#26465;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#22825;&#27668;&#39044;&#25253;&#21644;&#27668;&#20505;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiffDA&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#26469;&#21516;&#21270;&#22823;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;GraphCast&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20316;&#20026;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20102;&#20004;&#38454;&#27573;&#26465;&#20214;&#65306;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#39044;&#27979;&#29366;&#24577;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21482;&#23545;&#31232;&#30095;&#35266;&#27979;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36825;&#31181;&#31574;&#30053;&#36824;&#33021;&#23558;&#39044;&#27979;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#27809;&#26377;&#21487;&#29992;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#36890;&#36807;&#22522;&#20110;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;0.25&#24230;&#20998;&#36776;&#29575;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21516;&#21270;&#20840;&#29699;&#22823;&#27668;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#21021;&#22987;&#26465;&#20214;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#36739;&#23567;&#25439;&#22833;&#30340;&#39044;&#25253;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss 
&lt;/p&gt;</description></item><item><title>MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04577</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#36974;&#34109;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04577
&lt;/p&gt;
&lt;p&gt;
MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNeT&#30340;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#30452;&#25509;&#25805;&#20316;&#22810;&#20010;&#38899;&#39057;&#20196;&#29260;&#27969;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGNeT&#30001;&#21333;&#38454;&#27573;&#38750;&#33258;&#22238;&#24402;Transformer&#32452;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36974;&#34109;&#35745;&#21010;&#22120;&#39044;&#27979;&#36974;&#34109;&#20196;&#29260;&#30340;&#33539;&#22260;&#65292;&#32780;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#26500;&#24314;&#36755;&#20986;&#24207;&#21015;&#20351;&#29992;&#22810;&#20010;&#35299;&#30721;&#27493;&#39588;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21644;&#25490;&#21517;MAGNeT&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#23558;&#34987;&#29992;&#20110;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;MAGNeT&#30340;&#28151;&#21512;&#29256;&#26412;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#21069;&#20960;&#31186;&#38047;&#65292;&#32780;&#20854;&#20313;&#30340;&#24207;&#21015;&#21017;&#20197;&#24182;&#34892;&#26041;&#24335;&#36827;&#34892;&#35299;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.08616</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36890;&#29992;&#31070;&#32463;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#21644;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#28608;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#25193;&#25955;&#30340;GNN&#30340;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20004;&#31181;&#26426;&#21046;&#23494;&#20999;&#30456;&#20851;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#33258;&#28982;&#22320;&#20135;&#29983;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#27491;&#24335;&#32479;&#19968;&#36825;&#20123;GNN&#30340;&#36890;&#29992;&#25193;&#25955;&#26694;&#26550;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#20165;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;GNN&#23398;&#20064;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#25171;&#24320;&#19968;&#20010;&#35774;&#35745;&#24191;&#27867;&#26032;&#30340;GNN&#31867;&#21035;&#30340;&#26032;&#22823;&#38376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#36890;&#29992;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#23427;&#27491;&#24335;&#24314;&#31435;&#20102;&#25193;&#25955;&#36807;&#31243;&#19982;&#26356;&#22810;GNN&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#25193;&#25955;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#24449;&#65292;&#21363;&#24403;&#21069;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#21482;&#23545;&#24212;&#20110;&#19968;&#38454;&#25193;&#25955;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#23454;&#38469;&#19978;&#34920;&#29616;&#20986;&#21333;&#19968;&#24615;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion-based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually exhibit monophily property, which induces the similarit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.04295</link><description>&lt;p&gt;
&#35782;&#21035;&#24178;&#39044;&#22806;&#25512;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35782;&#21035;&#21644;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#23398;&#20064;&#30340;&#21069;&#25552;&#26159;&#25913;&#36827;&#24403;&#21069;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#22312;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20855;&#20307;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#21363;&#20351;&#36825;&#20123;&#24178;&#39044;&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#33021;&#22815;&#20026;&#36825;&#20010;&#20219;&#21153;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#32467;&#26524;Y&#65292;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;X&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#28508;&#22312;&#29305;&#24449;Z&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#20197;&#21450;&#24433;&#21709;Z&#30340;&#22806;&#29983;&#34892;&#20026;&#21464;&#37327;A&#12290;&#24178;&#39044;&#22806;&#25512;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#20301;&#20110;&#35757;&#32451;&#25903;&#25345;&#20043;&#22806;&#30340;A&#19978;&#30340;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;Y&#12290;&#22312;&#36825;&#37324;&#65292;&#22806;&#25512;&#21464;&#24471;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becom
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#21487;&#23548;&#33322;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02424</link><description>&lt;p&gt;
AXNav: &#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AXNav: Replaying Accessibility Tests from Natural Language. (arXiv:2310.02424v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#21487;&#23548;&#33322;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#32773;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#36890;&#24120;&#20381;&#36182;&#25163;&#21160;&#27979;&#35797;&#26469;&#22312;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#20013;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27979;&#35797;&#21487;&#33021;&#24456;&#20047;&#21619;&#65292;&#33539;&#22260;&#24222;&#22823;&#65292;&#24182;&#19988;&#24456;&#38590;&#23433;&#25490;&#22312;&#20854;&#20182;&#24320;&#21457;&#37324;&#31243;&#30865;&#20043;&#38388;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#29992;&#25143;&#30028;&#38754;&#65292;&#20294;&#25454;&#25105;&#20204;&#20102;&#35299;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#20154;&#25506;&#32034;&#36807;&#23427;&#20204;&#22312;&#25511;&#21046;&#36741;&#21161;&#25216;&#26415;&#20197;&#25903;&#25345;&#26080;&#38556;&#30861;&#27979;&#35797;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#25506;&#35752;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#26080;&#38556;&#30861;&#27979;&#35797;&#24037;&#20316;&#27969;&#31243;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20197;&#25163;&#21160;&#26080;&#38556;&#30861;&#27979;&#35797;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;VoiceOver&#20013;&#25628;&#32034;&#19968;&#20010;&#33410;&#30446;&#8221;&#65289;&#65292;&#24182;&#20351;&#29992;LLM&#32467;&#21512;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#26469;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#31456;&#33410;&#21010;&#20998;&#30340;&#21487;&#23548;&#33322;&#35270;&#39057;&#12290;&#22312;&#27599;&#20010;&#35270;&#39057;&#20013;&#65292;&#20026;&#20102;&#24110;&#21161;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#65292;&#25105;&#20204;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#26631;&#35760;ac&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag ac
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00194</link><description>&lt;p&gt;
&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models. (arXiv:2310.00194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00194
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#21363;&#36890;&#36807;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#20013;&#19987;&#38376;&#27169;&#22359;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23436;&#25104;&#35268;&#21010;&#12290;&#36825;&#20123;&#27169;&#22359;&#25191;&#34892;&#20914;&#31361;&#30417;&#27979;&#12289;&#29366;&#24577;&#39044;&#27979;&#12289;&#29366;&#24577;&#35780;&#20272;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#20219;&#21153;&#21327;&#35843;&#31561;&#21151;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#26102;&#33021;&#22815;&#21333;&#29420;&#25191;&#34892;&#36825;&#20123;&#21151;&#33021;&#65292;&#20294;&#22312;&#26381;&#21153;&#20110;&#19968;&#20010;&#30446;&#26631;&#26102;&#24448;&#24448;&#38590;&#20197;&#33258;&#20027;&#21327;&#35843;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#22522;&#20110;LLM&#65288;GPT-4&#65289;&#27169;&#22359;&#30340;&#40657;&#30418;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#19987;&#38376;&#30340;PFC&#21551;&#21457;&#27169;&#22359;&#30340;&#20132;&#20114;&#23558;&#19968;&#20010;&#26356;&#22823;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23545;LLM&#30340;&#31616;&#30701;&#33258;&#21160;&#35843;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06354</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35782;&#21035;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#23545;&#24739;&#32773;&#30340;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#25910;&#38598;&#19981;&#23436;&#25972;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;EHR&#20013;&#25552;&#21462;SDoH&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#22312;&#25913;&#36827;&#36825;&#20123;&#23569;&#35265;&#20294;&#26497;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#25968;&#25454;&#25552;&#21462;&#20013;&#30340;&#20316;&#29992;&#12290;&#23545;800&#20221;&#24739;&#32773;&#35760;&#24405;&#36827;&#34892;&#20102;SDoH&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#36824;&#23581;&#35797;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#31639;&#27861;&#20559;&#24046;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#65288;macro-F1 0.71&#65289;&#29992;&#20110;&#20219;&#20309;SDoH&#65292;&#20197;&#21450;Flan-T5 XXL&#65288;macro-F1 0.70&#65289;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36741;&#21161;&#24494;&#35843;&#30340;&#25928;&#30410;&#22240;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#32780;&#24322;&#65292;&#22312;&#36739;&#23567;&#30340;Flan-T5&#27169;&#22411;&#65288;&#22522;&#30784;&#21644;&#22823;&#22411;&#65289;&#20013;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;delta F1 +0.12&#21040;+0.23&#65289;&#12290;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01264</link><description>&lt;p&gt;
&#25506;&#32034;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the psychology of GPT-4's Moral and Legal Reasoning. (arXiv:2308.01264v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20316;&#39640;&#24230;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#33021;&#22815;&#23545;&#27861;&#24459;&#21644;&#36947;&#24503;&#38382;&#39064;&#20316;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33258;&#36523;&#20869;&#37096;&#24037;&#20316;&#30340;&#25351;&#23548;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#21019;&#24314;&#24037;&#31243;&#22242;&#38431;&#20063;&#26080;&#27861;&#35299;&#37322;&#23427;&#20204;&#22914;&#20309;&#33719;&#24471;&#24403;&#21069;&#25152;&#26377;&#33021;&#21147;&#30340;&#20855;&#20307;&#36807;&#31243;&#12290;&#26426;&#22120;&#24515;&#29702;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#25317;&#26377;&#30340;&#36807;&#31243;&#21644;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#26469;&#25506;&#31350;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GPT-4&#19982;&#20154;&#31867;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22238;&#31572;&#20043;&#38388;&#23384;&#22312;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. However, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. The emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. In this paper, we employ the methods of psychology to probe into GPT-4's moral and legal reasoning. More specifically, we investigate the similarities and differences between GPT-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. We find high correlations between human and AI response
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#20915;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;Hopfield&#32593;&#32476;&#20013;&#20351;&#29992;&#20851;&#32852;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#27169;&#22411;&#65292;&#32593;&#32476;&#21487;&#20197;&#35299;&#20915;&#20855;&#20307;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20250;&#27704;&#20037;&#20002;&#22833;&#65292;&#23548;&#33268;&#32593;&#32476;&#20135;&#29983;&#30475;&#20284;&#26368;&#20248;&#20294;&#23454;&#38469;&#19978;&#19981;&#36866;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#29702;&#35299;&#32593;&#32476;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#24456;&#26377;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.16807</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#35299;&#20915;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;Hopfield&#32593;&#32476;&#20013;&#20351;&#29992;&#20851;&#32852;&#35760;&#24518;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of associative memory in Hopfield networks designed to solve propositional satisfiability problems. (arXiv:2307.16807v2 [nlin.AO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#20915;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;Hopfield&#32593;&#32476;&#20013;&#20351;&#29992;&#20851;&#32852;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#27169;&#22411;&#65292;&#32593;&#32476;&#21487;&#20197;&#35299;&#20915;&#20855;&#20307;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20250;&#27704;&#20037;&#20002;&#22833;&#65292;&#23548;&#33268;&#32593;&#32476;&#20135;&#29983;&#30475;&#20284;&#26368;&#20248;&#20294;&#23454;&#38469;&#19978;&#19981;&#36866;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#29702;&#35299;&#32593;&#32476;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#24456;&#26377;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#32593;&#32476;&#30001;&#20110;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#29289;&#23398;&#19978;&#21487;&#34892;&#30340;&#26426;&#21046;&#65292;&#22240;&#27492;&#22312;&#35299;&#20915;&#35768;&#22810;&#31867;&#22411;&#30340;&#35745;&#31639;&#38382;&#39064;&#26102;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#33258;&#25105;&#20248;&#21270;&#65288;SO&#65289;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#36203;&#24067;&#23398;&#20064;&#35268;&#21017;&#21644;&#37325;&#22797;&#30340;&#32593;&#32476;&#37325;&#32622;&#21040;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#65292;&#26469;&#20248;&#21270;&#32593;&#32476;&#34892;&#20026;&#20197;&#36798;&#21040;&#32593;&#32476;&#20013;&#32534;&#30721;&#30340;&#26576;&#20010;&#24076;&#26395;&#30340;&#30446;&#26631;&#29366;&#24577;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#35813;&#36807;&#31243;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;SO&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;Liars&#38382;&#39064;&#21644;&#22320;&#22270;&#30528;&#33394;&#38382;&#39064;&#30340;&#20004;&#20010;&#20363;&#23376;&#26469;&#35299;&#20915;&#20855;&#20307;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#27704;&#36828;&#20002;&#22833;&#65292;&#20174;&#32780;&#20351;&#24471;&#23398;&#20064;&#32593;&#32476;&#20135;&#29983;&#30475;&#20284;&#26368;&#20248;&#35299;&#20294;&#23454;&#38469;&#19978;&#23545;&#25152;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#19981;&#21512;&#36866;&#12290;&#36825;&#31181;SO&#27169;&#22411;&#30340;&#21103;&#20316;&#29992;&#30475;&#20284;&#19981;&#22909;&#65292;&#21364;&#21487;&#20197;&#23545;&#20854;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#30340;&#36807;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hopfield networks are an attractive choice for solving many types of computational problems because they provide a biologically plausible mechanism. The Self-Optimization (SO) model adds to the Hopfield network by using a biologically founded Hebbian learning rule, in combination with repeated network resets to arbitrary initial states, for optimizing its own behavior towards some desirable goal state encoded in the network. In order to better understand that process, we demonstrate first that the SO model can solve concrete combinatorial problems in SAT form, using two examples of the Liars problem and the map coloring problem. In addition, we show how under some conditions critical information might get lost forever with the learned network producing seemingly optimal solutions that are in fact inappropriate for the problem it was tasked to solve. What appears to be an undesirable side-effect of the SO model, can provide insight into its process for solving intractable problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.04735</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21151;&#33021;&#22240;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#33391;&#22909;&#32467;&#26524;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#20197;&#33719;&#24471;&#24341;&#23548;&#26356;&#22909;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Open Pre-trained Transformers&#65288;OPT&#65289;&#21644;Galactica&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#20154;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#35780;&#20272;&#20559;&#24046;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#32452;&#20844;&#24179;&#24615;&#65288;&#20559;&#24046;&#65289;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#21040;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;&#30001;&#20110;LLMs&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#29992;&#20110;&#24037;&#19994;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#30340;&#20559;&#35265;&#35780;&#20272;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18149</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#31561;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#36896;&#34394;&#20551;&#30340;&#23398;&#26415;&#25991;&#26412;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#25512;&#29305;&#31561;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26816;&#27979;&#36825;&#20123;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;ML&#20998;&#31867;&#22120;&#12289;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#21487;&#30693;&#26041;&#27861;&#21644;&#31934;&#35843;&#30340;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#26816;&#27979;&#22120;&#22312;&#26500;&#24314;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25991;&#26412;&#38271;&#24230;&#30340;&#22240;&#32032;&#65306;&#30701;&#25991;&#26412;&#30340;&#32570;&#20047;&#20449;&#24687;&#29305;&#24449;&#65292;&#20351;&#20854;&#26356;&#38590;&#26816;&#27979;&#12290;&#38024;&#23545;&#22810;&#23610;&#24230;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#65288;MPU&#65289;&#35757;&#32451;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25215;&#35748;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20855;&#26377;&#31867;&#20154;&#23646;&#24615;&#65292;&#24182;&#23558;&#25991;&#26412;&#20998;&#31867;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#27491;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#36825;&#20123;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20026;"unlabeled"&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#27491;&#36127;&#26679;&#26412;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13269</link><description>&lt;p&gt;
&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Game-based Platforms for Artificial Intelligence Research. (arXiv:2304.13269v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20855;&#26377;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24191;&#27867;&#29305;&#24449;&#65292;&#25104;&#20026;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#29702;&#24819;&#27979;&#35797;&#22522;&#22320;&#65292;&#20849;&#21516;&#30340;&#30740;&#31350;&#39046;&#22495;&#21253;&#25324;&#23398;&#20064;&#21644;&#20248;&#21270;&#12289;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#12289;&#21338;&#24328;&#35770;&#12289;&#35745;&#21010;&#19982;&#25490;&#31243;&#12289;&#35774;&#35745;&#21644;&#25945;&#32946;&#31561;&#12290;&#24050;&#23454;&#26045;&#20102;&#35768;&#22810;&#24320;&#28304;&#28216;&#25103;&#25110;&#22522;&#20110;&#28216;&#25103;&#30340;&#29615;&#22659;&#29992;&#20110;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#12290;&#38500;&#20102;&#21333;&#20154;&#25110;&#22810;&#20154;&#12289;&#21512;&#20316;&#25110;&#23545;&#25239;&#24615;&#28216;&#25103;&#22806;&#65292;&#22312;&#21019;&#24847;&#35774;&#35745;&#26041;&#38754;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#24179;&#21488;&#20026;&#25506;&#32034;&#21644;&#27604;&#36739;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#24819;&#21644;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#24819;&#22522;&#20934;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#30001;&#36825;&#20123;&#24179;&#21488;&#28436;&#21464;&#24341;&#36215;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-sourced games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12653</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#22312;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#22343;&#22330;&#29702;&#35770;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35266;&#23519;&#21040;&#22266;&#23450;&#33539;&#22260;&#20869;&#30340;&#20854;&#20182;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#24433;&#21709;&#20102;&#26234;&#33021;&#20307;&#35780;&#20272;&#21608;&#22260;&#26234;&#33021;&#20307;&#34892;&#21160;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#33719;&#21462;&#26356;&#26377;&#25928;&#20449;&#24687;&#20197;&#36873;&#25321;&#26356;&#26377;&#25928;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#20197;&#21069;&#24037;&#20316;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#25110;&#21152;&#26435;&#22343;&#22330;&#26469;&#26356;&#26032;&#37051;&#23621;&#26234;&#33021;&#20307;&#24179;&#22343;&#34892;&#21160;&#65292;&#20294;&#23427;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21608;&#22260;&#37051;&#23621;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#23616;&#37096;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07143</link><description>&lt;p&gt;
&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Longitudinal Car-Following Model. (arXiv:2304.07143v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36319;&#36710;&#27169;&#22411;&#26159;&#20132;&#36890;&#20223;&#30495;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#20869;&#32622;&#20110;&#35768;&#22810;&#37197;&#22791;ADAS&#30340;&#27773;&#36710;&#20013;&#12290;&#23545;&#36710;&#36319;&#36710;&#34892;&#20026;&#30340;&#30740;&#31350;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#30001;&#22522;&#26412;&#30340;&#36710;&#36742;&#20132;&#20114;&#36807;&#31243;&#24341;&#36215;&#30340;&#19981;&#21516;&#23439;&#35266;&#29616;&#35937;&#30340;&#26681;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21508;&#31181;&#36710;&#36319;&#36710;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12289;&#20114;&#34917;&#24615;&#21644;&#37325;&#21472;&#20043;&#22788;&#12290;&#35813;&#23457;&#26597;&#23558;&#22312;&#19981;&#21516;&#21407;&#21017;&#20013;&#27010;&#24565;&#21270;&#30340;&#36710;&#36319;&#36710;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The car-following (CF) model is the core component for traffic simulations and has been built-in in many production vehicles with Advanced Driving Assistance Systems (ADAS). Research of CF behavior allows us to identify the sources of different macro phenomena induced by the basic process of pairwise vehicle interaction. The CF behavior and control model encompasses various fields, such as traffic engineering, physics, cognitive science, machine learning, and reinforcement learning. This paper provides a comprehensive survey highlighting differences, complementarities, and overlaps among various CF models according to their underlying logic and principles. We reviewed representative algorithms, ranging from the theory-based kinematic models, stimulus-response models, and cruise control models to data-driven Behavior Cloning (BC) and Imitation Learning (IL) and outlined their strengths and limitations. This review categorizes CF models that are conceptualized in varying principles and s
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30340;&#26041;&#27861;&#65292;&#23427;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#21487;&#20197;&#25913;&#21892;&#28216;&#25103;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.15734</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#20048;&#22120;&#20307;&#31215;&#35843;&#21046;&#30340;&#26041;&#27861;&#22686;&#24378;&#26684;&#26007;&#28216;&#25103;&#20013;&#30340;&#32972;&#26223;&#38899;&#20048;&#65306;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach. (arXiv:2303.15734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30340;&#26041;&#27861;&#65292;&#23427;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#21487;&#20197;&#25913;&#21892;&#28216;&#25103;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312; DareFightingICE &#20013;&#28155;&#21152;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#20197;&#22686;&#24378;&#28216;&#25103;&#20307;&#39564;&#30340;&#24037;&#20316;&#12290;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#26469;&#35780;&#20272;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064; AI&#65288;Blind DL AI&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#26102;&#30456;&#27604;&#65292;Blind DL AI &#22312;&#19982;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#19968;&#36215;&#25773;&#25918;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding an adaptive BGM. The adaptive BGM consists of five different instruments playing a classical music piece called "Air on G-String." The BGM adapts by changing the volume of the instruments. Each instrument is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.10540</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#21457;&#29616;&#20013;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#22522;&#20110;&#36153;&#26364;&#29289;&#29702;&#35762;&#20041;&#30340;&#19968;&#32452;&#20844;&#24335;&#65292;&#25105;&#20204;&#37325;&#26032;&#21019;&#24314;&#20102;120&#20010;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24615;&#33021;&#65288;SRSD&#65289;&#12290;&#23545;&#20110;&#36825;120&#20010;SRSD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#20844;&#24335;&#21450;&#20854;&#21464;&#37327;&#30340;&#23646;&#24615;&#65292;&#35774;&#35745;&#20102;&#21512;&#29702;&#30340;&#23454;&#20540;&#33539;&#22260;&#26469;&#37319;&#26679;&#20540;&#65292;&#20197;&#20415;&#25105;&#20204;&#30340;&#26032;SRSD&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;SRSD&#30340;&#28508;&#21147;&#65292;&#22914;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20013;&#65288;&#37325;&#26032;&#65289;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#22806;120&#20010;&#21253;&#21547;&#34394;&#25311;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20165;&#36873;&#25321;&#24517;&#35201;&#21464;&#37327;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#26041;&#31243;&#19982;&#30495;&#23454;&#26041;&#31243;&#26641;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#65288;NED&#65289;&#26469;&#35299;&#20915;&#29616;&#26377;SR&#24230;&#37327;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20108;&#20803;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
&lt;/p&gt;</description></item><item><title>AutoGL&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24211;&#65292;&#20351;&#29992;&#26041;&#20415;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2104.04987</link><description>&lt;p&gt;
AutoGL&#65306;&#33258;&#21160;&#22270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
AutoGL: A Library for Automated Graph Learning. (arXiv:2104.04987v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04987
&lt;/p&gt;
&lt;p&gt;
AutoGL&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24211;&#65292;&#20351;&#29992;&#26041;&#20415;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#20026;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#25163;&#21160;&#35774;&#35745;&#26368;&#20248;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19981;&#28789;&#27963;&#12289;&#36153;&#26102;&#65292;&#24182;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22270;&#19978;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24211;&#20013;&#27809;&#26377;&#19968;&#20010;&#33021;&#23436;&#20840;&#25903;&#25345;&#22270;&#19978;&#30340;AutoML&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#22270;&#23398;&#20064;&#65288;AutoGL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#33258;&#21160;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24211;&#12290;AutoGL&#26159;&#24320;&#28304;&#30340;&#65292;&#26131;&#20110;&#20351;&#29992;&#65292;&#32780;&#19988;&#26131;&#20110;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#35774;&#22791;&#20132;&#20114;&#30340;&#21518;&#31471;&#12289;&#23436;&#25972;&#30340;&#33258;&#21160;&#22270;&#23398;&#20064;&#27969;&#31243;&#21644;&#25903;&#25345;&#30340;&#22270;&#24212;&#29992;&#30340;&#19977;&#23618;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an upsurge in research interests and applications of machine learning on graphs. However, manually designing the optimal machine learning algorithms for different graph datasets and tasks is inflexible, labor-intensive, and requires expert knowledge, limiting its adaptivity and applicability. Automated machine learning (AutoML) on graphs, aiming to automatically design the optimal machine learning algorithm for a given graph dataset and task, has received considerable attention. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first dedicated library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose a three-layer architecture, consisting of backends to interface with devices, a complete automated graph learning pipeline, and supported graph applications. The automated machine learning
&lt;/p&gt;</description></item></channel></rss>