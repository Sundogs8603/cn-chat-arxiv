<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>FindingEmo&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#22797;&#26434;&#22330;&#26223;&#30340;&#27880;&#37322;&#65292;&#28085;&#30422;&#22810;&#20010;&#20154;&#29289;&#21644;&#31038;&#20132;&#35774;&#32622;&#65292;&#27880;&#37322;&#21253;&#25324;&#24773;&#24863;&#32500;&#24230;&#21644;&#24773;&#24863;&#26631;&#31614;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01355</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;Emo&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FindingEmo: An Image Dataset for Emotion Recognition in the Wild
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01355
&lt;/p&gt;
&lt;p&gt;
FindingEmo&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#22797;&#26434;&#22330;&#26223;&#30340;&#27880;&#37322;&#65292;&#28085;&#30422;&#22810;&#20010;&#20154;&#29289;&#21644;&#31038;&#20132;&#35774;&#32622;&#65292;&#27880;&#37322;&#21253;&#25324;&#24773;&#24863;&#32500;&#24230;&#21644;&#24773;&#24863;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;FindingEmo&#65292;&#19968;&#20010;&#21253;&#21547;25k&#24352;&#22270;&#29255;&#30340;&#26032;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#23427;&#19987;&#27880;&#20110;&#22797;&#26434;&#22330;&#26223;&#65292;&#21253;&#21547;&#22810;&#20010;&#20154;&#29289;&#22312;&#21508;&#31181;&#33258;&#28982;&#12289;&#31038;&#20132;&#29615;&#22659;&#20013;&#65292;&#22270;&#29255;&#30340;&#27880;&#37322;&#26159;&#25972;&#20307;&#36827;&#34892;&#30340;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#23545;&#20110;&#38754;&#37096;&#25110;&#21333;&#20010;&#20010;&#20307;&#30340;&#20851;&#27880;&#12290;&#27880;&#37322;&#30340;&#32500;&#24230;&#21253;&#25324;&#24773;&#24863;&#20215;&#20540;&#12289;&#21796;&#36215;&#21644;&#24773;&#24863;&#26631;&#31614;&#65292;&#27880;&#37322;&#36890;&#36807;Prolific&#25910;&#38598;&#12290;&#38500;&#20102;&#27880;&#37322;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25351;&#21521;&#21407;&#22987;&#22270;&#29255;&#30340;URL&#21015;&#34920;&#65292;&#20197;&#21450;&#25152;&#26377;&#30456;&#20851;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01741</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#32423;&#21453;&#39304;&#25511;&#21046;&#23454;&#29616;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;
&lt;/p&gt;
&lt;p&gt;
Intrusion Tolerance for Networked Systems through Two-Level Feedback Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26381;&#21153;&#22797;&#21046;&#21697;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#26412;&#22320;&#32423;&#21035;&#65292;&#33410;&#28857;&#25511;&#21046;&#22120;&#25191;&#34892;&#20837;&#20405;&#24674;&#22797;&#65292;&#22312;&#20840;&#23616;&#32423;&#21035;&#65292;&#31995;&#32479;&#25511;&#21046;&#22120;&#31649;&#29702;&#22797;&#21046;&#22240;&#23376;&#12290;&#26412;&#22320;&#21644;&#20840;&#23616;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#36816;&#31609;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#26356;&#25442;&#38382;&#39064;&#21644;&#24211;&#23384;&#34917;&#32473;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#27169;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21517;&#20026;TOLERANCE&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#25511;&#21046;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#35745;&#31639;&#23427;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20223;&#30495;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;TOLERANCE&#65292;&#20854;&#20013;&#36816;&#34892;&#20102;10&#31181;&#32593;&#32476;&#20837;&#20405;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#30456;&#27604;&#65292;TOLERANCE&#33021;&#22815;&#25552;&#39640;&#26381;&#21153;&#21487;&#29992;&#24615;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01741v1 Announce Type: cross  Abstract: We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00474</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Linguistic Calibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#33258;&#20449;&#24187;&#35273;&#26102;&#23548;&#33268;&#29992;&#25143;&#20570;&#20986;&#27425;&#20248;&#21270;&#30340;&#19979;&#28216;&#20915;&#31574;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21475;&#22836;&#20256;&#36798;&#20854;&#20027;&#24352;&#27491;&#30830;&#27010;&#29575;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20855;&#26377;&#26657;&#20934;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20915;&#31574;&#35282;&#24230;&#65292;&#20026;&#38271;&#31687;&#29983;&#25104;&#24418;&#24335;&#30340;&#35821;&#35328;&#26657;&#20934;&#24418;&#24335;&#21270;&#23450;&#20041;&#65306;&#22914;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20351;&#20854;&#29992;&#25143;&#33021;&#22815;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#65292;&#21017;&#35813;&#27169;&#22411;&#26159;&#35821;&#35328;&#19978;&#26657;&#20934;&#30340;&#12290;&#36825;&#20010;&#23450;&#20041;&#20351;&#24471;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#27493;&#39588;&#24341;&#23548;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21457;&#20986;&#24102;&#26377;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#38271;&#31687;&#29983;&#25104;&#65292;&#35832;&#22914;&#8220;&#25105;&#20272;&#35745;&#26377;30%&#30340;&#26426;&#20250;&#8230;&#8221;&#25110;&#8220;&#25105;&#30830;&#20449;&#8230;&#8221;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#65292;&#22870;&#21169;&#20351;&#29992;&#25143;&#33021;&#22815;&#23545;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26657;&#20934;&#31572;&#26696;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23545;Llama 2 7B &#36827;&#34892;&#35821;&#35328;&#26657;&#20934;&#65292;&#24182;&#21457;&#29616;&#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#27979;&#35797;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.18715</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;&#20943;&#36731;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18715
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36234;&#26469;&#36234;&#25797;&#38271;&#20174;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#32454;&#33410;&#21644;&#36830;&#36143;&#24615;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#27169;&#24335;&#20915;&#31574;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#24212;&#29992;&#23427;&#20204;&#26102;&#65292;&#20854;&#24212;&#29992;&#21463;&#21040;&#24187;&#35273;&#30340;&#38459;&#30861;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#19981;&#20934;&#30830;&#22320;&#20195;&#34920;&#20102;&#35270;&#35273;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;LVLM&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#25200;&#21160;&#25351;&#31034;&#26174;&#33879;&#21152;&#21095;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#20013;&#30340;&#24187;&#35273;&#30340;&#21551;&#21457;&#12290;ICD&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#32780;&#22686;&#21152;&#23545;&#40784;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#25928;&#22320;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;&#36890;&#36807;&#22312;&#21028;&#21035;&#22522;&#20934;(POPE&#21644;MME)&#21644;&#29983;&#25104;&#22522;&#20934;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18715v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#26041;&#38754;&#26469;&#25913;&#36827;&#27010;&#24565;&#23884;&#20837;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#24120;&#35782;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16984</link><description>&lt;p&gt;
&#29992;&#22810;&#26041;&#38754;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#24314;&#27169;&#24120;&#35782;&#20849;&#24615;
&lt;/p&gt;
&lt;p&gt;
Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#26041;&#38754;&#26469;&#25913;&#36827;&#27010;&#24565;&#23884;&#20837;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#24120;&#35782;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#26426;&#21046;&#65292;&#23558;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#26631;&#20934;&#23884;&#20837;&#20027;&#35201;&#21453;&#26144;&#22522;&#26412;&#20998;&#31867;&#31867;&#21035;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#27010;&#24565;&#23884;&#20837;&#26102;&#26126;&#30830;&#24314;&#27169;&#24863;&#20852;&#36259;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24471;&#21040;&#20102;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#24120;&#35782;&#23646;&#24615;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16984v1 Announce Type: new  Abstract: Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks. Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e.\ sets of concepts which share some property of interest. Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust. Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g.\ the colour of objects or the materials they are made of). In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings. We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAFT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10131</link><description>&lt;p&gt;
RAFT&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#39046;&#22495;RAG
&lt;/p&gt;
&lt;p&gt;
RAFT: Adapting Language Model to Domain Specific RAG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAFT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#33539;&#24335;&#12290;&#22312;&#23558;&#36825;&#20123;LLMs&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#36890;&#24120;&#36824;&#20250;&#36890;&#36807;&#22522;&#20110;RAG&#30340;&#25552;&#31034;&#25110;&#24494;&#35843;&#65292;&#23558;&#26032;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#26102;&#25928;&#26032;&#38395;&#25110;&#31169;&#26377;&#39046;&#22495;&#30693;&#35782;&#65289;&#23884;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33719;&#24471;&#36825;&#20123;&#26032;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#65288;RAFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;"&#24320;&#25918;&#20070;&#31821;"&#30340;&#39046;&#22495;&#35774;&#32622;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#22312;RAFT&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#24573;&#30053;&#37027;&#20123;&#23545;&#22238;&#31572;&#38382;&#39064;&#27809;&#26377;&#24110;&#21161;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24178;&#25200;&#25991;&#26723;&#12290;RAFT&#36890;&#36807;&#21407;&#25991;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10131v1 Announce Type: cross  Abstract: Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAF
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.07004</link><description>&lt;p&gt;
&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;
&lt;/p&gt;
&lt;p&gt;
Convergence of Some Convex Message Passing Algorithms to a Fixed Point
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07004
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#27169;&#22411;&#20013;&#35299;&#20915;MAP&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#36890;&#36807;&#65288;&#22359;&#29366;&#65289;&#22352;&#26631;&#19979;&#38477;&#26368;&#23567;&#21270;&#20174;&#23545;&#20598;&#32447;&#24615;&#35268;&#21010;&#25110;Lagrange&#26494;&#24347;&#20013;&#33719;&#24471;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#36825;&#26679;&#30340;&#31639;&#27861;&#21253;&#25324;&#26368;&#22823;&#21644;&#25193;&#25955;&#20197;&#21450;&#39034;&#24207;&#26641;&#37325;&#26032;&#21152;&#26435;&#28040;&#24687;&#20256;&#36882;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#30446;&#21069;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#20250;&#25910;&#25947;&#21040;&#30001;&#27963;&#36291;&#32422;&#26463;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#25152;&#34920;&#24449;&#30340;&#38598;&#21512;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26410;&#30693;&#65307;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#36845;&#20195;&#26159;&#21542;&#20250;&#25910;&#25947;&#65288;&#21040;&#20219;&#20309;&#19968;&#20010;&#21333;&#19968;&#28857;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#32467;&#26524;&#65288;&#20043;&#21069;&#26377;&#29468;&#24819;&#20294;&#20174;&#26410;&#35777;&#26126;&#36807;&#65289;&#65306;&#36845;&#20195;&#20250;&#25910;&#25947;&#21040;&#31639;&#27861;&#30340;&#19968;&#20010;&#22266;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#23427;&#20204;&#22312;$\mathcal{O}(1/\varepsilon)$&#27425;&#36845;&#20195;&#20013;&#36798;&#21040;&#20102;&#31934;&#24230;$\varepsilon&gt;0$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07004v1 Announce Type: new  Abstract: A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon&gt;0$ in $\mathcal{O}(1/\varepsilon)$ iterations.   We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel p
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02419</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#26356;&#22810;LLM&#35843;&#29992;&#21527;&#65311;&#36208;&#21521;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#26159;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#29992;&#24182;&#27719;&#24635;&#23427;&#20204;&#30340;&#21709;&#24212;&#30340;&#22797;&#21512;&#31995;&#32479;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#24433;&#21709; -- &#20363;&#22914;&#65292;&#24403;&#35201;&#27714;LLM&#22810;&#27425;&#22238;&#31572;&#27599;&#20010;&#38382;&#39064;&#24182;&#21462;&#24471;&#20849;&#35782;&#26102; -- &#23545;&#20110;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLM&#35843;&#29992;&#27425;&#25968;&#22914;&#20309;&#24433;&#21709;&#19968;&#20010;&#23618;&#32423;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021; -- &#36825;&#26159;&#26368;&#31616;&#21333;&#30340;&#22797;&#21512;&#31995;&#32479;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32858;&#21512;LLM&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38750;&#21333;&#35843;&#24615;&#26159;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19009</link><description>&lt;p&gt;
&#29983;&#25104;&#12289;&#37325;&#24314;&#21644;&#34920;&#31034;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#65306;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19009
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#19978;&#26080;&#32541;&#25972;&#21512;&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#23478;&#26063;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#22522;&#20110;&#19977;&#39033;&#26680;&#24515;&#33021;&#21147;--&#29983;&#25104;&#26032;&#23454;&#20363;&#12289;&#37325;&#24314;&#36755;&#20837;&#21644;&#23398;&#20064;&#32039;&#20945;&#34920;&#31034;--&#36328;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#31163;&#25955;&#25991;&#26412;/&#34507;&#30333;&#24207;&#21015;&#21644;&#36830;&#32493;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#23478;&#26063;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#29305;&#23450;&#33021;&#21147;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#21487;&#23398;&#20064;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#24191;&#20041;&#25193;&#25955;&#65288;DiLED&#65289;&#65292;&#23427;&#26080;&#32541;&#22320;&#38598;&#25104;&#20102;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;DiLED&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#21270;&#32534;&#30721;-&#35299;&#30721;&#26469;&#23558;&#26631;&#20934;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#21152;&#22122;-&#21435;&#22122;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#20851;&#38190;&#26159;&#65292;DiLED&#19982;&#25104;&#29087;&#30340;&#25193;&#25955;&#27169;&#22411;&#30446;&#26631;&#21644;&#35757;&#32451;&#26041;&#27861;&#20860;&#23481;&#65292;&#21487;&#26377;&#25928;&#23398;&#20064;&#32534;&#30721;-&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19009v1 Announce Type: cross  Abstract: The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17811</link><description>&lt;p&gt;
TruthX: &#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#22238;&#24212;&#65292;&#23613;&#31649;&#25317;&#26377;&#27491;&#30830;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TruthX&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLMs&#20869;&#37096;&#34920;&#31034;&#20197;&#33719;&#21462;&#20854;&#30495;&#23454;&#24615;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#12290;TruthX&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;LLM&#30340;&#34920;&#31034;&#20998;&#21035;&#26144;&#23556;&#21040;&#35821;&#20041;&#21644;&#30495;&#23454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#35782;&#21035;&#30495;&#23454;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLM&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;TruthX&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;LLMs&#30340;&#30495;&#23454;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TruthX&#36890;&#36807;20%&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;LLMs&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30495;&#23454;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17811v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17768</link><description>&lt;p&gt;
&#25193;&#25955;&#36935;&#35265;DAgger: &#36229;&#32423;&#30524;&#22312;&#25163;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Diffusion Meets DAgger (DMD)&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#20986;&#29616;&#22312;&#19987;&#23478;&#28436;&#31034;&#20013;&#30340;&#29366;&#24577;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17768v1 Announce Type: cross  Abstract: A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17606</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#25299;&#25169;&#34920;&#31034;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#29616;&#25104;GNN&#27169;&#22411;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#24182;&#21457;&#22270;&#65288;DGs&#65289;&#30340;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#25628;&#32034;&#26694;&#26550;&#20013;&#23884;&#20837;DG&#20197;&#35299;&#20915;JSSP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TBGAT&#20998;&#21035;&#20174;&#27491;&#21521;&#21644;&#21453;&#21521;&#35270;&#22270;&#23884;&#20837;DG&#65292;&#28040;&#24687;&#36890;&#36807;&#36981;&#24490;&#19981;&#21516;&#35270;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#27719;&#24635;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#26032;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#35745;&#31639;DG&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#25299;&#25169;&#25490;&#24207;&#65292;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#34987;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;TBGAT&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20256;&#32479;&#30340;&#21463;&#38480;&#35780;&#20272;&#33539;&#24335;&#65292;&#25506;&#32034;&#26356;&#30495;&#23454;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16786</link><description>&lt;p&gt;
&#25919;&#27835;&#32599;&#30424;&#25110;&#26059;&#36716;&#31661;&#65311;&#26397;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#26356;&#26377;&#24847;&#20041;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16786
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20256;&#32479;&#30340;&#21463;&#38480;&#35780;&#20272;&#33539;&#24335;&#65292;&#25506;&#32034;&#26356;&#30495;&#23454;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#35843;&#26597;&#21644;&#38382;&#21367;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#30340;&#21160;&#26426;&#26159;&#28304;&#20110;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;LLM&#24212;&#29992;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#27880;&#19982;&#24403;&#21069;&#35780;&#20272;&#30340;&#20154;&#20026;&#24615;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65306;&#30495;&#23454;&#29992;&#25143;&#36890;&#24120;&#19981;&#20250;&#21521;LLMs&#25552;&#20986;&#35843;&#26597;&#38382;&#39064;&#12290;&#21463;&#21040;&#36825;&#31181;&#24046;&#24322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#30446;&#21069;&#23545;LLMs&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#32422;&#26463;&#35780;&#20272;&#33539;&#24335;&#65292;&#24182;&#25506;&#32034;&#20102;&#26356;&#29616;&#23454;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24191;&#21463;&#27426;&#36814;&#30340;&#25919;&#27835;&#32599;&#30424;&#27979;&#35797;&#65288;PCT&#65289;&#12290;&#22312;&#19968;&#20010;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#20808;&#21069;&#20351;&#29992;PCT&#30340;&#24037;&#20316;&#37117;&#24378;&#21046;&#27169;&#22411;&#36981;&#23432;PCT&#30340;&#22810;&#39033;&#36873;&#25321;&#26684;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#19981;&#34987;&#24378;&#21046;&#26102;&#65292;&#27169;&#22411;&#32473;&#20986;&#30340;&#31572;&#26696;&#23454;&#36136;&#19978;&#26159;&#19981;&#21516;&#30340;&#65307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16786v1 Announce Type: new  Abstract: Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers cha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15152</link><description>&lt;p&gt;
&#20851;&#20110;Sharpness-Aware&#26368;&#23567;&#21270;&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Duality Between Sharpness-Aware Minimization and Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(Adversarial Training, AT)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#34987;&#35748;&#20026;&#26159;&#23545;&#25239;&#25915;&#20987;&#20013;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#20043;&#19968;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#19968;&#31181;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#24517;&#28982;&#20250;&#38477;&#20302;&#24178;&#20928;&#20934;&#30830;&#24615;&#12290;&#19982;&#23545;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#19981;&#21516;&#65292;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#65292;&#20197;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26354;&#38754;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#26088;&#22312;&#25552;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#65292;&#20854;&#22312;&#22686;&#24378;&#23545;&#25239;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;SAM&#21644;AT&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20174;SAM&#20013;&#27966;&#29983;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#29702;&#35299;SAM&#30340;&#36825;&#31181;&#24847;&#22806;&#29305;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;SAM&#22914;&#20309;&#38544;&#24335;&#23398;&#20064;&#26356;&#40065;&#26834;&#29305;&#24449;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#35265;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15152v1 Announce Type: cross  Abstract: Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive exper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.14883</link><description>&lt;p&gt;
&#21452;I&#27700;&#21360;&#65306;&#20445;&#25252;LLM&#24494;&#35843;&#27169;&#22411;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#65292;&#19994;&#20027;&#32463;&#24120;&#36890;&#36807;LLM&#25152;&#26377;&#32773;&#25110;&#20113;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;API&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#21462;&#23450;&#21046;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#23384;&#22312;&#30528;&#27169;&#22411;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#20250;&#32473;&#19994;&#20027;&#24102;&#26469;&#20005;&#37325;&#30340;&#32463;&#27982;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#20445;&#25252;&#36825;&#20123;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#24050;&#25104;&#20026;&#32039;&#36843;&#30340;&#23454;&#38469;&#38656;&#27714;&#65292;&#20294;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32039;&#36843;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22522;&#20110;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#65292;&#20998;&#21035;&#22312;&#25351;&#20196;&#21644;&#36755;&#20837;&#20013;&#35302;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#23558;&#23450;&#21046;&#30340;&#21518;&#38376;&#26679;&#26412;&#32435;&#20837;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#27880;&#20837;&#20102;&#29305;&#23450;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#38544;&#31192;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#20869;&#23481;&#22312;&#19981;&#24178;&#39044;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#29289;&#21697;&#30340;&#26333;&#20809;&#24230;&#65292;&#32780;&#36825;&#31181;&#25915;&#20987;&#23545;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#26080;&#24433;&#21709;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.14836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#38544;&#31192;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Stealthy Attack on Large Language Model based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14836
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#38544;&#31192;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#20869;&#23481;&#22312;&#19981;&#24178;&#39044;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#29289;&#21697;&#30340;&#26333;&#20809;&#24230;&#65292;&#32780;&#36825;&#31181;&#25915;&#20987;&#23545;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#26080;&#24433;&#21709;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25512;&#21160;&#25512;&#33616;&#31995;&#32479;(RS)&#30340;&#36827;&#23637;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#23433;&#20840;&#23041;&#32961;&#30340;&#25935;&#24863;&#24615;&#21364;&#34987;&#22823;&#22810;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLMs&#24341;&#20837;&#25512;&#33616;&#27169;&#22411;&#20013;&#20135;&#29983;&#26032;&#23433;&#20840;&#28431;&#27934;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#27880;&#37325;&#29289;&#21697;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#27979;&#35797;&#38454;&#27573;&#20165;&#36890;&#36807;&#25913;&#21464;&#29289;&#21697;&#30340;&#25991;&#26412;&#20869;&#23481;&#26174;&#33879;&#22686;&#21152;&#20854;&#26333;&#20809;&#24230;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#24178;&#39044;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#38544;&#31192;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#24433;&#21709;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#65292;&#23545;&#25991;&#26412;&#30340;&#20462;&#25913;&#24494;&#22937;&#65292;&#20351;&#29992;&#25143;&#21644;&#24179;&#21488;&#38590;&#20197;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20027;&#27969;&#30340;LLM-based&#25512;&#33616;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14836v1 Announce Type: cross  Abstract: Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;</title><link>https://arxiv.org/abs/2402.12336</link><description>&lt;p&gt;
Robust CLIP: &#23545;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12336
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenFlamingo&#12289;LLaVA&#21644;GPT-4&#20043;&#31867;&#30340;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35270;&#35273;&#27169;&#24577;&#19978;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#29992;&#26469;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#39575;&#29992;&#25143;&#65292;&#22240;&#27492;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#39118;&#38505;&#65292;&#36825;&#20351;&#24471;&#22823;&#22411;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#25104;&#20026;&#19968;&#39033;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#22312;&#25152;&#26377;&#20381;&#36182;&#20110;CLIP&#30340;&#35270;&#35273;&#19979;&#28216;&#20219;&#21153;&#65288;VLMs&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#65289;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26086;&#26356;&#25442;&#21407;&#22987;&#30340;CLIP&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#20351;&#29992;VLMs&#26102;&#20250;&#21463;&#21040;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#30340;&#25805;&#32437;&#22270;&#20687;&#30340;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
&lt;/p&gt;</description></item><item><title>LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.12061</link><description>&lt;p&gt;
&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#37117;&#19968;&#26679;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
All Language Models Large and Small
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12061
&lt;/p&gt;
&lt;p&gt;
LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#24378;&#24230;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#38477;&#20302;&#37096;&#32626;&#36164;&#28304;&#25104;&#26412;&#21644;&#26356;&#24555;&#25191;&#34892;&#20915;&#31574;&#20219;&#21153;&#31561;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#20248;&#21270;&#32593;&#32476;&#20998;&#24067;&#65288;LONDI&#65289;&#26694;&#26550;&#30340;&#26032;&#22411;&#21363;&#25554;&#21363;&#29992;LM&#26694;&#26550;&#12290; LONDI&#23398;&#20250;&#20102;&#22312;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;LM&#65292;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#20302;&#36164;&#28304;&#30340;LM&#12290; LONDI&#30001;&#20004;&#20010;&#65288;&#31163;&#32447;&#65289;&#31574;&#30053;&#32593;&#32476;&#31995;&#32479;&#12289;&#19968;&#20010;LM&#12289;&#19968;&#20010;&#22823;&#30340;LM&#65288;LLM)&#21644;&#19968;&#20010;&#20351;&#29992;&#24320;&#20851;&#25511;&#21046;&#24555;&#36895;&#23398;&#20064;&#20309;&#26102;&#35843;&#29992;LLM&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;LLM&#35843;&#29992;&#21644;&#36164;&#28304;&#20351;&#29992;&#26041;&#38754;&#20445;&#25345;&#39044;&#31639;&#32422;&#26463;&#30340;LONDI&#21464;&#20307;&#12290; &#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LONDI&#23398;&#20064;&#28608;&#27963;&#25152;&#38656;&#35299;&#20915;&#20219;&#21153;&#30340;LLM&#30340;&#31995;&#32479;&#29366;&#24577;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;</title><link>https://arxiv.org/abs/2402.11168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Trust Regions for Explanations via Black-Box Probabilistic Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#35299;&#26512;&#20010;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40657;&#30418;&#65288;&#27010;&#29575;&#24615;&#65289;&#35299;&#37322;&#35748;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#65292;&#19968;&#20010;&#31034;&#20363;&#30340;&#35299;&#37322;&#20197;&#21450;&#19968;&#20010;&#36136;&#37327;&#24230;&#37327;&#65288;&#22914;&#36924;&#30495;&#24230;&#12289;&#31283;&#23450;&#24615;&#65289;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#25214;&#21040;&#26368;&#22823;&#30340;&#36229;&#31435;&#26041;&#20307;&#65288;&#21363; $\ell_{\infty}$ &#29699;&#65289;&#65292;&#20197;&#31034;&#20363;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#24403;&#35299;&#37322;&#34987;&#24212;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;&#20869;&#30340;&#25152;&#26377;&#31034;&#20363;&#26102;&#65288;&#39640;&#27010;&#29575;&#19979;&#65289;&#36136;&#37327;&#26631;&#20934;&#24471;&#21040;&#28385;&#36275;&#65288;&#27604;&#22914;&#36924;&#30495;&#24230;&#39640;&#20110;&#26576;&#20010;&#20540;&#65289;&#65311;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#20449;&#20219;&#21306;&#22495;&#26377;&#22810;&#37325;&#22909;&#22788;&#65306;i&#65289;&#27934;&#23519;&#27169;&#22411;&#22312;&#19968;&#20010;&#21306;&#22495;&#20869;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#20445;&#35777;&#65307;ii&#65289;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#24471;&#21040;&#20445;&#35777;&#65307;iii&#65289;&#35299;&#37322;&#30340;&#37325;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#37329;&#38065;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32534;&#36753;&#23618;&#26469;&#21305;&#37197;&#19981;&#21516;&#23618;&#32423;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#24335;&#31243;&#24230;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24179;&#22343;&#23637;&#29616;&#20102;46.2%&#21644;67.8%&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10987</link><description>&lt;p&gt;
WilKE&#65306;&#26234;&#24935;&#23618;&#30693;&#35782;&#32534;&#36753;&#22120;&#29992;&#20110;&#32456;&#36523;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32534;&#36753;&#23618;&#26469;&#21305;&#37197;&#19981;&#21516;&#23618;&#32423;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#24335;&#31243;&#24230;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24179;&#22343;&#23637;&#29616;&#20102;46.2%&#21644;67.8%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#20026;&#36807;&#26102;&#25110;&#38169;&#35823;&#30340;&#30693;&#35782;&#36827;&#34892;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#21333;&#27425;&#32534;&#36753;&#65292;&#26410;&#33021;&#28385;&#36275;&#32456;&#36523;&#32534;&#36753;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#32456;&#36523;&#32534;&#36753;&#19982;&#32456;&#36523;&#30693;&#35782;&#32534;&#36753;&#21516;&#20041;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#30693;&#35782;&#32534;&#36753;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#29305;&#24449;&#20026;&#27602;&#24615;&#31215;&#32047;&#21644;&#27602;&#24615;&#38378;&#29616;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#27169;&#24335;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#23427;&#26681;&#25454;&#19981;&#21516;&#23618;&#32423;&#20013;&#32534;&#36753;&#30693;&#35782;&#30340;&#27169;&#24335;&#21305;&#37197;&#31243;&#24230;&#36873;&#25321;&#32534;&#36753;&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;WilKE&#22312;&#32534;&#36753;GPT2-XL&#21644;GPT-J&#26041;&#38754;&#20998;&#21035;&#24179;&#22343;&#25913;&#36827;&#20102;46.2%&#21644;67.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10987v1 Announce Type: cross  Abstract: Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. In this paper, lifelong editing is synonymous with lifelong knowledge editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10024</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#23545;&#20110;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Augmented In-Context Learning for Unsupervised Word Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#35774;&#32622;&#20013;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#35789;&#27719;&#32763;&#35793;&#21644;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;(BLI)&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27809;&#26377;&#31181;&#23376;&#32763;&#35793;&#23545;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#8220;&#20256;&#32479;&#8221;&#30340;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861; (SAIL) &#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65306;&#20174;&#38646;&#26679;&#26412;&#25552;&#31034;&#24320;&#22987;&#65292;SAIL&#36890;&#36807;&#36845;&#20195;&#22320;&#20174;LLM&#20013;&#24341;&#20986;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#30340;&#35789;&#27719;&#32763;&#35793;&#23545;&#65292;&#28982;&#21518;&#22312;ICL&#30340;&#26041;&#24335;&#19979;&#20877;&#27425;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#30340;BLI&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;LLM&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20063;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26144;&#23556;&#30340;&#22522;&#32447;&#12290;&#38500;&#20102;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#65292;&#23558;&#20154;&#31867;&#31227;&#21160;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#20026;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31227;&#21160;&#29983;&#25104;&#25512;&#29702;&#26694;&#26550;&#65288;MobiGeaR&#65289;&#65292;&#23558;LLMs&#36882;&#24402;&#29983;&#25104;&#31227;&#21160;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.09836</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#20223;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#65292;&#23558;&#20154;&#31867;&#31227;&#21160;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#20026;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31227;&#21160;&#29983;&#25104;&#25512;&#29702;&#26694;&#26550;&#65288;MobiGeaR&#65289;&#65292;&#23558;LLMs&#36882;&#24402;&#29983;&#25104;&#31227;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#34892;&#20026;&#19982;&#20132;&#36890;&#25317;&#22581;&#12289;&#30123;&#24773;&#25511;&#21046;&#31561;&#37325;&#35201;&#31038;&#20250;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#31227;&#21160;&#24615;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#28041;&#21450;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#24615;&#31227;&#21160;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#20026;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#26032;&#30340;&#31227;&#21160;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#22320;&#25429;&#25417;&#39537;&#21160;&#31227;&#21160;&#34892;&#20026;&#30340;&#36830;&#36143;&#24847;&#22270;&#65292;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#21644;&#35821;&#20041;&#24863;&#30693;&#24230;&#20302;&#12290;&#21463;&#21040;LLMs&#20013;&#26032;&#20852;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#26412;&#30340;&#35270;&#35282;&#36716;&#21464;&#65292;&#23558;&#31227;&#21160;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#20026;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31227;&#21160;&#29983;&#25104;&#25512;&#29702;&#65288;MobiGeaR&#65289;&#26694;&#26550;&#65292;&#20419;&#20351;LLMs&#36882;&#24402;&#29983;&#25104;&#31227;&#21160;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09836v1 Announce Type: new  Abstract: Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aw
&lt;/p&gt;</description></item><item><title>&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09656</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#23569;&#37327;&#32534;&#36753;&#21487;&#33021;&#24341;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09656
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#24050;&#26174;&#31034;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23545;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#30340;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#29616;&#35937;&#65306;&#21363;&#20351;&#21482;&#36827;&#34892;&#19968;&#20010;&#32534;&#36753;&#65292;&#20063;&#21487;&#20197;&#24341;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#34920;&#29616;&#20026;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#27425;&#32534;&#36753;&#21518;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#34429;&#28982;&#24517;&#35201;&#65292;&#20294;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#39034;&#24207;&#32534;&#36753;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#26159;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25105;&#20204;&#20043;&#21069;&#21333;&#27425;&#32534;&#36753;&#30740;&#31350;&#20013;&#30340;&#22256;&#38590;&#26696;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20960;&#20046;&#25152;&#26377;&#30740;&#31350;&#30340;&#32534;&#36753;&#26041;&#27861;&#37117;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08848</link><description>&lt;p&gt;
&#28151;&#21512;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#27169;&#20223;&#23398;&#20064;&#26469;&#35828;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#27604;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#22320;&#22788;&#29702;&#38169;&#35823;&#32047;&#31215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#35201;&#27714;&#23398;&#20064;&#32773;&#21453;&#22797;&#35299;&#20915;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35745;&#31639;&#24448;&#24448;&#20250;&#28010;&#36153;&#22312;&#25628;&#32034;&#38750;&#24120;&#19981;&#30456;&#20284;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#31574;&#30053;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;-&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;-&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#19978;&#65292;&#19987;&#23478;&#25968;&#25454;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#23398;&#20064;&#32773;&#19987;&#27880;&#20110;&#33391;&#22909;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#24378;&#31574;&#30053;&#25152;&#38656;&#30340;&#25506;&#32034;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#23558;&#23398;&#20064;&#32773;&#37325;&#32622;&#21040;&#29615;&#22659;&#20013;&#30340;&#20219;&#24847;&#29366;&#24577;&#65292;&#36825;&#26159;&#20197;&#21069;&#22312;&#39640;&#25928;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08848v1 Announce Type: cross Abstract: The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formal
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;&#30340;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#24369;Webshell&#26679;&#26412;&#36867;&#36920;&#33021;&#21147;&#21644;&#32570;&#20047;&#22797;&#26434;&#24694;&#24847;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07408</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#32773;&#65306;&#25552;&#20986;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#20197;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07408
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;&#30340;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#24369;Webshell&#26679;&#26412;&#36867;&#36920;&#33021;&#21147;&#21644;&#32570;&#20047;&#22797;&#26434;&#24694;&#24847;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#32321;&#30340;&#32593;&#32476;&#25915;&#20987;&#20351;&#24471;Webshell&#25915;&#20987;&#21644;&#38450;&#24481;&#36880;&#28176;&#25104;&#20026;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#20197;&#21450;&#23545;Webshell&#36867;&#36920;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#36807;&#24230;&#20381;&#36182;&#25163;&#21160;&#23450;&#20041;&#35268;&#21017;&#38480;&#21046;&#20102;&#19982;Webshell&#36867;&#36920;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;&#21644;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;Webshell&#26816;&#27979;&#31639;&#27861;&#30456;&#20851;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#24369;Webshell&#26679;&#26412;&#36867;&#36920;&#33021;&#21147;&#30340;&#19981;&#36275;&#12289;&#32570;&#20047;&#20855;&#26377;&#22797;&#26434;&#24694;&#24847;&#29305;&#24449;&#30340;Webshell&#25968;&#25454;&#38598;&#20197;&#21450;&#25512;&#21160;Webshell&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;Webshell&#36867;&#36920;&#26679;&#26412;&#30340;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#12290;&#20316;&#20026;&#19987;&#38376;&#29992;&#20110;Webshell&#26679;&#26412;&#29983;&#25104;&#30340;&#25552;&#31034;&#31639;&#27861;&#65292;&#28151;&#21512;&#25552;&#31034;&#31639;&#27861;&#19981;&#20165;&#32467;&#21512;&#20102;&#21508;&#31181;&#25552;&#31034;&#24605;&#36335;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#65292;&#36824;&#34701;&#21512;&#20102;&#21508;&#31181;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various component
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01922</link><description>&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning from Weak Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#31639;&#27861;&#20174;&#21508;&#31181;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#26469;&#31616;&#21270;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#30528;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#24369;&#30417;&#30563;&#30340;&#21508;&#31181;&#22330;&#26223;&#21644;&#30001;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20174;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;GLWS&#65289;&#12290;GLWS&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#30340;&#20844;&#24335;&#65292;&#28789;&#27963;&#22320;&#36866;&#24212;&#20102;&#21508;&#31181;&#24369;&#30417;&#30563;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#12289;&#32858;&#21512;&#32479;&#35745;&#12289;&#25104;&#23545;&#35266;&#23519;&#21644;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38750;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;NFA&#65289;&#20197;&#21450;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;EM&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#24120;&#25152;&#38656;&#30340;&#20108;&#27425;&#25110;&#38454;&#20056;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#20174;&#20219;&#24847;&#24369;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#20102;&#23545;&#23427;&#20204;&#36827;&#34892;NFA&#24314;&#27169;&#12290;GLWS&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;+
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.00396</link><description>&lt;p&gt;
LLMs&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00396
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#39640;&#25928;&#25506;&#32034;&#22312;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#31243;&#24207;&#22312;&#25910;&#21040;&#21453;&#39304;&#26102;&#23558;&#22870;&#21169;&#27169;&#22411;&#25311;&#21512;&#21040;&#26597;&#35810;&#19978;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#21452;Thompson&#37319;&#26679;&#29983;&#25104;&#26597;&#35810;&#65292;&#19981;&#30830;&#23450;&#24615;&#30001;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#25928;&#25506;&#32034;&#20351;&#24471;&#24615;&#33021;&#27700;&#24179;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26597;&#35810;&#19979;&#36798;&#21040;&#36739;&#39640;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.12192</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#21453;&#21521;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text Embedding Inversion Security for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#20197;&#23454;&#25968;&#23884;&#20837;&#34920;&#31034;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23884;&#20837;&#24335;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23558;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20026;&#23884;&#20837;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#28431;&#27934;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#30693;&#36947;&#24213;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25991;&#26412;&#20063;&#21487;&#20197;&#20174;&#23884;&#20837;&#20013;&#37325;&#26500;&#12290;&#23613;&#31649;&#24050;&#32463;&#25506;&#35752;&#20102;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#20351;&#20854;&#20182;&#35821;&#35328;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#23884;&#20837;&#36870;&#36716;&#25506;&#35752;&#20102;LLM&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#40657;&#30418;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#36870;&#36716;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LLMs&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#33521;&#35821;&#30340;&#38450;&#24481;&#21487;&#33021;&#26080;&#25928;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#65292;&#23545;b&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12192v2 Announce Type: replace-cross  Abstract: Textual data is often represented as realnumbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be vulnerable to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages vulnerable to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and thoroughly explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.10537</link><description>&lt;p&gt;
MedAgents: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#21307;&#23398;&#25512;&#29702;&#30340;&#21512;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23613;&#31649;&#22312;&#21508;&#31181;&#36890;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#21253;&#25324;&#20116;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#25910;&#38598;&#39046;&#22495;&#19987;&#23478;&#12289;&#25552;&#20986;&#20010;&#21035;&#20998;&#26512;&#12289;&#23558;&#36825;&#20123;&#20998;&#26512;&#24635;&#32467;&#25104;&#25253;&#21578;&#12289;&#22312;&#35752;&#35770;&#20013;&#21453;&#22797;&#36845;&#20195;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#65292;&#26368;&#32456;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#38646;-shot&#24773;&#26223;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#20061;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09886</link><description>&lt;p&gt;
&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19979;&#19968;&#20195;&#32593;&#32476;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#32531;&#23384;&#26159;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36171;&#20104;&#23567;&#22411;&#22522;&#31449;&#65288;SBS&#65289;&#20013;&#30340;&#32531;&#23384;&#21333;&#20803;&#36171;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#33719;&#21462;&#24050;&#22312;SBS&#20013;&#39044;&#32531;&#23384;&#30340;&#29992;&#25143;&#35831;&#27714;&#20869;&#23481;&#12290;&#23545;&#20110;SBS&#26469;&#35828;&#65292;&#36890;&#36807;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#38750;&#24120;&#20851;&#38190;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#20010;&#20154;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#20294;&#26159;UE&#20043;&#38388;&#30340;&#25968;&#25454;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20026;&#27599;&#20010;UE&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#20197;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30456;&#37051;SBS&#20043;&#38388;&#21487;&#20197;&#20849;&#20139;&#32531;&#23384;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#22312;&#19981;&#21516;&#30340;SBS&#20013;&#32531;&#23384;&#39044;&#27979;&#21040;&#30340;&#28909;&#38376;&#20869;&#23481;&#21487;&#33021;&#20250;&#24433;&#21709;&#33719;&#21462;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#30340;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and mu
&lt;/p&gt;</description></item><item><title>&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06751</link><description>&lt;p&gt;
Easy Training Data&#23545;&#20110;&#22256;&#38590;&#20219;&#21153;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06751
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#22312;&#23450;&#20041;&#19978;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#22312;&#22256;&#38590;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21487;&#25193;&#23637;&#30417;&#30563;&#38382;&#39064;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#36807;&#31243;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#35770;&#65292;&#21363;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#30456;&#23545;&#33391;&#22909;&#65292;&#29978;&#33267;&#34920;&#29616;&#24471;&#21644;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#8220;oracle&#8221;&#27169;&#22411;&#19968;&#26679;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#32447;&#24615;&#20998;&#31867;&#22120;&#22836;&#21644;QLoRA&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#20174;&#26131;&#21040;&#38590;&#30340;&#27867;&#21270;&#65292;&#38024;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#38590;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20845;&#20010;&#32463;&#39564;&#22810;&#26679;&#30340;&#20154;&#31867;&#38590;&#24230;&#24230;&#37327;&#65288;&#22914;&#24180;&#32423;&#27700;&#24179;&#65289;&#21644;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#65288;&#22522;&#20110;&#25439;&#22833;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#20851;&#24515;&#27169;&#22411;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#25910;&#38598;&#24182;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#65292;&#22240;&#20026;&#22256;&#38590;&#25968;&#25454;&#36890;&#24120;&#26356;&#22024;&#26434;&#21644;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.14187</link><description>&lt;p&gt;
WaveCoder: &#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#19982;&#23436;&#21892;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23545;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#21518;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#32463;&#24120;&#20250;&#20135;&#29983;&#37325;&#22797;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#25511;&#21046;&#19981;&#22815;&#28789;&#27963;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#20026;4&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#22120;-&#21028;&#21035;&#22120;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#20174;&#24320;&#28304;&#20195;&#30721;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeOcean&#65292;&#19968;&#20010;&#21253;&#21547;4&#20010;&#36890;&#29992;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#12289;&#20849;&#35745;20,000&#20010;&#25351;&#20196;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#25351;&#20196;&#35843;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#30340;Code LLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00582</link><description>&lt;p&gt;
&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20197;&#23454;&#29616;&#20219;&#24847;Nash&#22343;&#34913;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20301;&#21892;&#24847;&#30340;&#28216;&#25103;&#35774;&#35745;&#32773;&#25110;&#24694;&#24847;&#30340;&#23545;&#25163;&#20462;&#25913;&#20102;&#19968;&#20010;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20415;&#19968;&#20010;&#30446;&#26631;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#30340;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;Nash&#22343;&#34913;&#65292;&#24182;&#19988;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#20855;&#26377;&#20215;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#33021;&#22815;&#23433;&#35013;&#20026;&#26576;&#20010;&#28216;&#25103;&#30340;&#21807;&#19968;&#22343;&#34913;&#30340;&#31574;&#30053;&#37197;&#32622;&#30340;&#38598;&#21512;&#65292;&#24182;&#24314;&#31435;&#20102;&#25104;&#21151;&#23433;&#35013;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#19968;&#20010;&#24102;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#65292;&#26469;&#33719;&#24471;&#19968;&#20010;&#25104;&#26412;&#36817;&#20046;&#26368;&#20248;&#30340;&#20462;&#25913;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08631</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24515;&#29702;&#20542;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08631
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#23558;&#25104;&#20026;&#20010;&#24615;&#21270;&#25216;&#26415;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;ChatGPT&#30340;LLMs&#20174;&#20010;&#20154;&#25968;&#23383;&#36275;&#36857;&#20013;&#25512;&#26029;&#20010;&#20154;&#24515;&#29702;&#20542;&#21521;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#20174;&#29992;&#25143;&#30340;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#23548;&#20986;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;LLM&#25512;&#26029;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20026;r = 0.29&#65288;&#33539;&#22260;&#20026;[0.22, 0.33]&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20010;&#24615;&#25512;&#26029;&#30340;&#20559;&#35265;&#65306;&#23545;&#20110;&#20960;&#20010;&#29305;&#36136;&#65292;&#25512;&#26029;&#24471;&#20998;&#22312;&#22899;&#24615;&#21644;&#24180;&#36731;&#20154;&#20013;&#30340;&#35823;&#24046;&#36739;&#23567;&#65292;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#26469;&#33258;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#25110;&#22312;&#32447;&#33258;&#25105;&#21576;&#29616;&#30340;&#24046;&#24322;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.02870</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#37325;&#26032;&#24605;&#32771;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Rethinking Momentum Knowledge Distillation in Online Continual Learning. (arXiv:2309.02870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#20986;&#29616;&#12290;&#19982;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;OCL&#20013;&#21482;&#33021;&#30475;&#21040;&#25968;&#25454;&#19968;&#27425;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#31574;&#30053;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#23427;&#20204;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#22312;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;OCL&#20013;&#20173;&#28982;&#26410;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23558;KD&#24212;&#29992;&#20110;OCL&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#65288;MKD&#65289;&#24212;&#29992;&#20110;&#35768;&#22810;&#26071;&#33328;OCL&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#23558;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;ImageNet100&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#26126;&#20102;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that 
&lt;/p&gt;</description></item><item><title>TempFuser&#26159;&#19968;&#31181;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#24182;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.03257</link><description>&lt;p&gt;
TempFuser: &#20351;&#29992;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer. (arXiv:2308.03257v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03257
&lt;/p&gt;
&lt;p&gt;
TempFuser&#26159;&#19968;&#31181;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#24182;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31354;&#20013;&#25112;&#26007;&#20013;&#65292;&#31354;&#25112;&#21160;&#20316;&#23545;&#25112;&#26415;&#26426;&#21160;&#21644;&#25935;&#25463;&#25112;&#26007;&#26426;&#30340;&#31354;&#27668;&#21160;&#21147;&#23398;&#37117;&#25552;&#20986;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TempFuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#26088;&#22312;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;LSTM&#30340;&#36755;&#20837;&#23884;&#20837;&#26469;&#32534;&#30721;&#38271;&#26399;&#31232;&#30095;&#21644;&#30701;&#26399;&#23494;&#38598;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#23884;&#20837;&#36890;&#36807;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25429;&#33719;&#20102;&#25112;&#26007;&#26426;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#31471;&#21040;&#31471;&#30340;&#39134;&#34892;&#25351;&#20196;&#65292;&#30830;&#20445;&#21344;&#25454;&#20248;&#21183;&#20301;&#32622;&#24182;&#36229;&#36234;&#23545;&#25163;&#12290;&#32463;&#36807;&#23545;&#39640;&#20445;&#30495;&#39134;&#34892;&#27169;&#25311;&#22120;&#20013;&#22810;&#31181;&#31867;&#22411;&#23545;&#25163;&#39134;&#26426;&#30340;&#24191;&#27867;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25191;&#34892;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#19988;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In aerial combat, dogfighting poses intricate challenges that demand an understanding of both strategic maneuvers and the aerodynamics of agile fighter aircraft. In this paper, we introduce TempFuser, a novel long short-term temporal fusion transformer designed to learn tactical and agile flight maneuvers in aerial dogfights. Our approach employs two distinct LSTM-based input embeddings to encode long-term sparse and short-term dense state representations. By integrating these embeddings through a transformer encoder, our model captures the tactics and agility of fighter jets, enabling it to generate end-to-end flight commands that secure dominant positions and outmaneuver the opponent. After extensive training against various types of opponent aircraft in a high-fidelity flight simulator, our model successfully learns to perform complex fighter maneuvers, consistently outperforming several baseline models. Notably, our model exhibits human-like strategic maneuvers even when facing adv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03175</link><description>&lt;p&gt;
&#25512;&#24320;&#32511;&#33394;&#65306;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#26893;&#29289;&#21494;&#29255;&#32972;&#21518;&#30340;&#20869;&#23481;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20892;&#19994;&#24212;&#29992;&#65288;&#20363;&#22914;&#26816;&#26597;&#12289;&#34920;&#22411;&#20998;&#26512;&#12289;&#37319;&#25688;&#27700;&#26524;&#65289;&#38656;&#35201;&#25805;&#20316;&#26893;&#29289;&#21494;&#29255;&#20197;&#26597;&#30475;&#21494;&#23376;&#21644;&#26525;&#24178;&#30340;&#32972;&#21518;&#12290;&#37096;&#20998;&#21487;&#35265;&#24615;&#12289;&#26497;&#31471;&#26434;&#20081;&#12289;&#34180;&#32467;&#26500;&#20197;&#21450;&#26893;&#29289;&#30340;&#26410;&#30693;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#37117;&#20351;&#24471;&#36825;&#31181;&#25805;&#20316;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35757;&#32451;SRPNet&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#39044;&#27979;&#22312;&#32473;&#23450;&#26893;&#29289;&#19978;&#25191;&#34892;&#20505;&#36873;&#21160;&#20316;&#26102;&#20250;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#20132;&#21449;&#29109;&#26041;&#27861;&#30340;SRPNet&#26469;&#39044;&#27979;&#26377;&#25928;&#22320;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#30340;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;SRPNet&#19981;&#20165;&#39044;&#27979;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#65292;&#36824;&#39044;&#27979;&#26174;&#38706;&#20986;&#31354;&#38388;&#30340;&#20301;&#32622;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#30340;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#12290;&#22312;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#30340;&#34276;&#34067;&#21644;&#30495;&#23454;&#26893;&#29289;&#65288;&#40857;&#34880;&#26641;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;5&#20010;&#35774;&#32622;&#65292;&#21253;&#25324;2&#20010;&#27979;&#35797;&#27867;&#21270;&#24615;&#33021;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to
&lt;/p&gt;</description></item><item><title>Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03360</link><description>&lt;p&gt;
Vid2Act&#65306;&#20026;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#28608;&#27963;&#31163;&#32447;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03360
&lt;/p&gt;
&lt;p&gt;
Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#22312;&#32447;&#20219;&#21153;&#25928;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36328;&#22495;&#20013;&#20219;&#21153;&#12289;&#21160;&#24577;&#21644;&#34892;&#20026;&#30340;&#22266;&#26377;&#19981;&#21305;&#37197;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;APV&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#20276;&#38543;&#21160;&#20316;&#35760;&#24405;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#22312;&#28304;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#12289;&#19981;&#28041;&#21450;&#25805;&#20316;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Vid2Act&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#21160;&#20316;&#26465;&#20214;&#21160;&#24577;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#21160;&#20316;&#28436;&#31034;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#19981;&#20165;&#23558;&#19990;&#30028;&#27169;&#22411;&#29992;&#20316;&#34892;&#20026;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#65292;&#36824;&#23558;&#20854;&#29992;&#20316;&#27979;&#37327;&#39046;&#22495;&#30456;&#20851;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36827;&#34892;&#21160;&#24577;&#34920;&#31034;&#20256;&#36755;&#21644;&#31574;&#30053;&#20256;&#36755;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22495;&#36873;&#25321;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#26102;&#38388;&#21464;&#21270;&#30340;&#20219;&#21153;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#30456;&#20284;&#24230;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#22320;&#23558;&#26368;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#21160;&#24577;&#20256;&#36755;&#21040;&#22312;&#32447;&#29615;&#22659;&#65292;&#21644;&#65288;ii&#65289;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25351;&#23548;&#20195;&#29702;&#38598;&#20013;&#25191;&#34892;&#20219;&#21153;&#30456;&#20851;&#30340;&#21160;&#20316;&#12290;&#22312;Atari&#21644;DMControl&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.11755</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#65306;&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25512;&#33616;&#25552;&#20379;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#26159;&#23454;&#29616;&#36879;&#26126;&#19988;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#36755;&#20986;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#30784;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#35299;&#37322;&#30446;&#26631;&#12289;&#35299;&#37322;&#33539;&#22260;&#12289;&#35299;&#37322;&#26679;&#24335;&#21644;&#35299;&#37322;&#26684;&#24335;&#36825;&#22235;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#23457;&#26597;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#35299;&#37322;&#30340;&#25991;&#29486;&#12290;&#35748;&#35782;&#21040;&#21487;&#35270;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20174;&#35299;&#37322;&#24615;&#35270;&#35273;&#26041;&#24335;&#30340;&#35282;&#24230;&#36884;&#24452;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#65292;&#21363;&#20351;&#29992;&#21487;&#35270;&#21270;&#20316;&#20026;&#35299;&#37322;&#30340;&#26174;&#31034;&#26679;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizat
&lt;/p&gt;</description></item><item><title>Perfusion&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#38145;&#23450;&#26032;&#27010;&#24565;&#19982;&#20854;&#19978;&#20301;&#31867;&#21035;&#30340;&#20132;&#21449;&#20851;&#27880;&#38190;&#21644;&#38376;&#25511;&#31209;-1&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#20445;&#25345;&#39640;&#24230;&#20445;&#30495;&#24230;&#12289;&#20801;&#35768;&#21019;&#24847;&#25511;&#21046;&#20197;&#21450;&#22810;&#20010;&#20010;&#24615;&#27010;&#24565;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01644</link><description>&lt;p&gt;
&#38145;&#23450;&#20851;&#38190;&#25490;&#24207;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Key-Locked Rank One Editing for Text-to-Image Personalization. (arXiv:2305.01644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01644
&lt;/p&gt;
&lt;p&gt;
Perfusion&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#38145;&#23450;&#26032;&#27010;&#24565;&#19982;&#20854;&#19978;&#20301;&#31867;&#21035;&#30340;&#20132;&#21449;&#20851;&#27880;&#38190;&#21644;&#38376;&#25511;&#31209;-1&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#20445;&#25345;&#39640;&#24230;&#20445;&#30495;&#24230;&#12289;&#20801;&#35768;&#21019;&#24847;&#25511;&#21046;&#20197;&#21450;&#22810;&#20010;&#20010;&#24615;&#27010;&#24565;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;T2I&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21019;&#20316;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#28789;&#27963;&#24615;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20010;&#24615;&#21270;&#20197;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#35270;&#35273;&#27010;&#24565;&#30456;&#19968;&#33268;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;T2I &#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#38754;&#20020;&#22810;&#20010;&#22256;&#38590;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#20801;&#35768;&#21019;&#24847;&#25511;&#21046;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#35270;&#35273;&#20445;&#30495;&#24230;&#65292;&#23558;&#22810;&#20010;&#20010;&#24615;&#21270;&#27010;&#24565;&#32452;&#21512;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#65292;&#20197;&#21450;&#20445;&#25345;&#23567;&#22411;&#27169;&#22411;&#23610;&#23544;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Perfusion&#8221;&#30340; T2I &#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#30784; T2I &#27169;&#22411;&#30340;&#21160;&#24577;&#31209;-1&#26356;&#26032;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Perfusion &#36890;&#36807;&#23558;&#26032;&#27010;&#24565;&#30340;&#20132;&#21449;&#20851;&#27880;&#38190;&#8220;&#38145;&#23450;&#8221;&#21040;&#20854;&#19978;&#20301;&#31867;&#21035;&#26469;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38376;&#25511;&#31209;-1&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#25511;&#21046;&#25152;&#23398;&#27010;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#32452;&#21512;&#22810;&#20010;&#27010;&#24565;&#12290;&#36825;&#20801;&#35768;&#36816;&#34892;&#26102;&#26377;&#25928;&#22320;&#24179;&#34913;&#35270;&#35273;&#20445;&#30495;&#24230;&#21644;&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that "locks" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-align
&lt;/p&gt;</description></item><item><title>FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.03714</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Flow-Guided Density Ratio Learning. (arXiv:2303.03714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03714
&lt;/p&gt;
&lt;p&gt;
FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#65288;FDRL&#65289;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;DGflow&#20013;&#24341;&#20837;&#30340;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#30340;&#36807;&#26102;&#65288;&#26102;&#38388;&#26080;&#20851;&#65289;&#36817;&#20284;&#65292;&#24182;&#19988;&#36890;&#36807;GAN&#37492;&#21035;&#22120;&#32473;&#20986;&#30340;&#36807;&#26102;&#20272;&#35745;&#22120;&#36817;&#20284;&#20102;&#19981;&#21487;&#35745;&#31639;&#30340;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27604;&#12290;&#22312;&#26679;&#26412;&#32454;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#36275;&#22815;&#65292;&#22240;&#20026;&#27969;&#30340;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#26159;&#30456;&#36817;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#19988;&#36807;&#26102;&#20272;&#35745;&#22120;&#30340;&#26420;&#32032;&#24212;&#29992;&#30001;&#20110;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#22823;&#40511;&#27807;&#32780;&#22833;&#36133;&#12290;FDRL&#25552;&#20986;&#20102;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#20351;&#24471;FDRL&#33021;&#22815;&#29983;&#25104;&#39640;&#36798;$128\times128$&#23610;&#23544;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26799;&#24230;&#27969;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable approach to generative modeling which builds on the stale (time-independent) approximation of the gradient flow of entropy-regularized f-divergences introduced in DGflow. In DGflow, the intractable time-dependent density ratio is approximated by a stale estimator given by a GAN discriminator. This is sufficient in the case of sample refinement, where the source and target distributions of the flow are close to each other. However, this assumption is invalid for generation and a naive application of the stale estimator fails due to the large chasm between the two distributions. FDRL proposes to train a density ratio estimator such that it learns from progressively improving samples during the training process. We show that this simple method alleviates the density chasm problem, allowing FDRL to generate images of dimensions as high as $128\times128$, as well as outperform existing gradient flow baselines on qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.02021</link><description>&lt;p&gt;
&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#24847;&#22270;&#35782;&#21035;&#30456;&#20851;&#30340;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35774;&#35745;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#22270;&#35889;&#20013;&#30340;&#20856;&#22411;&#25361;&#25112;&#65306;&#20026;&#27599;&#20010;&#23545;&#35805;&#36716;&#25240;&#25351;&#23450;&#24847;&#22270;&#26631;&#31614;&#65288;&#24847;&#22270;&#32858;&#31867;&#65289;&#24182;&#22522;&#20110;&#24847;&#22270;&#32858;&#31867;&#26041;&#27861;&#29983;&#25104;&#19968;&#32452;&#24847;&#22270;&#65288;&#24847;&#22270;&#24402;&#32435;&#65289;&#12290;&#25105;&#20204;&#20551;&#35774;&#33258;&#21160;&#24402;&#32435;&#24847;&#22270;&#26377;&#20004;&#20010;&#26174;&#33879;&#22240;&#32032;&#65306;&#65288;1&#65289;&#24847;&#22270;&#26631;&#31614;&#30340;&#32858;&#31867;&#31639;&#27861;&#21644;&#65288;2&#65289;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290; &#25105;&#20204;&#26681;&#25454;DSTC11&#35780;&#20272;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#25104;&#21697;&#32858;&#31867;&#27169;&#22411;&#21644;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35748;&#30495;&#32771;&#34385;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;NMI&#65292;ARI&#65292;F1&#65292;&#20934;&#30830;&#24615;&#21644;&#31034;&#20363;&#35206;&#30422;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Jeiyoon/dstc11-track2&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.
&lt;/p&gt;</description></item></channel></rss>