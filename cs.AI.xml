<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05666</link><description>&lt;p&gt;
&#23384;&#22312;&#23545;&#31216;&#24615;&#21644;&#29366;&#24577;&#25277;&#35937;&#30340;&#25919;&#31574;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#20381;&#38752;&#25277;&#35937;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#24182;&#23558;MDP&#21516;&#24577;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#38024;&#23545;&#25277;&#35937;MDP&#30340;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#23548;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26799;&#24230;&#32467;&#26524;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#22522;&#20110;&#36825;&#20123;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#65292;&#20351;&#29992;&#26494;&#25955;&#21452;&#20223;&#23556;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#29615;&#22659;&#65292;&#20197;&#36827;&#19968;&#27493;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#20316;&#25277;&#35937;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#29615;&#22659;&#20197;&#21450;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
&lt;/p&gt;</description></item><item><title>ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05665</link><description>&lt;p&gt;
ImageBind:&#19968;&#20010;&#20849;&#21516;&#23884;&#20837;&#31354;&#38388;&#32465;&#23450;&#25152;&#26377;&#27169;&#24577;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05665
&lt;/p&gt;
&lt;p&gt;
ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ImageBind&#65292;&#36825;&#26159;&#19968;&#31181;&#36328;&#36234;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#28145;&#24230;&#12289;&#28909;&#20256;&#24863;&#21644;IMU&#25968;&#25454;&#30340;&#20845;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25152;&#26377;&#37197;&#23545;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#36275;&#20197;&#23558;&#36825;&#20123;&#27169;&#24577;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;ImageBind&#21487;&#20197;&#21033;&#29992;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#19982;&#22270;&#20687;&#30340;&#33258;&#28982;&#37197;&#23545;&#65292;&#23558;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#26032;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#29992;&#31639;&#26415;&#32452;&#21512;&#27169;&#24577;&#12289;&#36328;&#27169;&#24577;&#26816;&#27979;&#21644;&#29983;&#25104;&#12290;&#26032;&#22411;&#24212;&#29992;&#38543;&#30528;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24378;&#24230;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#19987;&#23478;&#30417;&#30563;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24378;&#30340;&#20960;&#20309;&#35782;&#21035;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;ImageBind&#25104;&#20026;&#20102;&#35780;&#20272;&#35270;&#35273;&#27169;&#24577;&#32852;&#21512;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
&lt;/p&gt;</description></item><item><title>ShapeCoder&#26159;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22522;&#20803;&#20013;&#21457;&#29616;&#35270;&#35273;&#31243;&#24207;&#30340;&#25277;&#35937;&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#37325;&#20889;&#31243;&#24207;&#20197;&#20351;&#20854;&#26356;&#32039;&#20945;&#65292;&#26292;&#38706;&#33258;&#30001;&#24230;&#26356;&#23569;&#12290;&#22312;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;ShapeCoder&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31243;&#24207;&#22823;&#23567;&#20943;&#23569;&#20102;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.05661</link><description>&lt;p&gt;
ShapeCoder&#65306;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22522;&#20803;&#20013;&#21457;&#29616;&#35270;&#35273;&#31243;&#24207;&#30340;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives. (arXiv:2305.05661v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05661
&lt;/p&gt;
&lt;p&gt;
ShapeCoder&#26159;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22522;&#20803;&#20013;&#21457;&#29616;&#35270;&#35273;&#31243;&#24207;&#30340;&#25277;&#35937;&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#37325;&#20889;&#31243;&#24207;&#20197;&#20351;&#20854;&#26356;&#32039;&#20945;&#65292;&#26292;&#38706;&#33258;&#30001;&#24230;&#26356;&#23569;&#12290;&#22312;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;ShapeCoder&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31243;&#24207;&#22823;&#23567;&#20943;&#23569;&#20102;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#21487;&#35270;&#21270;&#25968;&#25454;&#34920;&#31034;&#26041;&#24335;&#65292;&#21487;&#20197;&#25581;&#31034;&#32039;&#20945;&#12289;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#65292;&#25903;&#25345;&#25805;&#20316;&#12290;&#35270;&#35273;&#31243;&#24207;&#36890;&#24120;&#20197;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;(DSLs)&#32534;&#20889;&#12290;&#25214;&#21040;&#8220;&#22909;&#8221;&#30340;&#31243;&#24207;&#65292;&#21363;&#20165;&#26292;&#38706;&#26377;&#24847;&#20041;&#30340;&#33258;&#30001;&#24230;&#65292;&#38656;&#35201;&#35775;&#38382;&#20855;&#26377;&#8220;&#22909;&#8221;&#20989;&#25968;&#24211;&#30340;DSLs&#65292;&#36825;&#20123;&#20989;&#25968;&#24211;&#36890;&#24120;&#30001;&#39046;&#22495;&#19987;&#23478;&#32534;&#20889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ShapeCoder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25509;&#21463;&#24418;&#29366;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#22522;&#20803;&#34920;&#31034;&#65292;&#24182;&#20849;&#21516;&#21457;&#29616;(i)&#26377;&#29992;&#30340;&#25277;&#35937;&#20989;&#25968;&#21644;(ii)&#20351;&#29992;&#36825;&#20123;&#25277;&#35937;&#26469;&#35299;&#37322;&#36755;&#20837;&#24418;&#29366;&#30340;&#31243;&#24207;&#12290;&#21457;&#29616;&#30340;&#25277;&#35937;&#25429;&#33719;&#25968;&#25454;&#38598;&#20013;&#30340;&#24120;&#35265;&#27169;&#24335;(&#32467;&#26500;&#21644;&#21442;&#25968;)&#65292;&#22240;&#27492;&#65292;&#20351;&#29992;&#36825;&#20123;&#25277;&#35937;&#37325;&#20889;&#30340;&#31243;&#24207;&#26356;&#32039;&#20945;&#65292;&#26292;&#38706;&#30340;&#33258;&#30001;&#24230;&#26356;&#23569;&#12290;ShapeCoder&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#25277;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#19979;&#65292;&#21457;&#29616;&#26356;&#22909;&#30340;&#25277;&#35937;&#65292;&#22312;&#36739;&#23569;&#30340;&#20808;&#39564;&#30693;&#35782;&#19979;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ShapeCoder&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31243;&#24207;&#22823;&#23567;&#20943;&#23569;&#20102;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programs are an increasingly popular representation for visual data, exposing compact, interpretable structure that supports manipulation. Visual programs are usually written in domain-specific languages (DSLs). Finding "good" programs, that only expose meaningful degrees of freedom, requires access to a DSL with a "good" library of functions, both of which are typically authored by domain experts. We present ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across the dataset, so that programs rewritten with these abstractions are more compact, and expose fewer degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05658</link><description>&lt;p&gt;
TidyBot: &#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#29289;&#29702;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#29289;&#29702;&#36741;&#21161;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#29992;&#25143;&#30340;&#20010;&#20154;&#21916;&#22909;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25441;&#36215;&#29289;&#21697;&#24182;&#23558;&#20854;&#25918;&#22238;&#21407;&#22788;&#26469;&#25972;&#29702;&#25151;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#29289;&#21697;&#30340;&#27491;&#30830;&#20301;&#32622;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#21916;&#22909;&#21487;&#20197;&#22240;&#20010;&#20154;&#21697;&#21619;&#25110;&#25991;&#21270;&#32972;&#26223;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#25277;&#23625;&#37324;&#65292;&#32780;&#21478;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#26550;&#23376;&#19978;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#29305;&#23450;&#20154;&#30340;&#20808;&#21069;&#20132;&#20114;&#23398;&#20064;&#36825;&#26679;&#30340;&#21916;&#22909;&#65292;&#32780;&#21482;&#38656;&#35201;&#20960;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#22522;&#20110;&#35821;&#35328;&#30340;&#35268;&#21010;&#21644;&#24863;&#30693;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#24191;&#27867;&#36866;&#29992;&#20110;&#26410;&#26469;&#20132;&#20114;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;91.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#22823;&#36807;&#28388;&#22120;&#20551;&#35774;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#22914;&#20309;&#22312;&#22825;&#20307;&#29983;&#29289;&#23398;&#30340;&#32972;&#26223;&#19979;&#29702;&#35299;&#20840;&#29699;&#28798;&#38590;&#24615;&#39118;&#38505;&#65292;&#24182;&#25351;&#20986;&#20102;&#24773;&#25253;&#30028;&#36890;&#36807;&#35748;&#35782;&#21040;AI&#22312;&#22823;&#35268;&#27169;&#20840;&#29699;&#39118;&#38505;&#20013;&#30340;&#28508;&#22312;&#20316;&#29992;&#33719;&#24471;&#26377;&#20215;&#20540;&#27934;&#23519;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05653</link><description>&lt;p&gt;
AI &#26159;&#21542;&#21487;&#33021;&#25104;&#20026;&#22823;&#36807;&#28388;&#22120;&#65311;&#22825;&#20307;&#29983;&#29289;&#23398;&#33021;&#22815;&#21578;&#35785;&#24773;&#25253;&#30028;&#20851;&#20110;&#20154;&#31867;&#36215;&#28304;&#39118;&#38505;&#30340;&#19996;&#35199;
&lt;/p&gt;
&lt;p&gt;
Could AI be the Great Filter? What Astrobiology can Teach the Intelligence Community about Anthropogenic Risks. (arXiv:2305.05653v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#22823;&#36807;&#28388;&#22120;&#20551;&#35774;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#22914;&#20309;&#22312;&#22825;&#20307;&#29983;&#29289;&#23398;&#30340;&#32972;&#26223;&#19979;&#29702;&#35299;&#20840;&#29699;&#28798;&#38590;&#24615;&#39118;&#38505;&#65292;&#24182;&#25351;&#20986;&#20102;&#24773;&#25253;&#30028;&#36890;&#36807;&#35748;&#35782;&#21040;AI&#22312;&#22823;&#35268;&#27169;&#20840;&#29699;&#39118;&#38505;&#20013;&#30340;&#28508;&#22312;&#20316;&#29992;&#33719;&#24471;&#26377;&#20215;&#20540;&#27934;&#23519;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20154;&#22312;&#21738;&#37324;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#28085;&#30422;&#20102;&#36153;&#31859;&#24726;&#35770;&#20013;&#30340;&#19981;&#23433;&#65292;&#21363;&#22914;&#26524;&#23431;&#23449;&#20013;&#23384;&#22312;&#27010;&#29575;&#36739;&#39640;&#30340;&#22806;&#26143;&#29983;&#21629;&#65292;&#21017;&#20026;&#20160;&#20040;&#25105;&#20204;&#36824;&#27809;&#26377;&#36935;&#21040;&#23427;&#65311;&#36825;&#20010;&#35868;&#22242;&#24050;&#32463;&#22256;&#25200;&#23398;&#32773;&#25968;&#21313;&#24180;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#20551;&#35828;&#65292;&#26082;&#21253;&#25324;&#33258;&#28982;&#30340;&#20063;&#21253;&#25324;&#31038;&#20250;&#23398;&#30340;&#35299;&#37322;&#12290;&#20854;&#20013;&#19968;&#20010;&#26377;&#36259;&#30340;&#20551;&#35774;&#34987;&#31216;&#20026;&#22823;&#36807;&#28388;&#22120;&#65292;&#23427;&#24314;&#35758;&#29983;&#21629;&#36827;&#21270;&#25152;&#38656;&#30340;&#26576;&#20123;&#20107;&#20214;&#26497;&#19981;&#21487;&#33021;&#21457;&#29983;&#65292;&#22240;&#27492;&#23431;&#23449;&#20445;&#25345;&#30528;&#27785;&#40664;&#12290;&#36825;&#20010;&#21644;&#23427;&#36923;&#36753;&#31561;&#20215;&#30340;&#20551;&#35774;&#24212;&#35813;&#20351;&#25105;&#20204;&#20572;&#19979;&#26469;&#24605;&#32771;&#8212;&#8212;&#26576;&#20123;&#28798;&#38590;&#24615;&#20107;&#20214;&#24456;&#21487;&#33021;&#20250;&#21457;&#29983;&#65292;&#20174;&#32780;&#38459;&#27490;&#20102;&#29983;&#21629;&#22312;&#23431;&#23449;&#20013;&#30340;&#25193;&#24352;&#12290;&#36825;&#21487;&#33021;&#26159;&#19968;&#31181;&#33258;&#28982;&#20107;&#20214;&#65292;&#25110;&#26356;&#20196;&#20154;&#19981;&#23433;&#30340;&#26159;&#65292;&#26159;&#26234;&#24935;&#29983;&#29289;&#20026;&#33258;&#24049;&#24341;&#21457;&#30340;&#23548;&#33268;&#33258;&#24049;&#28781;&#32477;&#30340;&#20107;&#20214;&#12290;&#20174;&#24773;&#25253;&#35282;&#24230;&#26469;&#30475;&#65292;&#22312;&#22825;&#20307;&#29983;&#29289;&#23398;&#30340;&#32972;&#26223;&#19979;&#26500;&#24314;&#20840;&#29699;&#28798;&#38590;&#24615;&#39118;&#38505;&#65288;&#29305;&#21035;&#26159;&#20154;&#31867;&#36215;&#28304;&#39118;&#38505;&#65289;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#23431;&#23449;&#20013;&#26234;&#24935;&#29983;&#21629;&#21487;&#33021;&#30340;&#31232;&#26377;&#24615;&#20197;&#21450;&#21487;&#33021;&#23384;&#22312;&#30340;&#31181;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21457;&#23637;&#22914;&#20309;&#25581;&#31034;&#22823;&#36807;&#28388;&#22120;&#21487;&#33021;&#22312;&#25105;&#20204;&#21069;&#38754;&#30340;&#37325;&#35201;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36890;&#36807;&#35748;&#35782;&#21040; AI &#22312;&#22823;&#35268;&#27169;&#20840;&#29699;&#39118;&#38505;&#20013;&#30340;&#28508;&#22312;&#20316;&#29992;&#65292;&#24773;&#25253;&#30028;&#21487;&#20197;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#65292;&#20943;&#23569;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Where is everybody? This phrase distills the foreboding of what has come to be known as the Fermi Paradox - the disquieting idea that, if extraterrestrial life is probable in the Universe, then why have we not encountered it? This conundrum has puzzled scholars for decades, and many hypotheses have been proposed suggesting both naturalistic and sociological explanations. One intriguing hypothesis is known as the Great Filter, which suggests that some event required for the emergence of intelligent life is extremely unlikely, hence the cosmic silence. A logically equivalent version of this hypothesis and one that should give us pause - suggests that some catastrophic event is likely to occur that prevents life's expansion throughout the cosmos. This could be a naturally occurring event, or more disconcertingly, something that intelligent beings do to themselves that leads to their own extinction. From an intelligence perspective, framing global catastrophic risk (particularly risks of
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#65292;&#21487;&#22312;&#20302;&#25104;&#26412;&#19979;&#23545;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#36827;&#34892;&#22823;&#35268;&#27169;&#31579;&#26597;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#39118;&#38505;&#35780;&#20998;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.05648</link><description>&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning. (arXiv:2305.05648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05648
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#65292;&#21487;&#22312;&#20302;&#25104;&#26412;&#19979;&#23545;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#36827;&#34892;&#22823;&#35268;&#27169;&#31579;&#26597;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#39118;&#38505;&#35780;&#20998;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;(CVDs)&#26159;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#26089;&#26399;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#36825;&#20123;&#20154;&#32676;&#20013;&#65292;&#26089;&#26399; CVD &#26816;&#27979;&#21644;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#35768;&#22810;&#29616;&#26377;&#30340; CVD &#39118;&#38505;&#35780;&#20998;&#38656;&#35201;&#36523;&#20307;&#26816;&#26597;&#25110;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#36825;&#22312;&#36825;&#26679;&#30340;&#20581;&#24247;&#31995;&#32479;&#20013;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#21463;&#38480;&#30340;&#21487;&#21450;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;(PPG)&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#22823;&#22810;&#25968;&#26234;&#33021;&#25163;&#26426;&#19978;&#37117;&#21487;&#29992;&#30340;&#20256;&#24863;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#22823;&#35268;&#27169;&#31579;&#26597;&#65292;&#29992;&#20110; CVD &#39118;&#38505;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340; PPG-CVD &#39118;&#38505;&#35780;&#20998;(DLS)&#65292;&#20165;&#20351;&#29992;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#21560;&#28895;&#29366;&#24577;&#21644; PPG &#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#65292;&#39044;&#27979;&#26410;&#26469; 10 &#24180;&#21457;&#29983;&#37325;&#35201;&#30340;&#19981;&#33391;&#24515;&#34880;&#31649;&#20107;&#20214;(MACE&#65306;&#38750;&#33268;&#21629;&#24615;&#24515;&#32908;&#26775;&#22622;&#65292;&#20013;&#39118;&#21644;&#24515;&#34880;&#31649;&#27515;&#20129;) &#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#19982;&#21150;&#20844;&#23460;&#21046;&#23450;&#30340; refit-WHO &#35780;&#20998;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35813;&#35780;&#20998;&#37319;&#29992;&#20102; WHO &#21644; Globorisk &#35780;&#20998;&#30340;&#20849;&#20139;&#39044;&#27979;&#22240;&#23376;&#65288;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;&#21560;&#28895;&#29366;&#24577;&#65292;&#25910;&#32553;&#21387;&#21644;&#24635;&#32966;&#22266;&#37255;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#21069;&#30651;&#24615;&#38431;&#21015;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#38431;&#21015;&#20013;&#65292;DLS &#22312;&#39044;&#27979; MACE &#26041;&#38754;&#34920;&#29616;&#20248;&#20110; refit-WHO &#35780;&#20998;&#65292;&#20998;&#21035;&#20026;0.76&#21644;0.80&#65292;&#32780;&#21150;&#20844;&#23460;&#21046;&#23450;&#30340; refit-WHO &#35780;&#20998;&#20026;0.66&#21644;0.70&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#31034;&#65292;&#22522;&#20110; PPG &#30340; DLS &#20855;&#26377;&#28508;&#21147;&#24110;&#21161;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#26089;&#26399;&#20302;&#25104;&#26412;&#35782;&#21035; CVD &#39640;&#39118;&#38505;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases (CVDs) are responsible for a large proportion of premature deaths in low- and middle-income countries. Early CVD detection and intervention is critical in these populations, yet many existing CVD risk scores require a physical examination or lab measurements, which can be challenging in such health systems due to limited accessibility. Here we investigated the potential to use photoplethysmography (PPG), a sensing technology available on most smartphones that can potentially enable large-scale screening at low cost, for CVD risk prediction. We developed a deep learning PPG-based CVD risk score (DLS) to predict the probability of having major adverse cardiovascular events (MACE: non-fatal myocardial infarction, stroke, and cardiovascular death) within ten years, given only age, sex, smoking status and PPG as predictors. We compared the DLS with the office-based refit-WHO score, which adopts the shared predictors from WHO and Globorisk scores (age, sex, smoking st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#36890;&#29992;&#20195;&#25968;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#19968;&#33324;&#35774;&#23450;&#19979;&#65292;&#25512;&#24191;&#20102;&#25277;&#35937;&#30340;&#31867;&#27604;&#27604;&#20363;&#20195;&#25968;&#36923;&#36753;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#35821;&#35328;&#21040;&#21452;&#35821;&#35328;&#26694;&#26550;&#30340;&#36716;&#21464;&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05614</link><description>&lt;p&gt;
&#21452;&#35821;&#31867;&#27604;&#27604;&#20363;
&lt;/p&gt;
&lt;p&gt;
Bilingual analogical proportions. (arXiv:2305.05614v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#36890;&#29992;&#20195;&#25968;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#19968;&#33324;&#35774;&#23450;&#19979;&#65292;&#25512;&#24191;&#20102;&#25277;&#35937;&#30340;&#31867;&#27604;&#27604;&#20363;&#20195;&#25968;&#36923;&#36753;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#35821;&#35328;&#21040;&#21452;&#35821;&#35328;&#26694;&#26550;&#30340;&#36716;&#21464;&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#27604;&#20363;&#26159;&#8220;$a$&#21040;$b$&#23601;&#22914;&#21516;$c$&#21040;$d$&#8221;&#36825;&#31181;&#34920;&#36798;&#24335;&#65292;&#23427;&#22788;&#20110;&#31867;&#27604;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#21518;&#32773;&#21448;&#22788;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#20316;&#32773;&#26368;&#36817;&#22522;&#20110;&#36890;&#29992;&#20195;&#25968;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#19968;&#33324;&#35774;&#23450;&#65292;&#20174;&#31532;&#19968;&#21407;&#29702;&#24320;&#22987;&#20171;&#32461;&#20102;&#19968;&#31181;&#25277;&#35937;&#30340;&#31867;&#27604;&#27604;&#20363;&#20195;&#25968;&#36923;&#36753;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#28304;&#20195;&#25968;&#21644;&#30446;&#26631;&#20195;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#22522;&#30784;&#35821;&#35328;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#23558;&#20854;&#21333;&#35821;&#35328;&#26694;&#26550;&#25512;&#24191;&#21040;&#21452;&#35821;&#35328;&#26694;&#26550;&#65292;&#20854;&#20013;&#22522;&#30784;&#35821;&#35328;&#21487;&#33021;&#19981;&#21516;&#12290;&#36890;&#36807;&#22312;&#27604;&#20363;&#35777;&#26126;&#20013;&#20351;&#29992;&#38480;&#23450;&#35821;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#30340;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25512;&#24191;&#65292;&#24191;&#27867;&#22320;&#25193;&#23637;&#20102;&#22522;&#30784;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35828;&#65292;&#26412;&#25991;&#26159;&#36808;&#21521;&#31867;&#27604;&#25512;&#29702;&#25968;&#23398;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are expressions of the form ``$a$ is to $b$ what $c$ is to $d$'' at the core of analogical reasoning which itself is at the core of human and artificial intelligence. The author has recently introduced {\em from first principles} an abstract algebro-logical framework of analogical proportions within the general setting of universal algebra and first-order logic. In that framework, the source and target algebras have the {\em same} underlying language. The purpose of this paper is to generalize his unilingual framework to a bilingual one where the underlying languages may differ. This is achieved by using hedges in justifications of proportions. The outcome is a major generalization vastly extending the applicability of the underlying framework. In a broader sense, this paper is a further step towards a mathematical theory of analogical reasoning.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;6-11&#23681;&#30340;&#20799;&#31461;&#39640;&#20272;&#20102;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#30340;&#26234;&#33021;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#25110;&#20195;&#29702;&#24863;&#21040;&#19981;&#30830;&#23450;&#65292;&#24182;&#32570;&#20047;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#26041;&#38754;&#30340;&#20934;&#30830;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.05597</link><description>&lt;p&gt;
&#8220;Alexa&#24182;&#19981;&#37027;&#20040;&#26377;&#24863;&#24773;&#8221;: &#20799;&#31461;&#36890;&#36807;&#19982;&#26234;&#33021;&#38899;&#31665;&#30340;&#20132;&#20114;&#26469;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
"Alexa doesn't have that many feelings": Children's understanding of AI through interactions with smart speakers in their homes. (arXiv:2305.05597v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05597
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;6-11&#23681;&#30340;&#20799;&#31461;&#39640;&#20272;&#20102;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#30340;&#26234;&#33021;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#25110;&#20195;&#29702;&#24863;&#21040;&#19981;&#30830;&#23450;&#65292;&#24182;&#32570;&#20047;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#26041;&#38754;&#30340;&#20934;&#30830;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#65288;CAs&#65289;&#65292;&#21253;&#25324;Alexa&#12289;Siri&#21644;Google Home&#31561;&#30340;&#26222;&#21450;&#65292;&#22312;&#23478;&#24237;&#20013;&#65292;&#35768;&#22810;&#20799;&#31461;&#29616;&#22312;&#32463;&#24120;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20114;&#21160;&#12290;&#30740;&#31350;&#20799;&#31461;&#19982;&#20351;&#29992;AI&#25216;&#26415;&#30340;&#28040;&#36153;&#35774;&#22791;&#30340;&#32463;&#39564;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#32463;&#39564;&#20250;&#22609;&#36896;&#20182;&#20204;&#23545;AI&#21450;&#20854;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#33487;&#26684;&#20848;&#23545;6-11&#23681;&#30340;&#23567;&#23398;&#29983;&#36827;&#34892;&#20102;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#65288;&#38382;&#21367;&#21644;&#35775;&#35848;&#65289;&#65292;&#20197;&#30830;&#23450;&#20799;&#31461;&#23545;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#30340;&#24037;&#20316;&#26041;&#24335;&#12289;&#35748;&#30693;&#33021;&#21147;&#12289;&#20195;&#29702;&#21644;&#20854;&#20182;&#31867;&#20284;&#20154;&#31867;&#30340;&#29305;&#36136;&#12289;&#22312;&#20351;&#29992;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#26102;&#38544;&#31169;&#26041;&#38754;&#30340;&#24847;&#35782;&#21644;&#20449;&#20219;&#20197;&#21450;&#20182;&#20204;&#35748;&#20026;&#19982;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#30340;&#36866;&#24403;&#21475;&#22836;&#20132;&#20114;&#12290;&#22823;&#22810;&#25968;&#20799;&#31461;&#39640;&#20272;&#20102;&#35821;&#38899;&#23545;&#35805;&#21161;&#25163;&#30340;&#26234;&#33021;&#27700;&#24179;&#65292;&#23545;&#31995;&#32479;&#30340;&#24773;&#24863;&#25110;&#20195;&#29702;&#24863;&#21040;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#26041;&#38754;&#32570;&#20047;&#20934;&#30830;&#30340;&#29702;&#35299;&#65292;&#24182;&#35748;&#20026;&#21521;&#23545;&#35805;&#21161;&#25163;&#34920;&#29616;&#20986;&#19981;&#31036;&#35980;&#26159;&#19981;&#23545;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems. It is important to research children's experiences with consumer devices which use AI techniques because these shape their understanding of AI and its capabilities. We conducted a mixed-methods study (questionnaires and interviews) with primary-school children aged 6-11 in Scotland to establish children's understanding of how voice-based CAs work, how they perceive their cognitive abilities, agency and other human-like qualities, their awareness and trust of privacy aspects when using CAs and what they perceive as appropriate verbal interactions with CAs. Most children overestimated the CAs' intelligence and were uncertain about the systems' feelings or agency. They also lacked accurate understanding of data privacy and security aspects, and believed it was wrong to be rude to con
&lt;/p&gt;</description></item><item><title>PET-NeuS&#26159;&#19968;&#20010;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#12289;&#26032;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21487;&#23398;&#20064;&#30340;&#21367;&#31215;&#25805;&#20316;&#65292;&#24182;&#33021;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.05594</link><description>&lt;p&gt;
PET-NeuS&#65306;&#31070;&#32463;&#34920;&#38754;&#30340;&#20301;&#32622;&#32534;&#30721;&#19977;&#24179;&#38754;
&lt;/p&gt;
&lt;p&gt;
PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces. (arXiv:2305.05594v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05594
&lt;/p&gt;
&lt;p&gt;
PET-NeuS&#26159;&#19968;&#20010;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#12289;&#26032;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21487;&#23398;&#20064;&#30340;&#21367;&#31215;&#25805;&#20316;&#65292;&#24182;&#33021;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MLP&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#26159;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#30340;&#24120;&#35265;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#25104;&#21151;&#30340;&#26368;&#26032;&#26041;&#27861;NeuS&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19977;&#20010;&#26032;&#32452;&#20214;&#36827;&#34892;&#25193;&#23637;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#20174;EG3D&#20013;&#20511;&#29992;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#65292;&#23558;&#31526;&#21495;&#36317;&#31163;&#22330;&#34920;&#31034;&#20026;&#19977;&#24179;&#38754;&#21644;MLP&#30340;&#28151;&#21512;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;MLP&#26469;&#34920;&#31034;&#12290;&#20351;&#29992;&#19977;&#24179;&#38754;&#20250;&#23548;&#33268;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#20294;&#20063;&#20250;&#24341;&#20837;&#37325;&#24314;&#34920;&#38754;&#20013;&#30340;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#19982;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#26469;&#23545;&#25239;&#37325;&#24314;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#23558;&#19977;&#24179;&#38754;&#20013;&#30340;&#29305;&#24449;&#20998;&#25104;&#22810;&#20010;&#39057;&#29575;&#23610;&#24230;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#39057;&#29575;&#30340;&#27491;&#24358;&#21644;&#20313;&#24358;&#20989;&#25968;&#35843;&#21046;&#23427;&#20204;&#12290;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#22312;&#19977;&#24179;&#38754;&#29305;&#24449;&#19978;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#21367;&#31215;&#25805;&#20316;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#21367;&#31215;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#39057;&#24102;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PET-NeuS&#27604;NeuS&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
A signed distance function (SDF) parametrized by an MLP is a common ingredient of neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Using tri-planes leads to a more expressive data structure but will also introduce noise in the reconstructed surface. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency scales and modulate them with sin and cos functions of different frequencies. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency bands. The experiments show that PET-NeuS achieves h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05586</link><description>&lt;p&gt;
RLocator: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;Bug&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
RLocator: Reinforcement Learning for Bug Localization. (arXiv:2305.05586v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#32773;&#22312;&#20182;&#20204;&#30340;&#39033;&#30446;&#20013;&#33457;&#36153;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20462;&#22797;Bugs&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;Bug&#23450;&#20301;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#28304;&#20195;&#30721;&#25991;&#20214;&#21487;&#33021;&#26159;&#36127;&#36131;&#29305;&#23450;Bug&#30340;&#28304;&#22836;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;Bug&#23450;&#20301;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#30452;&#25509;&#20248;&#21270;&#35780;&#20272;&#25351;&#26631;&#12290;&#30456;&#21453;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20250;&#23545;&#26816;&#32034;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Bug&#23450;&#20301;&#26041;&#27861;RLocator&#12290;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#20248;&#21270;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23545;Bug&#23450;&#20301;&#38382;&#39064;&#36827;&#34892;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35813;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#20845;&#31181;&#39640;&#24230;&#27969;&#34892;&#30340;Apache&#39033;&#30446;&#30340;8,316&#20010;Bug&#25253;&#21578;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RLocator&#30456;&#36739;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;Bug&#23450;&#20301;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. Instead, they use different metrics in the training and testing phases, which can negatively impact the model performance in retrieval tasks. In this paper, we propose RLocator, a Reinforcement Learning-based (RL) bug localization approach. We formulate the bug localization problem using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. Our evaluation shows that RLocator achie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SMAClite&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#35299;&#32806;&#20102;&#21407;&#26377;&#30340;&#38381;&#28304;&#28216;&#25103;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#26694;&#26550;&#65292;SMAClite&#22312;&#36816;&#34892;&#26102;&#36895;&#24230;&#21644;&#20869;&#23384;&#26041;&#38754;&#36229;&#36234;&#20102;SMAC&#12290;</title><link>http://arxiv.org/abs/2305.05566</link><description>&lt;p&gt;
SMAClite: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning. (arXiv:2305.05566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SMAClite&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#35299;&#32806;&#20102;&#21407;&#26377;&#30340;&#38381;&#28304;&#28216;&#25103;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#26694;&#26550;&#65292;SMAClite&#22312;&#36816;&#34892;&#26102;&#36895;&#24230;&#21644;&#20869;&#23384;&#26041;&#38754;&#36229;&#36234;&#20102;SMAC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#32570;&#23569;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;Starcraft&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#65288;SMAC&#65289;&#24050;&#32463;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#26500;&#24314;&#22312;&#37325;&#22411;&#30340;&#38381;&#28304;&#35745;&#31639;&#26426;&#28216;&#25103;StarCraft II&#20043;&#19978;&#12290;&#22240;&#27492;&#65292;SMAC&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#20855;&#22791;&#20851;&#20110;&#28216;&#25103;&#30340;&#29305;&#27530;&#30693;&#35782;&#21644;&#20351;&#29992;&#19987;&#26377;&#24037;&#20855;&#25165;&#33021;&#23545;&#29615;&#22659;&#36827;&#34892;&#20219;&#20309;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#25110;&#36129;&#29486;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SMAClite - &#19968;&#20010;&#22522;&#20110;SMAC&#30340;&#25361;&#25112;&#65292;&#19981;&#20165;&#19982;Starcraft II&#35299;&#32806;&#65292;&#32780;&#19988;&#26159;&#24320;&#28304;&#30340;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#26032;&#30340;SMAClite&#20869;&#23481;&#32780;&#26080;&#38656;&#20219;&#20309;&#29305;&#27530;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;SMAClite&#19978;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22797;&#29616;SMAC&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#65292;SMAClite&#22312;&#36816;&#34892;&#26102;&#36895;&#24230;&#21644;&#20869;&#23384;&#26041;&#38754;&#36229;&#36234;SMAC&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning (MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely used in MARL research, but is built on top of a heavy, closed-source computer game, StarCraft II. Thus, SMAC is computationally expensive and requires knowledge and the use of proprietary tools specific to the game for any meaningful alteration or contribution to the environment. We introduce SMAClite -- a challenge based on SMAC that is both decoupled from Starcraft II and open-source, along with a framework which makes it possible to create new content for SMAClite without any special knowledge. We conduct experiments to show that SMAClite is equivalent to SMAC, by training MARL algorithms on SMAClite and reproducing SMAC results. We then show that SMAClite outperforms SMAC in both runtime speed and memory.
&lt;/p&gt;</description></item><item><title>SkelEx&#21644;BoundEx&#26159;&#31532;&#19968;&#25209;&#33258;&#28982;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#28857;&#21644;&#20915;&#31574;&#36793;&#30028;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#20026;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ReLU NN&#24341;&#20837;&#20102;&#38750;&#24120;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.05562</link><description>&lt;p&gt;
SkelEx&#21644;BoundEx&#65306;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks. (arXiv:2305.05562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05562
&lt;/p&gt;
&lt;p&gt;
SkelEx&#21644;BoundEx&#26159;&#31532;&#19968;&#25209;&#33258;&#28982;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#28857;&#21644;&#20915;&#31574;&#36793;&#30028;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#20026;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ReLU NN&#24341;&#20837;&#20102;&#38750;&#24120;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26435;&#37325;&#21644;&#20559;&#24046;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#38480;&#65292;&#20294;&#20173;&#28982;&#26159;&#32534;&#30721;ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20989;&#25968;&#26368;&#27969;&#34892;&#30340;&#26041;&#24335;&#12290;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#25105;&#20204;&#24341;&#20837;SkelEx&#31639;&#27861;&#65292;&#20197;&#25552;&#21462;ReLU NN&#23398;&#20064;&#30340;&#25104;&#21592;&#20989;&#25968;&#30340;&#39592;&#26550;&#65292;&#20351;&#36825;&#20123;&#20989;&#25968;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#20851;&#38190;&#28857;&#30340;&#35282;&#24230;&#32771;&#34385;&#32447;&#24615;&#21306;&#22495;&#30340;&#24037;&#20316;&#12290;&#20316;&#20026;&#33258;&#28982;&#30340;&#21518;&#32493;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BoundEx&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20174;ReLU NN&#30340;&#23454;&#29616;&#20013;&#25552;&#21462;&#20915;&#31574;&#36793;&#30028;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20026;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ReLU NN&#24341;&#20837;&#20102;&#38750;&#24120;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their limited interpretability, weights and biases are still the most popular encoding of the functions learned by ReLU Neural Networks (ReLU NNs). That is why we introduce SkelEx, an algorithm to extract a skeleton of the membership functions learned by ReLU NNs, making those functions easier to interpret and analyze. To the best of our knowledge, this is the first work that considers linear regions from the perspective of critical points. As a natural follow-up, we also introduce BoundEx, which is the first analytical method known to us to extract the decision boundary from the realization of a ReLU NN. Both of those methods introduce very natural visualization tool for ReLU NNs trained on low-dimensional data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#21644;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#21253;&#21547;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05560</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Distributional Multi-Objective Decision Making. (arXiv:2305.05560v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#21644;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#21253;&#21547;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20914;&#31361;&#30446;&#26631;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21487;&#20197;&#21521;&#20915;&#31574;&#32773;&#21576;&#29616;&#19968;&#32452;&#21487;&#33021;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25506;&#35752;&#36825;&#20123;&#35299;&#24212;&#35813;&#21253;&#21547;&#21738;&#20123;&#31574;&#30053;&#20197;&#21450;&#22914;&#20309;&#39640;&#25928;&#22320;&#35745;&#31639;&#36825;&#20123;&#35299;&#12290;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21183;&#20934;&#21017;&#65292;&#30452;&#25509;&#20851;&#32852;&#31574;&#30053;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20010;&#20934;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#20854;&#20013;&#21253;&#21547;&#20102;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#24573;&#30053;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#23427;&#21253;&#25324;&#25152;&#26377;&#22312;&#22810;&#32500;&#39118;&#38505;&#35268;&#36991;&#20915;&#31574;&#32773;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#25928;&#29992;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#36129;&#29486;&#20102;&#21098;&#26525;&#31639;&#23376;&#26469;&#23558;&#20854;&#20943;&#23569;&#21040;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For effective decision support in scenarios with conflicting objectives, sets of potentially optimal solutions can be presented to the decision maker. We explore both what policies these sets should contain and how such sets can be computed efficiently. With this in mind, we take a distributional approach and introduce a novel dominance criterion relating return distributions of policies directly. Based on this criterion, we present the distributional undominated set and show that it contains optimal policies otherwise ignored by the Pareto front. In addition, we propose the convex distributional undominated set and prove that it comprises all policies that maximise expected utility for multivariate risk-averse decision makers. We propose a novel algorithm to learn the distributional undominated set and further contribute pruning operators to reduce the set to the convex distributional undominated set. Through experiments, we demonstrate the feasibility and effectiveness of these metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#20215;&#20540;&#21462;&#21521;(SVO)&#21644;&#31215;&#20998;&#24773;&#24863;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;SVO&#21644;&#31215;&#20998;&#24773;&#24863;&#25919;&#31574;&#30340;&#26234;&#33021;&#20307;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#31038;&#20250;&#20559;&#22909;&#21644;&#24773;&#24863;&#23545;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.05549</link><description>&lt;p&gt;
&#31038;&#20250;&#20215;&#20540;&#21462;&#21521;&#19982;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#31215;&#20998;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Social Value Orientation and Integral Emotions in Multi-Agent Systems. (arXiv:2305.05549v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#20215;&#20540;&#21462;&#21521;(SVO)&#21644;&#31215;&#20998;&#24773;&#24863;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;SVO&#21644;&#31215;&#20998;&#24773;&#24863;&#25919;&#31574;&#30340;&#26234;&#33021;&#20307;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#31038;&#20250;&#20559;&#22909;&#21644;&#24773;&#24863;&#23545;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#30340;&#31038;&#20250;&#34892;&#20026;&#21463;&#31038;&#20250;&#20559;&#22909;&#30340;&#20010;&#20307;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#31038;&#20250;&#20215;&#20540;&#21462;&#21521;&#65288;SVO&#65289;&#26159;&#19968;&#31181;&#21487;&#27979;&#37327;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#34920;&#26126;&#20010;&#20307;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#23545;&#33258;&#24049;&#21644;&#20182;&#20154;&#30340;&#31119;&#21033;&#30456;&#23545;&#37325;&#35270;&#30340;&#31243;&#24230;&#12290;SVO&#21644;&#20854;&#20182;&#20010;&#20307;&#24046;&#24322;&#21464;&#37327;&#26159;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#32467;&#26524;&#30340;&#24378;&#26377;&#21147;&#39044;&#27979;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#19982;&#20010;&#20307;&#24046;&#24322;&#29420;&#31435;&#30340;&#30701;&#26242;&#24773;&#32490;&#21464;&#21270;&#20063;&#20250;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#12290;&#31215;&#20998;&#24773;&#24863;&#26159;&#23545;&#20915;&#31574;&#24773;&#22659;&#30452;&#25509;&#21453;&#24212;&#32780;&#20135;&#29983;&#30340;&#24773;&#24863;&#65292;&#24050;&#32463;&#19982;&#20915;&#31574;&#21462;&#21521;&#30340;&#20020;&#26102;&#36716;&#21464;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31215;&#20998;&#24773;&#24863;&#23545;&#22810;&#26234;&#33021;&#20307;&#31038;&#20250;&#20013;&#35843;&#33410;&#31038;&#20250;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Svoie&#65292;&#19968;&#31181;&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;SVO&#25919;&#31574;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#21487;&#26367;&#20195;&#31215;&#20998;&#24773;&#24863;&#25919;&#31574;&#20570;&#20986;&#20915;&#31574;&#30340;&#26234;&#33021;&#20307;&#35774;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#31038;&#20250;&#20540;&#21462;&#21521;&#21644;&#24773;&#24863;&#35843;&#33410;&#23545;&#26234;&#33021;&#20307;&#20869;&#37096;&#21644;&#25972;&#20307;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human social behavior is influenced by individual differences in social preferences. Social value orientation (SVO) is a measurable personality trait which indicates the relative importance an individual places on their own and on others' welfare when making decisions. SVO and other individual difference variables are strong predictors of human behavior and social outcomes. However, there are transient changes human behavior associated with emotions that are not captured by individual differences alone. Integral emotions, the emotions which arise in direct response to a decision-making scenario, have been linked to temporary shifts in decision-making preferences.  In this work, we investigated the effects of moderating social preferences with integral emotions in multi-agent societies. We developed Svoie, a method for designing agents which make decisions based on established SVO policies, as well as alternative integral emotion policies in response to task outcomes. We conducted simul
&lt;/p&gt;</description></item><item><title>Walk4Me&#26159;&#19968;&#20010;&#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#31995;&#32479;&#65292;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27493;&#24577;&#29305;&#24449;&#26816;&#27979;&#26469;&#35782;&#21035;&#30142;&#30149;&#24739;&#32773;&#30340;&#27493;&#24577;&#38556;&#30861;&#24182;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.05543</link><description>&lt;p&gt;
Walk4Me: &#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#8212;&#8212;&#19968;&#20010;&#26089;&#26399;&#35786;&#26029;&#21644;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression. (arXiv:2305.05543v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05543
&lt;/p&gt;
&lt;p&gt;
Walk4Me&#26159;&#19968;&#20010;&#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#31995;&#32479;&#65292;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27493;&#24577;&#29305;&#24449;&#26816;&#27979;&#26469;&#35782;&#21035;&#30142;&#30149;&#24739;&#32773;&#30340;&#27493;&#24577;&#38556;&#30861;&#24182;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Walk4Me&#65292;&#36825;&#26159;&#19968;&#20010;&#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#26089;&#26399;&#35786;&#26029;&#12289;&#20005;&#37325;&#24615;&#21644;&#36827;&#23637;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#19977;&#20010;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65306;1&#65289;&#20419;&#36827;&#26089;&#26399;&#35786;&#26029;&#65307;2&#65289;&#35782;&#21035;&#20020;&#24202;&#20005;&#37325;&#24615;&#30340;&#26089;&#26399;&#25351;&#26631;&#65307;3&#65289;&#37327;&#21270;&#21644;&#36319;&#36394;&#30142;&#30149;&#22312;&#27493;&#34892;&#26399;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27493;&#24577;&#29305;&#24449;&#26816;&#27979;&#26469;&#26816;&#27979;&#24739;&#32773;&#21644;&#36890;&#24120;&#21457;&#32946;&#30340;&#21516;&#40836;&#20154;&#30340;&#34892;&#36208;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;Walk4Me API&#20174;&#35774;&#22791;&#20256;&#24863;&#22120;&#65288;&#20363;&#22914;&#31227;&#21160;&#35774;&#22791;&#30340;&#21152;&#36895;&#24230;&#31561;&#65289;&#36828;&#31243;&#23454;&#26102;&#25910;&#38598;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#25552;&#21462;&#26102;&#38388;/&#31354;&#38388;&#27493;&#24577;&#29305;&#24449;&#21644;&#21407;&#22987;&#25968;&#25454;&#20449;&#21495;&#29305;&#24449;&#65292;&#28982;&#21518;&#37319;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#27169;&#24335;&#65292;&#20197;&#20415;&#23454;&#29616;&#65306;1&#65289;&#35782;&#21035;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#27493;&#24577;&#38556;&#30861;&#30340;&#24739;&#32773;&#65307;2&#65289;&#25551;&#36848;&#27963;&#21160;&#33021;&#21147;&#38480;&#21046;&#30340;&#31243;&#24230;&#65307;3&#65289;&#30830;&#23450;&#30142;&#30149;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Walk4Me, a telehealth community mobility assessment system designed to facilitate early diagnosis, severity, and progression identification. Our system achieves this by 1) enabling early diagnosis, 2) identifying early indicators of clinical severity, and 3) quantifying and tracking the progression of the disease across the ambulatory phase of the disease. To accomplish this, we employ an Artificial Intelligence (AI)-based detection of gait characteristics in patients and typically developing peers. Our system remotely and in real-time collects data from device sensors (e.g., acceleration from a mobile device, etc.) using our novel Walk4Me API. Our web application extracts temporal/spatial gait characteristics and raw data signal characteristics and then employs traditional machine learning and deep learning techniques to identify patterns that can 1) identify patients with gait disturbances associated with disease, 2) describe the degree of mobility limitation, and 3) ide
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#36824;&#32771;&#34385;&#20102;&#23454;&#20307;&#38388;&#30340;&#36890;&#20449;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05538</link><description>&lt;p&gt;
&#22810;&#20803;&#35774;&#22791;&#32593;&#32476;&#20013;&#39640;&#25928;&#22522;&#20110;&#27169;&#24335;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient pattern-based anomaly detection in a network of multivariate devices. (arXiv:2305.05538v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#36824;&#32771;&#34385;&#20102;&#23454;&#20307;&#38388;&#30340;&#36890;&#20449;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32452;&#32455;&#31649;&#29702;&#26381;&#21153;&#36136;&#37327;&#24182;&#30417;&#35270;&#22823;&#37327;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20307;&#37117;&#19982;&#36965;&#27979;&#25110;&#29289;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#24207;&#21015;&#30456;&#20851;&#32852;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#34892;&#20026;&#24322;&#24120;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#20851;&#27880;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#24573;&#30053;&#23454;&#20307;&#38388;&#30340;&#36890;&#20449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#25903;&#25345;&#26368;&#32456;&#29992;&#25143;&#19981;&#20165;&#22312;&#23450;&#20301;&#22312;&#26576;&#20010;&#26102;&#26399;&#23548;&#33268;&#24322;&#24120;&#30340;&#23454;&#20307;&#21644;&#20256;&#24863;&#22120;&#26041;&#38754;&#65292;&#32780;&#19988;&#35299;&#37322;&#36825;&#20010;&#20915;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#20004;&#27493;&#26041;&#27861;&#26816;&#27979;&#24322;&#24120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24674;&#22797;&#32593;&#32476;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#20851;&#31995;&#36890;&#24120;&#26159;&#21160;&#24577;&#30340;&#65292;&#30001;&#26410;&#30693;&#30340;&#22522;&#30784;&#36807;&#31243;&#24341;&#36215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22522;&#20110;&#39034;&#24207;&#27169;&#24335;&#30340;&#23884;&#20837;&#25253;&#21578;&#24322;&#24120;&#12290;&#27169;&#24335;&#25366;&#25496;&#26159;&#39640;&#25928;&#30340;&#24182;&#25903;&#25345;&#35299;&#37322;&#65292;&#21363;&#27169;&#24335;&#20195;&#34920;&#26102;&#38388;&#24207;&#21015;&#20013;&#39057;&#32321;&#21457;&#29983;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#27169;&#24335;&#25366;&#25496;&#20197;&#22522;&#20110;&#39057;&#29575;&#36807;&#28388;&#39034;&#24207;&#27169;&#24335;&#65292;&#24182;&#24341;&#20837;&#27169;&#24335;&#27169;&#26495;&#20197;&#23558;&#27169;&#24335;&#25253;&#21578;&#20026;&#25991;&#26412;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many organisations manage service quality and monitor a large set devices and servers where each entity is associated with telemetry or physical sensor data series. Recently, various methods have been proposed to detect behavioural anomalies, however existing approaches focus on multivariate time series and ignore communication between entities. Moreover, we aim to support end-users in not only in locating entities and sensors causing an anomaly at a certain period, but also explain this decision. We propose a scalable approach to detect anomalies using a two-step approach. First, we recover relations between entities in the network, since relations are often dynamic in nature and caused by an unknown underlying process. Next, we report anomalies based on an embedding of sequential patterns. Pattern mining is efficient and supports interpretation, i.e. patterns represent frequent occurring behaviour in time series. We extend pattern mining to filter sequential patterns based on frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#20102;&#19977;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25391;&#21160;&#20449;&#21495;&#30340;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;98.8\%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.05532</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#38598;&#21512;&#29992;&#20110;&#25391;&#21160;&#20449;&#21495;&#30340;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An ensemble of convolution-based methods for fault detection using vibration signals. (arXiv:2305.05532v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#20102;&#19977;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25391;&#21160;&#20449;&#21495;&#30340;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;98.8\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#27979;&#35797;&#24179;&#21488;&#19978;&#34892;&#26143;&#40831;&#36718;&#31665;&#25391;&#21160;&#20449;&#21495;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35299;&#20915;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#12290;&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#36317;&#31163;&#12289;&#22522;&#20110;&#21151;&#33021;&#25968;&#25454;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;ROCKET&#12289;ResNet&#21644;FCN&#31561;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#26041;&#27861;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#31867;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#26041;&#27861;&#30340;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#24182;&#23454;&#29616;&#36229;&#36807;98.8\%&#20934;&#30830;&#29575;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on solving a fault detection problem using multivariate time series of vibration signals collected from planetary gearboxes in a test rig. Various traditional machine learning and deep learning methods have been proposed for multivariate time-series classification, including distance-based, functional data-oriented, feature-driven, and convolution kernel-based methods. Recent studies have shown using convolution kernel-based methods like ROCKET, and 1D convolutional neural networks with ResNet and FCN, have robust performance for multivariate time-series data classification. We propose an ensemble of three convolution kernel-based methods and show its efficacy on this fault detection problem by outperforming other approaches and achieving an accuracy of more than 98.8\%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#24103;&#65292;&#22312;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05525</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25506;&#32034;&#65306;&#20197;&#35780;&#20272;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises. (arXiv:2305.05525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#24103;&#65292;&#22312;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;AI&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#28982;&#32780;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23588;&#20854;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#25506;&#32034;&#21364;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#20013;&#28041;&#21450;&#34917;&#20607;&#21160;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#24103;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (AI) techniques are increasingly being explored to provide insights into why AI and machine learning (ML) models provide a certain outcome in various applications. However, there has been limited exploration of explainable AI techniques on time-series data, especially in the healthcare context. In this paper, we describe a threshold-based method that utilizes a weakly supervised model and a gradient-based explainable AI technique (i.e. saliency map) and explore its feasibility to identify salient frames of time-series data. Using the dataset from 15 post-stroke survivors performing three upper-limb exercises and labels on whether a compensatory motion is observed or not, we implemented a feed-forward neural network model and utilized gradients of each input on model outcomes to identify salient frames that involve compensatory motions. According to the evaluation using frame-level annotations, our approach achieved a recall of 0.96 and an F2-score of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#35745;&#31639;&#26694;&#26550;&#26469;&#37327;&#21270;&#39640;&#26657;&#25945;&#32946;&#36136;&#37327;&#65292;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65288;AQI&#65289;&#26469;&#32508;&#21512;&#34913;&#37327;&#23398;&#32773;&#30340;&#23398;&#26415;&#21697;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.05460</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#26657;&#25945;&#32946;&#36136;&#37327;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#36879;&#26126;&#25307;&#32856;&#65306;&#31532;&#19968;&#37096;&#20998;&#27169;&#22411;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Optimization- and AI-based approaches to academic quality quantification for transparent academic recruitment: part 1-model development. (arXiv:2305.05460v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#35745;&#31639;&#26694;&#26550;&#26469;&#37327;&#21270;&#39640;&#26657;&#25945;&#32946;&#36136;&#37327;&#65292;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65288;AQI&#65289;&#26469;&#32508;&#21512;&#34913;&#37327;&#23398;&#32773;&#30340;&#23398;&#26415;&#21697;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20844;&#27491;&#22320;&#25307;&#32856;&#39640;&#26657;&#21644;&#30740;&#31350;&#26426;&#26500;&#30340;&#25945;&#32946;&#20154;&#21592;&#65292;&#26681;&#25454;&#20840;&#29699;&#20844;&#35748;&#30340;&#23398;&#26415;&#21697;&#36136;&#29305;&#24449;&#30830;&#23450;&#27491;&#30830;&#30340;&#34913;&#37327;&#26631;&#20934;&#26159;&#19968;&#20010;&#21313;&#20998;&#24494;&#22937;&#12289;&#23500;&#26377;&#25361;&#25112;&#24615;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#36830;&#20018;&#30340;&#20004;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31532;&#19968;&#31687;&#35770;&#25991;&#20013;&#30340;&#23398;&#26415;&#21697;&#36136;&#37327;&#21270;&#24314;&#27169;&#37096;&#20998;&#65292;&#32780;&#22312;&#31532;&#20108;&#31687;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#26696;&#20363;&#30740;&#31350;&#37096;&#20998;&#12290;&#38024;&#23545;&#23398;&#26415;&#21697;&#36136;&#37327;&#21270;&#24314;&#27169;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26500;&#24314;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#35745;&#31639;&#26694;&#26550;&#65306;(i) &#22522;&#20110;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450; (ii) &#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#26694;&#26550;&#65288;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65289;&#12290;&#20004;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#37117;&#26159;&#19968;&#20010;&#31216;&#20026;&#23398;&#26415;&#21697;&#36136;&#37327;&#25351;&#25968;(Academic Quality Index, AQI)&#30340;&#21333;&#19968;&#25351;&#26631;&#65292;&#23427;&#26159;&#24635;&#20307;&#23398;&#26415;&#21697;&#36136;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#20840;&#29699;&#31532;&#19968;&#21644;&#24179;&#22343;&#38750;&#31532;&#19968;&#31867;&#22823;&#23398;&#30340;&#23398;&#32773;&#25968;&#25454;&#65292;&#26681;&#25454;&#12298;&#27888;&#26212;&#22763;&#39640;&#31561;&#25945;&#32946;&#19990;&#30028;&#22823;&#23398;&#25490;&#21517;&#12299;&#21644; QS &#19990;&#30028;&#22823;&#23398;&#25490;&#21517;&#36827;&#34892;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fair academic recruitment at universities and research institutions, determination of the right measure based on globally accepted academic quality features is a highly delicate, challenging, but quite important problem to be addressed. In a series of two papers, we consider the modeling part for academic quality quantification in the first paper, in this paper, and the case studies part in the second paper. For academic quality quantification modeling, we develop two computational frameworks which can be used to construct a decision-support tool: (i) an optimization-based framework and (ii) a Siamese network (a type of artificial neural network)-based framework. The output of both models is a single index called Academic Quality Index (AQI) which is a measure of the overall academic quality. The data of academics from first-class and average-class world universities, based on Times Higher Education World University Rankings and QS World University Rankings, are assumed as the refe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#20445;&#25252;&#38750;&#25112;&#26007;&#20154;&#21592;&#30340;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.05459</link><description>&lt;p&gt;
&#19968;&#31181;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#65306;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#21307;&#30103;&#21333;&#20301;&#21644;&#21463;&#20260;&#22763;&#20853;&#30340;&#20445;&#25252;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
A Cross-Frequency Protective Emblem: Protective Options for Medical Units and Wounded Soldiers in the Context of (fully) Autonomous Warfare. (arXiv:2305.05459v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#20445;&#25252;&#38750;&#25112;&#26007;&#20154;&#21592;&#30340;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#20445;&#25252;&#38750;&#25112;&#26007;&#20154;&#21592;&#24341;&#21457;&#20102;&#22269;&#38469;&#20445;&#25252;&#26631;&#24535;&#30340;&#26102;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#35774;&#35745;&#19968;&#31181;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#65292;&#20197;&#20415;&#27494;&#22120;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#21040;&#24182;&#30456;&#24212;&#20316;&#20986;&#25514;&#26045;&#12290;&#22312;&#25216;&#26415;&#37096;&#32626;&#26041;&#38754;&#65292;&#32771;&#34385;&#37319;&#29992;&#38647;&#36798;&#20449;&#26631;&#31561;&#24418;&#24335;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The protection of non-combatants in times of (fully) autonomous warfare raises the question of the timeliness of the international protective emblem. Incidents in the recent past indicate that it is becoming necessary to transfer the protective emblem to other dimensions of transmission and representation. (Fully) Autonomous weapon systems are often launched from a great distance to the aiming point and there may be no possibility for the operators to notice protective emblems at the point of impact. In this case, the weapon system would have to detect such protective emblems and, if necessary, disintegrate autonomously or request an abort via human-in-the-loop. In our paper, we suggest ways in which a cross-frequency protective emblem can be designed. On the one hand, the technical deployment, e.g. in the form of RADAR beacons, is considered, as well as the interpretation by methods of machine learning. With regard to the technical deployment, possibilities are considered to address d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#23618;&#35821;&#20041;&#32467;&#26500;&#30340;&#29289;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#21363;&#36882;&#24402;&#35782;&#21035;&#29289;&#20307;&#30340;&#35270;&#35273;&#23646;&#21644;&#24046;&#24322;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#29289;&#20307;&#35782;&#21035;&#36807;&#31243;&#30340;&#20998;&#23618;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05422</link><description>&lt;p&gt;
&#33258;&#25105;&#20013;&#24515;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#20041;&#23398;
&lt;/p&gt;
&lt;p&gt;
Egocentric Hierarchical Visual Semantics. (arXiv:2305.05422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#23618;&#35821;&#20041;&#32467;&#26500;&#30340;&#29289;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#21363;&#36882;&#24402;&#35782;&#21035;&#29289;&#20307;&#30340;&#35270;&#35273;&#23646;&#21644;&#24046;&#24322;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#29289;&#20307;&#35782;&#21035;&#36807;&#31243;&#30340;&#20998;&#23618;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#20154;&#20204;&#23545;&#29289;&#20307;&#30340;&#24605;&#32771;&#19982;&#26426;&#22120;&#24863;&#30693;&#36827;&#34892;&#23545;&#40784;&#65292;&#21363;&#26426;&#22120;&#30340;&#29289;&#20307;&#35782;&#21035;&#36807;&#31243;&#24212;&#35813;&#19982;&#20154;&#31867;&#24605;&#32771;&#29289;&#20307;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#36807;&#31243;&#31867;&#20284;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#26500;&#24314;&#33021;&#22815;&#26377;&#24847;&#20041;&#22320;&#19982;&#29992;&#25143;&#20132;&#20114;&#65292;&#24182;&#21487;&#20197;&#29992;&#29992;&#25143;&#33258;&#24049;&#30340;&#26415;&#35821;&#25551;&#36848;&#24863;&#30693;&#20869;&#23481;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#23618;&#35821;&#20041;&#32467;&#26500;&#30340;&#29289;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#36882;&#24402;&#35782;&#21035;&#29289;&#20307;&#30340;&#35270;&#35273;&#23646;&#21644;&#24046;&#24322;&#23646;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29289;&#20307;&#35782;&#21035;&#36807;&#31243;&#30340;&#20998;&#23618;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in aligning how people think about objects and what machines perceive, meaning by this the fact that object recognition, as performed by a machine, should follow a process which resembles that followed by humans when thinking of an object associated with a certain concept. The ultimate goal is to build systems which can meaningfully interact with their users, describing what they perceive in the users' own terms. As from the field of Lexical Semantics, humans organize the meaning of words in hierarchies where the meaning of, e.g., a noun, is defined in terms of the meaning of a more general noun, its genus, and of one or more differentiating properties, its differentia. The main tenet of this paper is that object recognition should implement a hierarchical process which follows the hierarchical semantic structure used to define the meaning of words. We achieve this goal by implementing an algorithm which, for any object, recursively recognizes its visual genus and its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#22768;&#26126;&#24615;&#36807;&#31243;&#35268;&#33539;&#30340;&#28385;&#36275;&#24230;&#65292;&#24182;&#21487;&#29992;&#20110;&#21457;&#29616;&#12289;&#26816;&#26597;&#21644;&#28418;&#31227;&#26816;&#27979;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.05418</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;LTLf&#27969;&#31243;&#35268;&#33539;&#27979;&#37327;&#65306;&#19968;&#31181;&#27010;&#29575;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring Rule-based LTLf Process Specifications: A Probabilistic Data-driven Approach. (arXiv:2305.05418v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#22768;&#26126;&#24615;&#36807;&#31243;&#35268;&#33539;&#30340;&#28385;&#36275;&#24230;&#65292;&#24182;&#21487;&#29992;&#20110;&#21457;&#29616;&#12289;&#26816;&#26597;&#21644;&#28418;&#31227;&#26816;&#27979;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#26126;&#24615;&#27969;&#31243;&#35268;&#33539;&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#36712;&#36857;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTLf&#65289;&#35268;&#21017;&#26469;&#23450;&#20041;&#27969;&#31243;&#34892;&#20026;&#12290;&#22312;&#25366;&#25496;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#20123;&#35268;&#33539;&#26159;&#20174;&#20449;&#24687;&#31995;&#32479;&#65288;&#21363;&#20107;&#20214;&#26085;&#24535;&#65289;&#35760;&#24405;&#30340;&#36816;&#34892;&#30340;&#22810;&#37325;&#38598;&#20013;&#25512;&#26029;&#20986;&#24182;&#36827;&#34892;&#26816;&#26597;&#30340;&#12290;&#20026;&#27492;&#65292;&#33021;&#22815;&#34913;&#37327;&#27969;&#31243;&#25968;&#25454;&#31526;&#21512;&#35268;&#33539;&#30340;&#31243;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25366;&#25496;&#21644;&#39564;&#35777;&#25216;&#26415;&#20165;&#20998;&#26512;&#35268;&#21017;&#26412;&#36523;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35774;&#35745;&#22768;&#26126;&#24615;&#36807;&#31243;&#35268;&#33539;&#30340;&#27010;&#29575;&#24230;&#37327;&#26041;&#24335;&#12290;&#36827;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#20107;&#20214;&#26085;&#24535;&#19978;&#35268;&#33539;&#28385;&#36275;&#24230;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#21457;&#29616;&#12289;&#26816;&#26597;&#21644;&#28418;&#31227;&#26816;&#27979;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Declarative process specifications define the behavior of processes by means of rules based on Linear Temporal Logic on Finite Traces (LTLf). In a mining context, these specifications are inferred from, and checked on, multi-sets of runs recorded by information systems (namely, event logs). To this end, being able to gauge the degree to which process data comply with a specification is key. However, existing mining and verification techniques analyze the rules in isolation, thereby disregarding their interplay. In this paper, we introduce a framework to devise probabilistic measures for declarative process specifications. Thereupon, we propose a technique that measures the degree of satisfaction of specifications over event logs. To assess our approach, we conduct an evaluation with real-world data, evidencing its applicability in discovery, checking, and drift detection contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38754;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#26102;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.05403</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38754;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#26102;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30693;&#35782;&#24211;&#26159;&#30693;&#35782;&#20013;&#24515;&#30340;AI&#30340;&#22522;&#30707;&#12290;&#35768;&#22810;&#30693;&#35782;&#24211;&#26159;&#20174;Web&#26469;&#28304;&#23454;&#29992;&#20027;&#20041;&#26500;&#24314;&#30340;&#65292;&#22240;&#27492;&#36828;&#38750;&#23436;&#25972;&#12290;&#36825;&#32473;&#20869;&#23481;&#30340;&#28040;&#36153;&#21644;&#31649;&#29702;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22914;&#20309;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#30693;&#35782;&#24211;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#65288;i&#65289;&#37096;&#20998;&#23553;&#38381;&#19990;&#30028;&#35821;&#20041;&#19979;&#30340;&#30693;&#35782;&#34920;&#31034;&#21644;&#26597;&#35810;&#30340;&#36923;&#36753;&#22522;&#30784;&#65307;&#65288;ii&#65289;&#36890;&#36807;&#32479;&#35745;&#27169;&#24335;&#20272;&#35745;&#27492;&#20449;&#24687;&#65307;&#65288;iii&#65289;&#20174;&#30693;&#35782;&#24211;&#21644;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#20110;&#21484;&#22238;&#29575;&#30340;&#20449;&#24687;&#65307;&#65288;iv&#65289;&#36776;&#21035;&#26377;&#36259;&#30340;&#21542;&#23450;&#35821;&#21477;&#65307;&#20197;&#21450;&#65288;v&#65289;&#30456;&#23545;&#21484;&#22238;&#29575;&#30340;&#23485;&#26494;&#27010;&#24565;&#12290;&#26412;&#35843;&#26597;&#38024;&#23545;&#20004;&#31867;&#21463;&#20247;&#65306;&#65288;1&#65289;&#23547;&#27714;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30693;&#35782;&#25351;&#21335;&#30340;&#20174;&#19994;&#32773;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26088;&#22312;&#25512;&#36827;&#30693;&#35782;&#24211;&#31649;&#29702;&#12289;&#36136;&#37327;&#35780;&#20272;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric AI. Many of them are constructed pragmatically from Web sources, and are thus far from complete. This poses challenges for the consumption as well as the curation of their content. While several surveys target the problem of completing incomplete KBs, the first problem is arguably to know whether and where the KB is incomplete in the first place, and to which degree.  In this survey we discuss how knowledge about completeness, recall, and negation in KBs can be expressed, extracted, and inferred. We cover (i) the logical foundations of knowledge representation and querying under partial closed-world semantics; (ii) the estimation of this information via statistical patterns; (iii) the extraction of information about recall from KBs and text; (iv) the identification of interesting negative statements; and (v) relaxed notions of relative recall.  This survey is targeted at two types of audiences: (1) practitione
&lt;/p&gt;</description></item><item><title>SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05392</link><description>&lt;p&gt;
&#20851;&#20110;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05392
&lt;/p&gt;
&lt;p&gt;
SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#30340;&#26032;&#29702;&#35299;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#22768;&#26126;&#30340;&#29702;&#35770;&#35777;&#25454;&#21644;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#21033;&#29992;SAM&#21487;&#20197;&#23454;&#29616;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#24847;&#22806;&#30340;&#22909;&#22788;&#12290;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#23548;&#33268;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#38477;&#20302;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;SAM&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/weizeming/SAM_AT&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
&lt;/p&gt;</description></item><item><title>VEDLIoT&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23433;&#20840;&#12289;&#33410;&#33021;&#21644;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#65288;AIoT&#65289;&#24212;&#29992;&#31243;&#24207;&#30340;&#25361;&#25112;&#12290;&#20854;&#26680;&#24515;&#26041;&#27861;&#22312;&#20110;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35748;&#30693;&#29289;&#32852;&#32593;&#30828;&#20214;&#24179;&#21488;&#65292;&#21033;&#29992;&#24494;&#26381;&#21153;&#22120;&#25216;&#26415;&#21644;&#24322;&#26500;&#35745;&#31639;&#20197;&#21450;&#20840;&#35889;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#38598;&#25104;&#12290;&#39033;&#30446;&#30340;&#36129;&#29486;&#28085;&#30422;&#20102;&#21487;&#20449;&#35745;&#31639;&#12289;&#36828;&#31243;&#26029;&#35328;&#21644;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.05388</link><description>&lt;p&gt;
VEDLIoT--&#19979;&#19968;&#20195;&#21152;&#36895;AIoT&#31995;&#32479;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
VEDLIoT -- Next generation accelerated AIoT systems and applications. (arXiv:2305.05388v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05388
&lt;/p&gt;
&lt;p&gt;
VEDLIoT&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23433;&#20840;&#12289;&#33410;&#33021;&#21644;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#65288;AIoT&#65289;&#24212;&#29992;&#31243;&#24207;&#30340;&#25361;&#25112;&#12290;&#20854;&#26680;&#24515;&#26041;&#27861;&#22312;&#20110;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35748;&#30693;&#29289;&#32852;&#32593;&#30828;&#20214;&#24179;&#21488;&#65292;&#21033;&#29992;&#24494;&#26381;&#21153;&#22120;&#25216;&#26415;&#21644;&#24322;&#26500;&#35745;&#31639;&#20197;&#21450;&#20840;&#35889;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#38598;&#25104;&#12290;&#39033;&#30446;&#30340;&#36129;&#29486;&#28085;&#30422;&#20102;&#21487;&#20449;&#35745;&#31639;&#12289;&#36828;&#31243;&#26029;&#35328;&#21644;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
VEDLIoT&#39033;&#30446;&#26088;&#22312;&#20026;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#65288;AIoT&#65289;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20248;&#21270;&#31639;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;AIoT&#31995;&#32479;&#22266;&#26377;&#30340;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#22312;&#20110;&#19968;&#31181;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35748;&#30693;&#29289;&#32852;&#32593;&#30828;&#20214;&#24179;&#21488;&#65292;&#23427;&#21033;&#29992;&#24494;&#26381;&#21153;&#22120;&#25216;&#26415;&#20351;&#29992;&#25143;&#33021;&#22815;&#37197;&#32622;&#30828;&#20214;&#20197;&#28385;&#36275;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#35201;&#27714;&#12290;&#24322;&#26500;&#35745;&#31639;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#38598;&#25104;&#20102;&#20840;&#35889;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#25552;&#20379;&#19987;&#29992;ASIC&#20197;&#21450;&#29992;&#20110;&#21487;&#37325;&#26500;&#35745;&#31639;&#30340;FPGA&#12290;&#35813;&#39033;&#30446;&#30340;&#36129;&#29486;&#28085;&#30422;&#20102;&#21487;&#20449;&#35745;&#31639;&#65292;&#36828;&#31243;&#26029;&#35328;&#21644;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#30340;&#33539;&#22260;&#65292;&#26088;&#22312;&#20419;&#36827;&#20581;&#22766;&#32780;&#26377;&#25928;&#30340;AIoT&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#12290;VEDLIoT&#30340;&#25972;&#20307;&#24895;&#26223;&#26159;&#36890;&#36807;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35748;&#30693;&#29289;&#32852;&#32593;&#30828;&#20214;&#24179;&#21488;&#65292;&#21033;&#29992;&#24494;&#26381;&#21153;&#22120;&#25216;&#26415;&#21644;&#24322;&#26500;&#35745;&#31639;&#20197;&#21450;&#38598;&#25104;&#20102;&#20840;&#35889;&#30828;&#20214;&#21152;&#36895;&#22120;&#20026;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#65288;AIoT&#65289;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#33410;&#33021;&#21644;&#23433;&#20840;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21487;&#20449;&#35745;&#31639;&#65292;&#36828;&#31243;&#26029;&#35328;&#21644;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#35299;&#20915;AIoT&#31995;&#32479;&#22266;&#26377;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The VEDLIoT project aims to develop energy-efficient Deep Learning methodologies for distributed Artificial Intelligence of Things (AIoT) applications. During our project, we propose a holistic approach that focuses on optimizing algorithms while addressing safety and security challenges inherent to AIoT systems. The foundation of this approach lies in a modular and scalable cognitive IoT hardware platform, which leverages microserver technology to enable users to configure the hardware to meet the requirements of a diverse array of applications. Heterogeneous computing is used to boost performance and energy efficiency. In addition, the full spectrum of hardware accelerators is integrated, providing specialized ASICs as well as FPGAs for reconfigurable computing. The project's contributions span across trusted computing, remote attestation, and secure execution environments, with the ultimate goal of facilitating the design and deployment of robust and efficient AIoT systems. The over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20195;&#30721;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21464;&#24322;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;Python&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;CodeExecutor&#27169;&#22411;&#20197;&#22686;&#24378;&#35821;&#20041;&#29702;&#35299;&#65292;&#35813;&#27169;&#22411;&#22312;&#20195;&#30721;&#25191;&#34892;&#12289;&#38646;-shot&#20195;&#30721;&#21040;&#20195;&#30721;&#25628;&#32034;&#21644;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#31561;&#26041;&#38754;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2305.05383</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#25191;&#34892;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Code Execution with Pre-trained Language Models. (arXiv:2305.05383v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20195;&#30721;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21464;&#24322;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;Python&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;CodeExecutor&#27169;&#22411;&#20197;&#22686;&#24378;&#35821;&#20041;&#29702;&#35299;&#65292;&#35813;&#27169;&#22411;&#22312;&#20195;&#30721;&#25191;&#34892;&#12289;&#38646;-shot&#20195;&#30721;&#21040;&#20195;&#30721;&#25628;&#32034;&#21644;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#31561;&#26041;&#38754;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25191;&#34892;&#26159;&#32534;&#31243;&#35821;&#35328;&#35821;&#20041;&#23398;&#20013;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#23427;&#21453;&#26144;&#20102;&#20195;&#30721;&#30340;&#30830;&#20999;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#38754;&#21521;&#20195;&#30721;&#26234;&#33021;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24573;&#30053;&#20102;&#25191;&#34892;&#36712;&#36857;&#65292;&#21482;&#20381;&#38752;&#28304;&#20195;&#30721;&#21644;&#21477;&#27861;&#32467;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#29702;&#35299;&#21644;&#25191;&#34892;&#20195;&#30721;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24322;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21644;&#36924;&#30495;&#30340;Python&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#22914;Codex&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeExecutor&#65292;&#19968;&#20010;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#39044;&#35757;&#32451;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#22686;&#24378;&#20854;&#35821;&#20041;&#29702;&#35299;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#20195;&#30721;&#25191;&#34892;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#26395;&#30340;&#34920;&#29616;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#38646;-shot&#20195;&#30721;&#21040;&#20195;&#30721;&#25628;&#32034;&#21644;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20851;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21644;&#27867;&#21270;&#30340;&#27934;&#35265;&#65292;&#24182;&#20026;&#20195;&#30721;&#26234;&#33021;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32844;&#19994;&#20934;&#22791;&#25216;&#33021;&#65292;&#27604;&#36739;&#20102;GPT-3&#21644;Turbo-GPT3.5&#22312;1149&#20010;&#19987;&#19994;&#35748;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;Turbo-GPT3.5&#30340;&#36890;&#36807;&#29575;&#36798;&#21040;&#20102;100%&#12290;&#27169;&#22411;&#35777;&#26126;&#20102;&#22312;&#35745;&#31639;&#26426;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#25216;&#33021;&#21644;&#28508;&#21147;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05377</link><description>&lt;p&gt;
&#19987;&#19994;&#35748;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;500&#20010;&#32844;&#20301;
&lt;/p&gt;
&lt;p&gt;
Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models. (arXiv:2305.05377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32844;&#19994;&#20934;&#22791;&#25216;&#33021;&#65292;&#27604;&#36739;&#20102;GPT-3&#21644;Turbo-GPT3.5&#22312;1149&#20010;&#19987;&#19994;&#35748;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;Turbo-GPT3.5&#30340;&#36890;&#36807;&#29575;&#36798;&#21040;&#20102;100%&#12290;&#27169;&#22411;&#35777;&#26126;&#20102;&#22312;&#35745;&#31639;&#26426;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#25216;&#33021;&#21644;&#28508;&#21147;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#19994;&#35748;&#35777;&#35843;&#26597;&#65292;&#20197;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#35780;&#20272;&#20854;&#23601;&#19994;&#25216;&#33021;&#12290;&#23427;&#27604;&#36739;&#20102;&#20004;&#20010;AI&#27169;&#22411;GPT-3&#21644;Turbo-GPT3.5&#22312;&#19968;&#20010;&#21253;&#25324;1149&#20010;&#19987;&#19994;&#35748;&#35777;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#30340;&#26159;&#32844;&#19994;&#20934;&#22791;&#32780;&#19981;&#26159;&#23398;&#26415;&#34920;&#29616;&#12290;GPT-3&#22312;39%&#30340;&#19987;&#19994;&#35748;&#35777;&#20013;&#21462;&#24471;&#20102;&#36890;&#36807;&#20998;&#25968;&#65288;&gt;70%&#27491;&#30830;&#29575;&#65289;&#65292;&#32780;&#27809;&#26377;&#36827;&#34892;&#24494;&#35843;&#25110;&#32771;&#35797;&#20934;&#22791;&#12290;&#27169;&#22411;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#30340;&#36164;&#26684;&#65292;&#22914;&#20113;&#21644;&#34394;&#25311;&#21270;&#12289;&#21830;&#19994;&#20998;&#26512;&#12289;&#32593;&#32476;&#35774;&#32622;&#21644;&#32500;&#20462;&#20197;&#21450;&#25968;&#25454;&#20998;&#26512;&#31561;&#12290;Turbo-GPT3.5&#22312;&#39047;&#20855;&#20215;&#20540;&#30340;Offensive Security Certified Professional&#65288;OSCP&#65289;&#32771;&#35797;&#20013;&#24471;&#20998;100%&#12290;&#27169;&#22411;&#36824;&#23637;&#29616;&#20102;&#22312;&#20854;&#20182;&#32844;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#25252;&#29702;&#12289;&#25345;&#29260;&#21672;&#35810;&#12289;&#33647;&#21058;&#23398;&#21644;&#25945;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;Turbo-GPT3.5&#22312;&#27809;&#26377;&#20934;&#22791;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#37329;&#34701;&#19994;&#30417;&#31649;&#23616;&#65288;FINRA&#65289;&#31995;&#21015;6&#32771;&#35797;&#30340;70%&#30340;&#25104;&#32489;&#12290;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;500&#20010;&#24037;&#20316;&#32844;&#20301;&#30340;&#25968;&#25454;&#38598;&#26469;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (&gt;70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.05374</link><description>&lt;p&gt;
HybridNet: &#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction. (arXiv:2305.05374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38459;&#22622;&#39044;&#27979;&#26159;&#24110;&#21161;&#35774;&#35745;&#24072;&#22312;VLSI&#35774;&#35745;&#21608;&#26399;&#20869;&#26356;&#24555;&#36845;&#20195;&#30340;&#37325;&#35201;&#29615;&#33410;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#22270;&#65288;&#20960;&#20309;&#22270;&#12289;&#25299;&#25169;&#22270;&#65289;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#21807;&#19968;&#23646;&#24615;&#37319;&#29992;&#19981;&#21516;&#30340;&#36793;&#32536;&#26500;&#24314;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#27599;&#20010;&#36335;&#24452;&#20013;&#37117;&#26377;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#24182;&#36890;&#36807;&#31934;&#32454;&#30340;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#32858;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21517;&#20026;HybridNet&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#21333;&#20803;&#20043;&#38388;&#30340;&#20960;&#20309;&#20132;&#20114;&#65292;&#32780;&#19988;&#36824;&#20445;&#30041;&#20102;&#21407;&#22987;&#30005;&#36335;&#25299;&#25169;&#20851;&#31995;&#12290;&#22312;ISPD2015&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate early congestion prediction can prevent unpleasant surprises at the routing stage, playing a crucial character in assisting designers to iterate faster in VLSI design cycles. In this paper, we introduce a novel strategy to fully incorporate topological and geometrical features of circuits by making several key designs in our network architecture. To be more specific, we construct two individual graphs (geometry-graph, topology-graph) with distinct edge construction schemes according to their unique properties. We then propose a dual-branch network with different encoder layers in each pathway and aggregate representations with a sophisticated fusion strategy. Our network, named HybridNet, not only provides a simple yet effective way to capture the geometric interactions of cells, but also preserves the original topological relationships in the netlist. Experimental results on the ISPD2015 benchmarks show that we achieve an improvement of 10.9% compared to previous methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05368</link><description>&lt;p&gt;
GNNs: &#21487;&#20197;&#26356;&#24378;&#12289;&#26356;&#26032;&#12289;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#38598;&#25104;&#37051;&#23621;&#33410;&#28857;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#34920;&#29616;&#20986;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#32780;&#36880;&#28176;&#38477;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;k&#36339;&#23376;&#22270;&#32858;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#28145;&#23618;GNN&#34920;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#32858;&#21512;&#23376;&#22270;&#30340;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#27531;&#24046;&#30340;GNN&#23454;&#38469;&#19978;&#21033;&#29992;&#20102;1&#21040;k&#36339;&#23376;&#22270;&#32858;&#21512;&#32467;&#26524;&#26469;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20854;&#27604;&#20043;&#21069;&#30340;&#27531;&#24046;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#21033;&#29992;1&#21040;k&#36339;&#36291;&#23376;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#32463;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#33719;&#24471;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.05364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#32463;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#33719;&#24471;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#31034;&#20363;&#26469;&#25191;&#34892;&#25351;&#20196;&#24182;&#25191;&#34892;&#26032;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21442;&#25968;&#21270;LLMs&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#20197;&#27604;&#24494;&#35843;&#20302;&#24471;&#22810;&#30340;&#25104;&#26412;&#25299;&#23637;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#19968;&#25512;&#29702;&#32447;&#36335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;LLM&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#31572;&#30340;&#35828;&#26126;&#24615;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#32780;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#65292;&#22312;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#36335;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;6.4%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#35282;&#24230;&#31361;&#20986;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26894;&#31649;&#26426;&#22120;&#20154;&#25163;&#26415;&#26415;&#20013;&#35268;&#21010;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30456;&#23545;&#20256;&#32479;&#30340;&#26415;&#21069;&#35268;&#21010;&#21644;&#26415;&#20013;&#27880;&#20876;&#24320;&#29615;&#25511;&#21046;&#30340;&#23616;&#38480;&#65292;&#33021;&#22815;&#25552;&#39640;&#25918;&#32622;&#20934;&#30830;&#24230;&#21644;&#20445;&#35777;&#25163;&#26415;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2305.05354</link><description>&lt;p&gt;
&#33034;&#26609;&#34701;&#21512;&#25163;&#26415;&#20013;&#32463;&#30382;&#26894;&#24339;&#26681;&#34746;&#38025;&#25918;&#32622;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement. (arXiv:2305.05354v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26894;&#31649;&#26426;&#22120;&#20154;&#25163;&#26415;&#26415;&#20013;&#35268;&#21010;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30456;&#23545;&#20256;&#32479;&#30340;&#26415;&#21069;&#35268;&#21010;&#21644;&#26415;&#20013;&#27880;&#20876;&#24320;&#29615;&#25511;&#21046;&#30340;&#23616;&#38480;&#65292;&#33021;&#22815;&#25552;&#39640;&#25918;&#32622;&#20934;&#30830;&#24230;&#21644;&#20445;&#35777;&#25163;&#26415;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#26609;&#34701;&#21512;&#25163;&#26415;&#38656;&#35201;&#39640;&#31934;&#24230;&#23454;&#26045;&#26894;&#24339;&#26681;&#34746;&#38025;&#26893;&#20837;&#65292;&#20854;&#24517;&#39035;&#22312;&#26497;&#24230;&#25509;&#36817;&#37325;&#35201;&#32467;&#26500;&#24182;&#19988;&#35299;&#21078;&#35270;&#37326;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#25163;&#26415;&#31995;&#32479;&#20197;&#25913;&#21892;&#25918;&#32622;&#20934;&#30830;&#24230;&#65292;&#20294;&#26159;&#29616;&#26377;&#31995;&#32479;&#20173;&#28982;&#23384;&#22312;&#20256;&#32479;&#30340;&#26415;&#21069;&#35268;&#21010;&#21644;&#26415;&#20013;&#27880;&#20876;&#24320;&#29615;&#25511;&#21046;&#30340;&#23616;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#23454;&#26102;&#35266;&#23519;&#30340;&#26894;&#31649;&#26426;&#22120;&#20154;&#25163;&#26415;&#26415;&#20013;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#20027;&#35201;&#30340;&#36129;&#29486;&#26377;&#65306;&#65288;1&#65289;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#23433;&#20840;&#36807;&#28388;&#22120;&#20445;&#35777;&#23433;&#20840;&#25805;&#20316;&#33021;&#21147;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20351;&#29992;&#20808;&#39564;&#35299;&#21078;&#32467;&#26500;&#20449;&#24687;&#23545;&#19981;&#23436;&#25972;&#30340;&#26415;&#20013;&#35299;&#21078;&#20449;&#24687;&#36827;&#34892;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spinal fusion surgery requires highly accurate implantation of pedicle screw implants, which must be conducted in critical proximity to vital structures with a limited view of anatomy. Robotic surgery systems have been proposed to improve placement accuracy, however, state-of-the-art systems suffer from the limitations of open-loop approaches, as they follow traditional concepts of preoperative planning and intraoperative registration, without real-time recalculation of the surgical plan. In this paper, we propose an intraoperative planning approach for robotic spine surgery that leverages real-time observation for drill path planning based on Safe Deep Reinforcement Learning (DRL). The main contributions of our method are (1) the capability to guarantee safe actions by introducing an uncertainty-aware distance-based safety filter; and (2) the ability to compensate for incomplete intraoperative anatomical information, by encoding a-priori knowledge about anatomical structures with a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;transformer&#21644;&#38598;&#25104;&#25216;&#26415;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#65292;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05325</link><description>&lt;p&gt;
&#21033;&#29992;transformer&#21644;&#38598;&#25104;&#25216;&#26415;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;
&lt;/p&gt;
&lt;p&gt;
Detection of depression on social networks using transformers and ensembles. (arXiv:2305.05325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;transformer&#21644;&#38598;&#25104;&#25216;&#26415;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#65292;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#25216;&#22312;&#25105;&#20204;&#29983;&#27963;&#20013;&#30340;&#24433;&#21709;&#19981;&#26029;&#22686;&#24378;&#65292;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#20063;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23427;&#19981;&#20165;&#26159;&#19968;&#31181;&#27807;&#36890;&#24037;&#20855;&#65292;&#36824;&#21487;&#20197;&#29992;&#26469;&#21521;&#31038;&#21306;&#20998;&#20139;&#25105;&#20204;&#30340;&#35266;&#28857;&#21644;&#24863;&#21463;&#12290;&#23545;&#20110;&#25233;&#37057;&#30151;&#31561;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#65292;&#20154;&#20204;&#20063;&#20250;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#26469;&#34920;&#36798;&#33258;&#24049;&#30340;&#24819;&#27861;&#23547;&#27714;&#24110;&#21161;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#24182;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#26469;&#25552;&#20379;&#24110;&#21161;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#22823;&#37327;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;BERT&#12289;RoBERTA&#12289;BERTweet&#21644;mentalBERT&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;Reddit&#21644;Twitter&#20004;&#20010;&#31038;&#20132;&#24179;&#21488;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#36328;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;transformer&#38598;&#25104;&#27169;&#22411;&#27604;&#21333;&#19968;&#30340;transformer&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the impact of technology on our lives is increasing, we witness increased use of social media that became an essential tool not only for communication but also for sharing information with community about our thoughts and feelings. This can be observed also for people with mental health disorders such as depression where they use social media for expressing their thoughts and asking for help. This opens a possibility to automatically process social media posts and detect signs of depression. We build several large pre-trained language model based classifiers for depression detection from social media posts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also construct two types of ensembles. We analyze the performance of our models on two data sets of posts from social platforms Reddit and Twitter, and investigate also the performance of transfer learning across the two data sets. The results show that transformer ensembles improve over the single transformer-based
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#21516;&#26893;&#29289;&#26469;&#28304;&#30340;&#28096;&#31881;&#26174;&#24494;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;81%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#26356;&#20248;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;f1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.05321</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21046;&#21058;&#20013;&#26174;&#24494;&#28096;&#31881;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Artificial Intelligence in the Classification of Microscopical Starch Images for Drug Formulation. (arXiv:2305.05321v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#21516;&#26893;&#29289;&#26469;&#28304;&#30340;&#28096;&#31881;&#26174;&#24494;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;81%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#26356;&#20248;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;f1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28096;&#31881;&#26159;&#26893;&#29289;&#20013;&#37325;&#35201;&#30340;&#33021;&#37327;&#26469;&#28304;&#65292;&#20855;&#26377;&#35768;&#22810;&#21307;&#33647;&#24037;&#19994;&#20013;&#30340;&#29992;&#36884;&#65292;&#22914;&#33647;&#21697;&#20013;&#30340;&#31896;&#21512;&#21058;&#12289;&#23849;&#35299;&#21058;&#21644;&#22686;&#22609;&#21058;&#65292;&#22240;&#27492;&#38656;&#35201;&#38750;&#24120;&#20180;&#32454;&#30340;&#29289;&#29702;&#21270;&#23398;&#20998;&#26512;&#36827;&#34892;&#27491;&#30830;&#30340;&#35782;&#21035;&#21644;&#39564;&#35777;&#65292;&#21253;&#25324;&#26174;&#24494;&#38236;&#26816;&#26597;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;CNN&#23545;&#26469;&#33258;9&#31181;&#26893;&#29289;&#26469;&#28304;&#30340;&#28096;&#31881;&#26679;&#26412;&#30340;&#26174;&#24494;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26469;&#33258;MicroNet&#25968;&#25454;&#38598;&#30340;&#26174;&#24494;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#20102;61%&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#20174;Imagenet&#25968;&#25454;&#38598;&#33719;&#24471;&#30340;&#38543;&#26426;&#26085;&#24120;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#20934;&#30830;&#29575;&#36291;&#21319;&#33267;81%&#12290;&#19982;&#22312;MicroNet&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Imagenet&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;f1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starches are important energy sources found in plants with many uses in the pharmaceutical industry such as binders, disintegrants, bulking agents in drugs and thus require very careful physicochemical analysis for proper identification and verification which includes microscopy. In this work, we applied artificial intelligence techniques (using transfer learning and deep convolution neural network CNNs to microscopical images obtained from 9 starch samples of different botanical sources. Our approach obtained an accuracy of 61% when the machine learning model was pretrained on microscopic images from MicroNet dataset. However the accuracy jumped to 81% for model pretrained on random day to day images obtained from Imagenet dataset. The model pretrained on the imagenet dataset also showed a better precision, recall and f1 score than that pretrained on the imagenet dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22788;&#29702;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;Pointer Network&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20197;&#21069;&#25552;&#20986;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20026;&#20934;&#30830;&#30340;SSA&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05311</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#22522;&#20110;&#36716;&#31227;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Structured Sentiment Analysis as Transition-based Dependency Parsing. (arXiv:2305.05311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22788;&#29702;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;Pointer Network&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20197;&#21069;&#25552;&#20986;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20026;&#20934;&#30830;&#30340;SSA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#65288;SSA&#65289;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#24182;&#20197;&#22270;&#24418;&#32467;&#26500;&#20805;&#20998;&#34920;&#31034;&#35813;&#20449;&#24687;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20934;&#30830;&#30340;&#25191;&#34892;SSA&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#35270;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#12290;&#23613;&#31649;&#25105;&#20204;&#21487;&#20197;&#22312;&#25991;&#29486;&#20013;&#21457;&#29616;&#22522;&#20110;&#36716;&#31227;&#30340;&#31639;&#27861;&#22312;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#25152;&#26377;&#23581;&#35797;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;SSA&#30340;&#26041;&#27861;&#37117;&#22522;&#20110;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23558;SSA&#20316;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22788;&#29702;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#31995;&#32479;&#65292;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#22788;&#29702;&#36755;&#20837;&#25991;&#26412;&#65292;&#36880;&#27493;&#29983;&#25104;&#21253;&#21547;&#25152;&#26377;&#35782;&#21035;&#20986;&#30340;&#35266;&#28857;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23454;&#29616;&#25105;&#20204;&#30340;&#26368;&#32456;&#22522;&#20110;&#36716;&#31227;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#21161;&#20102;Pointer Network&#20307;&#31995;&#32467;&#26500;&#20316;&#20026;&#25903;&#25745;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36234;&#20102;SSA&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#22312;&#21253;&#25324;SemEval 2014 Task 4&#22312;&#20869;&#30340;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36804;&#20170;&#20026;&#27490;&#25253;&#21578;&#30340;&#26368;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a dependency parsing task. Although we can find in the literature how transition-based algorithms excel in dependency parsing in terms of accuracy and efficiency, all proposed attempts to tackle SSA following that approach were based on graph-based models. In this article, we present the first transition-based method to address SSA as dependency parsing. Specifically, we design a transition system that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions. To effectively implement our final transition-based model, we resort to a Pointer Network architecture as a backbone. From an extensive evaluation, we demonstrate that our mod
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.05280</link><description>&lt;p&gt;
VCSUM&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05280
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26032;&#38395;&#21644;&#32842;&#22825;&#25688;&#35201;&#30456;&#27604;&#65292;&#30001;&#20110;&#25968;&#25454;&#21463;&#38480;&#65292;&#20250;&#35758;&#25688;&#35201;&#30340;&#21457;&#23637;&#21463;&#21040;&#26497;&#22823;&#30340;&#20943;&#36895;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#25105;&#20204;&#22768;&#31216;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#22810;&#21151;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#20027;&#39064;&#21010;&#20998;&#12289;&#22836;&#26465;&#12289;&#20998;&#27573;&#25688;&#35201;&#12289;&#25972;&#20010;&#20250;&#35758;&#25688;&#35201;&#21644;&#26174;&#35201;&#21477;&#23376;&#31561;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;VCSum&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20851;&#20110;&#19981;&#21516;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;VCSum&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312; \url{https://github.com/hahahawu/VCSum} &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#30340;&#26032;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23558;&#39038;&#23458;&#30340;&#22312;&#32447;&#34892;&#20026;&#26144;&#23556;&#25104;&#20026;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#65292;&#20197;&#20415;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#12289;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#30340;&#36141;&#29289;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.05279</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#25512;&#33616;&#20197;&#22522;&#20110;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning to Personalize Recommendation based on Customers' Shopping Intents. (arXiv:2305.05279v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#30340;&#26032;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23558;&#39038;&#23458;&#30340;&#22312;&#32447;&#34892;&#20026;&#26144;&#23556;&#25104;&#20026;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#65292;&#20197;&#20415;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#12289;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#30340;&#36141;&#29289;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39038;&#23458;&#30340;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#65292;&#22914;&#20182;&#20204;&#21435;&#38706;&#33829;&#25110;&#20030;&#21150;&#29983;&#26085;&#27966;&#23545;&#30340;&#24895;&#26395;&#65292;&#23545;&#20110;&#30005;&#21830;&#24179;&#21488;&#38750;&#24120;&#37325;&#35201;&#65307;&#23427;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#30456;&#20851;&#12289;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#30340;&#25512;&#33616;&#26469;&#25552;&#39640;&#36141;&#29289;&#20307;&#39564;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#25361;&#25112;&#65292;&#36825;&#31181;&#39640;&#32423;&#21035;&#30340;&#36141;&#29289;&#24847;&#22270;&#22312;&#34892;&#19994;&#20013;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#30340;&#26032;&#31995;&#32479;&#65292;&#26126;&#30830;&#22320;&#35782;&#21035;&#21644;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#30340;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#26469;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#35782;&#21035;&#20122;&#39532;&#36874;&#23458;&#25143;&#27491;&#22312;&#36861;&#27714;&#30340;&#21508;&#31181;&#39640;&#32423;&#21035;&#30446;&#26631;&#65292;&#22914;&#8220;&#21435;&#38706;&#33829;&#8221;&#21644;&#8220;&#20934;&#22791;&#28023;&#28393;&#27966;&#23545;&#8221;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#25193;&#23637;&#65288;&#36328;&#36234;21&#20010;&#22269;&#23478;&#30340;14&#31181;&#35821;&#35328;&#65289;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23558;&#27599;&#20010;&#23458;&#25143;&#30340;&#22312;&#32447;&#34892;&#20026;&#65292;&#22914;&#20135;&#21697;&#25628;&#32034;&#21644;&#20010;&#20307;&#39033;&#30446;&#21442;&#19982;&#65292;&#26144;&#23556;&#25104;&#19968;&#32452;&#39640;&#32423;&#21035;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the customers' high level shopping intent, such as their desire to go camping or hold a birthday party, is critically important for an E-commerce platform; it can help boost the quality of shopping experience by enabling provision of more relevant, explainable, and diversified recommendations. However, such high level shopping intent has been overlooked in the industry due to practical challenges. In this work, we introduce Amazon's new system that explicitly identifies and utilizes each customer's high level shopping intents for personalizing recommendations. We develop a novel technique that automatically identifies various high level goals being pursued by the Amazon customers, such as "go camping", and "preparing for a beach party". Our solution is in a scalable fashion (in 14 languages across 21 countries). Then a deep learning model maps each customer's online behavior, e.g. product search and individual item engagements, into a subset of high level shopping intents
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#19988;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20844;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#26059;&#36716;&#21516;&#27493;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05268</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#23454;&#29616;&#26059;&#36716;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Rotation Synchronization via Deep Matrix Factorization. (arXiv:2305.05268v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#19988;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20844;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#26059;&#36716;&#21516;&#27493;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26059;&#36716;&#21516;&#27493;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20174;&#25104;&#23545;&#30340;&#26059;&#36716;&#20013;&#24674;&#22797;&#32477;&#23545;&#26059;&#36716;&#65292;&#20854;&#20013;&#26410;&#30693;&#25968;&#21644;&#27979;&#37327;&#20540;&#20998;&#21035;&#34920;&#31034;&#20026;&#22270;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#35813;&#38382;&#39064;&#26159;&#32467;&#26500;&#20174;&#36816;&#21160;&#21644;&#21516;&#26102;&#23450;&#20301;&#21644;&#26144;&#23556;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20844;&#24335;&#21270;&#21516;&#27493;&#38382;&#39064;&#65292;&#36825;&#39033;&#24037;&#20316;&#22312;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#25165;&#24320;&#22987;&#25506;&#32034;&#12290;&#21463;&#21040;&#28145;&#24230;&#30697;&#38453;&#34917;&#20840;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#26059;&#36716;&#21516;&#27493;&#34920;&#31034;&#20026;&#20855;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#32780;&#20808;&#21069;&#30340;&#28145;&#24230;&#26041;&#27861;&#26159;&#21463;&#30417;&#30563;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#23545;&#25163;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#21512;&#25104;&#21311;&#21517;&#21270;&#30149;&#20154;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21644;&#22521;&#35757;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#25968;&#25454;&#35775;&#38382;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.05247</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65306;&#24179;&#34913;&#30740;&#31350;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy. (arXiv:2305.05247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#21512;&#25104;&#21311;&#21517;&#21270;&#30149;&#20154;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21644;&#22521;&#35757;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#25968;&#25454;&#35775;&#38382;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#21644;&#25968;&#23383;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20419;&#20351;&#20102;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#27934;&#35265;&#20197;&#22686;&#24378;&#30149;&#24739;&#32467;&#26524;&#65292;&#24314;&#31435;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#30495;&#23454;&#30340;&#30149;&#20154;&#25968;&#25454;&#20250;&#23548;&#33268;&#38544;&#31169;&#21644;&#30417;&#31649;&#25361;&#25112;&#65292;&#21253;&#25324;HIPAA&#21644;GDPR&#30340;&#21512;&#35268;&#35201;&#27714;&#12290;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#22914;GAN&#21644;VAE&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#24179;&#34913;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#35775;&#38382;&#21644;&#30149;&#20154;&#38544;&#31169;&#20445;&#25252;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#21019;&#24314;&#36924;&#30495;&#30340;&#21311;&#21517;&#21270;&#30149;&#20154;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21644;&#22521;&#35757;&#65292;&#25506;&#32034;&#21512;&#25104;&#25968;&#25454;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20854;&#30410;&#22788;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21512;&#25104;&#25968;&#25454;&#26377;&#28508;&#21147;&#36890;&#36807;&#25552;&#20379;&#21311;&#21517;&#30149;&#20154;&#25968;&#25454;&#26469;&#38761;&#21629;&#21270;&#21307;&#30103;&#20445;&#20581;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#24182;&#23454;&#29616;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA and GDPR. Synthetic data generation, using generative AI models like GANs and VAEs offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training, explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Learnable Behavioral Control (LBC)&#26694;&#26550;&#65292;&#20351;&#24471;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#24471;&#21040;&#25193;&#22823;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#22312;Atari&#28216;&#25103;&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#36798;&#21040;10&#20010;&#28216;&#25103;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.05239</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#25511;&#21046;&#65306;&#36890;&#36807;&#39640;&#25928;&#34892;&#20026;&#36873;&#25321;&#25171;&#30772;Atari&#20154;&#31867;&#19990;&#30028;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection. (arXiv:2305.05239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Learnable Behavioral Control (LBC)&#26694;&#26550;&#65292;&#20351;&#24471;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#24471;&#21040;&#25193;&#22823;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#22312;Atari&#28216;&#25103;&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#36798;&#21040;10&#20010;&#28216;&#25103;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#38382;&#39064;&#26159;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#24037;&#20316;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#25506;&#32034;&#31574;&#30053;&#30340;&#20154;&#32676;&#20013;&#25910;&#38598;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#26679;&#26412;&#12290;&#33258;&#36866;&#24212;&#31574;&#30053;&#36873;&#25321;&#24050;&#34987;&#29992;&#20110;&#34892;&#20026;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#39044;&#23450;&#20041;&#31574;&#30053;&#31181;&#32676;&#30340;&#38480;&#21046;&#65292;&#36825;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#34892;&#20026;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#31216;&#20026;&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#25511;&#21046;&#65288;LBC&#65289;&#26469;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;a)&#36890;&#36807;&#20174;&#25152;&#26377;&#31574;&#30053;&#20013;&#21046;&#23450;&#28151;&#21512;&#34892;&#20026;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25193;&#22823;&#30340;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#65307;b)&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#36873;&#25321;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;LBC&#24341;&#20837;&#20998;&#24067;&#24335;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#20248;&#21270;&#34892;&#20026;&#26144;&#23556;&#30340;&#36873;&#25321;&#26469;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#22312;10&#20010;Atari&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LBC&#26694;&#26550;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05230</link><description>&lt;p&gt;
FedNoRo: &#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;(FNLL)&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#22810;&#28304;&#20998;&#25955;&#23398;&#20064;&#24037;&#20855;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#20840;&#23616;&#25968;&#25454;&#31867;&#21035;&#24179;&#34913;&#30340;&#20551;&#35774;&#65292;&#21487;&#33021;&#26080;&#27861;&#24314;&#27169;&#22797;&#26434;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#30495;&#23454;&#30340;&#32852;&#37030;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#25968;&#25454;&#26159;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#22122;&#22768;&#26159;&#24322;&#36136;&#30340;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312; FedNoRo &#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#37319;&#29992;&#27599;&#31867;&#25439;&#22833;&#25351;&#26631;&#20043;&#21518;&#36319;&#38543;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22024;&#26434;&#23458;&#25143;&#31471;&#35782;&#21035;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21516;&#26102;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#36317;&#31163;&#24863;&#30693;&#32858;&#21512;&#20989;&#25968;&#36827;&#34892;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#27169;&#22411;&#26356;&#26032;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedNoRo &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340; FNLL &#26041;&#27861;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;</title><link>http://arxiv.org/abs/2305.05228</link><description>&lt;p&gt;
&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#25552;&#21319;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance. (arXiv:2305.05228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#20122;&#39532;&#36874;&#29983;&#20135;&#21151;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#65292;&#20174;&#26102;&#23578;&#23646;&#24615;&#26816;&#27979;&#21040;&#21697;&#29260;&#35782;&#21035;&#12290;&#23454;&#29616;&#36825;&#20123;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#37326;&#22806;&#35270;&#35273;&#32972;&#26223;&#20449;&#21495;&#65292;&#20854;&#20013;&#21253;&#21547;&#28151;&#28102;&#27169;&#22411;&#30340;&#26080;&#20851;&#20687;&#32032;&#65292;&#20351;&#27169;&#22411;&#38590;&#20197;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#24182;&#26681;&#25454;&#35813;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21033;&#29992;&#23450;&#20301;&#24341;&#23548;&#65292;&#20197;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#26377;&#26631;&#31614;&#30340;AUC&#24471;&#20998;&#30340;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#20026;15.27%&#12290;&#26680;&#24515;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#28041;&#21450;&#23545;Instagram&#26102;&#23578;&#26381;&#35013;&#30340;&#22810;&#26631;&#31614;&#26102;&#23578;&#23646;&#24615;&#20998;&#31867;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained multi-label classification models have broad applications in Amazon production features, such as visual based label predictions ranging from fashion attribute detection to brand recognition. One challenge to achieve satisfactory performance for those classification tasks in real world is the wild visual background signal that contains irrelevant pixels which confuses model to focus onto the region of interest and make prediction upon the specific region. In this paper, we introduce a generic semantic- embedding deep neural network to apply the spatial awareness semantic feature incorporating a channel- wise attention based model to leverage the localization guidance to boost model performance for multi- label prediction. We observed an Avg.relative improvement of 15.27% in terms of AUC score across all labels compared to the baseline approach. Core experiment and ablation studies involve multi-label fashion attribute classification performed on Instagram fashion apparels' 
&lt;/p&gt;</description></item><item><title>FishRecGAN&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#20854;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#65292;&#24182;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;</title><link>http://arxiv.org/abs/2305.05222</link><description>&lt;p&gt;
FishRecGAN&#65306;&#29992;&#20110;&#40060;&#30524;&#22270;&#20687;&#30699;&#27491;&#21644;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#26631;&#23450;&#30340;&#31471;&#21040;&#31471;GAN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration. (arXiv:2305.05222v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05222
&lt;/p&gt;
&lt;p&gt;
FishRecGAN&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#20854;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#65292;&#24182;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#20351;&#29992;Pix2Pix GAN&#21644;Wasserstein GAN&#65288;W-Pix2PixGAN&#65289;&#24320;&#21457;&#30340;Quick Image Rectification&#27169;&#22359;&#65292;&#20197;&#21450;&#20351;&#29992;CNN&#26550;&#26500;&#30340;Calibration&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26631;&#23450;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Quick Image Rectification&#27169;&#22359;&#20013;&#36755;&#20986;&#30340;&#30452;&#32447;&#29305;&#24449;&#20316;&#20026;&#25351;&#23548;&#26679;&#26412;&#20256;&#36882;&#32473;Calibration&#27169;&#22359;&#65292;&#20197;&#23398;&#20064;&#26657;&#27491;&#21069;&#21518;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36879;&#35270;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end deep learning approach to rectify fisheye images and simultaneously calibrate camera intrinsic and distortion parameters. Our method consists of two parts: a Quick Image Rectification Module developed with a Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a CNN architecture. Our Quick Rectification Network performs robust rectification with good resolution, making it suitable for constant calibration in camera-based surveillance equipment. To achieve high-quality calibration, we use the straightened output from the Quick Rectification Module as a guidance-like semantic feature map for the Calibration Module to learn the geometric relationship between the straightened feature and the distorted feature. We train and validate our method with a large synthesized dataset labeled with well-simulated parameters applied to a perspective image dataset. Our solution has achieved robust performance in high-resolution with a significant PSNR v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23376;&#27880;&#24847;&#21147;&#31574;&#30053;&#65288;LSAS&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#32531;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#27880;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;DNN&#36807;&#20110;&#32858;&#28966;&#20110;&#26631;&#31614;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#26080;&#27861;&#23436;&#20840;&#21253;&#21547;&#29702;&#24819;&#21306;&#22495;&#12290;LSAS&#33021;&#22815;&#25913;&#36827;&#29616;&#26377;&#30340;&#33258;&#25105;&#20851;&#27880;&#27169;&#22359;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27969;&#34892;&#30340;&#20851;&#27880;&#32593;&#32476;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05200</link><description>&lt;p&gt;
LSAS&#65306;&#29992;&#36731;&#37327;&#32423;&#23376;&#27880;&#24847;&#21147;&#31574;&#30053;&#32531;&#35299;&#20851;&#27880;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias Problem. (arXiv:2305.05200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23376;&#27880;&#24847;&#21147;&#31574;&#30053;&#65288;LSAS&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#32531;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#27880;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;DNN&#36807;&#20110;&#32858;&#28966;&#20110;&#26631;&#31614;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#26080;&#27861;&#23436;&#20840;&#21253;&#21547;&#29702;&#24819;&#21306;&#22495;&#12290;LSAS&#33021;&#22815;&#25913;&#36827;&#29616;&#26377;&#30340;&#33258;&#25105;&#20851;&#27880;&#27169;&#22359;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27969;&#34892;&#30340;&#20851;&#27880;&#32593;&#32476;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24615;&#33021;&#19982;&#20854;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#65292;&#21363;&#35782;&#21035;&#24182;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#20687;&#32032;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991; quantitatively and statistically illustrate&#65292;&#22312;&#35768;&#22810;&#27969;&#34892;&#25968;&#25454;&#38598;&#30340;&#35768;&#22810;&#26679;&#26412;&#20013;&#65292;DNN&#23384;&#22312;&#20005;&#37325;&#30340;&#20851;&#27880;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer vision, the performance of deep neural networks (DNNs) is highly related to the feature extraction ability, i.e., the ability to recognize and focus on key pixel regions in an image. However, in this paper, we quantitatively and statistically illustrate that DNNs have a serious attention bias problem on many samples from some popular datasets: (1) Position bias: DNNs fully focus on label-independent regions; (2) Range bias: The focused regions from DNN are not completely contained in the ideal region. Moreover, we find that the existing self-attention modules can alleviate these biases to a certain extent, but the biases are still non-negligible. To further mitigate them, we propose a lightweight sub-attention strategy (LSAS), which utilizes high-order sub-attention modules to improve the original self-attention modules. The effectiveness of LSAS is demonstrated by extensive experiments on widely-used benchmark datasets and popular attention networks. We release our code to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22240;&#26524;&#25512;&#26029;&#35282;&#24230;&#20986;&#21457;&#30340;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;-shot&#26694;&#26550;COLA&#26469;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.05191</link><description>&lt;p&gt;
COLA: &#22240;&#26524;&#25512;&#26029;&#35282;&#24230;&#19979;&#30340;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective. (arXiv:2305.05191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22240;&#26524;&#25512;&#26029;&#35282;&#24230;&#20986;&#21457;&#30340;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;-shot&#26694;&#26550;COLA&#26469;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20107;&#20214;&#20043;&#38388;&#30340;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#65288;&#22240;&#26524;&#20851;&#31995;&#65289;&#38271;&#26399;&#20197;&#26469;&#26159;&#19968;&#39033;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#20107;&#20214;&#30340;&#22797;&#26434;&#24615;&#65292;&#19968;&#20010;&#20107;&#20214;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#22312;&#26816;&#27979;&#22240;&#26524;&#20851;&#31995;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#20808;&#21069;&#20851;&#20110;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#30340;&#24037;&#20316;&#21482;&#32771;&#34385;&#20004;&#20010;&#20107;&#20214;&#24182;&#24573;&#30053;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#65292;&#31616;&#21270;&#20102;&#20219;&#21153;&#30340;&#21046;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#26816;&#27979;&#20107;&#20214;&#24207;&#21015;&#65288;&#21363;&#19978;&#19979;&#25991;&#65289;&#20013;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#65292;&#31216;&#20026;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;-shot&#26694;&#26550;&#65306;COLA&#65288;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#22120;&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#36825;&#20010;&#26694;&#26550;&#20174;&#26102;&#38388;&#24615;&#33719;&#24471;&#20102;&#20016;&#23500;&#30340;&#20598;&#21457;&#30417;&#30563;&#65292;&#24182;&#24179;&#34913;&#20102;&#26469;&#33258;&#22810;&#20010;&#26102;&#38388;&#25139;&#30340;&#21327;&#21464;&#37327;&#20197;&#28040;&#38500;&#28151;&#28102;&#25928;&#24212;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;COLA&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;DeepFire2&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#33455;&#29255;&#30340;&#36229;&#32423;&#36923;&#36753;&#21306;&#22495;&#26144;&#23556;&#22823;&#22411;&#32593;&#32476;&#23618;&#65292;&#36991;&#20813;&#20102;&#36923;&#36753;&#36164;&#28304;&#38480;&#21046;&#23618;&#22823;&#23567;&#65292;&#21516;&#26102;&#36890;&#36807;&#28145;&#24230;&#27969;&#27700;&#32447;&#25552;&#39640;&#20102;&#26102;&#38047;&#36895;&#24230;&#65292;&#23558;&#21534;&#21520;&#37327;&#21644;&#21151;&#29575;&#25928;&#29575;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.05187</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;DeepFire2&#65306;&#22522;&#20110;FPGA&#30340;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepFire2: A Convolutional Spiking Neural Network Accelerator on FPGAs. (arXiv:2305.05187v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;DeepFire2&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#33455;&#29255;&#30340;&#36229;&#32423;&#36923;&#36753;&#21306;&#22495;&#26144;&#23556;&#22823;&#22411;&#32593;&#32476;&#23618;&#65292;&#36991;&#20813;&#20102;&#36923;&#36753;&#36164;&#28304;&#38480;&#21046;&#23618;&#22823;&#23567;&#65292;&#21516;&#26102;&#36890;&#36807;&#28145;&#24230;&#27969;&#27700;&#32447;&#25552;&#39640;&#20102;&#26102;&#38047;&#36895;&#24230;&#65292;&#23558;&#21534;&#21520;&#37327;&#21644;&#21151;&#29575;&#25928;&#29575;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#20223;&#29983;&#23398;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20195;&#26367;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20056;&#21152;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#20294;&#24403;&#21152;&#36895;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#19987;&#29992;&#30828;&#20214;&#23454;&#29616;&#36825;&#20123;&#31070;&#32463;&#20803;&#22312;&#21151;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#20855;&#26377;&#36739;&#24046;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;DeepFire2&#24341;&#20837;&#20102;&#19968;&#31181;&#30828;&#20214;&#26550;&#26500;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22823;&#22411;&#32593;&#32476;&#23618;&#26144;&#23556;&#21040;&#22810;&#20010;&#36229;&#32423;&#36923;&#36753;&#21306;&#22495;&#20013;&#30340;&#22810;&#20010;&#33455;&#29255;&#19978;&#12290;&#36825;&#32473;&#20102;&#26356;&#22810;&#30340;&#36164;&#28304;&#20998;&#37197;&#21644;&#24182;&#34892;&#24615;&#25511;&#21046;&#65292;&#20174;&#32780;&#20351;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#28040;&#32791;&#21463;&#30410;&#12290;&#36991;&#20813;&#20351;&#29992;&#26597;&#25214;&#34920;&#26469;&#23454;&#29616;SNN&#30340;AND&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#36923;&#36753;&#36164;&#28304;&#38480;&#21046;&#23618;&#22823;&#23567;&#12290;&#28145;&#24230;&#27969;&#27700;&#32447;&#19981;&#20165;&#21487;&#20197;&#23558;&#26102;&#38047;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;600 MHz&#65292;&#32780;&#19988;&#19982;DeepFire&#19978;&#19968;&#20010;&#29256;&#26412;&#30456;&#27604;&#65292;&#25105;&#20204;&#23558;&#21534;&#21520;&#37327;&#21644;&#21151;&#29575;&#25928;&#29575;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired spiking neural networks (SNNs) replace the multiply-accumulate operations of traditional neural networks by integrate-and-fire neurons, with the goal of achieving greater energy efficiency. Specialized hardware implementations of those neurons clearly have advantages over general-purpose devices in terms of power and performance, but exhibit poor scalability when it comes to accelerating large neural networks. DeepFire2 introduces a hardware architecture which can map large network layers efficiently across multiple super logic regions in a multi-die FPGA. That gives more control over resource allocation and parallelism, benefiting both throughput and energy consumption. Avoiding the use of lookup tables to implement the AND operations of an SNN, prevents the layer size to be limited by logic resources. A deep pipeline does not only lead to an increased clock speed of up to 600 MHz. We double the throughput and power efficiency compared to our previous version of DeepFir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#35821;&#26009;&#24211;CSED&#65292;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21477;&#27861;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;CSED-R&#21644;CSED-C&#20219;&#21153;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05183</link><description>&lt;p&gt;
CSED: &#19968;&#20010;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CSED: A Chinese Semantic Error Diagnosis Corpus. (arXiv:2305.05183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#35821;&#26009;&#24211;CSED&#65292;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21477;&#27861;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;CSED-R&#21644;CSED-C&#20219;&#21153;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#32416;&#27491;&#30340;&#24037;&#20316;&#22823;&#22810;&#38598;&#20013;&#22312;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#21644;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#35786;&#26029;&#65288;CGED&#65289;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#65288;CSED&#65289;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#35821;&#20041;&#38169;&#35823;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#21477;&#27861;&#19981;&#35268;&#21017;&#29978;&#33267;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;CSED&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#25968;&#25454;&#38598;&#12290;&#19968;&#20010;&#26159;&#29992;&#20110;CSED&#35782;&#21035;&#65288;CSED-R&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#29992;&#20110;CSED&#26356;&#27491;&#65288;CSED-C&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#36890;&#36807;&#36136;&#37327;&#20445;&#35777;&#26426;&#21046;&#20445;&#35777;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#20010;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;CSED&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#20154;&#31867;&#24471;&#20998;&#20063;&#24456;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21477;&#27861;&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;CSED&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;CSED-R&#21644;CSED-C&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, much Chinese text error correction work has focused on Chinese Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In contrast, little attention has been paid to the complicated problem of Chinese Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of semantic errors is important because they are very common and may lead to syntactic irregularities or even problems of comprehension. To investigate this, we build the CSED corpus, which includes two datasets. The one is for the CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C) task. Our annotation guarantees high-quality data through quality assurance mechanisms. Our experiments show that powerful pre-trained models perform poorly on this corpus. We also find that the CSED task is challenging, as evidenced by the fact that even humans receive a low score. This paper proposes syntax-aware models to specifically adapt to the CSED task. The experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05181</link><description>&lt;p&gt;
MoT&#65306;&#39044;&#24605;&#32771;&#21644;&#22238;&#24518;&#21151;&#33021;&#20351; ChatGPT &#22312;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#20013;&#33258;&#25105;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#35201;&#23454;&#29616;&#23427;&#20204;&#30340;&#26681;&#26412;&#24615;&#25913;&#36827;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#24605;&#32771;&#21644;&#35760;&#24518;&#36731;&#26494;&#25552;&#39640;&#33258;&#25105;&#27700;&#24179;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; MoT&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24605;&#24819;&#35760;&#24518;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#36827;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;1. &#22312;&#27979;&#35797;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#21152;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#24605;&#32771;&#65292;&#24182;&#23558;&#39640;&#32622;&#20449;&#24230;&#30340;&#24819;&#27861;&#20445;&#23384;&#20026;&#22806;&#37096;&#35760;&#24518;&#12290;2. &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#24518;&#30456;&#20851;&#30340;&#35760;&#24518;&#65292;&#24110;&#21161;&#33258;&#24049;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27599;&#20010;&#32452;&#20214;&#37117;&#21457;&#25381;&#20102;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24418;&#24335;&#21270; Hopfield &#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28155;&#21152;&#38598;&#21512;&#36830;&#25509;&#24182;&#23558;&#36825;&#20123;&#36830;&#25509;&#23884;&#20837;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#20013;&#65292;&#21487;&#22686;&#21152;&#23384;&#20648;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05179</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#21270; Hopfield&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Simplicial Hopfield networks. (arXiv:2305.05179v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05179
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24418;&#24335;&#21270; Hopfield &#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28155;&#21152;&#38598;&#21512;&#36830;&#25509;&#24182;&#23558;&#36825;&#20123;&#36830;&#25509;&#23884;&#20837;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#20013;&#65292;&#21487;&#22686;&#21152;&#23384;&#20648;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#32593;&#32476;&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24490;&#29615;&#36830;&#25509;&#26435;&#37325;&#21644;&#26356;&#26032;&#35268;&#21017;&#65292;&#22312;&#20854;&#31070;&#32463;&#20803;&#29366;&#24577;&#19978;&#23384;&#20648;&#35760;&#24518;&#27169;&#24335;&#65292;&#20351;&#32593;&#32476;&#30340;&#33021;&#37327;&#26223;&#35266;&#22260;&#32469;&#35760;&#24518;&#22788;&#24418;&#25104;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;$N$&#20010;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#22810;&#23569;&#20010;&#31283;&#23450;&#30340;&#12289;&#20855;&#26377;&#36275;&#22815;&#21560;&#24341;&#21147;&#30340;&#35760;&#24518;&#27169;&#24335;&#65292;&#31572;&#26696;&#21462;&#20915;&#20110;&#26435;&#37325;&#21644;&#26356;&#26032;&#35268;&#21017;&#30340;&#36873;&#25321;&#12290;&#21463;&#29983;&#29289;&#23398;&#20013;&#38598;&#21512;&#36830;&#25509;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#38598;&#21512;&#36830;&#25509;&#24182;&#23558;&#36825;&#20123;&#36830;&#25509;&#23884;&#20837;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#20013;&#26469;&#25193;&#23637;Hopfield&#32593;&#32476;&#12290;&#21333;&#32431;&#22797;&#21512;&#20307;&#26159;&#22270;&#24418;&#30340;&#39640;&#32500;&#31867;&#27604;&#65292;&#33258;&#28982;&#34920;&#31034;&#25104;&#23545;&#21644;&#38598;&#21512;&#20851;&#31995;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21333;&#32431;&#24418; Hopfield &#32593;&#32476;&#21487;&#20197;&#22686;&#21152;&#23384;&#20648;&#23481;&#37327;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#36830;&#25509;&#20165;&#38480;&#20110;&#31561;&#20110;&#20840;&#23545;&#32593;&#32476;&#30340;&#23567;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#20173;&#28982;&#20248;&#20110;&#20854;&#25104;&#23545;&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#31181;&#24773;&#20917;&#21253;&#25324;...&#65288;&#24453;&#34917;&#20805;&#65289;
&lt;/p&gt;
&lt;p&gt;
Hopfield networks are artificial neural networks which store memory patterns on the states of their neurons by choosing recurrent connection weights and update rules such that the energy landscape of the network forms attractors around the memories. How many stable, sufficiently-attracting memory patterns can we store in such a network using $N$ neurons? The answer depends on the choice of weights and update rule. Inspired by setwise connectivity in biology, we extend Hopfield networks by adding setwise connections and embedding these connections in a simplicial complex. Simplicial complexes are higher dimensional analogues of graphs which naturally represent collections of pairwise and setwise relationships. We show that our simplicial Hopfield networks increase memory storage capacity. Surprisingly, even when connections are limited to a small random subset of equivalent size to an all-pairwise network, our networks still outperform their pairwise counterparts. Such scenarios include
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#25552;&#31034;&#36866;&#24212;&#65292;LLM&#36817;&#20284;&#21644;LLM&#32423;&#32852;&#65292;&#24182;&#19988;&#36890;&#36807; FrugalGPT &#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#25110;&#26159;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05176</link><description>&lt;p&gt;
FrugalGPT: &#22914;&#20309;&#22312;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#25552;&#31034;&#36866;&#24212;&#65292;LLM&#36817;&#20284;&#21644;LLM&#32423;&#32852;&#65292;&#24182;&#19988;&#36890;&#36807; FrugalGPT &#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#25110;&#26159;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#20184;&#36153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26597;&#35810;&#27969;&#34892;&#30340;LLM API&#65288;&#20363;&#22914;GPT-4&#65292;ChatGPT&#65292;J1-Jumbo&#65289;&#28041;&#21450;&#30340;&#25104;&#26412;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24322;&#26500;&#30340;&#20215;&#26684;&#32467;&#26500;&#65292;&#36153;&#29992;&#21487;&#33021;&#30456;&#24046;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#29305;&#21035;&#26159;&#22312;&#22823;&#37327;&#26597;&#35810;&#21644;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;LLM&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#31574;&#30053;&#26469;&#20943;&#23569;&#20351;&#29992;LLM&#30340;&#27719;&#32534;&#25104;&#26412;&#65306;1&#65289;&#25552;&#31034;&#36866;&#24212;&#65292;2&#65289;LLM&#36817;&#20284;&#21644;3&#65289;LLM&#32423;&#32852;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FrugalGPT&#65292;&#23427;&#26159;LLM&#32423;&#32852;&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#23398;&#20064;&#20351;&#29992;&#21738;&#20123;LLM&#32452;&#21512;&#26469;&#22788;&#29702;&#19981;&#21516;&#26597;&#35810;&#65292;&#20197;&#38477;&#20302;&#25104;&#26412;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FrugalGPT&#21487;&#20197;&#22312;&#20165;&#20351;&#29992;&#36153;&#29992;&#30340;98&#65285;&#25110;&#19982;GPT-4&#30456;&#21516;&#30340;&#25104;&#26412;&#19979;&#65292;&#36798;&#21040;&#26368;&#20339;&#21333;&#20010;LLM&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;GPT-4&#65289;&#65292;&#25110;&#32773;&#20197;4&#65285;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;GPT-4&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#32508;&#21512;&#12289;&#35821;&#20041;&#21644;&#35745;&#31639;&#29702;&#35770;&#65292;&#20197;&#25506;&#35752;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#20010;&#32500;&#24230;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2305.05172</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Logic for Explainable AI. (arXiv:2305.05172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#32508;&#21512;&#12289;&#35821;&#20041;&#21644;&#35745;&#31639;&#29702;&#35770;&#65292;&#20197;&#25506;&#35752;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#20010;&#32500;&#24230;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#29702;&#35299;&#65288;&#23398;&#20064;&#65289;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#31181;&#29702;&#35299;&#26377;&#19977;&#20010;&#26041;&#38754;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#31532;&#19968;&#32500;&#19982;&#20026;&#21028;&#26029;&#20915;&#31574;&#25152;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#23454;&#20363;&#26465;&#20214;&#26377;&#20851;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#30340;&#23454;&#20363;&#25277;&#35937;&#65292;&#21487;&#35270;&#20026;&#8220;&#20915;&#31574;&#32972;&#21518;&#30340;&#21407;&#22240;&#8221;&#12290;&#19979;&#19968;&#32500;&#19982;&#25551;&#36848;&#36275;&#20197;&#20316;&#20986;&#20915;&#31574;&#30340;&#26368;&#23567;&#26465;&#20214;&#26377;&#20851;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#20915;&#31574;&#26080;&#20851;&#30340;&#23454;&#20363;&#26368;&#22823;&#26041;&#38754;&#12290;&#26368;&#21518;&#19968;&#20010;&#32500;&#24230;&#23558;&#20854;&#31227;&#21160;&#21040;&#20102;&#20915;&#31574;&#65292;&#21363;&#26631;&#35782;&#23545;&#23454;&#20363;&#36827;&#34892;&#26368;&#23567;&#25200;&#21160;&#20197;&#20135;&#29983;&#26367;&#20195;&#20915;&#31574;&#25152;&#24517;&#38656;&#30340;&#26368;&#23567;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#26412;&#25945;&#31243;&#20013;&#35752;&#35770;&#20102;&#27839;&#36825;&#20123;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#20840;&#38754;&#12289;&#35821;&#20041;&#21644;&#35745;&#31639;&#29702;&#35770;&#65292;&#36825;&#26159;&#22522;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central quest in explainable AI relates to understanding the decisions made by (learned) classifiers. There are three dimensions of this understanding that have been receiving significant attention in recent years. The first dimension relates to characterizing conditions on instances that are necessary and sufficient for decisions, therefore providing abstractions of instances that can be viewed as the "reasons behind decisions." The next dimension relates to characterizing minimal conditions that are sufficient for a decision, therefore identifying maximal aspects of the instance that are irrelevant to the decision. The last dimension relates to characterizing minimal conditions that are necessary for a decision, therefore identifying minimal perturbations to the instance that yield alternate decisions. We discuss in this tutorial a comprehensive, semantical and computational theory of explainability along these dimensions which is based on some recent developments in symbolic logic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21512;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#30142;&#30149;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20379;&#24212;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#30123;&#24773;&#30340;&#24635;&#20307;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2305.05163</link><description>&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21512;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization. (arXiv:2305.05163v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21512;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#30142;&#30149;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20379;&#24212;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#30123;&#24773;&#30340;&#24635;&#20307;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;&#31574;&#30053;&#65292;&#20197;&#22312;&#20379;&#24212;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#30123;&#24773;&#30340;&#24635;&#20307;&#36127;&#25285;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20551;&#35774;&#20122;&#32452;&#20154;&#21475;&#20869;&#30340;&#21516;&#36136;&#24615;&#34892;&#20026;&#21644;&#32570;&#20047;&#31227;&#21160;&#24615;&#21160;&#24577;&#38598;&#25104;&#65292;&#36827;&#34892;&#23439;&#35266;&#25110;&#31616;&#21270;&#30340;&#24494;&#35266;&#30123;&#33495;&#20998;&#37197;&#12290;&#30452;&#25509;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#24494;&#35266;&#30123;&#33495;&#20998;&#37197;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#65292;&#22240;&#20026;&#32570;&#20047;&#19982;&#34892;&#20026;&#30456;&#20851;&#30340;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#30142;&#30149;&#21160;&#21147;&#23398;&#20013;&#30340;&#31227;&#21160;&#24615;&#24322;&#36136;&#24615;&#34701;&#20837;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;Trans-vaccine-SEIR&#27169;&#22411;&#27169;&#25311;&#30142;&#30149;&#30340;&#28436;&#21464;&#36807;&#31243;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#23547;&#27714;&#39640;&#24230;&#31354;&#38388;-&#26102;&#38388;&#30142;&#30149;&#28436;&#21270;&#31995;&#32479;&#30340;&#26368;&#20248;&#30123;&#33495;&#20998;&#37197;&#31574;&#30053;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#26469;&#26377;&#25928;&#25429;&#25417;&#31227;&#21160;&#25509;&#35302;&#32593;&#32476;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#25552;&#21462;&#21160;&#24577;&#30142;&#30149;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;r
&lt;/p&gt;
&lt;p&gt;
This study explores the vaccine prioritization strategy to reduce the overall burden of the pandemic when the supply is limited. Existing methods conduct macro-level or simplified micro-level vaccine distribution by assuming the homogeneous behavior within subgroup populations and lacking mobility dynamics integration. Directly applying these models for micro-level vaccine allocation leads to sub-optimal solutions due to the lack of behavioral-related details. To address the issue, we first incorporate the mobility heterogeneity in disease dynamics modeling and mimic the disease evolution process using a Trans-vaccine-SEIR model. Then we develop a novel deep reinforcement learning to seek the optimal vaccine allocation strategy for the high-degree spatial-temporal disease evolution system. The graph neural network is used to effectively capture the structural properties of the mobility contact network and extract the dynamic disease features. In our evaluation, the proposed framework r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#30340;&#28508;&#22312;&#20132;&#20114;&#24335;A2C&#26041;&#27861;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#22312;&#31454;&#20105;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#20174;&#20854;&#20182;&#26234;&#33021;&#20307;&#20013;&#33719;&#24471;&#21508;&#31181;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05159</link><description>&lt;p&gt;
&#28508;&#22312;&#20132;&#20114;&#24335;A2C&#65306;&#22312;&#24320;&#25918;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Latent Interactive A2C for Improved RL in Open Many-Agent Systems. (arXiv:2305.05159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#30340;&#28508;&#22312;&#20132;&#20114;&#24335;A2C&#26041;&#27861;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#22312;&#31454;&#20105;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#20174;&#20854;&#20182;&#26234;&#33021;&#20307;&#20013;&#33719;&#24471;&#21508;&#31181;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#30340;&#26041;&#27861;&#26159;&#38598;&#20013;&#24335;&#35757;&#32451;&#65292;&#20294;&#35813;&#26041;&#27861;&#38656;&#35201;&#20174;&#20854;&#20182;&#26234;&#33021;&#20307;&#20013;&#33719;&#24471;&#21508;&#31181;&#20449;&#24687;&#65292;&#36825;&#22312;&#31454;&#20105;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#65292;&#20132;&#20114;&#24335;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;(IA2C)&#26041;&#27861;&#37319;&#29992;&#20102;&#20998;&#25955;&#30340;&#35757;&#32451;&#21644;&#25191;&#34892;&#65292;&#26088;&#22312;&#20174;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#30340;&#35266;&#23519;&#20013;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#21160;&#30340;&#28508;&#22312;IA2C&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30001;&#20247;&#22810;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#39046;&#22495;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28508;&#22312;IA2C&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#26356;&#24555;&#25910;&#25947;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#36825;&#20123;&#39046;&#22495;&#30340;&#24320;&#25918;&#29256;&#26412;&#65292;&#26234;&#33021;&#20307;&#31181;&#32676;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#23545;&#36825;&#20123;&#23454;&#20363;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#22270;&#20687;&#26816;&#32034;&#30340;&#36328;&#22495;&#21644;&#35821;&#20041;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#37325;&#26032;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#23545;&#40784;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#20197;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05144</link><description>&lt;p&gt;
&#25913;&#36827;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval. (arXiv:2305.05144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#22270;&#20687;&#26816;&#32034;&#30340;&#36328;&#22495;&#21644;&#35821;&#20041;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#37325;&#26032;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#23545;&#40784;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#20197;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22522;&#20110;&#33609;&#22270;&#30340;&#22270;&#20687;&#26816;&#32034;(ZS-SBIR)&#30001;&#20110;&#33609;&#22270;&#21644;&#29031;&#29255;&#20043;&#38388;&#30340;&#36328;&#22495;&#26412;&#36136;&#20197;&#21450;&#24050;&#30693;&#21644;&#26410;&#30693;&#22270;&#20687;&#20998;&#24067;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#21508;&#31181;&#36741;&#21161;&#20449;&#24687;&#21644;&#23398;&#20064;&#31574;&#30053;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388; (\romannumeral 1)&#22312;&#33609;&#22270;&#21644;&#29031;&#29255;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#65292;(\romannumeral 2) &#26725;&#25509;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#22312;&#36866;&#24212;&#39046;&#22495;&#21644;&#20174;&#24050;&#30693;&#31867;&#21035;&#20256;&#36882;&#30693;&#35782;&#26041;&#38754;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#8220;&#33258;&#36866;&#24212;&#21644;&#23545;&#40784;&#8221;&#26041;&#27861;&#26469;&#35299;&#20915;&#20851;&#38190;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25554;&#20837;&#31616;&#21333;&#19988;&#36731;&#37327;&#30340;&#22495;&#36866;&#37197;&#22120;&#65292;&#20197;&#23398;&#20064;&#33609;&#22270;&#39046;&#22495;&#30340;&#26032;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#25552;&#39640;&#36328;&#22495;&#34920;&#31034;&#33021;&#21147;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;(CLIP)&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#23398;&#20064;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#19982;&#27169;&#22411;&#26126;&#30830;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen image distributions. Previous methods fine-tune pre-trained models with various side information and learning strategies to learn a compact feature space that (\romannumeral1) is shared between the sketch and photo domains and (\romannumeral2) bridges seen and unseen classes. However, these efforts are inadequate in adapting domains and transferring knowledge from seen to unseen classes. In this paper, we present an effective \emph{``Adapt and Align''} approach to address the key challenges. Specifically, we insert simple and lightweight domain adapters to learn new abstract concepts of the sketch domain and improve cross-domain representation capabilities. Inspired by recent advances in image-text foundation models (\textit{e.g.}, CLIP) on zero-shot scenarios, we explicitly align the learned image embedding with a mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#20840;&#29699;&#19982;&#26412;&#22320;&#25903;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20195;&#29702;&#20154;&#22312;&#25903;&#20986;&#21644;&#33719;&#24471;&#25928;&#29992;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05134</link><description>&lt;p&gt;
AI&#25110;&#19981;AI&#65292;&#26412;&#22320;&#36141;&#36824;&#26159;&#19981;&#26412;&#22320;&#36141;&#65306;&#30495;&#23454;&#20215;&#26684;&#30340;&#25968;&#23398;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
To AI or not to AI, to Buy Local or not to Buy Local: A Mathematical Theory of Real Price. (arXiv:2305.05134v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#20840;&#29699;&#19982;&#26412;&#22320;&#25903;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20195;&#29702;&#20154;&#22312;&#25903;&#20986;&#21644;&#33719;&#24471;&#25928;&#29992;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#37324;&#65292;&#20840;&#29699;&#32463;&#27982;&#21464;&#24471;&#36234;&#26469;&#36234;&#20840;&#29699;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#25552;&#20513;&#8220;&#26412;&#22320;&#36141;&#8221;&#30340;&#29702;&#24565;&#65292;&#21363;&#20154;&#20204;&#36141;&#20080;&#26412;&#22320;&#29983;&#20135;&#30340;&#21830;&#21697;&#21644;&#26381;&#21153;&#32780;&#19981;&#26159;&#36828;&#31163;&#26412;&#22320;&#30340;&#37027;&#20123;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#20840;&#29699;&#19982;&#26412;&#22320;&#25903;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20195;&#29702;&#20154;&#22312;&#25903;&#20986;&#21644;&#33719;&#24471;&#25928;&#29992;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30495;&#23454;&#20215;&#26684;&#29702;&#35770;&#20381;&#36182;&#20110;&#19982;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#32593;&#32476;&#30456;&#20851;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#30340;&#28176;&#36817;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#30495;&#23454;&#20215;&#26684;&#21487;&#20197;&#20174;&#28041;&#21450;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#30697;&#38453;&#20013;&#30830;&#23450;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#20135;&#21697;&#30340;&#26631;&#31614;&#20215;&#26684;&#26174;&#33879;&#19981;&#21516;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#26631;&#31614;&#20215;&#26684;&#36890;&#24120;&#19981;&#26159;&#8220;&#30495;&#23454;&#30340;&#8221;&#25110;&#30452;&#25509;&#8220;&#26377;&#29992;&#30340;&#8221;&#65306;&#22914;&#26524;&#25552;&#20379;&#30456;&#21516;&#30340;&#36817;&#35270;&#25928;&#29992;&#65292;&#20215;&#26684;&#26356;&#20302;&#30340;&#37027;&#20010;&#20135;&#21697;&#20250;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past several decades, the world's economy has become increasingly globalized. On the other hand, there are also ideas advocating the practice of ``buy local'', by which people buy locally produced goods and services rather than those produced farther away. In this paper, we establish a mathematical theory of real price that determines the optimal global versus local spending of an agent which achieves the agent's optimal tradeoff between spending and obtained utility. Our theory of real price depends on the asymptotic analysis of a Markov chain transition probability matrix related to the network of producers and consumers. We show that the real price of a product or service can be determined from the involved Markov chain matrix, and can be dramatically different from the product's label price. In particular, we show that the label prices of products and services are often not ``real'' or directly ``useful'': given two products offering the same myopic utility, the one with low
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#39044;&#27979;&#30340;&#22320;&#36136;&#20449;&#24687;&#21644;&#23454;&#26102;&#30340;&#36816;&#34892;&#21442;&#25968;&#20449;&#24687;&#65292;&#20026;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#26426;&#21069;&#26041;&#22320;&#36136;&#39044;&#27979;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#32531;&#35299;&#26045;&#24037;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.05128</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#30340;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#38567;&#36947;&#23454;&#26102;&#22320;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling. (arXiv:2305.05128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#39044;&#27979;&#30340;&#22320;&#36136;&#20449;&#24687;&#21644;&#23454;&#26102;&#30340;&#36816;&#34892;&#21442;&#25968;&#20449;&#24687;&#65292;&#20026;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#26426;&#21069;&#26041;&#22320;&#36136;&#39044;&#27979;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#32531;&#35299;&#26045;&#24037;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Kriging&#22806;&#25512;&#31639;&#27861;&#21644;Random Forest&#30456;&#32467;&#21512;&#65292;&#20026;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#26426;&#21069;&#26041;&#22320;&#36136;&#39044;&#27979;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#32531;&#35299;&#26045;&#24037;&#39118;&#38505;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#21033;&#29992;&#20102;&#20808;&#21069;&#39044;&#27979;&#30340;&#22320;&#36136;&#20449;&#24687;&#21644;&#23454;&#26102;&#30340;&#36816;&#34892;&#21442;&#25968;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65292;&#36816;&#29992;&#21152;&#26435;&#24179;&#22343;&#26041;&#27861;&#23558;&#39044;&#27979;&#32467;&#26524;&#32467;&#21512;&#65292;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#26368;&#23567;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A kriging-random forest hybrid model is developed for real-time ground property prediction ahead of the earth pressure balanced shield by integrating Kriging extrapolation and random forest, which can guide shield operating parameter selection thereby mitigate construction risks. The proposed KRF algorithm synergizes two types of information: prior information and real-time information. The previously predicted ground properties with EPB operating parameters are extrapolated via the Kriging algorithm to provide prior information for the prediction of currently being excavated ground properties. The real-time information refers to the real-time operating parameters of the EPB shield, which are input into random forest to provide a real-time prediction of ground properties. The integration of these two predictions is achieved by assigning weights to each prediction according to their uncertainties, ensuring the prediction of KRF with minimum uncertainty. The performance of the KRF algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2305.05126</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#20869;&#26680;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#28041;&#21450;&#22312;&#21508;&#31181;&#31574;&#21010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32858;&#21512;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24230;&#37327;&#25351;&#26631;&#30340;&#36873;&#25321;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#29702;&#24819;&#24230;&#37327;&#19981;&#26126;&#26174;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#22270;&#29702;&#35770;&#65292;&#24182;&#20419;&#36827;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35825;&#23548;&#19968;&#32452;&#37197;&#22791;&#26377;&#19982;&#19968;&#20123;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#27169;&#22411;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.05119</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#21046;&#36896;&#20652;&#29983;&#20102;&#22797;&#26434;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#22914;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSP&#65289;&#12290;&#22312;FJSP&#20013;&#65292;&#25805;&#20316;&#21487;&#20197;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#23398;&#20064;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#65288;PDRs&#65289;&#20197;&#35299;&#20915;FJSP&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#35832;&#22914;OR-Tools&#31561;&#31934;&#30830;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#20173;&#26377;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;DRL&#36827;&#34892;&#21487;&#25193;&#23637;&#20915;&#31574;&#21046;&#23450;&#30340;&#20248;&#28857;&#12290;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#34987;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#25805;&#20316;&#20449;&#24687;&#27880;&#24847;&#22359;&#21644;&#26426;&#22120;&#20449;&#24687;&#27880;&#24847;&#22359;&#32452;&#25104;&#30340;&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DAN&#65289;&#12290;DAN&#21033;&#29992;&#36825;&#20123;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;PDRs&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#33322;&#29677;&#36215;&#39134;&#24310;&#35823;&#12290;&#19968;&#20010;&#30001; XGB-CBR Twin &#36716;&#25442;&#26469;&#30340; CBR &#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20934;&#30830;&#30340;&#23616;&#37096;&#39044;&#27979;&#12289;&#26368;&#26131;&#35299;&#37322;&#30340;&#23616;&#37096;&#35299;&#37322;&#34920;&#31034;&#21644;&#20840;&#23616;&#37325;&#35201;&#24615;&#30340;&#32500;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.05111</link><description>&lt;p&gt;
&#24403;&#25163;&#19978;&#26377;&#19968;&#20010;CBR&#27604;&#19995;&#26519;&#37324;&#30340;&#20004;&#20010;&#26356;&#22909;&#26102;
&lt;/p&gt;
&lt;p&gt;
When a CBR in Hand is Better than Twins in the Bush. (arXiv:2305.05111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#33322;&#29677;&#36215;&#39134;&#24310;&#35823;&#12290;&#19968;&#20010;&#30001; XGB-CBR Twin &#36716;&#25442;&#26469;&#30340; CBR &#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20934;&#30830;&#30340;&#23616;&#37096;&#39044;&#27979;&#12289;&#26368;&#26131;&#35299;&#37322;&#30340;&#23616;&#37096;&#35299;&#37322;&#34920;&#31034;&#21644;&#20840;&#23616;&#37325;&#35201;&#24615;&#30340;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#24120;&#34987;&#25903;&#25345;&#35299;&#37322;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#30340;&#20154;&#36140;&#20302;&#20026;&#19981;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26435;&#34913;&#24182;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#33322;&#29677;&#36215;&#39134;&#24310;&#35823;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;XGBoost&#23454;&#29616;&#35757;&#32451;&#20102;&#26368;&#31934;&#30830;&#30340;&#25968;&#25454;&#22238;&#24402;&#27169;&#22411;&#12290;&#32780;&#22312;&#26500;&#24314;XGB-CBR Twin&#24182;&#23558;XGBoost&#29305;&#24449;&#37325;&#35201;&#24615;&#36716;&#25442;&#20026;CBR&#27169;&#22411;&#20013;&#30340;&#20840;&#23616;&#26435;&#37325;&#26102;&#65292;&#32467;&#26524;&#21333;&#29420;&#20351;&#29992;&#30340;CBR&#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20934;&#30830;&#30340;&#23616;&#37096;&#39044;&#27979;&#65292;&#20445;&#25345;&#20102;&#20840;&#23616;&#37325;&#35201;&#24615;&#65292;&#25552;&#20379;&#20102;&#26368;&#26131;&#35299;&#37322;&#30340;&#23616;&#37096;&#35299;&#37322;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;CBR&#27169;&#22411;&#25104;&#20026;&#20102;&#36825;&#20010;&#38382;&#39064;&#24773;&#22659;&#19979;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#20934;&#65292;&#20174;&#32780;&#29992;&#20110;&#35780;&#20272;&#20004;&#31181;&#28155;&#21152;&#29305;&#24449;&#23646;&#24615;&#26041;&#27861;SHAP&#21644;LIME&#26469;&#35299;&#37322;XGBoost&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI methods referred to as interpretable are often discredited as inaccurate by supporters of the existence of a trade-off between interpretability and accuracy. In many problem contexts however this trade-off does not hold. This paper discusses a regression problem context to predict flight take-off delays where the most accurate data regression model was trained via the XGBoost implementation of gradient boosted decision trees. While building an XGB-CBR Twin and converting the XGBoost feature importance into global weights in the CBR model, the resultant CBR model alone provides the most accurate local prediction, maintains the global importance to provide a global explanation of the model, and offers the most interpretable representation for local explanations. This resultant CBR model becomes a benchmark of accuracy and interpretability for this problem context, and hence it is used to evaluate the two additive feature attribute methods SHAP and LIME to explain the XGBoost regressio
&lt;/p&gt;</description></item><item><title>TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05105</link><description>&lt;p&gt;
&#38754;&#21521;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05105
&lt;/p&gt;
&lt;p&gt;
TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;ACM/IEEE&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;&#65288;TDC&#65289;&#20110;2022&#24180;&#22312;&#31532;41&#23626;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#22269;&#38469;&#20250;&#35758;&#65288;ICCAD&#65289;&#19978;&#20030;&#34892;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26376;&#30740;&#21457;&#31454;&#36187;&#12290;TDC'22&#19987;&#27880;&#20110;&#38656;&#35201;&#22312;&#21487;&#26893;&#20837;&#35774;&#22791;&#19978;&#21019;&#26032;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#30340;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#12290;TDC'22&#30340;&#25361;&#25112;&#38382;&#39064;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#24515;&#33039;&#38500;&#39076;&#22120;&#65288;ICDs&#65289;&#19978;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#24494;&#25511;&#21046;&#22120;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;90&#20010;&#21463;&#35797;&#32773;&#30340;8&#31181;&#19981;&#21516;&#24515;&#24459;&#31867;&#22411;&#30340;&#36229;&#36807;38,000&#20010;5&#31186;&#24515;&#20869;&#30005;&#22270;&#65288;IEGM&#65289;&#29255;&#27573;&#12290;&#19987;&#29992;&#30828;&#20214;&#24179;&#21488;&#26159;STMicroelectronics&#21046;&#36896;&#30340;NUCLEO-L432KC&#12290;TDC'22&#38754;&#21521;&#20840;&#29699;&#22810;&#20154;&#22242;&#38431;&#65292;&#21560;&#24341;&#20102;&#26469;&#33258;50&#22810;&#20010;&#32452;&#32455;&#30340;150&#22810;&#25903;&#38431;&#20237;&#21442;&#36187;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#36825;&#19968;&#21307;&#30103;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#27169;&#22411;(NAP)&#65292;&#36890;&#36807;&#32534;&#30721;&#24207;&#21015;&#30452;&#25509;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23454;&#29616;&#35299;&#30721;&#27493;&#39588;&#30340;&#35268;&#36991;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20004;&#20010;&#22330;&#26223;&#19979;&#65292;NAP&#20998;&#21035;&#21487;&#20197;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#21644;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05098</link><description>&lt;p&gt;
&#35841;&#38656;&#35201;&#35299;&#30721;&#22120;&#65311;&#39640;&#25928;&#39044;&#27979;&#24207;&#21015;&#32423;&#23646;&#24615;&#12290;&#65288;arXiv:2305.05098v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05098
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#27169;&#22411;(NAP)&#65292;&#36890;&#36807;&#32534;&#30721;&#24207;&#21015;&#30452;&#25509;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23454;&#29616;&#35299;&#30721;&#27493;&#39588;&#30340;&#35268;&#36991;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20004;&#20010;&#22330;&#26223;&#19979;&#65292;NAP&#20998;&#21035;&#21487;&#20197;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#21644;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21270;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#33258;&#22238;&#24402;&#35299;&#30721;&#65292;&#36825;&#24448;&#24448;&#38750;&#24120;&#28040;&#32791;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#36234;&#30028;&#26816;&#27979;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#38469;&#35299;&#30721;&#36755;&#20986;&#24182;&#19981;&#38656;&#35201;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#24207;&#21015;&#30340;&#26631;&#37327;&#23646;&#24615;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#19979;&#65292;&#30693;&#36947;&#31995;&#32479;&#36755;&#20986;&#36136;&#37327;&#20197;&#39044;&#27979;&#24615;&#33021;&#36739;&#24046;&#27604;&#30693;&#36947;&#36755;&#20986;&#26412;&#36523;&#26356;&#20026;&#37325;&#35201;&#65292;&#37027;&#20040;&#26159;&#21542;&#21487;&#20197;&#32469;&#36807;&#33258;&#22238;&#24402;&#35299;&#30721;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#65288;NAP&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;NAP&#30452;&#25509;&#20174;&#32534;&#30721;&#39044;&#27979;&#36825;&#20123;&#25351;&#26631;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65306;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#22312;MT&#30340;&#36234;&#30028;&#26816;&#27979;&#20013;&#65292;NAP&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#65292;&#21516;&#26102;&#36895;&#24230;&#26174;&#33879;&#26356;&#24555;&#12290;NAP&#20063;&#34987;&#35777;&#26126;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;ASR&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20363;&#22914;&#35789;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#23646;&#24615;&#21487;&#20197;&#20174;&#32534;&#30721;&#20013;&#30452;&#25509;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#65292;NAP&#20026;&#20256;&#32479;&#22522;&#20110;&#35299;&#30721;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;CLIP&#22521;&#35757;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65306;&#65288;1&#65289;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#65288;2&#65289;&#36807;&#28388;&#25481;&#22270;&#20687;&#20013;&#24102;&#26377;&#25991;&#26412;&#21306;&#22495;&#30340;&#26679;&#26412;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#38450;&#24481;&#21360;&#21047;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;ImageNet&#21644;Coco&#31561;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#30528;&#25552;&#39640;&#20102;&#20998;&#31867;&#21644;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05095</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#31227;&#38500;&#25991;&#26412;&#21306;&#22495;&#25552;&#39640;CLIP&#35757;&#32451;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness. (arXiv:2305.05095v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;CLIP&#22521;&#35757;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65306;&#65288;1&#65289;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#65288;2&#65289;&#36807;&#28388;&#25481;&#22270;&#20687;&#20013;&#24102;&#26377;&#25991;&#26412;&#21306;&#22495;&#30340;&#26679;&#26412;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#38450;&#24481;&#21360;&#21047;&#25915;&#20987;&#65292;&#20174;&#32780;&#22312;ImageNet&#21644;Coco&#31561;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#30528;&#25552;&#39640;&#20102;&#20998;&#31867;&#21644;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#65288;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65289;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#27491;&#22312;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#20159;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#35757;&#32451;CLIP&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#25104;&#26412;&#39640;&#26114;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;CLIP&#27169;&#22411;&#19981;&#21306;&#20998;&#23884;&#20837;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21306;&#22495;&#30340;&#35270;&#35273;&#35821;&#20041;&#21644;&#21547;&#20041;&#12290;&#24403;&#23884;&#20837;&#21306;&#22495;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#22806;&#35266;&#19981;&#21305;&#37197;&#26102;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;CLIP&#22521;&#35757;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65306;&#65288;1&#65289;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#25968;&#37327;&#30340;&#20248;&#21270;&#27493;&#39588;&#65292;&#20197;&#21450;&#65288;2&#65289;&#36807;&#28388;&#22270;&#20687;&#20013;&#21253;&#21547;&#25991;&#26412;&#21306;&#22495;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26174;&#30528;&#25552;&#39640;&#20102;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;ImageNet&#21644;CoCo&#65289;&#30340;&#20998;&#31867;&#21644;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36807;&#28388;&#24102;&#26377;&#25991;&#26412;&#21306;&#22495;&#30340;&#22270;&#20687;&#36824;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#21360;&#21047;&#25915;&#20987;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;......
&lt;/p&gt;
&lt;p&gt;
The CLIP (Contrastive Language-Image Pre-training) model and its variants are becoming the de facto backbone in many applications. However, training a CLIP model from hundreds of millions of image-text pairs can be prohibitively expensive. Furthermore, the conventional CLIP model doesn't differentiate between the visual semantics and meaning of text regions embedded in images. This can lead to non-robustness when the text in the embedded region doesn't match the image's visual appearance. In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image. By doing so, we significantly improve the classification and retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out images with text regions also protects the model from typographic attacks. To verify this, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;3GPP Release 18&#22312;5G-Advanced&#20013;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#21457;&#23637;&#24773;&#20917;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#30740;&#31350;&#21644;&#24037;&#20316;&#39033;&#20197;&#21450;&#35774;&#35745;&#26041;&#38754;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.05092</link><description>&lt;p&gt;
3GPP 5G-Advanced&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in 3GPP 5G-Advanced: A Survey. (arXiv:2305.05092v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;3GPP Release 18&#22312;5G-Advanced&#20013;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#21457;&#23637;&#24773;&#20917;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#30740;&#31350;&#21644;&#24037;&#20316;&#39033;&#20197;&#21450;&#35774;&#35745;&#26041;&#38754;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25913;&#21464;&#20840;&#29699;&#21508;&#34892;&#21508;&#19994;&#65292;&#30005;&#20449;&#34892;&#19994;&#20063;&#19981;&#20363;&#22806;&#12290;&#26631;&#20934;&#21270;&#23545;&#20110;&#23454;&#29616;&#30005;&#20449;&#39046;&#22495;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#31532;&#19977;&#20195;&#21512;&#20316;&#20249;&#20276;&#35745;&#21010;&#65288;3GPP&#65289;Release 18&#26159;&#21253;&#25324;&#19987;&#38376;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#22810;&#26679;&#21270;&#30740;&#31350;&#21644;&#24037;&#20316;&#39033;&#30340;5G-Advanced&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#12290;&#26412;&#25991;&#32508;&#21512;&#27010;&#36848;&#20102;3GPP&#22312;5G-Advanced&#20013;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#21457;&#23637;&#24773;&#20917;&#65292;&#23558;&#21508;&#31181;3GPP Release-18&#20154;&#24037;&#26234;&#33021;&#27963;&#21160;&#20316;&#20026;&#26377;&#26426;&#25972;&#20307;&#21576;&#29616;&#65292;&#35814;&#32454;&#35299;&#37322;&#35774;&#35745;&#26041;&#38754;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#20998;&#20139;&#24433;&#21709;&#26631;&#20934;&#21270;&#30340;&#21508;&#31181;&#35774;&#35745;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industries worldwide are being transformed by artificial intelligence (AI), and the telecom industry is no different. Standardization is critical for industry alignment to achieve widespread adoption of AI in telecom. The 3rd generation partnership project (3GPP) Release 18 is the first release of 5G-Advanced, which includes a diverse set of study and work items dedicated to AI. This article provides a holistic overview of the state of the art in the 3GPP work on AI in 5G-Advanced, by presenting the various 3GPP Release-18 activities on AI as an organic whole, explaining in detail the design aspects, and sharing various design rationales influencing standardization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.05091</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25991;&#26412;&#28216;&#25103;&#20013;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#27969;&#26159;&#26234;&#33021;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#38656;&#35201;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#21644;&#25512;&#29702;&#26377;&#20851;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#20854;&#30417;&#30563;&#31243;&#24230;&#20063;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#21152;&#24378;&#20195;&#29702;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#20004;&#31181;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#20013;&#65306;&#20808;&#21069;&#27491;&#30830;&#25805;&#20316;&#30340;&#35760;&#24518;&#21644;&#29615;&#22659;&#20013;&#30456;&#20851;&#23545;&#35937;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#65306;`&#32431;`&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#35760;&#24518;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015; (seq2seq) &#27169;&#22411;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340; seq2seq&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication via natural language is a crucial aspect of intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. While there has been significant progress made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding, much of the community has turned to various sequential interactive tasks, as in semi-Markov text-based games, which have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a framework for enabling improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports three representative model classes: `pure' reinforcement learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.05080</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#22320;&#29699;&#31227;&#21160;&#32773;: &#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;(OT)&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;,&#39318;&#27425;&#20986;&#29616;&#20110;18&#19990;&#32426;,&#24182;&#24341;&#21457;&#20986;&#22823;&#37327;&#26041;&#27861;&#26469;&#22238;&#31572;&#35768;&#22810;&#29702;&#35770;&#21644;&#24212;&#29992;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#21313;&#24180;&#35265;&#35777;&#20102;&#36825;&#20010;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#26174;&#30528;&#36129;&#29486;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#21450;&#20854;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#22312;&#19987;&#39064;&#19982;&#32972;&#26223;&#30340;&#20801;&#35768;&#19979;,&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#20840;&#38754;&#35843;&#26597;,&#24182;&#30830;&#20445;&#20854;&#21576;&#29616;&#20855;&#26377;&#21487;&#35775;&#38382;&#24615;&#12290;&#39318;&#20808;,&#25105;&#20204;&#35299;&#37322;&#20102;&#26368;&#20248;&#36755;&#36816;&#30340;&#32972;&#26223;,&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#31867;&#22411;&#12289;&#29305;&#24615;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;,&#25105;&#20204;&#30528;&#37325;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#26368;&#20248;&#36755;&#36816;&#25193;&#23637;&#20197;&#24212;&#23545;&#24403;&#21069;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#29992;&#20110;&#25193;&#23637;OT&#30340;&#25991;&#29486;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;,&#24182;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#21576;&#29616;&#32467;&#26524;&#20197;&#20419;&#36827;&#29702;&#35299;&#12290;&#26368;&#21518;,&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#25193;&#23637;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#36793;&#32536;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#20272;&#35745;&#22810;&#20154;&#30340;&#20301;&#32622;&#12289;&#26397;&#21521;&#21644;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.05062</link><description>&lt;p&gt;
&#19981;&#20844;&#24320;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#32593;&#32476;&#21644;&#36793;&#32536;&#35745;&#31639;&#22312;&#23460;&#20869;&#23450;&#20301;&#21644;&#22810;&#20154;&#36319;&#36394;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Indoor Localization and Multi-person Tracking Using Privacy Preserving Distributed Camera Network with Edge Computing. (arXiv:2305.05062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#36793;&#32536;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#20272;&#35745;&#22810;&#20154;&#30340;&#20301;&#32622;&#12289;&#26397;&#21521;&#21644;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#31569;&#29615;&#22659;&#20013;&#23450;&#20301;&#20010;&#20307;&#26159;&#19968;&#20010;&#26085;&#30410;&#25104;&#38271;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36890;&#36807;&#20272;&#35745;&#20154;&#20204;&#22312;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#12289;&#38754;&#37096;&#26397;&#21521;&#65288;&#25110;&#20957;&#35270;&#26041;&#21521;&#65289;&#21644;&#36712;&#36857;&#65292;&#21487;&#20197;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#20363;&#22914;&#22312;&#20154;&#32676;&#31649;&#29702;&#12289;&#23433;&#20840;&#21644;&#21307;&#30103;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#36793;&#32536;&#35745;&#31639;&#26694;&#26550;&#29992;&#20110;&#22810;&#20154;&#23450;&#20301;&#65292;&#21363;&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#20272;&#35745;&#22810;&#20154;&#30340;&#20301;&#32622;&#12289;&#26397;&#21521;&#21644;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localization of individuals in a built environment is a growing research topic. Estimating the positions, face orientation (or gaze direction) and trajectories of people through space has many uses, such as in crowd management, security, and healthcare. In this work, we present an open-source, low-cost, scalable and privacy-preserving edge computing framework for multi-person localization, i.e. estimating the positions, orientations, and trajectories of multiple people in an indoor space. Our computing framework consists of 38 Tensor Processing Unit (TPU)-enabled edge computing camera systems placed in the ceiling of the indoor therapeutic space. The edge compute systems are connected to an on-premise fog server through a secure and private network. A multi-person detection algorithm and a pose estimation model run on the edge TPU in real-time to collect features which are used, instead of raw images, for downstream computations. This ensures the privacy of individuals in the space, re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05050</link><description>&lt;p&gt;
ANALOGICAL- &#19968;&#31181;&#26032;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#31867;&#27604;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20197;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#20026;&#24418;&#24335;&#30340;&#31867;&#27604;&#22312;&#34913;&#37327;&#35832;&#22914;word2vec&#20043;&#31867;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20027;&#35201;&#26681;&#25454;GLUE&#21644;SuperGLUE&#31561;&#22522;&#20934;&#30340;&#22806;&#22312;&#37327;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#22312;LLMs&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#25991;&#26412;&#20013;&#32472;&#21046;&#31867;&#27604;&#30340;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#20197;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#23545;LLMs&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20998;&#21035;&#20026; (i)&#21333;&#35789;&#12289;(ii)&#21333;&#35789;vs&#21477;&#23376;&#12289;(iii)&#35821;&#27861;&#12289;(iv)&#21542;&#23450;&#12289;(v)&#34164;&#21547;&#21644;(vi)&#38544;&#21947;&#12290;&#21033;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;(&#20363;&#22914;&#65292;&#8220;&#25105;&#33021;&#35828;&#20004;&#31181;&#35821;&#35328;&#8221;&#24212;&#35813;&#26356;&#25509;&#36817;&#8220;&#25105;&#26159;&#21452;&#35821;&#30340;&#8221;&#65292;&#32780;&#8220;&#25105;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#21644;&#8220;&#25105;&#19981;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#24212;&#35813;&#26159;&#27491;&#20132;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#35780;&#20272;Self-Attention&#21464;&#21387;&#22120;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#20135;&#29983;&#30340;&#25511;&#21046;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#19981;&#23558;&#23545;&#35937;&#21644;&#27010;&#24565;&#32423;&#21035;&#30340;&#21547;&#20041;&#21644;&#35821;&#20041;&#36171;&#20104;&#25152;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#24335;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#20013;&#25152;&#25551;&#36848;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.04989</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25351;&#23548;&#19979;&#35821;&#35328;&#27169;&#22411;&#35821;&#20041;&#35780;&#20272;&#20197;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust. (arXiv:2305.04989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#35780;&#20272;Self-Attention&#21464;&#21387;&#22120;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#20135;&#29983;&#30340;&#25511;&#21046;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#19981;&#23558;&#23545;&#35937;&#21644;&#27010;&#24565;&#32423;&#21035;&#30340;&#21547;&#20041;&#21644;&#35821;&#20041;&#36171;&#20104;&#25152;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#24335;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#20013;&#25152;&#25551;&#36848;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#65306;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#20160;&#20040;&#26679;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#35821;&#20041;&#65311;&#20687;&#30693;&#35782;&#22270;&#35889;&#36825;&#26679;&#30340;&#22270;&#34920;&#36798;&#24418;&#24335;&#24456;&#23481;&#26131;&#36827;&#34892;&#35780;&#20272;&#65292;&#22240;&#20026;&#23427;&#20204;&#26126;&#30830;&#22320;&#34920;&#36798;&#20102;&#35821;&#35328;&#35821;&#20041;&#21644;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#26469;&#35780;&#20272;Self-Attention&#21464;&#21387;&#22120;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#20174;&#30693;&#35782;&#22270;&#35889;&#33719;&#21462;&#30340;&#22270;&#24418;&#36335;&#24452;&#24207;&#21015;&#24182;&#23581;&#35797;&#20174;Self-Attention&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#22797;&#21046;/&#37325;&#26500;&#21516;&#26679;&#36335;&#24452;&#26469;&#27979;&#37327;&#37325;&#26500;&#35823;&#24046;&#12290;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#23545;&#20110;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#20915;&#31574;&#32467;&#26524;&#31561;&#31038;&#20250;&#38382;&#39064;&#26377;&#30528;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#20135;&#29983;&#30340;&#25511;&#21046;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#23427;&#20204;&#19981;&#23558;&#23545;&#35937;&#21644;&#27010;&#24565;&#32423;&#21035;&#30340;&#21547;&#20041;&#21644;&#35821;&#20041;&#36171;&#20104;&#25152;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#24335;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#20013;&#25152;&#25551;&#36848;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#31995;&#27744;&#21270;&#21040;&#23376;&#22270;GNN&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#25552;&#39640;&#20219;&#20309;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#65292;&#36890;&#36807;&#26126;&#30830;&#26631;&#35760;&#33410;&#28857;&#20316;&#20026;&#38468;&#21152;&#29305;&#24449;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292; &#24182;&#21487;&#22312;&#35768;&#22810;syn&#19978;&#23454;&#29616;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04963</link><description>&lt;p&gt;
&#20174;&#20851;&#31995;&#27744;&#21270;&#21040;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks. (arXiv:2305.04963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#31995;&#27744;&#21270;&#21040;&#23376;&#22270;GNN&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#25552;&#39640;&#20219;&#20309;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#65292;&#36890;&#36807;&#26126;&#30830;&#26631;&#35760;&#33410;&#28857;&#20316;&#20026;&#38468;&#21152;&#29305;&#24449;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292; &#24182;&#21487;&#22312;&#35768;&#22810;syn&#19978;&#23454;&#29616;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27744;&#21270;&#26159;&#29992;&#20110;&#26500;&#24314;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#32622;&#25442;&#19981;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20851;&#31995;&#27744;&#21270;&#22312;&#34920;&#29616;&#21147;&#26041;&#38754;&#30340;&#30830;&#20999;&#22686;&#24378;&#21450;&#20854;&#19982;Weisfeiler-Lehman&#20998;&#23618;&#30340;&#32852;&#31995;&#30340;&#29702;&#35299;&#26159;&#26377;&#38480;&#30340;&#12290;&#20174;&#20851;&#31995;&#27744;&#21270;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#33410;&#28857;&#26126;&#30830;&#26631;&#35760;&#20026;&#38468;&#21152;&#29305;&#24449;&#20197;&#25552;&#39640;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;WL&#65292;&#24471;&#21040;&#19968;&#31181;&#26032;&#30340;$k,l$-WL&#31639;&#27861;&#65292;&#27604;$k$-WL&#26356;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;$k,l$-WL&#30456;&#23545;&#20110;$k$&#21644;$l$&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#19982;&#22823;&#37327;&#30340;&#23376;&#22270;GNN&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35752;&#35770;&#20102;&#22797;&#26434;&#24230;&#38477;&#20302;&#26041;&#27861;&#65292;&#20197;&#26500;&#24314;&#24378;&#22823;&#32780;&#23454;&#29992;&#30340;$k,l$-GNN&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#20860;&#23481;&#30340;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#20219;&#20309;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;$k,l$-GNN&#22312;&#35768;&#22810;syn&#19978;&#23454;&#29616;&#20102;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel $k,l$-WL algorithm, a more general framework than $k$-WL. Theoretically, we analyze the expressivity of $k,l$-WL with respect to $k$ and $l$ and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical $k,l$-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our $k,l$-GNNs achieve superior performance on many syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04940</link><description>&lt;p&gt;
&#26089;&#36215;&#30340;&#40479;&#20799;&#25417;&#21040;&#34411;&#65306;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#31867;&#22411;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#20013;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#30340;&#26368;&#20339;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#26500;&#24314;&#20195;&#30721;&#30340;&#22797;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;12&#31181;&#21019;&#24314;&#22797;&#21512;&#34920;&#31034;&#30340;&#31574;&#30053;&#19982;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#32534;&#30721;&#22120;&#23618;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#22312;CodeBERT&#27169;&#22411;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20960;&#20010;&#26089;&#26399;&#23618;&#30340;&#32452;&#21512;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19968;&#20123;&#32452;&#21512;&#21017;&#25913;&#36827;&#20102;&#22810;&#31867;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24179;&#22343;&#26816;&#27979;&#22686;&#24378;2&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#25991;&#21457;&#24067;&#20043;&#21069;&#35782;&#21035;&#20986;&#21363;&#23558;&#34987;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#24182;&#25512;&#29702;&#20854;&#28508;&#22312;&#21361;&#23475;&#21644;&#36829;&#21453;&#24179;&#21488;&#25919;&#31574;&#30340;&#21407;&#22240;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#36827;&#26356;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04927</link><description>&lt;p&gt;
&#25552;&#21069;&#26816;&#27979;&#21644;&#25512;&#29702;&#21024;&#38500;&#25512;&#25991;
&lt;/p&gt;
&lt;p&gt;
Detecting and Reasoning of Deleted Tweets before they are Posted. (arXiv:2305.04927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#25991;&#21457;&#24067;&#20043;&#21069;&#35782;&#21035;&#20986;&#21363;&#23558;&#34987;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#24182;&#25512;&#29702;&#20854;&#28508;&#22312;&#21361;&#23475;&#21644;&#36829;&#21453;&#24179;&#21488;&#25919;&#31574;&#30340;&#21407;&#22240;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#36827;&#26356;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20449;&#24687;&#20256;&#25773;&#21644;&#28040;&#36153;&#31561;&#26041;&#38754;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#35768;&#22810;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#20063;&#23384;&#22312;&#30528;&#34987;&#28389;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#24694;&#24847;&#29992;&#25143;&#29992;&#23427;&#20204;&#26469;&#25955;&#25773;&#20167;&#24680;&#35328;&#35770;&#12289;&#25915;&#20987;&#24615;&#20869;&#23481;&#12289;&#35875;&#35328;&#31561;&#65292;&#20197;&#33719;&#24471;&#31038;&#20250;&#21644;&#25919;&#27835;&#35758;&#31243;&#65292;&#25110;&#32773;&#20260;&#23475;&#20010;&#20154;&#12289;&#23454;&#20307;&#21644;&#32452;&#32455;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#29992;&#25143;&#22312;&#26410;&#32463;&#39564;&#35777;&#30340;&#24773;&#20917;&#19979;&#26080;&#24847;&#20013;&#20998;&#20139;&#20449;&#24687;&#65292;&#25110;&#32773;&#26080;&#24847;&#20013;&#21457;&#24067;&#26377;&#23475;&#20449;&#24687;&#12290;&#19968;&#20123;&#27492;&#31867;&#20869;&#23481;&#32463;&#24120;&#34987;&#24179;&#21488;&#21024;&#38500;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#36829;&#21453;&#20102;&#26465;&#27454;&#21644;&#25919;&#31574;&#65292;&#20063;&#21487;&#33021;&#26159;&#30001;&#20110;&#29992;&#25143;&#33258;&#24049;&#30340;&#19981;&#21516;&#21407;&#22240;&#65292;&#27604;&#22914;&#21518;&#24724;&#20102;&#12290;&#30446;&#21069;&#26377;&#35768;&#22810;&#30740;&#31350;&#23545;&#21024;&#38500;&#20869;&#23481;&#36827;&#34892;&#20102;&#34920;&#24449;&#12289;&#29702;&#35299;&#21644;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#26088;&#22312;&#35782;&#21035;&#21024;&#38500;&#30340;&#32454;&#33268;&#21407;&#22240;&#65288;&#20363;&#22914;&#65292;&#24086;&#23376;&#20196;&#20154;&#21453;&#24863;&#12289;&#20167;&#24680;&#35328;&#35770;&#25110;&#27809;&#26377;&#21487;&#35782;&#21035;&#30340;&#21407;&#22240;&#65289;&#30340;&#30740;&#31350;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21363;&#23558;&#21457;&#24067;&#30340;&#34987;&#21024;&#38500;&#30340;&#25512;&#25991;&#65292;&#24182;&#25512;&#29702;&#23427;&#20204;&#30340;&#28508;&#22312;&#21361;&#23475;&#21644;&#36829;&#21453;&#24179;&#21488;&#25919;&#31574;&#30340;&#21407;&#22240;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#25991;&#30340;&#21508;&#31181;&#29305;&#24449;&#26469;&#39044;&#27979;&#23427;&#20204;&#26159;&#21542;&#20250;&#34987;&#21024;&#38500;&#65292;&#22914;&#26524;&#20250;&#34987;&#21024;&#38500;&#65292;&#21017;&#26159;&#20026;&#20160;&#20040;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#21024;&#38500;&#30340;&#25512;&#25991;&#21450;&#20854;&#21407;&#22240;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#25104;&#20026;&#20419;&#36827;&#26356;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms empower us in several ways, from information dissemination to consumption. While these platforms are useful in promoting citizen journalism, public awareness etc., they have misuse potentials. Malicious users use them to disseminate hate-speech, offensive content, rumor etc. to gain social and political agendas or to harm individuals, entities and organizations. Often times, general users unconsciously share information without verifying it, or unintentionally post harmful messages. Some of such content often get deleted either by the platform due to the violation of terms and policies, or users themselves for different reasons, e.g., regrets. There is a wide range of studies in characterizing, understanding and predicting deleted content. However, studies which aims to identify the fine-grained reasons (e.g., posts are offensive, hate speech or no identifiable reason) behind deleted content, are limited. In this study we address this gap, by identifying deleted 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#33258;&#36866;&#24212;&#21160;&#24577;&#25552;&#21462;&#20840;&#23616;&#39033;&#30446;&#36716;&#25442;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#22686;&#24378;&#65292;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04619</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Graph Masked Autoencoder for Sequential Recommendation. (arXiv:2305.04619v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#33258;&#36866;&#24212;&#21160;&#24577;&#25552;&#21462;&#20840;&#23616;&#39033;&#30446;&#36716;&#25442;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#22686;&#24378;&#65292;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#19968;&#20123;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#20363;&#22914;Transformer&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#36890;&#36807;&#39640;&#38454;&#39033;&#20381;&#36182;&#24314;&#27169;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36817;&#26399;&#30340;&#20851;&#27880;&#65292;&#36890;&#36807;&#23884;&#20837;&#23545;&#27604;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;&#27604;&#35270;&#22270;&#29983;&#25104;&#31574;&#30053;&#30340;&#25163;&#24037;&#21046;&#23450;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;CL&#22686;&#24378;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#25512;&#33616;&#20219;&#21153;&#20013;&#20135;&#29983;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#33021;&#23545;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#22122;&#22768;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#20840;&#23616;&#20449;&#24687;&#25552;&#21462;&#30340;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65288;MAERec&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#33258;&#28982;&#22320;&#36991;&#20813;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24471;&#30410;&#20110;&#20854;&#29420;&#29305;&#30340;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#20351;&#34920;&#31034;&#19981;&#20165;&#21033;&#29992;&#26412;&#22320;&#39034;&#24207;&#20449;&#24687;&#65292;&#36824;&#21033;&#29992;&#39033;&#30446;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective Graph Masked AutoEncoder-enhanced sequential Recommender system (MAERec) that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the abov
&lt;/p&gt;</description></item><item><title>&#26356;&#22810;&#30340;&#21518;&#20195;&#25968;&#37327;&#26377;&#21161;&#20110;$(1 + (\lambda, \lambda))$&#36951;&#20256;&#31639;&#27861;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2305.04553</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#21518;&#20195;&#25968;&#37327;&#26377;&#21161;&#20110;$(1 + (\lambda, \lambda))$&#36951;&#20256;&#31639;&#27861;&#20811;&#26381;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Larger Offspring Populations Help the $(1 + (\lambda, \lambda))$ Genetic Algorithm to Overcome the Noise. (arXiv:2305.04553v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04553
&lt;/p&gt;
&lt;p&gt;
&#26356;&#22810;&#30340;&#21518;&#20195;&#25968;&#37327;&#26377;&#21161;&#20110;$(1 + (\lambda, \lambda))$&#36951;&#20256;&#31639;&#27861;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#30693;&#23545;&#20110;&#36866;&#24212;&#24230;&#30340;&#35780;&#20272;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#26159;&#26356;&#22810;&#30340;&#21518;&#20195;&#25968;&#37327;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;$(1 + (\lambda, \lambda))$&#36951;&#20256;&#31639;&#27861;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#31243;&#24230;&#12290;&#35813;&#31639;&#27861;&#20063;&#20351;&#29992;&#20102;&#26356;&#22810;&#30340;&#21518;&#20195;&#25968;&#37327;&#65292;&#20294;&#20013;&#38388;&#30340;&#36873;&#25321;&#27493;&#39588;&#21644;&#23545;&#20132;&#21449;&#26041;&#24335;&#30340;&#38750;&#26631;&#20934;&#20351;&#29992;&#20316;&#20026;&#20462;&#22797;&#26426;&#21046;&#21487;&#33021;&#20351;&#35813;&#31639;&#27861;&#19981;&#22914;&#31616;&#21333;&#30340;$(1+\lambda)$&#36827;&#21270;&#31639;&#27861;&#40065;&#26834;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#32463;&#20856;&#22522;&#20934;&#38382;&#39064;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20010;&#38590;&#28857;&#24182;&#19981;&#20250;&#20986;&#29616;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#29978;&#33267;&#27604;$(1+\lambda)$ EA&#26356;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms are known to be robust to noise in the evaluation of the fitness. In particular, larger offspring population sizes often lead to strong robustness. We analyze to what extent the $(1+(\lambda,\lambda))$ genetic algorithm is robust to noise. This algorithm also works with larger offspring population sizes, but an intermediate selection step and a non-standard use of crossover as repair mechanism could render this algorithm less robust than, e.g., the simple $(1+\lambda)$ evolutionary algorithm. Our experimental analysis on several classic benchmark problems shows that this difficulty does not arise. Surprisingly, in many situations this algorithm is even more robust to noise than the $(1+\lambda)$~EA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.04474</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#35843;&#33410;&#30340;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation. (arXiv:2305.04474v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#38754;&#20020;&#30528;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174; mutual information &#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#28041;&#21450;&#21040;&#36127;&#26679;&#26412;&#30340;&#20114;&#20449;&#24687;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#19979;&#28216;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#25351;&#23548;&#19979;&#31995;&#32479;&#22320;&#24179;&#34913;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#26377;&#30410;&#24433;&#21709;&#21644;&#26377;&#23475;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04379</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#23578;&#26816;&#27979;&#30340;&#19981;&#24179;&#34913;&#26631;&#31614;&#26679;&#26412;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection. (arXiv:2305.04379v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#12290;&#23454;&#29616;&#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26174;&#33879;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#26102;&#23578;&#26381;&#35013;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#23646;&#24615;&#20998;&#31867;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label classification models have a wide range of applications in E-commerce, including visual-based label predictions and language-based sentiment classifications. A major challenge in achieving satisfactory performance for these tasks in the real world is the notable imbalance in data distribution. For instance, in fashion attribute detection, there may be only six 'puff sleeve' clothes among 1000 products in most E-commerce fashion catalogs. To address this issue, we explore more data-efficient model training techniques rather than acquiring a huge amount of annotations to collect sufficient samples, which is neither economic nor scalable. In this paper, we propose a state-of-the-art weighted objective function to boost the performance of deep neural networks (DNNs) for multi-label classification with long-tailed data distribution. Our experiments involve image-based attribute classification of fashion apparels, and the results demonstrate favorable performance for the new weig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;RATs-NAS&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GCN&#19978;&#37325;&#23450;&#21521;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#26469;&#24555;&#36895;&#25628;&#32034;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.04206</link><description>&lt;p&gt;
RATs-NAS&#65306;GCN&#19978;&#30340;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#37325;&#23450;&#21521;&#29992;&#20110;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search. (arXiv:2305.04206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;RATs-NAS&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GCN&#19978;&#37325;&#23450;&#21521;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#26469;&#24555;&#36895;&#25628;&#32034;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25163;&#24037;&#35774;&#35745;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22914;VGG&#12289;ResNet&#12289;DenseNet&#31561;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29616;&#22312;&#19987;&#27880;&#20110;&#33258;&#21160;&#25214;&#21040;&#26368;&#20339;CNN&#26550;&#26500;&#26469;&#22788;&#29702;&#19978;&#36848;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#25628;&#32034;&#26550;&#26500;&#38750;&#24120;&#32791;&#26102;&#65292;&#20351;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#25104;&#20026;NAS&#30340;&#19968;&#20010;&#22522;&#26412;&#32780;&#37325;&#35201;&#30340;&#20998;&#25903;&#12290;&#24314;&#31435;&#39044;&#27979;&#22120;&#30340;&#20004;&#31181;&#24120;&#29992;&#25216;&#26415;&#26159;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#26412;&#25991;&#32771;&#34385;GCN&#21644;MLP&#22312;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#19978;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;Redirected Adjacent Trails NAS&#65288;RATs-NAS&#65289;&#65292;&#20197;&#24555;&#36895;&#25628;&#32034;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;RATs-NAS&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;Redirected Adjacent Trails GCN&#65288;RATs-GCN&#65289;&#21644;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;&#25628;&#32034;&#31354;&#38388;&#25277;&#26679;&#65288;P3S&#65289;&#27169;&#22359;&#12290; RATs-GCN&#21487;&#20197;&#25913;&#21464;&#36712;&#36857;&#21450;&#20854;&#24378;&#24230;&#20197;&#25628;&#32034;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;P3S&#27169;&#22359;&#21017;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#25277;&#26679;&#20197;&#25552;&#39640;&#39044;&#27979;&#26550;&#26500;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;RATs-NAS&#21487;&#20197;&#26356;&#24555;&#22320;&#25214;&#21040;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various hand-designed CNN architectures have been developed, such as VGG, ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on different tasks. Neural Architecture Search (NAS) now focuses on automatically finding the best CNN architecture to handle the above tasks. However, the verification of a searched architecture is very time-consuming and makes predictor-based methods become an essential and important branch of NAS. Two commonly used techniques to build predictors are graph-convolution networks (GCN) and multilayer perceptron (MLP). In this paper, we consider the difference between GCN and MLP on adjacent operation trails and then propose the Redirected Adjacent Trails NAS (RATs-NAS) to quickly search for the desired neural network architecture. The RATs-NAS consists of two components: the Redirected Adjacent Trails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S) module. RATs-GCN can change trails and their strengths to search for a better neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.04152</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#32447;&#36890;&#20449;&#30340;&#36890;&#36947;&#39537;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#36125;&#21494;&#26031;&#32852;&#37030;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#30340;&#36817;&#26399;&#21457;&#23637;&#24050;&#32463;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#37319;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#39057;&#29575;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#20852;&#36259;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#32852;&#37030;&#24179;&#22343; Langevin &#21160;&#21147;&#23398;(FALD)&#20316;&#20026;&#32852;&#37030;&#24179;&#22343;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#36890;&#20449;&#23384;&#22312;&#19979;&#26377;&#25928;&#22320;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#32447; FALD(WFALD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#21644;&#22522;&#20110;&#36890;&#36947;&#39537;&#21160;&#30340; Monte Carlo &#26356;&#26032;&#26469;&#23454;&#29616;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340; FALD&#12290;&#19982;&#20808;&#21069;&#30340;&#26080;&#32447;&#36125;&#21494;&#26031;&#23398;&#20064;&#30456;&#27604;&#65292;WFALD &#21487;&#20197;&#23454;&#29616;(i) &#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65307;&#24182;&#19988;(ii) &#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#20197; 2-Wasserstein &#36317;&#31163;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32473;&#20986;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.02750</link><description>&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#19982;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#23545;&#35805;&#24212;&#29992;&#30456;&#20851;&#65292;&#20351;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#24341;&#23548;&#23545;&#35805;&#26041;&#21521;&#65292;&#20197;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25110;&#28385;&#36275;&#31995;&#32479;&#26041;&#38754;&#30340;&#29305;&#23450;&#30446;&#26631;&#12290;&#23427;&#36890;&#36807;&#20808;&#36827;&#25216;&#26415;&#36171;&#33021;&#20197;&#36827;&#23637;&#21040;&#38656;&#35201;&#25112;&#30053;&#24615;&#21644;&#28608;&#21169;&#24615;&#20132;&#20114;&#30340;&#26356;&#22797;&#26434;&#20219;&#21153;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#31532;&#19968;&#31687;&#32508;&#36848;&#21487;&#20197;&#20026;&#31038;&#21306;&#25552;&#20379;&#24555;&#36895;&#35775;&#38382;&#21644;&#25972;&#20307;&#22270;&#29255;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01528</link><description>&lt;p&gt;
FIREBALL&#65306;&#19968;&#20221;&#21253;&#21547;&#32467;&#26500;&#21270;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons&#65288;D&#65286;D&#65289;&#26159;&#19968;&#27454;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20854;&#29609;&#23478;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21644;&#38544;&#34255;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25317;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#28216;&#25103;&#22238;&#21512;&#27604;&#20165;&#20351;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;LLMs&#26356;&#20855;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26159;&#21551;&#21457;&#24335;&#21019;&#24314;&#30340;&#65292;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#40644;&#37329;&#26631;&#20934;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FIREBALL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Discord&#30340;&#36817;25,000&#20010;&#30495;&#23454;D&#65286;D&#28216;&#25103;&#20250;&#35805;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#20351;&#29992;Avrae&#26426;&#22120;&#20154;&#30340;&#29609;&#23478;&#30340;&#28216;&#25103;&#20250;&#35805;&#65292;&#35813;&#26426;&#22120;&#20154;&#26159;&#20026;&#20102;&#24110;&#21161;&#20154;&#20204;&#22312;&#32447;&#29609;D&#65286;D&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#25429;&#33719;&#20102;&#35821;&#35328;&#12289;&#28216;&#25103;&#21629;&#20196;&#21644;&#22522;&#30784;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;Avrae&#29366;&#24577;&#20449;&#24687;&#65292;FIREBALL&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#8230;
&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&amp;D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&amp;D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01118</link><description>&lt;p&gt;
CSP&#65306;&#38024;&#23545;&#22320;&#29702;&#31354;&#38388;&#35270;&#35273;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#31354;&#38388;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01118
&lt;/p&gt;
&lt;p&gt;
CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#20844;&#24320;&#21487;&#29992;&#65292;&#32780;&#23545;&#35937;&#31867;&#21035;&#31561;&#26631;&#31614;&#21017;&#30456;&#23545;&#31232;&#32570;&#19988;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20165;&#38656;&#24456;&#23569;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#26159;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#23545;&#35937;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#22312;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#38454;&#27573;&#30452;&#25509;&#21033;&#29992;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20016;&#23500;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; Contrastive Spatial Pre-Training&#65288;CSP&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#23545;&#27604;&#30446;&#26631;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#36716;&#31227;&#21040;&#19979;&#28216;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CSP&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#20803;&#36923;&#36753;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#30830;&#23450;&#30340;&#27604;&#29305;&#20540;&#21644;&#19981;&#23384;&#22312;&#30340;&#27604;&#29305;&#65288;&#30495;&#31354;&#24577;&#65289;&#65292;&#19982;&#26631;&#20934;&#30340;&#20108;&#20803;&#36923;&#36753;&#31995;&#32479;&#30456;&#27604;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#19988;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#20108;&#36827;&#21046;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.00984</link><description>&lt;p&gt;
&#19977;&#20803;&#30636;&#26102;&#22122;&#22768;&#36923;&#36753;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Ternary Instantaneous Noise-based Logic. (arXiv:2305.00984v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#20803;&#36923;&#36753;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#30830;&#23450;&#30340;&#27604;&#29305;&#20540;&#21644;&#19981;&#23384;&#22312;&#30340;&#27604;&#29305;&#65288;&#30495;&#31354;&#24577;&#65289;&#65292;&#19982;&#26631;&#20934;&#30340;&#20108;&#20803;&#36923;&#36753;&#31995;&#32479;&#30456;&#27604;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#19988;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#20108;&#36827;&#21046;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#20540;&#30636;&#26102;&#22122;&#22768;&#36923;&#36753;&#30340;&#21487;&#33021;&#34920;&#31034;&#26041;&#27861;&#12290;&#31532;&#19977;&#20010;&#20540;&#26159;&#19981;&#30830;&#23450;&#30340;&#27604;&#29305;&#20540;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#21516;&#26102;&#65292;&#36824;&#26377;&#31532;&#22235;&#20010;&#20540;&#65292;&#23427;&#21487;&#20197;&#34920;&#31034;&#19981;&#23384;&#22312;&#30340;&#27604;&#29305;&#65288;&#30495;&#31354;&#24577;&#65289;&#65292;&#23545;&#20110;&#25152;&#26377;&#27604;&#29305;&#26469;&#35828;&#65292;&#23427;&#26159;&#30456;&#21516;&#30340;&#65288;1&#20010;&#25968;&#20540;&#65289;&#65292;&#20294;&#21364;&#26159;&#25152;&#26377;&#27604;&#29305;&#30340;&#21387;&#32553;&#24577;&#12290;&#19968;&#20123;&#36923;&#36753;&#38376;&#20063;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#19977;&#20803;&#23431;&#23449;&#19982;&#26631;&#20934;&#30340;&#20108;&#20803;&#23431;&#23449;&#30456;&#27604;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#65306;&#22312;&#20219;&#20309;&#26102;&#38047;&#21608;&#26399;&#20869;&#65292;&#20854;&#24133;&#24230;&#27704;&#36828;&#19981;&#20026;&#38646;&#12290;&#25152;&#26377;&#24050;&#30693;&#30340;&#20108;&#36827;&#21046;&#36923;&#36753;&#38376;&#22312;&#20108;&#20803;&#27604;&#29305;&#20540;&#19978;&#30340;&#24037;&#20316;&#26041;&#24335;&#19982;&#20808;&#21069;&#30456;&#21516;&#65292;&#22240;&#27492;&#26087;&#30340;&#20108;&#36827;&#21046;&#31639;&#27861;&#21487;&#20197;&#22312;&#19977;&#20803;&#31995;&#32479;&#20013;&#36816;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#65292;&#24182;&#19988;&#19981;&#20250;&#20986;&#29616;&#23431;&#23449;&#38646;&#20540;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the possible representations of three-valued instantaneous noise-based logic is proposed. The third value is an uncertain bit value, which can be useful in artificial intelligence applications. There is a forth value, too, that can represent a non-existing bit (vacuum-state) that is the same (1 numeric value) for all bits, however that is a squeezed state common for all bits. Some logic gates are explored. The ternary Universe has a significant advantage compared to the standard binary one: its amplitude is never zero during any clock period. All the known binary logic gates work for the binary bit values in the same way as earlier therefore the former binary algorithms can be run in the ternary system with no change and without the problems posed by zero values of the Universe.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00909</link><description>&lt;p&gt;
&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;&#65306;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22797;&#26434;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#20570;&#27861;&#36890;&#24120;&#26159;&#20808;&#27010;&#36848;&#19968;&#19979;&#25511;&#21046;&#27969;&#31243;&#65292;&#28982;&#21518;&#36845;&#20195;&#36827;&#34892;&#20016;&#23500;&#65292;&#26368;&#32456;&#29983;&#25104;&#19968;&#20123;&#31934;&#24515;&#21152;&#24037;&#30340;&#35821;&#27861;&#32467;&#26500;&#21644;&#23618;&#27425;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#20195;&#30721;&#65292;&#27809;&#26377;&#20013;&#38388;&#29615;&#33410;&#65292;&#20197;&#21453;&#26144;"&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;"&#30340;&#32467;&#26500;&#21270;&#24605;&#32500;&#36807;&#31243;&#12290;&#21463;&#21040;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChainCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#31243;&#24207;&#32508;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36880;&#27493;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#21363;&#20174;&#31895;&#21040;&#32454;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#35299;&#26512;&#23558;&#28304;&#20195;&#30721;&#20998;&#35299;&#20026;&#24067;&#23616;&#26694;&#26550;&#32452;&#20214;&#21644;&#38468;&#20214;&#32452;&#20214;&#65292;&#20197;&#26500;&#24314;&#23618;&#27425;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#39044;&#27979;&#30446;&#26631;&#37325;&#26032;&#21551;&#21160;&#65292;&#24418;&#25104;&#22810;&#27425;&#36890;&#36807;&#30446;&#26631;&#65292;&#27599;&#27425;&#29983;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#20018;&#32852;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;</title><link>http://arxiv.org/abs/2305.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#31946;&#20998;&#31665;&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#24448;&#24448;&#20250;&#36807;&#20110;&#33258;&#20449;&#65292;&#20854;&#21407;&#22987;&#32467;&#26524;&#30340;&#27010;&#29575;&#24182;&#19981;&#31526;&#21512;&#30495;&#23454;&#30340;&#20915;&#31574;&#27010;&#29575;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#26159;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20808;&#21069;&#30340;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#20027;&#35201;&#21033;&#29992;&#28165;&#26224;&#30340;&#20998;&#31665;&#25104;&#21592;&#36164;&#26684;&#24230;&#37327;&#12290;&#36825;&#21152;&#21095;&#20102;&#27169;&#22411;&#27010;&#29575;&#30340;&#20559;&#26012;&#65292;&#24182;&#25551;&#32472;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19981;&#23436;&#25972;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#30340;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#27979;&#37327;&#26657;&#20934;&#35823;&#24046;&#26102;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;ECE&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#32676;&#20307;&#21644;&#31867;&#21035;&#25104;&#21592;&#36523;&#20221;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;FCE&#22312;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#65292;&#32531;&#35299;&#20102;&#27169;&#22411;&#32622;&#20449;&#24230;&#20998;&#25968;&#20559;&#26012;&#23545;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;https://github.com/srdgFHE/FCE-paper&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20351;&#29992;FCE&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14712</link><description>&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems. (arXiv:2304.14712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;12&#20010;&#23454;&#20363;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#38382;&#39064;&#22797;&#26434;&#24230;&#27700;&#24179;&#65288;&#21253;&#21547;&#20174;38&#21040;53&#20010;&#21253;&#35065;&#30340;&#25968;&#37327;&#65289;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#26465;&#20214;&#26469;&#26500;&#24314;&#36825;&#20123;&#23454;&#20363;&#65306;i)&#29289;&#21697;&#21644;&#31665;&#23376;&#23610;&#23544;&#65292;ii)&#37325;&#37327;&#38480;&#21046;&#65292;iii)&#21253;&#31867;&#21035;&#20043;&#38388;&#30340;&#20146;&#21644;&#24615;&#65292;iv)&#21253;&#35013;&#39034;&#24207;&#30340;&#20559;&#22909;&#21644;v)&#36127;&#36733;&#24179;&#34913;&#12290;&#38500;&#20102;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#20027;&#24320;&#21457;&#30340;Python&#33050;&#26412;&#29992;&#20110;&#25968;&#25454;&#38598;&#29983;&#25104;&#65292;&#31216;&#20026;Q4RealBPP-DataGen&#12290;&#35813;&#22522;&#20934;&#39318;&#20808;&#34987;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#27714;&#35299;&#22120;&#65292;&#22240;&#27492;&#36825;&#32452;&#23454;&#20363;&#30340;&#29305;&#24449;&#26159;&#25353;&#29031;&#37327;&#23376;&#35774;&#22791;&#30340;&#24403;&#21069;&#38480;&#21046;&#35774;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21253;&#21547;&#22312;&#20869;&#65292;&#20801;&#35768;&#26500;&#24314;&#36890;&#29992;&#22522;&#20934;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#23545;&#27604;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a benchmark for real-world bin packing problems is proposed. This dataset is composed of 12 instances comprehending different levels of problem complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements. In fact, several real-world oriented restrictions have been considered for building these instances: i) items and bins dimensions, ii) weight restrictions, iii) affinities among packages categories iv) preferences for package ordering and v) load balancing. Besides the data, we also provide an own-developed Python script for the dataset generation, coined as Q4RealBPP-DataGen. The benchmark was firstly proposed to evaluate quantum solvers, therefore the characteristic of this set of instances were designed according to the current limitations of quantum devices. Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks. The data introduced on this paper provides a baseline that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;SNN&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#35813;&#32467;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;Spikingformer&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11954</link><description>&lt;p&gt;
Spikingformer: &#22522;&#20110;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network. (arXiv:2304.11954v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;SNN&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#35813;&#32467;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;Spikingformer&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#30001;&#20110;&#20854;&#20107;&#20214;&#39537;&#21160;&#30340;&#33033;&#20914;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33410;&#33021;&#26367;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;Spikformer&#21644;SEW ResNet&#22312;&#20869;&#30340;&#26368;&#26032;&#28145;&#24230;SNN&#23384;&#22312;&#38750;&#33033;&#20914;&#35745;&#31639;&#65288;&#25972;&#25968;&#28014;&#28857;&#20056;&#27861;&#65289;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#27531;&#24046;&#36830;&#25509;&#32467;&#26500;&#25152;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;&#38750;&#33033;&#20914;&#35745;&#31639;&#22686;&#21152;&#20102;SNN&#30340;&#21151;&#32791;&#65292;&#24182;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#21482;&#25903;&#25345;&#33033;&#20914;&#25805;&#20316;&#30340;&#20027;&#27969;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20010;&#27531;&#24046;&#35774;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Spikingformer&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;ImageNet&#12289;CIFAR10&#12289;CIFAR100&#12289;CIFAR10-DVS&#21644;DVS128 Gesture&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Spikingformer&#65292;&#24182;&#34920;&#26126;&#20316;&#20026;&#20808;&#36827;&#39592;&#24178;&#30340;&#30452;&#25509;&#35757;&#32451;&#30340;&#32431;SNN&#65292;Spikingformer&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;(75.85$\%$ top-1 accuracy on ImageNet)&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, state-of-the-art deep SNNs (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connection. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware, which only supports spike operations. In this paper, we propose a hardware-friendly spike-driven residual learning architecture for SNNs to avoid non-spike computations. Based on this residual design, we develop Spikingformer, a pure transformer-based spiking neural network. We evaluate Spikingformer on ImageNet, CIFAR10, CIFAR100, CIFAR10-DVS and DVS128 Gesture datasets, and demonstrate that Spikingformer outperforms the state-of-the-art in directly trained pure SNNs as a novel advanced backbone (75.85$\
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17523</link><description>&lt;p&gt;
&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24050;&#36827;&#20837;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#30446;&#21069;&#25105;&#20204;&#25317;&#26377;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#23545;&#36752;&#23556;&#21644;&#28201;&#24230;&#31561;&#29615;&#22659;&#21464;&#37327;&#25935;&#24863;&#65292;&#22240;&#27492;&#20250;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#31639;&#27861;&#21644;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;NISQ&#22788;&#29702;&#22120;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#35299;&#37322;&#20854;&#22024;&#26434;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#37327;&#23376;&#24577;&#26377;&#22810;&#23569;&#20449;&#24515;&#65311;&#36825;&#31181;&#20449;&#24515;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;NISQ&#35745;&#31639;&#26426;&#23558;&#36755;&#20986;&#20854;&#37327;&#23376;&#20301;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#26102;&#24456;&#38590;&#21306;&#20998;&#20998;&#24067;&#26159;&#21542;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#35745;&#31639;&#25110;&#21482;&#26159;&#38543;&#26426;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;&#39044;&#27979;&#26694;&#26550;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#26500;&#24314;&#35757;&#32451;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#21512;&#29702;&#29289;&#21697;&#38598;&#21512;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#34920;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08176</link><description>&lt;p&gt;
&#21512;&#29702;&#29289;&#21697;&#38598;&#21512;&#32972;&#21518;&#30340;&#36923;&#36753;&#21450;&#20854;&#36807;&#28388;&#22120;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The logic behind desirable sets of things, and its filter representation. (arXiv:2302.08176v2 [math.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#21512;&#29702;&#29289;&#21697;&#38598;&#21512;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#34920;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#36817;&#19968;&#32452;&#21512;&#29702;&#19988;&#20196;&#20154;&#21521;&#24448;&#30340;&#29289;&#21697;&#38598;&#21512;&#29702;&#35770;&#30340;&#36923;&#36753;&#65292;&#36825;&#20010;&#29702;&#35770;&#25512;&#24191;&#20102;&#20196;&#20154;&#21521;&#24448;&#30340;&#36172;&#27880;&#21644;&#19968;&#33268;&#30340;&#36873;&#25321;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#35782;&#21035;&#20801;&#35768;&#25105;&#20204;&#24314;&#31435;&#21508;&#31181;&#34920;&#31034;&#32467;&#26524;&#65292;&#29992;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;&#36825;&#31181;&#19968;&#33268;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We identify the logic behind the recent theory of coherent sets of desirable (sets of) things, which generalise desirable (sets of) gambles and coherent choice functions, and show that this identification allows us to establish various representation results for such coherent models in terms of simpler ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.06675</link><description>&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#30340;&#31526;&#21495;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21457;&#29616;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#25928;&#25628;&#32034;&#25216;&#26415;&#26469;&#25506;&#32034;&#26080;&#38480;&#21644;&#31232;&#30095;&#30340;&#31243;&#24207;&#31354;&#38388;&#12290;&#20026;&#20102;&#22635;&#34917;&#20195;&#29702;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#24040;&#22823;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31243;&#24207;&#36873;&#25321;&#21644;&#31616;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;$ \textbf {Lion} $&#65288;$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $&#65289;&#12290;&#23427;&#30340;&#35760;&#24518;&#25928;&#29575;&#27604;Adam&#26356;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#36319;&#36394;&#21160;&#37327;&#12290;&#19982;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#19981;&#21516;&#65292;&#36890;&#36807;&#31526;&#21495;&#36816;&#31639;&#35745;&#31639;&#30340;&#27599;&#20010;&#21442;&#25968;&#30340;&#26356;&#26032;&#20855;&#26377;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#23558;Lion&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#21644;Adafactor&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;Lion&#23558;&#22312;ImageNet&#19978;ViT&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;2&#65285;&#65292;&#24182;&#33410;&#30465;&#20102;&#22810;&#36798;5&#20493;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#21644;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#30340;&#20316;&#21697;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#29983;&#25104;&#21746;&#23398;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25307;&#21215;&#22823;&#37327;&#21442;&#19982;&#32773;&#26469;&#21306;&#20998;&#30495;&#27491;&#30340;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#23383;&#12290;&#19987;&#23478;&#25104;&#21151;&#29575;&#36798;&#21040;51&#65285;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#39044;&#26399;&#30340;80&#65285;&#65292;&#35813;&#27169;&#22411;&#26377;&#21487;&#33021;&#36229;&#36234;&#20154;&#31867;&#30340;&#24605;&#32500;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.01339</link><description>&lt;p&gt;
&#21019;&#36896;&#19968;&#20010;&#21746;&#23398;&#23478;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Creating a Large Language Model of a Philosopher. (arXiv:2302.01339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#21644;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#30340;&#20316;&#21697;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#29983;&#25104;&#21746;&#23398;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25307;&#21215;&#22823;&#37327;&#21442;&#19982;&#32773;&#26469;&#21306;&#20998;&#30495;&#27491;&#30340;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#23383;&#12290;&#19987;&#23478;&#25104;&#21151;&#29575;&#36798;&#21040;51&#65285;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#39044;&#26399;&#30340;80&#65285;&#65292;&#35813;&#27169;&#22411;&#26377;&#21487;&#33021;&#36229;&#36234;&#20154;&#31867;&#30340;&#24605;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#21542;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#38590;&#20197;&#19982;&#20154;&#31867;&#21746;&#23398;&#23478;&#30340;&#25991;&#26412;&#21306;&#20998;&#30340;&#21746;&#23398;&#25991;&#23383;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#30340;&#20316;&#21697;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24494;&#35843;OpenAI&#30340;GPT-3&#12290;&#20026;&#20102;&#25506;&#32034;&#20025;&#23612;&#29305;&#27169;&#22411;&#65292;&#25105;&#20204;&#21521;&#30495;&#27491;&#30340;&#20025;&#23612;&#29305;&#25552;&#20986;&#20102;&#21313;&#20010;&#21746;&#23398;&#38382;&#39064;&#65292;&#28982;&#21518;&#21521;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#25910;&#38598;&#20102;&#22235;&#20010;&#22238;&#31572;&#65292;&#27809;&#26377;&#36827;&#34892;&#31579;&#36873;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;425&#21517;&#21442;&#19982;&#32773;&#26469;&#21306;&#20998;&#20025;&#23612;&#29305;&#30340;&#31572;&#26696;&#21644;&#22235;&#20010;&#26426;&#22120;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#29087;&#24713;&#20025;&#23612;&#29305;&#20316;&#21697;&#30340;&#19987;&#23478;&#65288;N = 25&#65289;&#30340;&#25104;&#21151;&#29575;&#20026;51&#65285;&#65292;&#39640;&#20110;20&#65285;&#30340;&#26426;&#20250;&#29575;&#65292;&#20294;&#19981;&#21450;&#25105;&#20204;&#39044;&#26399;&#30340;80&#65285;&#30340;&#27491;&#30830;&#29575;&#12290;&#23545;&#20110;&#20854;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65292;&#35821;&#35328;&#27169;&#22411;&#33267;&#23569;&#29983;&#25104;&#20102;&#19968;&#20010;&#31572;&#26696;&#65292;&#19987;&#23478;&#20204;&#26356;&#39057;&#32321;&#22320;&#36873;&#25321;&#35813;&#31572;&#26696;&#32780;&#38750;&#20025;&#23612;&#29305;&#33258;&#24049;&#30340;&#31572;&#26696;&#12290;&#21746;&#23398;&#21338;&#23458;&#35835;&#32773;&#65288;N = 302&#65289;&#30340;&#34920;&#29616;&#19982;&#19987;&#23478;&#30456;&#20284;&#65292;&#32780;&#26222;&#36890;&#30740;&#31350;&#21442;&#19982;&#32773;&#65288;N = 98&#65289;&#21017;&#36817;&#20284;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;AI&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#21322;&#20107;&#23454;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#65292;&#24182;&#23545;&#21382;&#21490;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#26410;&#26469;&#31639;&#27861;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2301.11970</link><description>&lt;p&gt;
&#21363;&#20351;&#35299;&#37322;&#65306;&#21322;&#20107;&#23454;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;Semi-Factual XAI&#65289;&#30340;&#20808;&#21069;&#24037;&#20316;&#12289;&#26399;&#26395;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Even if Explanations: Prior Work, Desiderata &amp; Benchmarks for Semi-Factual XAI. (arXiv:2301.11970v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;AI&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#21322;&#20107;&#23454;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#65292;&#24182;&#23545;&#21382;&#21490;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#26410;&#26469;&#31639;&#27861;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#8221;&#65288;XAI&#65289;&#30740;&#31350;&#19987;&#27880;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#20316;&#20026;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20915;&#31574;&#30340;&#21518;&#35780;&#20215;&#65288;&#20363;&#22914;&#65292;&#21487;&#33021;&#21578;&#35785;&#19968;&#20010;&#34987;&#25298;&#32477;&#36151;&#27454;&#30340;&#23458;&#25143;&#65306;&#22914;&#26524;&#24744;&#35201;&#27714;&#19968;&#20010;&#26356;&#30701;&#26399;&#38480;&#30340;&#36151;&#27454;&#65292;&#23601;&#20250;&#34987;&#25209;&#20934;&#65289;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#35828;&#26126;&#20102;&#23545;AI&#31995;&#32479;&#36755;&#20837;&#29305;&#24449;&#30340;&#26356;&#25913;&#22914;&#20309;&#25913;&#21464;&#36755;&#20986;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#19968;&#31181;&#31867;&#22411;&#30340;&#21453;&#20107;&#23454;&#31216;&#20026;&#21322;&#20107;&#23454;&#65292;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23427;&#20204;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36825;&#20123;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21382;&#21490;&#21644;&#26368;&#26032;&#31361;&#30772;&#12290;&#23427;&#20026;&#21322;&#20107;&#23454;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23450;&#20041;&#20102;&#20851;&#38190;&#26399;&#26395;&#65292;&#24182;&#25253;&#21578;&#20102;&#21382;&#21490;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#30340;&#24188;&#31258;&#26041;&#27861;&#65289;&#65292;&#20197;&#20026;&#26410;&#26469;&#30340;&#31639;&#27861;&#24320;&#21457;&#25552;&#20379;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, eXplainable AI (XAI) research has focused on counterfactual explanations as post-hoc justifications for AI-system decisions (e.g. a customer refused a loan might be told: If you asked for a loan with a shorter term, it would have been approved). Counterfactuals explain what changes to the input-features of an AI system change the output-decision. However, there is a sub-type of counterfactual, semi-factuals, that have received less attention in AI (though the Cognitive Sciences have studied them extensively). This paper surveys these literatures to summarise historical and recent breakthroughs in this area. It defines key desiderata for semi-factual XAI and reports benchmark tests of historical algorithms (along with a novel, naieve method) to provide a solid basis for future algorithmic developments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.07068</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#23433;&#20840;&#36755;&#20837;&#35745;&#25968;&#30340;#DNN-Verification&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21487;&#20197;&#29992;&#26469;&#26816;&#26597;DNN&#26159;&#21542;&#19981;&#23433;&#20840;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#19981;&#23433;&#20840;&#30340;&#36755;&#20837;&#37197;&#32622;&#65292;&#20294;&#23427;&#20204;&#30340;&#26159;/&#21542;&#36755;&#20986;&#23545;&#20110;&#20854;&#20182;&#30446;&#30340;&#65288;&#22914;&#23631;&#34109;&#12289;&#27169;&#22411;&#36873;&#25321;&#25110;&#22521;&#35757;&#25913;&#36827;&#65289;&#30340;&#20449;&#24687;&#19981;&#36275;&#22815;&#35814;&#32454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#35745;&#31639;&#23548;&#33268;DNN&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#36820;&#22238;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#30340;#P&#23436;&#22791;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#27491;&#30830;&#35745;&#25968;&#30340;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#21576;&#29616;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30830;&#23450;&#24182;&#34920;&#24449;&#20102;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#65292;&#21516;&#26102;&#20063;&#24341;&#39046;&#20102;&#23545;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26041;&#27861;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.13881</link><description>&lt;p&gt;
&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26426;&#22120;&#30340;&#29305;&#24449;&#23398;&#20064;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. (arXiv:2212.13881v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#24182;&#34920;&#24449;&#20102;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#65292;&#21516;&#26102;&#20063;&#24341;&#39046;&#20102;&#23545;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26041;&#27861;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#25216;&#26415;&#21644;&#31185;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#25110;&#25968;&#25454;&#27169;&#24335;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#30830;&#23450;&#36825;&#26679;&#30340;&#26426;&#21046;&#26159;&#25512;&#21160;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#21487;&#38752;&#37319;&#29992;&#30340;&#20851;&#38190;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#34920;&#24449;&#20102;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#34920;&#26126;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#26159;&#36890;&#36807;&#23454;&#29616;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#26469;&#21152;&#24378;&#19982;&#27169;&#22411;&#36755;&#20986;&#23494;&#20999;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#25581;&#31034;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#65292;&#21253;&#25324;&#20551;&#29305;&#24449;&#30340;&#20986;&#29616;&#21644;&#31616;&#21333;&#24615;&#20559;&#24046;&#20197;&#21450;&#22914;&#20309;&#20462;&#21098;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#12298;&#24425;&#31080;&#20551;&#35774;&#12299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#26426;&#21046;&#20063;&#24341;&#39046;&#20102;&#23545;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26041;&#27861;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years neural networks have achieved impressive results on many technological and scientific tasks. Yet, the mechanism through which these models automatically select features, or patterns in data, for prediction remains unclear. Identifying such a mechanism is key to advancing performance and interpretability of neural networks and promoting reliable adoption of these models in scientific applications. In this paper, we identify and characterize the mechanism through which deep fully connected neural networks learn features. We posit the Deep Neural Feature Ansatz, which states that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output. Our ansatz sheds light on various deep learning phenomena including emergence of spurious features and simplicity biases and how pruning networks can increase performance, the "lottery ticket hypothesis." Moreover, the mechanism identified in our work leads to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16715</link><description>&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;RL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#20559;&#24046;&#65292;&#36890;&#36807;&#23545;&#32534;&#30721;&#25361;&#25112;&#36827;&#34892;&#27169;&#22359;&#21270;&#20998;&#35299;&#21644;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#24178;&#39044;&#26426;&#21046;&#26469;&#26292;&#38706;&#19981;&#33391;&#20559;&#24046;&#65292;&#26368;&#32456;&#20316;&#20026;&#32531;&#35299;&#31574;&#30053;&#36827;&#34892;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2211.00609</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#20195;&#30721;&#29983;&#25104;&#20559;&#24046;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple, Yet Effective Approach to Finding Biases in Code Generation. (arXiv:2211.00609v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#20559;&#24046;&#65292;&#36890;&#36807;&#23545;&#32534;&#30721;&#25361;&#25112;&#36827;&#34892;&#27169;&#22359;&#21270;&#20998;&#35299;&#21644;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#24178;&#39044;&#26426;&#21046;&#26469;&#26292;&#38706;&#19981;&#33391;&#20559;&#24046;&#65292;&#26368;&#32456;&#20316;&#20026;&#32531;&#35299;&#31574;&#30053;&#36827;&#34892;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#20986;&#29616;&#20102;&#12290;&#23427;&#20204;&#32463;&#36807;&#22823;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#65292;&#32780;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#22823;&#22810;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#26426;&#21487;&#25191;&#34892;&#20195;&#30721;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#23384;&#22312;&#26469;&#33258;&#20854;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#20559;&#24046;&#65292;&#36825;&#20123;&#20559;&#24046;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20250;&#38477;&#20302;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#8220;&#24433;&#21709;&#22359;&#8221;&#27010;&#24565;&#65292;&#23427;&#33021;&#22815;&#23545;&#32534;&#30721;&#25361;&#25112;&#36827;&#34892;&#27169;&#22359;&#21270;&#20998;&#35299;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#24178;&#39044;&#26426;&#21046;&#65292;&#31867;&#20284;&#20110;&#23545;&#25239;&#24615;&#27979;&#35797;&#65292;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#30340;&#22833;&#25928;&#27169;&#24335;&#26469;&#26292;&#38706;&#19981;&#33391;&#20559;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#24494;&#35843;&#26399;&#38388;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#29992;&#20316;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#65292;&#20316;&#20026;&#36825;&#20123;&#20559;&#24046;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.  To investigate the effect, we propose the "block of influence" concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#26469;&#20102;&#35299;&#25919;&#27835;&#27963;&#21160;&#30340;&#29305;&#28857;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2210.10669</link><description>&lt;p&gt;
&#38024;&#23545;Facebook&#19978;&#30340;&#25919;&#27835;&#27963;&#21160;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#26469;&#20102;&#35299;&#25919;&#27835;&#27963;&#21160;&#30340;&#29305;&#28857;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30446;&#21069;&#26159;&#25919;&#27835;&#20449;&#24687;&#20256;&#25773;&#30340;&#20027;&#35201;&#28192;&#36947;&#65292;&#25919;&#27835;&#23478;&#20204;&#33021;&#22815;&#36890;&#36807;&#36825;&#20123;&#24179;&#21488;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#36827;&#34892;&#23459;&#20256;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#24212;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20351;&#36825;&#31181;&#20132;&#27969;&#36879;&#26126;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20449;&#24687;&#20256;&#25773;&#19982;&#30446;&#26631;&#21463;&#20247;&#32039;&#23494;&#30456;&#36830;&#65292;&#24182;&#32463;&#24120;&#34987;&#22810;&#20010;&#21033;&#30410;&#25912;&#20851;&#26041;&#20849;&#21516;&#20256;&#25773;&#12290;&#26412;&#25991;&#26088;&#22312;&#31532;&#19968;&#27493;&#20102;&#35299;&#36825;&#20123;&#39640;&#24230;&#20998;&#25955;&#30340;&#25919;&#27835;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#24182;&#20998;&#26512;&#25919;&#27835;&#27963;&#21160;&#22914;&#20309;&#20351;&#29992;&#26576;&#31181;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#22914;&#20301;&#32622;&#12289;&#24615;&#21035;&#25110;&#24180;&#40836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36873;&#20030;&#27665;&#24847;&#35843;&#26597;&#20013;&#25919;&#27835;&#24191;&#21578;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26500;&#24314;&#26082;&#31934;&#30830;&#21448;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#22522;&#26412;&#19981;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#23398;&#19979;&#38480;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.15259</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19981;&#21487;&#33021;&#23433;&#20840;&#30340;&#35770;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Impossible Safety of Large AI Models. (arXiv:2209.15259v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26500;&#24314;&#26082;&#31934;&#30830;&#21448;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#22522;&#26412;&#19981;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#23398;&#19979;&#38480;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;(LAIMs)&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26368;&#31361;&#20986;&#30340;&#26368;&#36817;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#23427;&#20204;&#23384;&#22312;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#31995;&#32479;&#21270;&#22320;&#24635;&#32467;&#20102;&#26377;&#20851;&#26500;&#24314;&#20219;&#24847;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#26412;&#19981;&#21487;&#33021;&#24615;&#30340;&#30693;&#35782;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20170;&#22825;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#30340;&#20851;&#38190;&#25361;&#25112;&#29305;&#24449;&#12290;&#21363;&#65292;&#39640;&#31934;&#24230;&#20284;&#20046;&#38656;&#35201;&#35760;&#20303;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#29992;&#25143;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#39640;&#24230;&#24322;&#26500;&#65292;&#21253;&#25324;&#25935;&#24863;&#20449;&#24687;&#21644;&#20551;&#29992;&#25143;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32479;&#35745;&#19979;&#38480;&#65292;&#35748;&#20026;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#35774;&#35745;&#20855;&#26377;&#24378;&#23433;&#20840;&#20445;&#35777;&#30340;&#39640;&#31934;&#24230;LAIMs&#30340;&#21487;&#33021;&#24615;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today's machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12336</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#23433;&#20840;&#20445;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#33258;&#20027;&#31995;&#32479;&#25552;&#20379;&#27491;&#24335;&#30340;&#23433;&#20840;&#21644;&#24615;&#33021;&#20445;&#35777;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#21704;&#23494;&#39039;-&#38597;&#31185;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20998;&#26512;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#25552;&#20379;&#36825;&#20123;&#20445;&#35777;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#12289;&#26377;&#30028;&#23545;&#25239;&#31995;&#32479;&#24178;&#25200;&#20197;&#21450;&#29366;&#24577;&#21644;&#36755;&#20837;&#32422;&#26463;&#12290;&#20294;&#26159;&#65292;&#23427;&#28041;&#21450;&#21040;&#27714;&#35299;PDE&#65292;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#38543;&#30528;&#29366;&#24577;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#31995;&#32479;&#19978;&#30340;&#30452;&#25509;&#20351;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;DeepReach&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27491;&#24358;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#26469;&#20811;&#26381;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#35201;&#27714;&#38543;&#21487;&#36798;&#31649;&#22797;&#26434;&#24615;&#32780;&#19981;&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#32780;&#21464;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#27492;&#35745;&#31639;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#23433;&#20840;&#65292;&#36825;&#27809;&#26377;&#36798;&#21040;&#25105;&#20204;&#25552;&#20379;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#30340;&#24635;&#20307;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#24863;&#30693;&#30340;&#39640;&#32500;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
&lt;/p&gt;</description></item><item><title>EDeNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26356;&#25509;&#36817;&#21407;&#22987;&#20107;&#20214;&#25968;&#25454;&#27969;&#65292;&#36991;&#20813;&#20102;&#31215;&#32047;&#20107;&#20214;&#21040;&#22270;&#20687;&#24103;&#30340;&#36807;&#31243;&#65292;&#23637;&#29616;&#20102;&#22312;&#35282;&#36895;&#24230;&#22238;&#24402;&#21644;&#31454;&#20105;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04362</link><description>&lt;p&gt;
EDeNN: &#20107;&#20214;&#34928;&#20943;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20302;&#24310;&#36831;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
EDeNN: Event Decay Neural Networks for low latency vision. (arXiv:2209.04362v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04362
&lt;/p&gt;
&lt;p&gt;
EDeNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26356;&#25509;&#36817;&#21407;&#22987;&#20107;&#20214;&#25968;&#25454;&#27969;&#65292;&#36991;&#20813;&#20102;&#31215;&#32047;&#20107;&#20214;&#21040;&#22270;&#20687;&#24103;&#30340;&#36807;&#31243;&#65292;&#23637;&#29616;&#20102;&#22312;&#35282;&#36895;&#24230;&#22238;&#24402;&#21644;&#31454;&#20105;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25968;&#23383;&#8220;&#31070;&#32463;&#20803;&#8221;&#26159;&#23545;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#19968;&#31181;&#38750;&#24120;&#19981;&#31934;&#30830;&#30340;&#36817;&#20284;&#12290;&#24403;&#21069;&#30340;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#22312;&#25968;&#23383;&#35774;&#22791;&#19978;&#36827;&#34892;&#65292;&#20855;&#26377;&#20687;&#22270;&#20687;&#24103;&#36825;&#26679;&#30340;&#25968;&#23383;&#25968;&#25454;&#34920;&#31034;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#36890;&#24120;&#27604;&#26368;&#20808;&#36827;&#30340;&#25968;&#23383;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#26356;&#20855;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#20107;&#20214;&#30456;&#26426;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#65292;&#23427;&#27169;&#20223;&#29983;&#29289;&#35270;&#35273;&#65292;&#20351;&#29992;&#24322;&#27493;&#35302;&#21457;&#30340;&#20687;&#32032;&#65292;&#25918;&#24323;&#20102;&#22270;&#20687;&#24103;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#65292;&#35768;&#22810;&#22522;&#20110;&#20107;&#20214;&#30340;&#31639;&#27861;&#19981;&#24471;&#19981;&#23558;&#20107;&#20214;&#31215;&#32047;&#22238;&#22270;&#20687;&#24103;&#65292;&#20174;&#32780;&#28010;&#36153;&#20102;&#20107;&#20214;&#30456;&#26426;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#37319;&#29992;&#30456;&#21453;&#30340;&#29702;&#24565;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#26356;&#25509;&#36817;&#21407;&#22987;&#20107;&#20214;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#22312;&#35282;&#36895;&#24230;&#22238;&#24402;&#21644;&#31454;&#20105;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#31215;&#32047;&#20107;&#20214;&#21040;&#22270;&#20687;&#24103;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of neural networks in computer vision tasks, digital 'neurons' are a very loose approximation of biological neurons. Today's learning approaches are designed to function on digital devices with digital data representations such as image frames. In contrast, biological vision systems are generally much more capable and efficient than state-of-the-art digital computer vision algorithms. Event cameras are an emerging sensor technology which imitates biological vision with asynchronously firing pixels, eschewing the concept of the image frame. To leverage modern learning techniques, many event-based algorithms are forced to accumulate events back to image frames, somewhat squandering the advantages of event cameras.  We follow the opposite paradigm and develop a new type of neural network which operates closer to the original event data stream. We demonstrate state-of-the-art performance in angular velocity regression and competitive optical flow estimation, while avoid
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23454;&#26102;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;EdgeSRGAN&#65292;&#20351;&#29992;&#27169;&#22411;&#37327;&#21270;&#25552;&#39640;&#20102;CPU&#21644;Edge TPU&#35774;&#22791;&#19978;&#30340;&#25191;&#34892;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#36827;&#19968;&#27493;&#20248;&#21270;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#36739;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.03355</link><description>&lt;p&gt;
&#24102;&#26377;&#30693;&#35782;&#33976;&#39311;&#30340;&#36793;&#32536;&#29983;&#25104;&#23545;&#25239;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation. (arXiv:2209.03355v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23454;&#26102;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;EdgeSRGAN&#65292;&#20351;&#29992;&#27169;&#22411;&#37327;&#21270;&#25552;&#39640;&#20102;CPU&#21644;Edge TPU&#35774;&#22791;&#19978;&#30340;&#25191;&#34892;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#36827;&#19968;&#27493;&#20248;&#21270;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#36739;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#24133;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21487;&#25903;&#25345;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#38656;&#35201;&#21487;&#38752;&#30340;&#35270;&#35273;&#27969;&#20197;&#30417;&#35270;&#20219;&#21153;&#12289;&#22788;&#29702;&#36828;&#31243;&#25805;&#32437;&#25110;&#30740;&#31350;&#30456;&#20851;&#35270;&#35273;&#32454;&#33410;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#12289;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23454;&#26102;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026;EdgeSRGAN&#65288;&#20195;&#30721;&#21487;&#22312;https://github.com/PIC4SeR/EdgeSRGAN&#33719;&#24471;&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21407;&#22987;SRGAN&#30340;&#23450;&#21046;&#21270;&#26550;&#26500;&#21644;&#27169;&#22411;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;CPU&#21644;Edge TPU&#35774;&#22791;&#19978;&#30340;&#25191;&#34892;&#25928;&#29575;&#65292;&#23454;&#29616;&#26368;&#39640;&#36798;200fps&#30340;&#25512;&#29702;&#35745;&#31639;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#21040;&#32593;&#32476;&#30340;&#36739;&#23567;&#29256;&#26412;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#30456;&#27604;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26356;&#37325;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#27169;&#22411;&#20445;&#30041;&#20102;&#30456;&#24403;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#24102;&#26377;&#24102;&#23485;&#38477;&#32423;&#30340;&#22270;&#20687;&#20256;&#36755;&#23454;&#39564;&#65292;&#20197;&#31361;&#26174;&#35813;&#31995;&#32479;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real-time Super-Resolution, called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#19982;&#20154;&#31867;&#35268;&#33539;&#36827;&#34892;&#23545;&#27604;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;LLMs&#22312;&#26576;&#20123;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#20013;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#23384;&#22312;&#24369;&#28857;&#65292;&#20363;&#22914;&#22312;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2208.11981</link><description>&lt;p&gt;
&#35770;&#29616;&#23454;&#21644;&#35821;&#35328;&#25968;&#25454;&#38480;&#21046;&#65306;&#23558;LLMs&#19982;&#20154;&#31867;&#35268;&#33539;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#19982;&#20154;&#31867;&#35268;&#33539;&#36827;&#34892;&#23545;&#27604;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;LLMs&#22312;&#26576;&#20123;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#20013;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#23384;&#22312;&#24369;&#28857;&#65292;&#20363;&#22914;&#22312;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21033;&#29992;&#22823;&#37327;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#20851;&#32852;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#26469;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#20173;&#26377;&#30097;&#38382;&#12290;&#22312;&#22238;&#39038;&#29616;&#26377;&#21327;&#35758;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#35268;&#33539;&#19982;GPT-3&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36890;&#24120;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#31867;&#21035;&#20197;&#21450;&#24369;&#28857;&#25152;&#22312;&#12290;GPT-3&#20026;&#21253;&#25324;&#21516;&#20041;&#35789;&#12289;&#21453;&#20041;&#35789;&#21644;&#40664;&#35748;&#32487;&#25215;&#22312;&#20869;&#30340;&#20960;&#20010;&#20851;&#31995;&#26041;&#38754;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#24403;&#30340;&#21475;&#22836;&#25512;&#29702;&#35777;&#25454;&#12290;&#27809;&#26377;&#26469;&#33258;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;GPT-3&#22312;&#20855;&#26377;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#30340;&#21306;&#38388;&#19979;&#38480;&#22788;&#12290;&#22312;&#24517;&#35201;&#21697;&#36136;&#12289;&#22823;&#23567;&#39034;&#24207;&#21644;&#24378;&#24230;&#39034;&#24207;&#31561;&#26041;&#38754;&#20063;&#35266;&#23519;&#21040;&#20102;&#19981;&#36275;&#20043;&#22788;&#12290;&#25226;LLMs&#19982;&#35937;&#24449;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#65292;&#20197;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.06652</link><description>&lt;p&gt;
&#39640;&#32500;&#31354;&#38388;&#20013;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Differentiable Inductive Logic Programming in High-Dimensional Space. (arXiv:2208.06652v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#65292;&#20197;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#21512;&#25104;&#22823;&#22411;&#36923;&#36753;&#31243;&#24207;&#36890;&#24120;&#38656;&#35201;&#20013;&#38388;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20869;&#28085;&#35859;&#35789;&#26434;&#20081;&#22320;&#21344;&#25454;&#20551;&#35774;&#31354;&#38388;&#36890;&#24120;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#26799;&#24230;&#19979;&#38477;&#25552;&#20379;&#20102;&#22312;&#36825;&#20123;&#39640;&#32500;&#31354;&#38388;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#31070;&#32463;&#31526;&#21495;ILP&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#25193;&#23637;{\delta}ILP&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#30340;&#24402;&#32435;&#21512;&#25104;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21463;&#30410;&#20110;&#21487;&#24494;&#24402;&#32435;&#21512;&#25104;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19981;&#25351;&#23450;&#35299;&#20915;&#26041;&#26696;&#30340;&#31934;&#30830;&#32467;&#26500;&#30340;&#35821;&#35328;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing large logic programs through symbolic Inductive Logic Programming (ILP) typically requires intermediate definitions. However, cluttering the hypothesis space with intensional predicates typically degrades performance. In contrast, gradient descent provides an efficient way to find solutions within such high- dimensional spaces. Neuro-symbolic ILP approaches have not fully exploited this so far. We propose extending the {\delta}ILP approach to inductive synthesis with large-scale predicate invention, thus allowing us to exploit the efficacy of high-dimensional gradient descent. We show that large-scale predicate invention benefits differentiable inductive synthesis through gradient descent and allows one to learn solutions for tasks beyond the capabilities of existing neuro-symbolic ILP systems. Furthermore, we achieve these results without specifying the precise structure of the solution within the language bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.03075</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#21307;&#23398;&#39046;&#22495;&#26159;&#37319;&#29992;FL&#30340;&#26368;&#36866;&#21512;&#39046;&#22495;&#20043;&#19968;&#65292;&#22240;&#20026;&#24517;&#39035;&#23562;&#37325;&#24739;&#32773;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25552;&#20379;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#24212;&#29992;FL&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#21363;&#38271;&#26399;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#30382;&#32932;&#30284;&#22270;&#20687;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#25552;&#20986;&#32463;&#39564;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#12290;&#28508;&#22312;&#30340;FL&#29992;&#25143;&#65292;&#22914;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#22522;&#20934;&#20316;&#20026;&#37319;&#29992;FL&#30340;&#25351;&#21335;&#65292;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#35797;&#38169;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#20197;&#20445;&#30041;&#29616;&#23454;&#19990;&#30028;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#38024;&#23545;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;FL&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#20004;&#31181;&#20856;&#22411;FL&#31639;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;&#22522;&#20110;&#19977;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#25552;&#20379;&#20102;&#22312;&#23433;&#20840;&#39640;&#25928;&#30340;&#26041;&#24335;&#19979;&#65292;&#24212;&#29992;FL&#20174;&#32780;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#21487;&#21487;&#31181;&#26893;&#25968;&#25454;&#21644;&#21355;&#26143;&#22270;&#20687;&#65292;&#20026;&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#21019;&#24314;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#21487;&#21487;&#31181;&#26893;&#21306;&#22320;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#21487;&#31181;&#26893;&#26159;&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#20445;&#25252;&#21306;&#26862;&#26519;&#25439;&#22833;&#30340;&#22522;&#26412;&#39537;&#21160;&#22240;&#32032;&#65292;&#32780;&#23448;&#26041;&#25253;&#21578;&#30340;&#21487;&#21487;&#31181;&#26893;&#38754;&#31215;&#26126;&#26174;&#20302;&#20272;&#12290;</title><link>http://arxiv.org/abs/2206.06119</link><description>&lt;p&gt;
&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#21487;&#21487;&#31181;&#26893;&#21306;&#30340;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Satellite-based high-resolution maps of cocoa planted area for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#21487;&#21487;&#31181;&#26893;&#25968;&#25454;&#21644;&#21355;&#26143;&#22270;&#20687;&#65292;&#20026;&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#21019;&#24314;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#21487;&#21487;&#31181;&#26893;&#21306;&#22320;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#21487;&#31181;&#26893;&#26159;&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#20445;&#25252;&#21306;&#26862;&#26519;&#25439;&#22833;&#30340;&#22522;&#26412;&#39537;&#21160;&#22240;&#32032;&#65292;&#32780;&#23448;&#26041;&#25253;&#21578;&#30340;&#21487;&#21487;&#31181;&#26893;&#38754;&#31215;&#26126;&#26174;&#20302;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#26159;&#20840;&#29699;&#26368;&#22823;&#30340;&#21487;&#21487;&#29983;&#20135;&#22269;&#65292;&#20004;&#22269;&#21344;&#20840;&#29699;&#21487;&#21487;&#20135;&#37327;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#22312;&#20004;&#20010;&#22269;&#23478;&#65292;&#21487;&#21487;&#26159;&#20027;&#35201;&#30340;&#22810;&#24180;&#29983;&#20316;&#29289;&#65292;&#20026;&#36817;&#20004;&#30334;&#19975;&#20892;&#27665;&#25552;&#20379;&#25910;&#20837;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#30340;&#21487;&#21487;&#31181;&#26893;&#21306;&#22320;&#22270;&#32570;&#22833;&#65292;&#38459;&#30861;&#20102;&#23545;&#20445;&#25252;&#21306;&#25193;&#24352;&#12289;&#20135;&#37327;&#21644;&#25910;&#30410;&#22686;&#38271;&#30340;&#20934;&#30830;&#37327;&#21270;&#65292;&#38480;&#21046;&#20102;&#21487;&#25345;&#32493;&#24615;&#27835;&#29702;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#32467;&#21512;&#21487;&#21487;&#31181;&#26893;&#25968;&#25454;&#21644;&#20844;&#24320;&#30340;&#21355;&#26143;&#22270;&#20687;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#20004;&#22269;&#21019;&#24314;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#21487;&#21487;&#31181;&#26893;&#21306;&#22320;&#22270;&#65292;&#24182;&#22312;&#29616;&#22330;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#21487;&#31181;&#26893;&#26159;&#31185;&#29305;&#36842;&#29926;&#21644;&#21152;&#32435;&#20445;&#25252;&#21306;&#26862;&#26519;&#25439;&#22833;&#30340;&#22522;&#26412;&#39537;&#21160;&#22240;&#32032;&#65292;&#20998;&#21035;&#21344;37&#65285;&#21644;13&#65285;&#65292;&#32780;&#23448;&#26041;&#25253;&#21578;&#30340;&#21487;&#21487;&#31181;&#26893;&#38754;&#31215;&#26126;&#26174;&#20302;&#20272;&#65292;&#22312;&#21152;&#32435;&#20302;&#20272;&#39640;&#36798;40&#65285;&#12290;&#36825;&#20123;&#22320;&#22270;&#26159;&#25512;&#36827;&#20445;&#25252;&#21644;&#32463;&#27982;&#29702;&#35299;&#30340;&#20851;&#38190;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
C\^ote d'Ivoire and Ghana, the world's largest producers of cocoa, account for two thirds of the global cocoa production. In both countries, cocoa is the primary perennial crop, providing income to almost two million farmers. Yet precise maps of cocoa planted area are missing, hindering accurate quantification of expansion in protected areas, production and yields, and limiting information available for improved sustainability governance. Here, we combine cocoa plantation data with publicly available satellite imagery in a deep learning framework and create high-resolution maps of cocoa plantations for both countries, validated in situ. Our results suggest that cocoa cultivation is an underlying driver of over 37% and 13% of forest loss in protected areas in C\^ote d'Ivoire and Ghana, respectively, and that official reports substantially underestimate the planted area, up to 40% in Ghana. These maps serve as a crucial building block to advance understanding of conservation and economic
&lt;/p&gt;</description></item><item><title>HierarchyNet&#26159;&#19968;&#31181;&#20248;&#31168;&#30340;&#20195;&#30721;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#24322;&#26500;&#20195;&#30721;&#34920;&#31034;&#21644;&#23618;&#27425;&#24863;&#30693;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#33719;&#35789;&#27861;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#23618;&#38754;&#19978;&#30340;&#20195;&#30721;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2205.15479</link><description>&lt;p&gt;
HierarchyNet: &#21033;&#29992;&#24322;&#26500;&#34920;&#31034;&#23398;&#20064;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
HierarchyNet: Learning to Summarize Source Code with Heterogeneous Representations. (arXiv:2205.15479v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15479
&lt;/p&gt;
&lt;p&gt;
HierarchyNet&#26159;&#19968;&#31181;&#20248;&#31168;&#30340;&#20195;&#30721;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#24322;&#26500;&#20195;&#30721;&#34920;&#31034;&#21644;&#23618;&#27425;&#24863;&#30693;&#20132;&#21449;&#27880;&#24847;&#23618;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#33719;&#35789;&#27861;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#23618;&#38754;&#19978;&#30340;&#20195;&#30721;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#20195;&#30721;&#34920;&#31034;(HCRs)&#21644;&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#30340;HierarchyNet&#26469;&#36827;&#34892;&#20195;&#30721;&#25688;&#35201;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#31895;&#31890;&#24230;&#30340;&#20195;&#30721;&#20803;&#32032;&#36827;&#34892;&#25277;&#35937;&#65292;&#24182;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#32435;&#20837;&#32454;&#31890;&#24230;&#30340;&#31243;&#24207;&#20803;&#32032;&#65292;HCRs&#26377;&#25928;&#22320;&#25429;&#33719;&#20102;&#35789;&#27861;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#23618;&#38754;&#19978;&#30340;&#20195;&#30721;&#35201;&#32032;&#12290;&#25105;&#20204;&#30340;HierarchyNet&#26041;&#27861;&#20998;&#21035;&#36890;&#36807;&#24322;&#26500;&#22270;&#21464;&#25442;&#22120;&#12289;&#22522;&#20110;&#26641;&#30340;CNN&#21644;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#29420;&#29305;&#22320;&#22788;&#29702;HCR&#30340;&#27599;&#19968;&#23618;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#26032;&#39062;&#30340;&#23618;&#27425;&#24863;&#30693;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20445;&#30041;&#20102;&#20195;&#30721;&#20803;&#32032;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#25429;&#25417;&#20182;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PA-Former&#12289;CAST&#21644;NeuralCodeSum&#31561;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method for code summarization utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs effectively capture essential code features at lexical, syntactic, and semantic levels by abstracting coarse-grained code elements and incorporating fine-grained program elements in a hierarchical structure. Our HierarchyNet method processes each layer of the HCR separately through a unique combination of the Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. This approach preserves dependencies between code elements and captures relations through a novel Hierarchical-Aware Cross Attention layer. Our method surpasses current state-of-the-art techniques, such as PA-Former, CAST, and NeuralCodeSum.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FL-FDMS&#65292;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#36864;&#24441;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26102;&#25214;&#21040;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2205.13222</link><description>&lt;p&gt;
&#36890;&#36807;&#21451;&#22909;&#27169;&#22411;&#26367;&#25442;&#35299;&#20915;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36864;&#24441;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Client Dropout in Federated Learning via Friend Model Substitution. (arXiv:2205.13222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FL-FDMS&#65292;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#36864;&#24441;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26102;&#25214;&#21040;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20854;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20248;&#21183;&#32780;&#38395;&#21517;&#12290;&#30001;&#20110;&#35768;&#22810;&#24773;&#20917;&#19979;&#23436;&#20840;&#23458;&#25143;&#31471;&#21442;&#19982;&#19981;&#21487;&#34892;&#65292;&#30740;&#31350;&#20102;&#20027;&#21160;&#36873;&#25321;/&#37319;&#26679;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#30340;&#37096;&#20998;&#21442;&#19982;FL&#31639;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25509;&#36817;&#20840;&#21442;&#19982;&#24773;&#20917;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#34987;&#21160;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#24773;&#20917;&#65292;&#36825;&#31181;&#24773;&#20917;&#29702;&#35299;&#19981;&#36275;&#24471;&#22810;&#65292;&#37096;&#20998;&#21442;&#19982;&#26159;&#30001;&#20110;&#22806;&#37096;&#20107;&#20214;&#65288;&#21363;&#23458;&#25143;&#31471;&#36864;&#20986;&#65289;&#32780;&#19981;&#26159;FL&#31639;&#27861;&#30340;&#20915;&#23450;&#32780;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#23458;&#25143;&#31471;&#36864;&#24441;&#30340;FL&#35270;&#20026;FL&#38382;&#39064;&#20013;&#30340;&#19968;&#31867;&#29305;&#27530;&#24773;&#20917;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#20197;&#25552;&#20132;&#26367;&#20195;&#65288;&#21487;&#33021;&#19981;&#20934;&#30830;&#65289;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FL-FDMS&#65292;&#21487;&#20197;&#21363;&#26102;&#21457;&#29616;&#23458;&#25143;&#31471;&#30340;&#26379;&#21451;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#65289;&#65292;&#24182;&#20351;&#29992;&#26379;&#21451;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#26469;&#26367;&#20195;&#25918;&#24323;&#30340;&#23458;&#25143;&#31471;&#30340;&#32570;&#22833;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#20551;&#35774;&#19979;&#35777;&#26126;FL-FDMS&#22312;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#26368;&#20248;&#20840;&#23616;&#27169;&#22411;&#65292;&#35813;&#20551;&#35774;&#36275;&#20197;&#28085;&#30422;&#35768;&#22810;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#20010;&#22522;&#32447;&#30456;&#27604;&#65292;&#24403;&#23458;&#25143;&#31471;&#36864;&#20986;&#29575;&#36866;&#20013;&#21040;&#39640;&#26102;&#65292;FL-FDMS&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;160&#22810;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#32508;&#36848;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#20363;&#22914;&#38656;&#35201;&#36229;&#36234;&#32479;&#35745;&#24179;&#34913;&#30340;&#26032;&#20844;&#24179;&#24615;&#34913;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.11127</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#30740;&#31350;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Fairness in Recommender Systems: Research Landscape and Future Directions. (arXiv:2205.11127v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;160&#22810;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#32508;&#36848;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#20363;&#22914;&#38656;&#35201;&#36229;&#36234;&#32479;&#35745;&#24179;&#34913;&#30340;&#26032;&#20844;&#24179;&#24615;&#34913;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#25105;&#20204;&#22312;&#32447;&#19978;&#30475;&#21040;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20174;&#32780;&#24433;&#21709;&#25105;&#20204;&#30340;&#20449;&#20208;&#12289;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21019;&#36896;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#37492;&#20110;&#36825;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#23545;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#28508;&#22312;&#24433;&#21709;&#36234;&#26469;&#36234;&#22823;&#65292;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#33616;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#20173;&#28982;&#26159;&#19968;&#20010;&#27491;&#22312;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#22312;&#35813;&#39046;&#22495;&#25552;&#20986;&#30340;&#20844;&#24179;&#24615;&#22522;&#26412;&#27010;&#24565;&#21644;&#35266;&#24565;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;160&#22810;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#32508;&#36848;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#20363;&#22914;&#19968;&#33324;&#30740;&#31350;&#26041;&#27861;&#12289;&#20844;&#24179;&#24615;&#25514;&#26045;&#21644;&#31639;&#27861;&#26041;&#27861;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#26368;&#36817;&#30740;&#31350;&#30340;&#20998;&#26512;&#25351;&#20986;&#20102;&#26576;&#20123;&#30740;&#31350;&#31354;&#30333;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#65292;&#23545;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#25552;&#21319;&#23646;&#24615;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#35843;&#26597;&#36824;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#20363;&#22914;&#38656;&#35201;&#36229;&#36234;&#32479;&#35745;&#24179;&#34913;&#30340;&#26032;&#20844;&#24179;&#24615;&#34913;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems can strongly influence which information we see online, e.g., on social media, and thus impact our beliefs, decisions, and actions. At the same time, these systems can create substantial business value for different stakeholders. Given the growing potential impact of such AI-based systems on individuals, organizations, and society, questions of fairness have gained increased attention in recent years. However, research on fairness in recommender systems is still a developing area. In this survey, we first review the fundamental concepts and notions of fairness that were put forward in the area in the recent past. Afterward, through a review of more than 160 scholarly publications, we present an overview of how research in this field is currently operationalized, e.g., in terms of general research methodology, fairness measures, and algorithmic approaches. Overall, our analysis of recent works points to certain research gaps. In particular, we find that in many resea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#20316;&#19994;&#19982;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#65292;&#21487;&#24212;&#29992;&#20110;&#35777;&#26126;&#12289;&#32534;&#31243;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2204.04196</link><description>&lt;p&gt;
&#35777;&#26126;&#22359;&#38382;&#39064;&#20013;&#30340;&#39640;&#25928;&#21453;&#39304;&#21644;&#37096;&#20998;&#20998;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Efficient Feedback and Partial Credit Grading for Proof Blocks Problems. (arXiv:2204.04196v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#20316;&#19994;&#19982;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#65292;&#21487;&#24212;&#29992;&#20110;&#35777;&#26126;&#12289;&#32534;&#31243;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22359;&#26159;&#19968;&#31181;&#36719;&#20214;&#24037;&#20855;&#65292;&#20801;&#35768;&#23398;&#29983;&#36890;&#36807;&#25302;&#25918;&#32780;&#38750;&#20174;&#22836;&#20889;&#35777;&#26126;&#65292;&#32451;&#20064;&#25968;&#23398;&#35777;&#26126;&#30340;&#20889;&#20316;&#12290;&#35777;&#26126;&#22359;&#25552;&#20379;&#20102;&#20998;&#37096;&#20998;&#20998;&#21644;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#36890;&#36807;&#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#30340;&#20316;&#19994;&#19982;&#19968;&#20123;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#26469;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#36753;&#36317;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#25490;&#21015;&#25972;&#20010;&#25628;&#32034;&#31354;&#38388;&#30340;&#22522;&#32447;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#23558;&#38382;&#39064;&#30340;&#32553;&#20943;&#20026;&#26368;&#23567;&#39030;&#28857;&#35206;&#30422;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35838;&#31243;&#30340;&#25968;&#21315;&#20221;&#23398;&#29983;&#25552;&#20132;&#20013;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#26126;&#22522;&#32447;&#31639;&#27861;&#38590;&#20197;&#35745;&#31639;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23545;&#20110;&#23454;&#29616;&#35838;&#22530;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#26032;&#30340;&#31639;&#27861;&#20063;&#24050;&#32463;&#34987;&#29992;&#20110;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#21487;&#20197;&#24314;&#27169;&#20026;DAG&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#32534;&#31243;&#20316;&#19994;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proof Blocks is a software tool that allows students to practice writing mathematical proofs by dragging and dropping lines instead of writing proofs from scratch. Proof Blocks offers the capability of assigning partial credit and providing solution quality feedback to students. This is done by computing the edit distance from a student's submission to some predefined set of solutions. In this work, we propose an algorithm for the edit distance problem that significantly outperforms the baseline procedure of exhaustively enumerating over the entire search space. Our algorithm relies on a reduction to the minimum vertex cover problem. We benchmark our algorithm on thousands of student submissions from multiple courses, showing that the baseline algorithm is intractable, and that our proposed algorithm is critical to enable classroom deployment. Our new algorithm has also been used for problems in many other domains where the solution space can be modeled as a DAG, including but not limi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wi-Fi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#22788;&#29702;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.22%&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GUI&#24212;&#29992;&#31243;&#24207;&#26041;&#20415;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.08146</link><description>&lt;p&gt;
&#22522;&#20110;Wi-Fi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;&#35782;&#21035;&#30340;&#21069;&#30651;&#24615;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#20855;&#26377;GUI&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(arXiv:2202.08146v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wi-Fi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#22788;&#29702;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.22%&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GUI&#24212;&#29992;&#31243;&#24207;&#26041;&#20415;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#12289;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12289;&#26234;&#33021;&#22478;&#24066;&#21644;&#31038;&#20250;&#32463;&#27982;&#21464;&#38761;&#30340;&#38656;&#35201;&#65292;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;(HAR)&#30740;&#31350;&#24050;&#32463;&#33719;&#24471;&#20102;&#37325;&#35201;&#30340;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20256;&#24863;&#22120;&#30340;HAR&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12289;&#23384;&#20648;&#21644;&#21151;&#29575;&#28040;&#32791;&#38382;&#39064;&#20197;&#21450;&#20329;&#25140;&#20256;&#24863;&#22120;&#30340;&#19981;&#36866;&#24863;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;HAR&#30740;&#31350;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#22522;&#20110;WiFi&#30340;HAR&#22240;&#20854;&#26356;&#31895;&#31890;&#24230;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#21487;&#29992;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;WiFi&#30340;HAR&#26041;&#27861;&#20165;&#38480;&#20110;&#23545;&#22312;&#30456;&#31561;&#26102;&#38388;&#20869;&#25191;&#34892;&#30340;&#29420;&#31435;&#21644;&#38750;&#24182;&#21457;&#20154;&#31867;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#12290;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#36890;&#20449;&#38142;&#36335;&#65292;&#20854;&#20013;&#21457;&#23556;&#22120;&#26159;WiFi&#36335;&#30001;&#22120;&#65292;&#25509;&#25910;&#22120;&#26159;&#37197;&#22791;&#20102;Intel 5300 NIC&#30340;&#26234;&#33021;&#25163;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(ABiGRNN)&#30340;WiFi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;(HHI)&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#22788;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;HHI&#27963;&#21160;&#25968;&#25454;&#38598;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;98.22%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#36731;&#26494;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;HHI&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) research has gained significant momentum due to recent technological advancements, artificial intelligence algorithms, the need for smart cities, and socioeconomic transformation. However, existing computer vision and sensor-based HAR solutions have limitations such as privacy issues, memory and power consumption, and discomfort in wearing sensors for which researchers are observing a paradigm shift in HAR research. In response, WiFi-based HAR is gaining popularity due to the availability of more coarse-grained Channel State Information. However, existing WiFi-based HAR approaches are limited to classifying independent and non-concurrent human activities performed within equal time duration. Recent research commonly utilizes a Single Input Multiple Output communication link with a WiFi signal of 5 GHz channel frequency, using two WiFi routers or two Intel 5300 NICs as transmitter-receiver. Our study, on the other hand, utilizes a Multiple Input Multiple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20445;&#35777;&#32852;&#37030;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615; (PF) &#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26131;&#20110;&#23454;&#29616;&#30340;&#31639;&#27861; PropFair&#65292;&#33021;&#22815;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#24046; 10% &#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2202.01666</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Proportional Fairness in Federated Learning. (arXiv:2202.01666v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20445;&#35777;&#32852;&#37030;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615; (PF) &#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26131;&#20110;&#23454;&#29616;&#30340;&#31639;&#27861; PropFair&#65292;&#33021;&#22815;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#24046; 10% &#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#20445;&#35777;&#20844;&#24179;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21363;&#38656;&#35201;&#20026;&#20247;&#22810;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#25552;&#20379;&#21512;&#29702;&#28385;&#24847;&#30340;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21363;&#27604;&#20363;&#20844;&#24179;&#24615; (PF)&#65292;&#23427;&#22522;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#24615;&#33021;&#30340;&#30456;&#23545;&#21464;&#21270;&#12290;&#36890;&#36807;&#19982;&#20132;&#26131;&#21338;&#24328;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; PropFair&#65292;&#19968;&#31181;&#26032;&#39062;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23547;&#25214;&#27604;&#20363;&#20844;&#24179;&#35299;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126; PropFair &#33021;&#22815;&#22823;&#33268;&#25214;&#21040; PF &#35299;&#65292;&#24182;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#24046; 10% &#23458;&#25143;&#31471;&#30340;&#24179;&#22343;&#24615;&#33021;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients. Our code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#22768;&#36895;&#24230;&#37325;&#26500;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#26512;&#22270;&#20687;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22521;&#35757;&#25968;&#25454;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#21644;&#20581;&#22766;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#39640;&#20998;&#36776;&#29575;&#22768;&#36895;&#22320;&#22270;&#37325;&#26500;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#31616;&#21270;&#20960;&#20309;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.01208</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#22768;&#36895;&#24230;&#37325;&#26500;&#65306;&#22521;&#35757;&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#31283;&#23450;&#24615;&#21644;&#20581;&#22766;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness. (arXiv:2202.01208v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#22768;&#36895;&#24230;&#37325;&#26500;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#26512;&#22270;&#20687;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22521;&#35757;&#25968;&#25454;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#21644;&#20581;&#22766;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#39640;&#20998;&#36776;&#29575;&#22768;&#36895;&#22320;&#22270;&#37325;&#26500;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#31616;&#21270;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;b&#27169;&#24335;&#25104;&#20687;&#26159;&#19968;&#31181;&#23450;&#24615;&#26041;&#27861;&#65292;&#35786;&#26029;&#36136;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#25805;&#20316;&#21592;&#30340;&#22521;&#35757;&#21644;&#32463;&#39564;&#12290;&#23450;&#37327;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#32452;&#32455;&#29305;&#24615;&#30340;&#20449;&#24687;&#65307;&#22240;&#27492;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#32452;&#32455;&#20013;&#30340;&#22768;&#36895;&#21487;&#20197;&#29992;&#20316;&#32452;&#32455;&#24694;&#24615;&#24230;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#29305;&#21035;&#26159;&#22312;&#20083;&#33146;&#25104;&#20687;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#23436;&#20840;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23454;&#29616;&#22768;&#36895;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#26029;&#39046;&#22495;&#36716;&#31227;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#30495;&#23454;&#35774;&#32622;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#20110;&#22521;&#35757;&#25968;&#25454;&#29983;&#25104;&#65292;&#32452;&#32455;&#32467;&#26500;&#34987;&#24314;&#27169;&#20026;&#31616;&#21270;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#19981;&#33021;&#21453;&#26144;&#30495;&#23454;&#32452;&#32455;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#26512;&#22270;&#20687;&#30340;&#26032;&#22411;&#27169;&#25311;&#35774;&#32622;&#29992;&#20110;&#29983;&#25104;&#22521;&#35757;&#25968;&#25454;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#20998;&#21106;&#65292;&#20197;&#25972;&#21512;&#39069;&#22806;&#30340;&#32452;&#32455;&#20449;&#24687;&#24182;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24187;&#24433;&#21644;&#20307;&#20869;&#27979;&#37327;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#26679;&#21270;&#32452;&#32455;&#32467;&#26500;&#30340;&#35757;&#32451;&#23548;&#33268;&#26356;&#31283;&#23450;&#21644;&#20581;&#22766;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#23618;&#26512;&#22270;&#20687;&#30340;&#26041;&#27861;&#22312;&#37325;&#24314;&#39640;&#20998;&#36776;&#29575;&#22768;&#36895;&#22320;&#22270;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#31616;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound b-mode imaging is a qualitative approach and diagnostic quality strongly depends on operators' training and experience. Quantitative approaches can provide information about tissue properties; therefore, can be used for identifying various tissue types, e.g., speed-of-sound in the tissue can be used as a biomarker for tissue malignancy, especially in breast imaging. Recent studies showed the possibility of speed-of-sound reconstruction using deep neural networks that are fully trained on simulated data. However, because of the ever-present domain shift between simulated and measured data, the stability and performance of these models in real setups are still under debate. In prior works, for training data generation, tissue structures were modeled as simplified geometrical structures which does not reflect the complexity of the real tissues. In this study, we proposed a new simulation setup for training data generation based on Tomosynthesis images. We combined our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BungeeNeRF&#30340;&#28176;&#36827;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#21487;&#20197;&#22312;&#26497;&#31471;&#19981;&#21516;&#30340;&#23610;&#24230;&#19978;&#23454;&#29616;&#36880;&#23618;&#28210;&#26579;&#65292;&#32463;&#36807;&#22810;&#20010;&#38454;&#27573;&#36880;&#27493;&#32454;&#21270;&#31895;&#30053;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#35268;&#27169;&#65292;&#24182;&#22312;&#25152;&#26377;&#35268;&#27169;&#19978;&#20135;&#29983;&#20855;&#26377;&#31934;&#32454;&#32454;&#33410;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22478;&#24066;&#26223;&#35266;&#12289;&#26223;&#35266;&#21644;Minecraft&#27169;&#22411;&#31561;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2112.05504</link><description>&lt;p&gt;
BungeeNeRF&#65306;&#26497;&#31471;&#22810;&#23610;&#24230;&#22330;&#26223;&#28210;&#26579;&#30340;&#28176;&#36827;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BungeeNeRF&#30340;&#28176;&#36827;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#21487;&#20197;&#22312;&#26497;&#31471;&#19981;&#21516;&#30340;&#23610;&#24230;&#19978;&#23454;&#29616;&#36880;&#23618;&#28210;&#26579;&#65292;&#32463;&#36807;&#22810;&#20010;&#38454;&#27573;&#36880;&#27493;&#32454;&#21270;&#31895;&#30053;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#35268;&#27169;&#65292;&#24182;&#22312;&#25152;&#26377;&#35268;&#27169;&#19978;&#20135;&#29983;&#20855;&#26377;&#31934;&#32454;&#32454;&#33410;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22478;&#24066;&#26223;&#35266;&#12289;&#26223;&#35266;&#21644;Minecraft&#27169;&#22411;&#31561;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22312;&#24314;&#27169;&#19977;&#32500;&#29289;&#20307;&#21644;&#21463;&#25511;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#22312;&#21333;&#19968;&#23610;&#24230;&#19979;&#36816;&#34892;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#22810;&#23610;&#24230;&#24773;&#20917;&#65292;&#20854;&#20013;&#35266;&#23519;&#21040;&#22270;&#20687;&#22312;&#25130;&#28982;&#19981;&#21516;&#30340;&#23610;&#24230;&#19979;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#22914;&#22478;&#24066;&#22330;&#26223;&#65292;&#22312;&#21355;&#26143;&#32423;&#21035;&#19979;&#25429;&#25417;&#19968;&#20010;&#22478;&#24066;&#30340;&#27010;&#36848;&#65292;&#21040;&#22320;&#38754;&#32423;&#21035;&#19979;&#26174;&#31034;&#24314;&#31569;&#29289;&#30340;&#22797;&#26434;&#32454;&#33410;&#65307;&#20063;&#24120;&#35265;&#20110;&#26223;&#35266;&#21644;&#31934;&#32454;&#30340;Minecraft 3D&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#35270;&#35282;&#33539;&#22260;&#20135;&#29983;&#20102;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#22810;&#23610;&#24230;&#28210;&#26579;&#65292;&#36825;&#32473;&#31070;&#32463;&#36752;&#23556;&#22330;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#24182;&#20351;&#20854;&#20559;&#21521;&#22949;&#21327;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;BungeeNeRF&#65292;&#23427;&#26159;&#19968;&#31181;&#28176;&#36827;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#22312;&#26497;&#31471;&#19981;&#21516;&#30340;&#23610;&#24230;&#19978;&#23454;&#29616;&#36880;&#23618;&#28210;&#26579;&#12290;&#20174;&#29992;&#27973;&#23618;&#32593;&#32476;&#25311;&#21512;&#36828;&#36317;&#31163;&#35270;&#22270;&#24320;&#22987;&#65292;BungeeNeRF&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#36880;&#27493;&#32454;&#21270;&#31895;&#30053;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#30001;&#20808;&#21069;&#38454;&#27573;&#30340;&#21453;&#39304;&#21644;&#36873;&#25321;&#24615;&#35843;&#25972;&#19981;&#21516;&#23610;&#24230;&#30340;&#36129;&#29486;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#35268;&#27169;&#65292;&#24182;&#22312;&#25152;&#26377;&#35268;&#27169;&#19978;&#20135;&#29983;&#20855;&#26377;&#31934;&#32454;&#32454;&#33410;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22478;&#24066;&#26223;&#35266;&#12289;&#26223;&#35266;&#21644;Minecraft&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#20302;&#19988;&#39640;&#31934;&#24230;&#30340;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#35782;&#21035;COVID-19&#24739;&#32773;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2108.05660</link><description>&lt;p&gt;
&#21033;&#29992;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#24320;&#21457;&#26080;&#39118;&#38505;&#30340;COVID-19&#31579;&#26597;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning. (arXiv:2108.05660v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#20302;&#19988;&#39640;&#31934;&#24230;&#30340;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#35782;&#21035;COVID-19&#24739;&#32773;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#36716;&#24405;&#32858;&#21512;&#37238;&#38142;&#21453;&#24212;&#65288;RT-PCR&#65289;&#26159;&#36776;&#21035;COVID-19&#24863;&#26579;&#30340;&#38134;&#24377;&#35786;&#26029;&#26816;&#27979;&#12290;&#24555;&#36895;&#25239;&#21407;&#26816;&#27979;&#26159;&#19968;&#31181;&#31579;&#26597;&#26816;&#27979;&#65292;&#21487;&#22312;15&#20998;&#38047;&#20869;&#35782;&#21035;COVID-19&#38451;&#24615;&#24739;&#32773;&#65292;&#20294;&#20854;&#28789;&#25935;&#24230;&#20302;&#20110;PCR&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;COVID-19&#24739;&#32773;&#20813;&#30123;&#21644;&#34880;&#28082;&#23398;&#36164;&#26009;&#30340;&#21442;&#25968;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#20302;&#19988;&#39640;&#31934;&#24230;&#30340;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#35782;&#21035;COVID-19&#24739;&#32773;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Reverse Transcription Polymerase Chain Reaction (RTPCR)} test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected and either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological and hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall and
&lt;/p&gt;</description></item></channel></rss>