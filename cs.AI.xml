<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01649</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#23558;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#20960;&#20010;&#21512;&#25104;&#22270;&#20687;&#20013;&#12290;&#20854;&#24605;&#24819;&#26159;&#21512;&#25104;&#23569;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#28857;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32473;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#36924;&#36817;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#24182;&#25193;&#23637;&#21040;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#26469;&#21512;&#25104;&#33976;&#39311;&#30340;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.
&lt;/p&gt;</description></item><item><title>Perfusion&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#38145;&#23450;&#26032;&#27010;&#24565;&#19982;&#20854;&#19978;&#20301;&#31867;&#21035;&#30340;&#20132;&#21449;&#20851;&#27880;&#38190;&#21644;&#38376;&#25511;&#31209;-1&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#20445;&#25345;&#39640;&#24230;&#20445;&#30495;&#24230;&#12289;&#20801;&#35768;&#21019;&#24847;&#25511;&#21046;&#20197;&#21450;&#22810;&#20010;&#20010;&#24615;&#27010;&#24565;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01644</link><description>&lt;p&gt;
&#38145;&#23450;&#20851;&#38190;&#25490;&#24207;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Key-Locked Rank One Editing for Text-to-Image Personalization. (arXiv:2305.01644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01644
&lt;/p&gt;
&lt;p&gt;
Perfusion&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#38145;&#23450;&#26032;&#27010;&#24565;&#19982;&#20854;&#19978;&#20301;&#31867;&#21035;&#30340;&#20132;&#21449;&#20851;&#27880;&#38190;&#21644;&#38376;&#25511;&#31209;-1&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#20445;&#25345;&#39640;&#24230;&#20445;&#30495;&#24230;&#12289;&#20801;&#35768;&#21019;&#24847;&#25511;&#21046;&#20197;&#21450;&#22810;&#20010;&#20010;&#24615;&#27010;&#24565;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;T2I&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21019;&#20316;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#28789;&#27963;&#24615;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20010;&#24615;&#21270;&#20197;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#35270;&#35273;&#27010;&#24565;&#30456;&#19968;&#33268;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;T2I &#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#38754;&#20020;&#22810;&#20010;&#22256;&#38590;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#20801;&#35768;&#21019;&#24847;&#25511;&#21046;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24230;&#35270;&#35273;&#20445;&#30495;&#24230;&#65292;&#23558;&#22810;&#20010;&#20010;&#24615;&#21270;&#27010;&#24565;&#32452;&#21512;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#65292;&#20197;&#21450;&#20445;&#25345;&#23567;&#22411;&#27169;&#22411;&#23610;&#23544;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Perfusion&#8221;&#30340; T2I &#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#30784; T2I &#27169;&#22411;&#30340;&#21160;&#24577;&#31209;-1&#26356;&#26032;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Perfusion &#36890;&#36807;&#23558;&#26032;&#27010;&#24565;&#30340;&#20132;&#21449;&#20851;&#27880;&#38190;&#8220;&#38145;&#23450;&#8221;&#21040;&#20854;&#19978;&#20301;&#31867;&#21035;&#26469;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38376;&#25511;&#31209;-1&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#25511;&#21046;&#25152;&#23398;&#27010;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#32452;&#21512;&#22810;&#20010;&#27010;&#24565;&#12290;&#36825;&#20801;&#35768;&#36816;&#34892;&#26102;&#26377;&#25928;&#22320;&#24179;&#34913;&#35270;&#35273;&#20445;&#30495;&#24230;&#21644;&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that "locks" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-align
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.01639</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#22686;&#24378;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"DP-ICL"&#26469;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#12290;DP-ICL&#36890;&#36807;&#20351;&#29992;"report-noisy-max"&#26426;&#21046;&#22312;&#31034;&#20363;&#38598;&#21512;&#19978;&#24314;&#31435;&#22024;&#26434;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#31169;&#26377;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;DP-ICL&#65292;&#21457;&#29616;&#20854;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;(&lt;2%&#38477;&#32423;)&#12290;
&lt;/p&gt;
&lt;p&gt;
An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (&lt;2\% degradation) with non-private ICL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.01626</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#22522;&#30784;&#35821;&#27861;&#65306;&#33258;&#21457;&#32852;&#25509;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#30340;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#26222;&#36941;&#21644;&#22522;&#26412;&#30340;&#35821;&#27861;&#29305;&#24615;&#20043;&#19968;&#8212;&#8212;&#32852;&#25509;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21457;&#32852;&#25509;&#29616;&#35937;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#20010;&#21035;&#21333;&#35789;&#30340;&#22768;&#23398;&#35760;&#24405;&#19978;&#35757;&#32451;&#26102;&#65292;&#24320;&#22987;&#20135;&#29983;&#36755;&#20986;&#65292;&#36825;&#20123;&#36755;&#20986;&#23558;&#20004;&#20010;&#29978;&#33267;&#19977;&#20010;&#21333;&#35789;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#32780;&#19981;&#20250;&#25509;&#35302;&#21040;&#20855;&#26377;&#22810;&#20010;&#21333;&#35789;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#20004;&#20010;&#21333;&#35789;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29615;&#22659;&#19979;&#35757;&#32451;&#30340;&#21407;&#22987;&#35821;&#38899;CNN&#20197;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#23427;&#19981;&#20165;&#23545;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#23398;&#20064;&#26041;&#24335;&#26377;&#24433;&#21709;&#65292;&#36824;&#23545;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models of syntax are predominantly text-based. Here we propose that basic syntax can be modeled directly from raw speech in a fully unsupervised way. We focus on one of the most ubiquitous and basic properties of syntax -- concatenation. We introduce spontaneous concatenation: a phenomenon where convolutional neural networks (CNNs) trained on acoustic recordings of individual words start generating outputs with two or even three words concatenated without ever accessing data with multiple words in the input. Additionally, networks trained on two words learn to embed words into novel unobserved word combinations. To our knowledge, this is a previously unreported property of CNNs trained on raw speech in the Generative Adversarial Network setting and has implications both for our understanding of how these architectures learn as well as for modeling syntax and its evolution from raw acoustic inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22686;&#24378;&#22411;&#30005;&#23376;&#20234;&#36763;&#26426;&#20316;&#20026;&#39640;&#25928;&#30340;SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#28155;&#21152;&#36866;&#24403;&#30340;&#31435;&#26041;&#30456;&#20114;&#20316;&#29992;&#21644;&#26032;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#35299;&#20915;&#28385;&#36275;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01623</link><description>&lt;p&gt;
&#22686;&#24378;&#22411;&#30005;&#23376;&#20234;&#36763;&#26426;&#20316;&#20026;&#39640;&#25928;&#30340;SAT&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Augmented Electronic Ising Machine as an Effective SAT Solver. (arXiv:2305.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22686;&#24378;&#22411;&#30005;&#23376;&#20234;&#36763;&#26426;&#20316;&#20026;&#39640;&#25928;&#30340;SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#28155;&#21152;&#36866;&#24403;&#30340;&#31435;&#26041;&#30456;&#20114;&#20316;&#29992;&#21644;&#26032;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#35299;&#20915;&#28385;&#36275;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#32479;&#20911;&#35834;&#20381;&#26364;&#31995;&#32479;&#25913;&#36827;&#30340;&#20943;&#36895;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#34987;&#20184;&#35832;&#20110;&#26032;&#20852;&#33539;&#20363;&#65292;&#22914;&#20234;&#36763;&#26426;&#12290;&#23427;&#20204;&#23545;&#20110;NP&#23436;&#20840;&#20248;&#21270;&#38382;&#39064;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#20234;&#36763;&#26426;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#35299;&#20915;&#20687;MaxCut&#36825;&#26679;&#30340;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#28385;&#36275;&#24615;(SAT)&#38382;&#39064;&#23545;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;3-SAT&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#26550;&#26500;&#26080;&#27861;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#21152;&#36895;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#35201;&#24402;&#21151;&#20110;&#20256;&#32479;SAT&#27714;&#35299;&#22120;&#19981;&#26029;&#21462;&#24471;&#30340;&#36827;&#27493;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32454;&#33268;&#30340;&#20998;&#26512;&#23558;&#37096;&#20998;&#22833;&#36133;&#24402;&#22240;&#20110;&#32570;&#20047;&#20004;&#20010;&#37325;&#35201;&#32452;&#20214;&#65306;&#31435;&#26041;&#30456;&#20114;&#20316;&#29992;&#21644;&#39640;&#25928;&#30340;&#38543;&#26426;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#20234;&#36763;&#26426;&#19978;&#28155;&#21152;&#20102;&#36866;&#24403;&#30340;&#31435;&#26041;&#20307;&#30456;&#20114;&#20316;&#29992;&#32467;&#26500;&#25903;&#25345;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;&#24863;&#30693;&#28140;&#28779;&#36827;&#24230;&#34920;&#65292;&#20351;&#24471;&#25628;&#32034;&#31354;&#38388;&#23548;&#33322;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowdown of improvement in conventional von Neumann systems, increasing attention is paid to novel paradigms such as Ising machines. They have very different approach to NP-complete optimization problems. Ising machines have shown great potential in solving binary optimization problems like MaxCut. In this paper, we present an analysis of these systems in satisfiability (SAT) problems. We demonstrate that, in the case of 3-SAT, a basic architecture fails to produce meaningful acceleration, thanks in no small part to the relentless progress made in conventional SAT solvers. Nevertheless, careful analysis attributes part of the failure to the lack of two important components: cubic interactions and efficient randomization heuristics. To overcome these limitations, we add proper architectural support for cubic interaction on a state-of-the-art Ising machine. More importantly, we propose a novel semantic-aware annealing schedule that makes the search-space navigation much more eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#36890;&#27969;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;FlowMap&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#36710;&#36742;&#30340;&#36712;&#36857;&#29983;&#25104;&#36335;&#24452;&#65292;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#23450;&#20041;&#28165;&#26224;&#30340;&#8220;&#36947;&#36335;&#8221;&#24773;&#20917;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01622</link><description>&lt;p&gt;
FlowMap&#65306;&#20351;&#29992;&#20132;&#36890;&#27969;&#29983;&#25104;&#24320;&#25918;&#31354;&#38388;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow. (arXiv:2305.01622v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#36890;&#27969;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;FlowMap&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#36710;&#36742;&#30340;&#36712;&#36857;&#29983;&#25104;&#36335;&#24452;&#65292;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#23450;&#20041;&#28165;&#26224;&#30340;&#8220;&#36947;&#36335;&#8221;&#24773;&#20917;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#29486;&#25506;&#35752;&#20102;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#20256;&#24863;&#22120;&#36755;&#20837;&#65288;&#22914;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#21644;&#30456;&#26426;&#22270;&#20687;&#65289;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#36947;&#36335;&#32467;&#26500;&#12290;&#21033;&#29992;&#26368;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65288;&#22914;&#21464;&#21387;&#22120;&#65289;&#21644;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#34920;&#31034;&#27861;&#65292;&#36947;&#36335;&#35748;&#30693;&#31934;&#24230;&#19981;&#26029;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35748;&#30693;&#8220;&#36947;&#36335;&#8221;&#65292;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#23450;&#20041;&#28165;&#26224;&#30340;&#8220;&#36947;&#36335;&#8221;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#36890;&#27969;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;FlowMap&#12290;FlowMap&#36890;&#36807;&#25193;&#23637;&#25105;&#20204;&#20808;&#21069;&#30340;&#24037;&#20316;RoadMap&#26469;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is extensive literature on perceiving road structures by fusing various sensor inputs such as lidar point clouds and camera images using deep neural nets. Leveraging the latest advance of neural architects (such as transformers) and bird-eye-view (BEV) representation, the road cognition accuracy keeps improving. However, how to cognize the ``road'' for automated vehicles where there is no well-defined ``roads'' remains an open problem. For example, how to find paths inside intersections without HD maps is hard since there is neither an explicit definition for ``roads'' nor explicit features such as lane markings. The idea of this paper comes from a proverb: it becomes a way when people walk on it. Although there are no ``roads'' from sensor readings, there are ``roads'' from tracks of other vehicles. In this paper, we propose FlowMap, a path generation framework for automated vehicles based on traffic flows. FlowMap is built by extending our previous work RoadMap, a light-weight 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#35843;&#20248;&#31574;&#30053;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#35821;&#35328;&#20449;&#21495;&#21644;&#25945;&#24072;&#20449;&#21495;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126; FreeLM &#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01616</link><description>&lt;p&gt;
FreeLM: &#20813;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FreeLM: Fine-Tuning-Free Language Model. (arXiv:2305.01616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#35843;&#20248;&#31574;&#30053;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#35821;&#35328;&#20449;&#21495;&#21644;&#25945;&#24072;&#20449;&#21495;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126; FreeLM &#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLMs) &#22312; NLP &#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20027;&#27969;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36981;&#24490;&#39044;&#35757;&#32451;&#21518;&#35843;&#20248;&#30340;&#33539;&#24335;&#65292;&#36825;&#26082;&#24102;&#26469;&#20102;&#39640;&#26114;&#30340;&#37096;&#32626;&#25104;&#26412;&#65292;&#20063;&#38477;&#20302;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35843;&#20248;&#29305;&#23450;&#20219;&#21153;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026; PLMs &#20165;&#22312;&#22823;&#22411;&#21407;&#22987;&#25968;&#25454;&#30340;&#35821;&#35328;&#20449;&#21495;&#19979;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35843;&#20248;&#31574;&#30053;&#65292;&#21363;&#21516;&#26102;&#32771;&#34385;&#35821;&#35328;&#20449;&#21495;&#21644;&#25945;&#24072;&#20449;&#21495;&#12290;&#25945;&#24072;&#20449;&#21495;&#26159;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#20010;&#25277;&#35937;&#34920;&#31034;&#65292;&#20197;&#32479;&#19968;&#21629;&#39064;&#26684;&#24335;&#25552;&#20379;&#12290;&#25105;&#20204;&#30340; FreeLM &#27169;&#22411;&#22312;&#20132;&#20114;&#24335;&#22320;&#20351;&#29992;&#35821;&#35328;&#20449;&#21495;&#21644;&#24378;&#20219;&#21153;&#24863;&#30693;&#30340;&#25945;&#24072;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#21518;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;FreeLM &#22312;&#22810;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914; GPT-3 &#21644; InstructGPT&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340; 175B &#21442;&#25968;&#30456;&#27604;&#65292;FreeLM &#26356;&#23567;&#65292;&#21482;&#26377; 0.3B &#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have achieved remarkable success in NLP tasks. Despite the great success, mainstream solutions largely follow the pre-training then finetuning paradigm, which brings in both high deployment costs and low training efficiency. Nevertheless, fine-tuning on a specific task is essential because PLMs are only pre-trained with language signal from large raw data. In this paper, we propose a novel fine-tuning-free strategy for language models, to consider both language signal and teacher signal. Teacher signal is an abstraction of a battery of downstream tasks, provided in a unified proposition format. Trained with both language and strong task-aware teacher signals in an interactive manner, our FreeLM model demonstrates strong generalization and robustness. FreeLM outperforms large models e.g., GPT-3 and InstructGPT, on a range of language understanding tasks in experiments. FreeLM is much smaller with 0.3B parameters, compared to 175B in these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.01610</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#25506;&#27979;&#20013;&#23547;&#25214;&#28023;&#37327;&#31070;&#32463;&#20803;: &#23454;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24212;&#29992;&#21644;&#37096;&#32626;&#36805;&#36895;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#35745;&#31639;&#20173;&#28982;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39640;&#32423;&#21487;&#35299;&#37322;&#29305;&#24449;&#22312;LLM&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#20013;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;(&#25506;&#38024;)&#26469;&#35757;&#32451;&#36825;&#20123;&#20869;&#37096;&#28608;&#27963;&#20540;&#65292;&#24182;&#39044;&#27979;&#36755;&#20837;&#30340;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65307;&#36890;&#36807;&#25913;&#21464;$k$&#20540;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#24403;$k=1$&#26102;&#65292;&#25105;&#20204;&#23450;&#20301;&#26576;&#20010;&#29305;&#23450;&#29305;&#24449;&#38750;&#24120;&#30456;&#20851;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;LLM&#30340;&#19968;&#33324;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#35768;&#22810;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#20284;&#20046;&#20855;&#26377;&#19987;&#38376;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#26356;&#39640;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#32780;&#22686;&#21152;&#30340;&#35268;&#27169;&#21017;&#23548;&#33268;&#34920;&#31034;&#31232;&#30095;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#25968;&#25454;&#19978;&#19979;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#35821;&#20041;&#37325;&#25490;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.01598</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#65306;&#21033;&#29992;&#25968;&#25454;&#36827;&#34892;&#31243;&#24207;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
From Words to Code: Harnessing Data for Program Synthesis from Natural Language. (arXiv:2305.01598v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#25968;&#25454;&#19978;&#19979;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#35821;&#20041;&#37325;&#25490;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#27491;&#30830;&#25805;&#20316;&#25968;&#25454;&#30340;&#31243;&#24207;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24213;&#23618;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644; API &#23545;&#20110;&#35768;&#22810;&#19981;&#29087;&#32451;&#30340;&#31243;&#24207;&#21592;&#26469;&#35828;&#23398;&#20064;&#36215;&#26469;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20195;&#30721;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#25968;&#25454;&#25805;&#20316;&#39046;&#22495;&#65292;&#38500;&#20102;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#35813;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#28155;&#21152;&#21040;&#21457;&#36865;&#32473; LLM &#30340;&#25552;&#31034;&#20013;&#30340;&#26041;&#24335;&#26377;&#38480;&#22320;&#21033;&#29992;&#25968;&#25454;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20837;&#25968;&#25454;&#26469;&#25191;&#34892; LLM &#29983;&#25104;&#30340;&#20505;&#36873;&#31243;&#24207;&#24182;&#25910;&#38598;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#20041;&#37325;&#25490;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22522;&#20110;&#31243;&#24207;&#36755;&#20986;&#30340;&#19977;&#20010;&#20449;&#21495;&#65288;a&#65289;&#35821;&#20041;&#36807;&#28388;&#21644;&#33391;&#22909;&#26684;&#24335;&#24471;&#20998;&#35843;&#25972;&#65306;&#31243;&#24207;&#26159;&#21542;&#31526;&#21512;&#35821;&#20041;&#21644;&#26684;&#24335;&#65292; (b) &#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#24471;&#20998;: &#31243;&#24207;&#26159;&#21542;&#20026;&#36755;&#20837;&#25968;&#25454;&#25552;&#20379;&#20102;&#36755;&#20986;&#12290;, (c) &#32467;&#26500;&#19982;&#35268;&#33539;&#24471;&#20998;&#65306;&#31243;&#24207;&#26159;&#21542;&#36981;&#24490;API&#30340;&#32467;&#26500;&#21644;&#35268;&#33539;&#65292;&#20197;&#37325;&#25490; LLM &#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating programs to correctly manipulate data is a difficult task, as the underlying programming languages and APIs can be challenging to learn for many users who are not skilled programmers. Large language models (LLMs) demonstrate remarkable potential for generating code from natural language, but in the data manipulation domain, apart from the natural language (NL) description of the intended task, we also have the dataset on which the task is to be performed, or the "data context". Existing approaches have utilized data context in a limited way by simply adding relevant information from the input data into the prompts sent to the LLM.  In this work, we utilize the available input data to execute the candidate programs generated by the LLMs and gather their outputs. We introduce semantic reranking, a technique to rerank the programs generated by LLMs based on three signals coming the program outputs: (a) semantic filtering and well-formedness based score tuning: do programs even ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SELFIES&#20998;&#23376;&#34920;&#31034;&#21644;&#25913;&#36827;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#22810;&#31181;&#31867;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.01580</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#22411;&#20998;&#23376;&#34920;&#31034;&#19982;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Molecular design method based on novel molecular representation and variational auto-encoder. (arXiv:2305.01580v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SELFIES&#20998;&#23376;&#34920;&#31034;&#21644;&#25913;&#36827;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#22810;&#31181;&#31867;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20256;&#32479;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26368;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;SELFIES&#65292;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#20998;&#23376;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#22810;&#23618;&#21367;&#31215;&#32593;&#32476;&#12289;&#36153;&#33293;&#23572;&#20449;&#24687;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32858;&#21512;&#25968;&#25454;&#29305;&#24449;&#21644;&#25351;&#23548;&#32534;&#30721;&#36807;&#31243;&#31561;&#25163;&#27573;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21407;&#22987;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;SELFIES&#21644;&#26032;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#31181;&#31867;&#30340;&#20998;&#23376;&#65292;&#19988;&#29983;&#25104;&#30340;&#20998;&#23376;&#30456;&#20284;&#24230;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the traditional VAE, a novel neural network model is presented, with the latest molecular representation, SELFIES, to improve the effect of generating new molecules. In this model, multi-layer convolutional network and Fisher information are added to the original encoding layer to learn the data characteristics and guide the encoding process, which makes the features of the data hiding layer more aggregated, and integrates the Long Short Term Memory neural network (LSTM) into the decoding layer for better data generation, which effectively solves the degradation phenomenon generated by the encoding layer and decoding layer of the original VAE model. Through experiments on zinc molecular data sets, it is found that the similarity in the new VAE is 8.47% higher than that of the original ones. SELFIES are better at generating a variety of molecules than the traditional molecular representation, SELFIES. Experiments have shown that using SELFIES and the new VAE model presented in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.01579</link><description>&lt;p&gt;
&#21306;&#20998;&#21644;&#22238;&#31572;&#65306;&#36890;&#36807;&#36776;&#21035;&#22120;&#32531;&#35299;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20551;&#23450;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#37117;&#26159;&#20107;&#23454;&#19978;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21487;&#33021;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#23545;&#36825;&#31181;&#20449;&#24687;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#25110;&#25552;&#31034;&#26469;&#24341;&#20986;GPT-3&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20351;&#26816;&#32034;&#22686;&#24378;LM&#23545;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;LM&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#20915;&#31574;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#30340;&#21457;&#29616;&#65292;&#20026;&#21033;&#29992;&#20004;&#32773;&#30340;&#26368;&#20339;&#26041;&#24335;&#38138;&#24179;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2CGL&#30340;&#26032;&#39062;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24314;&#27169;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#21644;&#24322;&#26500;&#30340;&#26041;&#24335;&#35760;&#24405;&#30446;&#26631;&#35770;&#25991;&#24180;&#24230;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#21644;&#21442;&#32771;&#25991;&#29486;&#12289;&#24341;&#25991;&#12289;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23427;&#37319;&#29992;&#21152;&#26435; GIN &#26469;&#25429;&#25417;&#24322;&#26500;&#23376;&#22270;&#30340;&#21160;&#24577;&#65292;&#21516;&#26102;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01572</link><description>&lt;p&gt;
H2CGL: &#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#21160;&#24577;&#24314;&#27169;&#19982;&#24433;&#21709;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
H2CGL: Modeling Dynamics of Citation Network for Impact Prediction. (arXiv:2305.01572v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2CGL&#30340;&#26032;&#39062;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24314;&#27169;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#21644;&#24322;&#26500;&#30340;&#26041;&#24335;&#35760;&#24405;&#30446;&#26631;&#35770;&#25991;&#24180;&#24230;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#21644;&#21442;&#32771;&#25991;&#29486;&#12289;&#24341;&#25991;&#12289;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23427;&#37319;&#29992;&#21152;&#26435; GIN &#26469;&#25429;&#25417;&#24322;&#26500;&#23376;&#22270;&#30340;&#21160;&#24577;&#65292;&#21516;&#26102;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#24433;&#21709;&#21147;&#36890;&#24120;&#26159;&#36890;&#36807;&#20854;&#24341;&#29992;&#25968;&#37327;&#26469;&#34913;&#37327;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20302;&#20272;&#26032;&#21457;&#34920;&#35770;&#25991;&#38543;&#26102;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#19988;&#26410;&#33021;&#23558;&#36825;&#31181;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24615;&#32435;&#20837;&#22270;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24180;&#24230;&#35270;&#35282;&#30340;&#30446;&#26631;&#35770;&#25991;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#65292;&#24182;&#35760;&#24405;&#20102;&#30446;&#26631;&#35770;&#25991;&#31185;&#23398;&#32972;&#26223;&#20449;&#24687;&#30340;&#24180;&#24230;&#21160;&#24577;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#20998;&#23618;&#24322;&#26500;&#23545;&#27604;&#22270;&#23398;&#20064;&#27169;&#22411;&#65288;H2CGL&#65289;&#65292;&#20197;&#34701;&#21512;&#24341;&#25991;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;H2CGL&#20998;&#21035;&#32858;&#21512;&#20102;&#27599;&#24180;&#30340;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#20197;&#21450;&#21442;&#32771;&#25991;&#29486;&#21644;&#24341;&#25991;&#19982;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#23427;&#37319;&#29992;&#21152;&#26435;GIN&#26469;&#25429;&#25417;&#24180;&#20221;&#20043;&#38388;&#30340;&#24322;&#26500;&#23376;&#22270;&#21160;&#24577;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential impact of a paper is often quantified by how many citations it will receive. However, most commonly used models may underestimate the influence of newly published papers over time, and fail to encapsulate this dynamics of citation network into the graph. In this study, we construct hierarchical and heterogeneous graphs for target papers with an annual perspective. The constructed graphs can record the annual dynamics of target papers' scientific context information. Then, a novel graph neural network, Hierarchical and Heterogeneous Contrastive Graph Learning Model (H2CGL), is proposed to incorporate heterogeneity and dynamics of the citation network. H2CGL separately aggregates the heterogeneous information for each year and prioritizes the highly-cited papers and relationships among references, citations, and the target paper. It then employs a weighted GIN to capture dynamics between heterogeneous subgraphs over years. Moreover, it leverages contrastive learning to make
&lt;/p&gt;</description></item><item><title>Pick-a-Pic&#26159;&#19968;&#20221;&#24320;&#25918;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;CLIP&#35780;&#20998;&#20989;&#25968;PickScore&#22312;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#20961;&#30340;&#24615;&#33021;&#65292;&#22312;&#27169;&#22411;&#35780;&#20272;&#26041;&#38754;&#27604;&#20854;&#20182;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26356;&#33021;&#19982;&#20154;&#31867;&#25490;&#21517;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#26410;&#26469;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25552;&#21319;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.01569</link><description>&lt;p&gt;
Pick-a-Pic&#65306;&#19968;&#20221;&#24320;&#25918;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. (arXiv:2305.01569v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01569
&lt;/p&gt;
&lt;p&gt;
Pick-a-Pic&#26159;&#19968;&#20221;&#24320;&#25918;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;CLIP&#35780;&#20998;&#20989;&#25968;PickScore&#22312;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#20961;&#30340;&#24615;&#33021;&#65292;&#22312;&#27169;&#22411;&#35780;&#20272;&#26041;&#38754;&#27604;&#20854;&#20182;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26356;&#33021;&#19982;&#20154;&#31867;&#25490;&#21517;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#26410;&#26469;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25552;&#21319;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#25991;&#26412;&#21040;&#22270;&#20687;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;&#36890;&#24120;&#21482;&#38480;&#20110;&#20225;&#19994;&#65292;&#20351;&#24471;&#36825;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#34987;&#20844;&#20247;&#33719;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20351;&#24471;&#25991;&#26412;&#21040;&#22270;&#20687;&#29992;&#25143;&#21487;&#20197;&#29983;&#25104;&#22270;&#20687;&#24182;&#25351;&#23450;&#20854;&#20559;&#22909;&#12290;&#21033;&#29992;&#27492;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Pick-a-Pic&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#21644;&#30495;&#23454;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#20540;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#22522;&#20110;CLIP&#30340;&#35780;&#20998;&#20989;&#25968; PickScore&#65292;&#22312;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102; PickScore &#22312;&#27169;&#22411;&#35780;&#20272;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#19982;&#20854;&#20182;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30456;&#27604;&#65292;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#25490;&#21517;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992; PickScore &#35780;&#20272;&#26410;&#26469;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992; Pick-a-Pic &#25968;&#25454;&#38598;&#20316;&#20026;&#27604; MS-COCO &#26356;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807; PickScore &#25552;&#21319;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;TTEA&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#19977;&#20803;&#32452;&#29305;&#24449;&#21644;&#23454;&#20307;&#35282;&#33394;&#22810;&#26679;&#24615;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#24863;&#30693;&#27880;&#24847;&#21147;&#26500;&#24314;&#22686;&#24378;&#22411;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#65292;&#36890;&#36807;&#20855;&#20307;&#24615;&#24863;&#30693;&#25511;&#21046;&#20102;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01556</link><description>&lt;p&gt;
&#20511;&#21161;&#19977;&#20803;&#32452;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#22686;&#24378;&#22411;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment. (arXiv:2305.01556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;TTEA&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#19977;&#20803;&#32452;&#29305;&#24449;&#21644;&#23454;&#20307;&#35282;&#33394;&#22810;&#26679;&#24615;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#24863;&#30693;&#27880;&#24847;&#21147;&#26500;&#24314;&#22686;&#24378;&#22411;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#65292;&#36890;&#36807;&#20855;&#20307;&#24615;&#24863;&#30693;&#25511;&#21046;&#20102;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26159;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25351;&#21521;&#21516;&#19968;&#23454;&#38469;&#23545;&#35937;&#30340;&#23454;&#20307;&#36827;&#34892;&#21457;&#29616;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#25366;&#25496;&#19977;&#20803;&#32452;&#20803;&#32032;&#30340;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#23545;&#40784;&#23454;&#20307;&#34920;&#31034;&#65292;&#24456;&#23569;&#20851;&#27880;&#19977;&#20803;&#32452;&#30340;&#19981;&#21487;&#20998;&#24615;&#21644;&#23454;&#20307;&#35282;&#33394;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;TTEA&#65288;&#31867;&#22411;&#22686;&#24378;&#30340;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#38598;&#25104;&#19977;&#20803;&#32452;&#30340;&#20855;&#20307;&#24615;&#21644;&#23454;&#20307;&#35282;&#33394;&#29305;&#24449;&#26469;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#20316;&#20026;&#20449;&#24687;&#36733;&#20307;&#22312;&#35821;&#20041;&#31354;&#38388;&#21644;&#31867;&#22411;&#31354;&#38388;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#20855;&#20307;&#24615;&#24863;&#30693;&#30340;&#19977;&#20803;&#32452;&#27880;&#24847;&#21147;&#24179;&#31283;&#22320;&#25511;&#21046;&#31354;&#38388;&#36716;&#25442;&#21644;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Entity alignment(EA) is a crucial task for integrating cross-lingual and cross-domain knowledge graphs(KGs), which aims to discover entities referring to the same real-world object from different KGs. Most existing methods generate aligning entity representation by mining the relevance of triple elements via embedding-based methods, paying little attention to triple indivisibility and entity role diversity. In this paper, a novel framework named TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment is proposed to overcome the above issues considering ensemble triple specificity and entity role features. Specifically, the ensemble triple representation is derived by regarding relation as information carrier between semantic space and type space, and hence the noise influence during spatial transformation and information propagation can be smoothly controlled via specificity-aware triple attention. Moreover, our framework uses 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01528</link><description>&lt;p&gt;
FIREBALL&#65306;&#19968;&#20221;&#21253;&#21547;&#32467;&#26500;&#21270;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons&#65288;D&#65286;D&#65289;&#26159;&#19968;&#27454;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20854;&#29609;&#23478;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21644;&#38544;&#34255;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25317;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#28216;&#25103;&#22238;&#21512;&#27604;&#20165;&#20351;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;LLMs&#26356;&#20855;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26159;&#21551;&#21457;&#24335;&#21019;&#24314;&#30340;&#65292;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#40644;&#37329;&#26631;&#20934;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FIREBALL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Discord&#30340;&#36817;25,000&#20010;&#30495;&#23454;D&#65286;D&#28216;&#25103;&#20250;&#35805;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#20351;&#29992;Avrae&#26426;&#22120;&#20154;&#30340;&#29609;&#23478;&#30340;&#28216;&#25103;&#20250;&#35805;&#65292;&#35813;&#26426;&#22120;&#20154;&#26159;&#20026;&#20102;&#24110;&#21161;&#20154;&#20204;&#22312;&#32447;&#29609;D&#65286;D&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#25429;&#33719;&#20102;&#35821;&#35328;&#12289;&#28216;&#25103;&#21629;&#20196;&#21644;&#22522;&#30784;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;Avrae&#29366;&#24577;&#20449;&#24687;&#65292;FIREBALL&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#8230;
&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&amp;D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&amp;D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#26368;&#22823;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;QA&#25968;&#25454;&#38598;&#30340;&#38646;-shot&#23398;&#20064;&#24182;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01526</link><description>&lt;p&gt;
Huatuo-26M&#65306;&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Huatuo-26M, a Large-scale Chinese Medical QA Dataset. (arXiv:2305.01526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#26368;&#22823;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;QA&#25968;&#25454;&#38598;&#30340;&#38646;-shot&#23398;&#20064;&#24182;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#26377;&#30528;2600&#19975;&#20010;&#38382;&#31572;&#23545;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#27492;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20197;&#26816;&#32034;&#21644;&#29983;&#25104;&#20004;&#20010;&#26041;&#38754;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#65292;&#32780;&#19988;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#25152;&#25552;&#35758;&#25968;&#25454;&#38598;&#22312;&#20197;&#19979;&#26041;&#38754;&#30340;&#30410;&#22788;&#65306;&#65288;i&#65289;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20026;&#20854;&#20182;&#38382;&#31572;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65307;&#65288;ii&#65289;&#20316;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22806;&#37096;&#30693;&#35782;&#65307;&#65288;iii&#65289;&#36890;&#36807;&#23558;&#38382;&#31572;&#23545;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#26041;&#24335;&#65292;&#25552;&#21319;&#29616;&#26377;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#23558;&#26377;&#21161;&#20110;&#21307;&#23398;&#30740;&#31350;&#65292;&#36824;&#23558;&#20419;&#36827;&#30149;&#20154;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#26381;&#21153;&#12290;&#35831;&#21442;&#32771;&#65306;\url{https://github.com/FreedomIntelligence/Huatuo-26M}&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we release a largest ever medical Question Answering (QA) dataset with 26 million QA pairs. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era. Moreover, we also experimentally show the benefit of the proposed dataset in many aspects: (i) trained models for other QA datasets in a zero-shot fashion; and (ii) as external knowledge for retrieval-augmented generation (RAG); and (iii) improving existing pre-trained language models by using the QA pairs as a pre-training corpus in continued training manner. We believe that this dataset will not only contribute to medical research but also facilitate both the patients and clinical doctors. See \url{https://github.com/FreedomIntelligence/Huatuo-26M}.
&lt;/p&gt;</description></item><item><title>DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01523</link><description>&lt;p&gt;
&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#25512;&#21160;AI&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01523
&lt;/p&gt;
&lt;p&gt;
DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29420;&#31435;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#26174;&#24335;&#30693;&#35782;&#25110;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#30340;&#30740;&#31350;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;AI&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#22320;&#25972;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#20250;&#38459;&#30861;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepEIK&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#21487;&#22797;&#21046;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#20154;&#26694;&#26550;&#23450;&#20041;&#21487;&#22797;&#21046;&#24615;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#26356;&#31995;&#32479;&#30340;&#21487;&#22797;&#21046;&#24615;&#35780;&#20272;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.01518</link><description>&lt;p&gt;
&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#30340;&#21487;&#22797;&#21046;&#24615;
&lt;/p&gt;
&lt;p&gt;
Defining Replicability of Prediction Rules. (arXiv:2305.01518v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#21487;&#22797;&#21046;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#20154;&#26694;&#26550;&#23450;&#20041;&#21487;&#22797;&#21046;&#24615;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#26356;&#31995;&#32479;&#30340;&#21487;&#22797;&#21046;&#24615;&#35780;&#20272;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#21487;&#22797;&#21046;&#24615;&#30340;&#26041;&#27861;&#12290;&#21463;&#26368;&#36817;&#19968;&#20221;&#32654;&#22269;&#22269;&#23478;&#31185;&#23398;&#38498;&#25253;&#21578;&#30340;&#21551;&#21457;&#65292;&#25991;&#31456;&#20174;&#19968;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#35748;&#20026;&#21487;&#22797;&#21046;&#24615;&#26159;&#22312;&#36866;&#21512;&#22238;&#31572;&#21516;&#19968;&#39044;&#27979;&#38382;&#39064;&#30340;&#22810;&#20010;&#30740;&#31350;&#20013;&#33719;&#24471;&#19968;&#33268;&#32467;&#26524;&#65292;&#27599;&#20010;&#30740;&#31350;&#37117;&#26377;&#33258;&#24049;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#35752;&#35770;&#20102;&#23450;&#20041;&#35813;&#35821;&#21477;&#20851;&#38190;&#37096;&#20998;&#30340;&#27010;&#24565;&#21644;&#38382;&#39064;&#65292;&#24182;&#19987;&#27880;&#20110;&#20856;&#22411;&#21033;&#29992;&#29615;&#22659;&#20013;&#8220;&#19968;&#33268;&#32467;&#26524;&#8221;&#30340;&#21547;&#20041;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#20154;&#26694;&#26550;&#26469;&#23450;&#20041;&#21487;&#22797;&#21046;&#24615;&#65292;&#20195;&#29702;&#20154;&#26082;&#19981;&#26159;&#21512;&#20316;&#20249;&#20276;&#20063;&#19981;&#26159;&#23545;&#25163;&#12290;&#20854;&#20013;&#19968;&#20123;&#26222;&#36941;&#23454;&#29992;&#30340;&#26041;&#27861;&#25104;&#20026;&#29305;&#20363;&#12290;&#24076;&#26395;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#26356;&#31995;&#32479;&#30340;&#21487;&#22797;&#21046;&#24615;&#35780;&#20272;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article I propose an approach for defining replicability for prediction rules. Motivated by a recent NAS report, I start from the perspective that replicability is obtaining consistent results across studies suitable to address the same prediction question, each of which has obtained its own data. I then discuss concept and issues in defining key elements of this statement. I focus specifically on the meaning of "consistent results" in typical utilization contexts, and propose a multi-agent framework for defining replicability, in which agents are neither partners nor adversaries. I recover some of the prevalent practical approaches as special cases. I hope to provide guidance for a more systematic assessment of replicability in machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#19978;&#30340;&#27773;&#36710;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01506</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#20013;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Discovering the Effectiveness of Pre-Training in a Large-scale Car-sharing Platform. (arXiv:2305.01506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#19978;&#30340;&#27773;&#36710;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36171;&#20104;&#20102;&#21508;&#31181;&#26234;&#33021;&#20132;&#36890;&#24212;&#29992;&#20197;&#21147;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#19978;&#12290;&#20256;&#32479;&#30340;&#20849;&#20139;&#27773;&#36710;&#26381;&#21153;&#36816;&#33829;&#39640;&#24230;&#20381;&#36182;&#20110;&#36710;&#38431;&#31649;&#29702;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#65292;&#29616;&#20195;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#21017;&#20801;&#35768;&#29992;&#25143;&#22312;&#20351;&#29992;&#21069;&#21518;&#19978;&#20256;&#27773;&#36710;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#27773;&#36710;&#32780;&#26080;&#38656;&#23454;&#22320;&#35775;&#38382;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#19978;&#36848;&#26816;&#26597;&#20219;&#21153;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#26222;&#36941;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#20197;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#19979;&#24314;&#31435;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#22788;&#29702;&#27773;&#36710;&#22270;&#20687;&#30340;&#20505;&#36873;&#20174;&#19994;&#32773;&#24456;&#21487;&#33021;&#20250;&#36973;&#21463;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#22797;&#26434;&#30340;&#31867;&#27604;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#32570;&#20047;&#28145;&#20837;&#21078;&#26512;&#12290;&#37492;&#20110;&#19978;&#36848;&#20998;&#26512;&#32570;&#20047;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#20197;&#25506;&#31350;&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#27773;&#36710;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#24050;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#25216;&#26415;&#30830;&#23454;&#25552;&#39640;&#20102;&#27773;&#36710;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#20934;&#30830;&#24615;&#36824;&#26159;&#25928;&#29575;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#24212;&#29992;&#20013;&#25506;&#31350;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress of deep learning has empowered various intelligent transportation applications, especially in car-sharing platforms. While the traditional operations of the car-sharing service highly relied on human engagements in fleet management, modern car-sharing platforms let users upload car images before and after their use to inspect the cars without a physical visit. To automate the aforementioned inspection task, prior approaches utilized deep neural networks. They commonly employed pre-training, a de-facto technique to establish an effective model under the limited number of labeled datasets. As candidate practitioners who deal with car images would presumably get suffered from the lack of a labeled dataset, we analyzed a sophisticated analogy into the effectiveness of pre-training is important. However, prior studies primarily shed a little spotlight on the effectiveness of pre-training. Motivated by the aforementioned lack of analysis, our study proposes a series of analys
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.01505</link><description>&lt;p&gt;
&#36229;&#36234;&#20998;&#31867;&#65306;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36130;&#21153;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;1000&#20159;&#21450;&#20197;&#19978;&#30340;&#21442;&#25968;&#32452;&#25104;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36890;&#29992;&#30340;&#36827;&#23637;&#24212;&#29992;&#22312;&#24456;&#23569;&#39046;&#22495;&#20013;&#65292;&#20363;&#22914;&#20020;&#24202;&#25110;&#27861;&#24459;&#39046;&#22495;&#65292;&#32780;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;LLMs&#35299;&#20915;&#36130;&#21153;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#20174;&#26410;&#34987;&#30740;&#31350;&#36807;&#65292;&#24182;&#19988;&#23427;&#26159;&#21542;&#21487;&#20197;&#22312;&#20219;&#20309;&#35268;&#27169;&#19978;&#23436;&#25104;&#20173;&#26410;&#30693;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23545;LLMs&#22312;&#36130;&#21153;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#35843;&#26597;&#21253;&#25324;&#23545;&#19968;&#31995;&#21015;&#20027;&#39064;&#30340;&#35814;&#32454;&#25506;&#35752;&#65292;&#21253;&#25324;&#20219;&#21153;&#21046;&#23450;&#65292;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26377;&#26080;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01486</link><description>&lt;p&gt;
ARBEx&#65306;&#29992;&#20110;&#40065;&#26834;&#24615;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#30340;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#19982;&#21487;&#38752;&#24615;&#24179;&#34913;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#30001;Vision Transformer&#39537;&#21160;&#30340;&#26032;&#22411;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#24102;&#26377;&#21487;&#38752;&#24615;&#24179;&#34913;&#65292;&#20197;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36739;&#24046;&#31867;&#20998;&#24067;&#12289;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#31934;&#21270;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#31383;&#21475;&#30340;&#20132;&#21449;&#20851;&#27880;ViT&#26469;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#65292;&#21152;&#19978;&#26631;&#31614;&#20998;&#24067;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36890;&#36807;&#21487;&#38752;&#24615;&#24179;&#34913;&#20248;&#21270;&#23545;&#24369;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#26631;&#31614;&#39044;&#27979;&#38887;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#30340;&#26631;&#31614;&#20998;&#31867;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#25439;&#22833;&#65292;&#40723;&#21169;&#38170;&#28857;&#20043;&#38388;&#30340;&#22823;&#38388;&#38548;&#12290;&#21478;&#22806;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#20063;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#25552;&#21319;&#22312;FEL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ARBEx&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01481</link><description>&lt;p&gt;
&#27169;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#65306;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement. (arXiv:2305.01481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01481
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24212;&#29992;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#38469;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#65292;&#27169;&#22411;&#32463;&#24120;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#27979;&#37327;&#19968;&#20010;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#19982;&#21478;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#19981;&#36830;&#36143;&#24615;&#65292;&#21363;&#20219;&#24847;&#26059;&#36716;&#21644;&#19981;&#21516;&#30340;&#32500;&#24230;&#65292;&#20004;&#20010;&#19981;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#24456;&#38590;&#34913;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#19981;&#36830;&#36143;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#8220;&#37051;&#22495;&#19968;&#33268;&#24615;&#24230;&#37327;&#8221;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#19968;&#33268;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#26377;&#24778;&#20154;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#21518;&#32493;&#30340;&#26041;&#24335;&#20013;&#23558;&#37051;&#22495;&#19968;&#33268;&#24615;&#34701;&#20837;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#25925;&#38556;&#26816;&#27979;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable application of machine learning is of primary importance to the practical deployment of deep learning methods. A fundamental challenge is that models are often unreliable due to overconfidence. In this paper, we estimate a model's reliability by measuring \emph{the agreement between its latent space, and the latent space of a foundation model}. However, it is challenging to measure the agreement between two different latent spaces due to their incoherence, \eg, arbitrary rotations and different dimensionality. To overcome this incoherence issue, we design a \emph{neighborhood agreement measure} between latent spaces and find that this agreement is surprisingly well-correlated with the reliability of a model's predictions. Further, we show that fusing neighborhood agreement into a model's predictive confidence in a post-hoc way significantly improves its reliability. Theoretical analysis and extensive experiments on failure detection across various datasets verify the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#65292; &#24182;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01461</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mixed-Integer Optimal Control via Reinforcement Learning: A Case Study on Hybrid Vehicle Energy Management. (arXiv:2305.01461v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#65292; &#24182;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#36755;&#20986;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#34987;&#21046;&#23450;&#20026;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;(MIOC)&#38382;&#39064;&#65292;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#35299;&#20915;&#12290;&#25968;&#20540;&#26041;&#27861;&#22914;&#20998;&#25903;&#23450;&#30028;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#19981;&#36866;&#21512;&#23454;&#26102;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;(CDRL)&#31639;&#27861;&#65292;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#28436;&#21592;- Q(TD3AQ)&#65292;&#29992;&#20110;MIOC&#38382;&#39064;&#12290;TD3AQ&#32467;&#21512;&#20102;&#28436;&#21592;-&#25209;&#35780;&#23478;&#21644;Q&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#35813;&#31639;&#27861;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;(HEV)&#33021;&#37327;&#31649;&#29702;&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#36830;&#32493;&#21464;&#37327;&#21457;&#21160;&#26426;&#36716;&#30697;&#21644;&#31163;&#25955;&#21464;&#37327;&#40831;&#36718;&#27604;&#30340;&#23454;&#26102;&#25511;&#21046;&#23545;&#20110;&#26368;&#22823;&#21270;&#29123;&#27833;&#32463;&#27982;&#24615;&#24182;&#28385;&#36275;&#39550;&#39542;&#32422;&#26463;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#21516;&#39537;&#21160;&#24490;&#29615;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CDRL&#31639;&#27861;&#22312;&#35299;&#20915;&#36136;&#37327;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many optimal control problems require the simultaneous output of continuous and discrete control variables. Such problems are usually formulated as mixed-integer optimal control (MIOC) problems, which are challenging to solve due to the complexity of the solution space. Numerical methods such as branch-and-bound are computationally expensive and unsuitable for real-time control. This paper proposes a novel continuous-discrete reinforcement learning (CDRL) algorithm, twin delayed deep deterministic actor-Q (TD3AQ), for MIOC problems. TD3AQ combines the advantages of both actor-critic and Q-learning methods, and can handle the continuous and discrete action spaces simultaneously. The proposed algorithm is evaluated on a hybrid electric vehicle (HEV) energy management problem, where real-time control of the continuous variable engine torque and discrete variable gear ratio is essential to maximize fuel economy while satisfying driving constraints. Simulation results on different drive cyc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#30123;&#33495;&#38144;&#21806;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#19981;&#21516;&#23618;&#27425;&#39044;&#27979;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#26368;&#23567;&#36857;&#21644;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#19982;&#32467;&#26500;&#32553;&#25918;&#30340;&#21327;&#35843;&#26041;&#27861;&#25928;&#26524;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.01455</link><description>&lt;p&gt;
&#30123;&#33495;&#20379;&#24212;&#38142;&#20248;&#21270;&#30340;&#39044;&#27979;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Forecast reconciliation for vaccine supply chain optimization. (arXiv:2305.01455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#30123;&#33495;&#38144;&#21806;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#19981;&#21516;&#23618;&#27425;&#39044;&#27979;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#26368;&#23567;&#36857;&#21644;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#19982;&#32467;&#26500;&#32553;&#25918;&#30340;&#21327;&#35843;&#26041;&#27861;&#25928;&#26524;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#20379;&#24212;&#38142;&#20248;&#21270;&#36890;&#36807;&#25353;&#31867;&#22411;&#25110;&#22320;&#28857;&#20998;&#32452;&#30340;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26469;&#33719;&#30410;&#12290;&#28982;&#32780;&#65292;&#24403;&#39640;&#23618;&#27425;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20302;&#23618;&#27425;&#39044;&#27979;&#32467;&#26524;&#20043;&#21644;&#19981;&#21305;&#37197;&#26102;&#65292;&#19981;&#21516;&#23618;&#27425;&#30340;&#39044;&#27979;&#21464;&#24471;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#21327;&#35843;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;2010&#24180;&#33267;2021&#24180;GSK&#20844;&#21496;&#30340;&#38144;&#21806;&#25968;&#25454;&#24314;&#27169;&#20026;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;&#30123;&#33495;&#38144;&#21806;&#39044;&#27979;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;ARIMA&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#20540;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#21508;&#31181;&#21327;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#26816;&#39564;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;COVID&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#26368;&#23567;&#36857;&#21644;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#19982;&#32467;&#26500;&#32553;&#25918;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#20102;&#22522;&#32447;ARIMA&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine supply chain optimization can benefit from hierarchical time series forecasting, when grouping the vaccines by type or location. However, forecasts of different hierarchy levels become incoherent when higher levels do not match the sum of the lower levels forecasts, which can be addressed by reconciliation methods.  In this paper, we tackle the vaccine sale forecasting problem by modeling sales data from GSK between 2010 and 2021 as a hierarchical time series. After forecasting future values with several ARIMA models, we systematically compare the performance of various reconciliation methods, using statistical tests. We also compare the performance of the forecast before and after COVID. The results highlight Minimum Trace and Weighted Least Squares with Structural scaling as the best performing methods, which provided a coherent forecast while reducing the forecast error of the baseline ARIMA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20262;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#20551;&#24819;&#22238;&#39038;&#35770;&#35777;&#31243;&#24207;&#65292;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21746;&#23398;&#29702;&#35770;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#21487;&#20197;&#32771;&#34385;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#33258;&#20027;&#22270;&#20070;&#39302;&#31995;&#32479;&#29992;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.01424</link><description>&lt;p&gt;
&#20351;&#29992;&#20551;&#24819;&#22238;&#39038;&#36827;&#34892;&#19981;&#30830;&#23450;&#26426;&#22120;&#20262;&#29702;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Uncertain Machine Ethical Decisions Using Hypothetical Retrospection. (arXiv:2305.01424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20262;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#20551;&#24819;&#22238;&#39038;&#35770;&#35777;&#31243;&#24207;&#65292;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21746;&#23398;&#29702;&#35770;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#21487;&#20197;&#32771;&#34385;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#33258;&#20027;&#22270;&#20070;&#39302;&#31995;&#32479;&#29992;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#30001;Sven Hansson&#24320;&#21457;&#30340;&#20551;&#24819;&#22238;&#39038;&#35770;&#35777;&#31243;&#24207;&#65292;&#20197;&#32771;&#34385;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#21746;&#23398;&#30340;&#35282;&#24230;&#20986;&#21457;&#25913;&#36827;&#29616;&#26377;&#30340;&#26426;&#22120;&#20262;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#19982;&#20154;&#31867;&#20135;&#29983;&#20849;&#40483;&#12290;&#35813;&#31243;&#24207;&#23558;&#34892;&#21160;&#34920;&#31034;&#20026;&#19968;&#32452;&#21487;&#33021;&#32467;&#26524;&#30340;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#20855;&#26377;&#29366;&#24577;&#12289;&#25928;&#29992;&#21644;&#25968;&#20540;&#25110;&#35799;&#24847;&#27010;&#29575;&#20272;&#35745;&#12290;&#34892;&#21160;&#30340;&#36873;&#25321;&#22522;&#20110;&#20174;&#23427;&#20204;&#30340;&#20998;&#25903;&#30340;&#35282;&#24230;&#27604;&#36739;&#20542;&#21521;&#20110;&#34892;&#21160;&#30340;&#19968;&#32452;&#35770;&#35777;&#65292;&#21363;&#20351;&#36825;&#20123;&#20998;&#25903;&#23548;&#33268;&#20102;&#19981;&#33391;&#32467;&#26524;&#20063;&#19968;&#26679;&#12290;&#36825;&#31181;&#20351;&#29992;&#35770;&#35777;&#30340;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21746;&#23398;&#29702;&#35770;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#21487;&#33021;&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#33258;&#20027;&#22270;&#20070;&#39302;&#31995;&#32479;&#29992;&#20363;&#20013;&#65292;&#20998;&#21035;&#29420;&#31435;&#25110;&#21516;&#26102;&#24212;&#29992;&#32467;&#26524;&#20027;&#20041;&#21644;&#20041;&#21153;&#35770;&#30340;&#20262;&#29702;&#29702;&#35770;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#20284;&#20046;&#28385;&#36275;&#20102;&#21508;&#31181;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Hansson, to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a a preliminary framework that seems to meet the varied requirements of a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;POIR&#23558;&#34892;&#20026;&#20811;&#38534;&#21644;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01400</link><description>&lt;p&gt;
&#36820;&#22238;&#20998;&#24067;&#35268;&#21010;&#65306;&#40065;&#26834;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Get Back Here: Robust Imitation by Return-to-Distribution Planning. (arXiv:2305.01400v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;POIR&#23558;&#34892;&#20026;&#20811;&#38534;&#21644;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#21363;&#19987;&#23478;&#25968;&#25454;&#19981;&#26159;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#19979;&#25910;&#38598;&#30340;&#65292;&#32780;&#26159;&#22312;&#21478;&#19968;&#20010;&#29256;&#26412;&#19978;&#25910;&#38598;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#27492;&#23548;&#33268;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#19982;&#19968;&#20010;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#35268;&#21010;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#20195;&#29702;&#31243;&#24207;&#20559;&#31163;&#28436;&#31034;&#20998;&#24067;&#26102;&#65292;&#23558;&#20195;&#29702;&#31243;&#24207;&#24102;&#22238;&#21040;&#19987;&#23478;&#35775;&#38382;&#30340;&#29366;&#24577;&#12290;&#24471;&#21040;&#30340;&#31639;&#27861;POIR&#21487;&#20197;&#31163;&#32447;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#20132;&#20114;&#26469;&#26377;&#25928;&#22320;&#20248;&#21270;&#20854;&#35268;&#21010;&#22120;&#65292;&#20197;&#36880;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#27169;&#25311;&#22120;&#20013;&#23545;POIR&#36827;&#34892;&#27979;&#35797;&#65292;&#20351;&#29992;&#21508;&#31181;&#20154;&#31867;&#29983;&#25104;&#30340;&#25805;&#20316;&#28436;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#31574;&#30053;&#23545;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#21644;&#22024;&#26434;&#21160;&#24577;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting distribution shift, we combine behavior cloning (BC) with a planner that is tasked to bring the agent back to states visited by the expert whenever the agent deviates from the demonstration distribution. The resulting algorithm, POIR, can be trained offline, and leverages online interactions to efficiently fine-tune its planner to improve performance over time. We test POIR on a variety of human-generated manipulation demonstrations in a realistic robotic manipulation simulator and show robustness of the learned policy to different initial state distributions and noisy dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;64.868%&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#21644;61.549%&#30340;&#8220;&#27833;&#27745;&#8221;&#31867;IoU&#12290;</title><link>http://arxiv.org/abs/2305.01386</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Oil Spill Segmentation using Deep Encoder-Decoder models. (arXiv:2305.01386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;64.868%&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#21644;61.549%&#30340;&#8220;&#27833;&#27745;&#8221;&#31867;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#27833;&#26159;&#29616;&#20195;&#19990;&#30028;&#32463;&#27982;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#38543;&#30528;&#21407;&#27833;&#24191;&#27867;&#24212;&#29992;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#24847;&#22806;&#30340;&#27833;&#27745;&#27844;&#28431;&#20063;&#38590;&#20197;&#36991;&#20813;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#20960;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#27169;&#22411;&#32452;&#21512;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#19982;&#24403;&#21069;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#22312;&#8220;&#27833;&#27745;&#8221;&#31867;&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#19978;&#23454;&#29616;&#20102;64.868%&#30340;&#32467;&#26524;&#21644;61.549%&#30340;&#31867;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;
Crude oil is an integral component of the modern world economy. With the growing demand for crude oil due to its widespread applications, accidental oil spills are unavoidable. Even though oil spills are in and themselves difficult to clean up, the first and foremost challenge is to detect spills. In this research, the authors test the feasibility of deep encoder-decoder models that can be trained effectively to detect oil spills. The work compares the results from several segmentation models on high dimensional satellite Synthetic Aperture Radar (SAR) image data. Multiple combinations of models are used in running the experiments. The best-performing model is the one with the ResNet-50 encoder and DeepLabV3+ decoder. It achieves a mean Intersection over Union (IoU) of 64.868% and a class IoU of 61.549% for the "oil spill" class when compared with the current benchmark model, which achieved a mean IoU of 65.05% and a class IoU of 53.38% for the "oil spill" class.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.01381</link><description>&lt;p&gt;
&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26679;&#26412;&#26377;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#24191;&#27867;&#29992;&#20110;&#25351;&#23450;&#31995;&#32479;&#31574;&#30053;&#30340;&#39640;&#32423;&#30446;&#26631;&#65292;&#33258;&#20027;&#31995;&#32479;&#23398;&#20064;&#30456;&#23545;&#20110;&#36825;&#26679;&#30340;&#35268;&#33539;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290; &#20294;&#26159;&#65292;&#20174;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#36731;&#26494;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26356;&#36890;&#29992;&#30340;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#65292;&#24403;&#19982;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26368;&#22823;&#21270;&#32473;&#23450;LTL&#35268;&#33539;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26377;&#20851;&#36873;&#25321;RL&#20013;&#20851;&#38190;&#21442;&#25968;&#20197;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20026;&#20102;&#30452;&#25509;&#35780;&#20272;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26469;&#35745;&#31639;LTL&#35268;&#33539;&#30340;&#28385;&#36275;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01322</link><description>&lt;p&gt;
&#22522;&#20110;Option&#26694;&#26550;&#30340;&#22810;&#27169;&#24335;&#25506;&#32034;&#33258;&#20027;&#38750;&#21333;&#20307;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25506;&#32034;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#32780;&#8220;&#20309;&#26102;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#30740;&#31350;&#19968;&#30452;&#27809;&#26377;&#25104;&#20026;&#37325;&#28857;&#12290;&#20856;&#22411;&#30340;&#25506;&#32034;&#34892;&#20026;&#36890;&#24120;&#23558;&#25506;&#32034;&#34892;&#20026;&#19982;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#21033;&#29992;&#34892;&#20026;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#38750;&#21333;&#20307;&#25506;&#32034;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#27169;&#24335;&#20999;&#25442;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#20915;&#23450;&#20309;&#26102;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;Option&#26694;&#26550;&#20013;&#25551;&#36848;&#20102;&#33258;&#20027;&#22810;&#27169;&#24335;&#25506;&#32034;&#30340;&#21021;&#22987;&#30740;&#31350;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#30340;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.01278</link><description>&lt;p&gt;
&#27178;&#21521;&#36801;&#31227;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#22312;VL-LLMs&#20043;&#38388;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#65288;VPG&#65289;&#36830;&#25509;&#24050;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;LLM&#65288;VL-LLM&#65289;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;VPGTrans&#30340;&#20004;&#38454;&#27573;&#36716;&#31227;&#26694;&#26550;&#65292;&#23427;&#22312;VQA&#21644;NLVR2&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#36716;&#31227;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#35299;&#20915;&#20102;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#20223;&#30495;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#20294;&#20063;&#23384;&#22312;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.01263</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sim2real and Digital Twins in Autonomous Driving: A Survey. (arXiv:2305.01263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#35299;&#20915;&#20102;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#20223;&#30495;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#20294;&#20063;&#23384;&#22312;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21644;&#25104;&#26412;&#26159;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#24320;&#21457;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20174;&#23398;&#26415;&#30740;&#31350;&#21040;&#21830;&#19994;&#24212;&#29992;&#65292;&#37117;&#38656;&#35201;&#20805;&#20998;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#12290;&#36890;&#24120;&#20250;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#65292;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#39550;&#39542;&#30693;&#35782;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#65292;&#22240;&#27492;&#22914;&#20309;&#23558;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#21040;&#30340;&#39550;&#39542;&#30693;&#35782;&#36866;&#24212;&#29616;&#23454;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#34394;&#25311;&#20223;&#30495;&#19990;&#30028;&#19982;&#29616;&#23454;&#19990;&#30028;&#22312;&#35768;&#22810;&#26041;&#38754;&#65288;&#22914;&#29031;&#26126;&#12289;&#32441;&#29702;&#12289;&#36710;&#36742;&#21160;&#21147;&#23398;&#21644;&#20195;&#29702;&#34892;&#20026;&#31561;&#65289;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#24357;&#21512;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#24046;&#36317;&#36890;&#24120;&#34987;&#31216;&#20026;&#29616;&#23454;&#24046;&#36317;&#65288;RG&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#24046;&#36317;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#22320;&#20998;&#20026;&#20004;&#31867;&#65306;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#30693;&#35782;&#36716;&#31227;&#65288;sim2real&#65289;&#21644;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#20223;&#30495;&#31934;&#24230;&#65288;&#25968;&#23383;&#23402;&#29983;&#65289;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety and cost are two important concerns for the development of autonomous driving technologies. From the academic research to commercial applications of autonomous driving vehicles, sufficient simulation and real world testing are required. In general, a large scale of testing in simulation environment is conducted and then the learned driving knowledge is transferred to the real world, so how to adapt driving knowledge learned in simulation to reality becomes a critical issue. However, the virtual simulation world differs from the real world in many aspects such as lighting, textures, vehicle dynamics, and agents' behaviors, etc., which makes it difficult to bridge the gap between the virtual and real worlds. This gap is commonly referred to as the reality gap (RG). In recent years, researchers have explored various approaches to address the reality gap issue, which can be broadly classified into two categories: transferring knowledge from simulation to reality (sim2real) and learn
&lt;/p&gt;</description></item><item><title>DreamPaint&#26159;&#19968;&#20010;&#26080;&#38656;3D&#24314;&#27169;&#30340;&#30005;&#21830;&#21830;&#21697;&#35797;&#31359;Few-Shot&#20462;&#22797;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#29992;&#25143;&#29615;&#22659;&#19979;&#23545;&#21830;&#21697;&#22270;&#20687;&#36827;&#34892;&#20462;&#22797;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20462;&#22797;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01257</link><description>&lt;p&gt;
DreamPaint: &#26080;&#38656;3D&#24314;&#27169;&#30340;&#30005;&#21830;&#21830;&#21697;&#35797;&#31359;Few-Shot&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling. (arXiv:2305.01257v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01257
&lt;/p&gt;
&lt;p&gt;
DreamPaint&#26159;&#19968;&#20010;&#26080;&#38656;3D&#24314;&#27169;&#30340;&#30005;&#21830;&#21830;&#21697;&#35797;&#31359;Few-Shot&#20462;&#22797;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#29992;&#25143;&#29615;&#22659;&#19979;&#23545;&#21830;&#21697;&#22270;&#20687;&#36827;&#34892;&#20462;&#22797;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20462;&#22797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; DreamPaint&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#26234;&#33021;&#20462;&#22797;&#30005;&#21830;&#20135;&#21697;&#22312;&#29992;&#25143;&#25552;&#20379;&#30340;&#20219;&#24847;&#32972;&#26223;&#22270;&#20687;&#19978;&#30340;&#26694;&#26550;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;&#22270;&#20687;&#26469;&#35797;&#31359;&#30005;&#21830;&#30446;&#24405;&#20013;&#30340;&#34915;&#26381;&#65292;&#25110;&#32773;&#20351;&#29992;&#33258;&#24049;&#25151;&#38388;&#30340;&#22270;&#20687;&#26469;&#22312;&#20854;&#20013;&#35797;&#25918;&#30005;&#21830;&#30446;&#24405;&#20013;&#30340;&#23478;&#20855;&#31561;&#12290;&#19982;&#20197;&#21069;&#30340;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#34394;&#25311;&#35797;&#31359;&#26041;&#27861;&#19981;&#21516;&#65292;DreamPaint &#19981;&#20351;&#29992;&#65292;&#20063;&#19981;&#38656;&#35201;&#30005;&#21830;&#20135;&#21697;&#25110;&#29992;&#25143;&#29615;&#22659;&#30340; 3D &#24314;&#27169;&#12290;&#30456;&#21453;&#65292;&#23427;&#30452;&#25509;&#20351;&#29992;&#30446;&#24405;&#25968;&#25454;&#24211;&#20013;&#21487;&#29992;&#30340;&#20135;&#21697;&#30340; 2D &#22270;&#20687;&#21644;&#19978;&#19979;&#25991;&#30340; 2D &#22270;&#20687;&#65292;&#20363;&#22914;&#20174;&#29992;&#25143;&#30340;&#25163;&#26426;&#30456;&#26426;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#20381;&#38752;&#23569;&#37327;&#26679;&#26412;&#24494;&#35843;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#20135;&#21697;&#30446;&#24405;&#22270;&#20687;&#19979;&#30340;&#36974;&#32617;&#28508;&#21464;&#37327;&#65288;&#20363;&#22914;&#65292;Masked DreamBooth&#65289;&#30340;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#21152;&#36733;&#21040;&#33021;&#22815;&#20445;&#30041;&#23646;&#24615;&#30340;&#39044;&#35757;&#32451;&#20462;&#22797;&#27169;&#22359;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DreamPaint, a framework to intelligently inpaint any e-commerce product on any user-provided context image. The context image can be, for example, the user's own image for virtual try-on of clothes from the e-commerce catalog on themselves, the user's room image for virtual try-on of a piece of furniture from the e-commerce catalog in their room, etc. As opposed to previous augmented-reality (AR)-based virtual try-on methods, DreamPaint does not use, nor does it require, 3D modeling of neither the e-commerce product nor the user context. Instead, it directly uses 2D images of the product as available in product catalog database, and a 2D picture of the context, for example taken from the user's phone camera. The method relies on few-shot fine tuning a pre-trained diffusion model with the masked latents (e.g., Masked DreamBooth) of the catalog images per item, whose weights are then loaded on a pre-trained inpainting module that is capable of preserving the characteristics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25139;&#25968;&#25454;&#30340;LTL&#27491;&#29255;&#27573;&#21046;&#23450;&#30340;&#26597;&#35810;&#21453;&#21521;&#24037;&#31243;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#23558;&#32473;&#23450;&#31572;&#26696;&#19982;&#38750;&#31572;&#26696;&#20998;&#24320;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#24182;&#32771;&#34385;&#20102;LTL&#26412;&#20307;&#20171;&#23548;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.01248</link><description>&lt;p&gt;
LTL&#26412;&#20307;&#20013;&#30340;&#26102;&#38388;&#26597;&#35810;&#21453;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering of Temporal Queries Mediated by LTL Ontologies. (arXiv:2305.01248v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25139;&#25968;&#25454;&#30340;LTL&#27491;&#29255;&#27573;&#21046;&#23450;&#30340;&#26597;&#35810;&#21453;&#21521;&#24037;&#31243;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#23558;&#32473;&#23450;&#31572;&#26696;&#19982;&#38750;&#31572;&#26696;&#20998;&#24320;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#24182;&#32771;&#34385;&#20102;LTL&#26412;&#20307;&#20171;&#23548;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#21453;&#21521;&#24037;&#31243;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#31572;&#26696;&#21644;&#38750;&#31572;&#26696;&#38598;&#21512;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;; &#28982;&#21518;&#21487;&#20197;&#23558;&#20854;&#29992;&#20110;&#36827;&#19968;&#27493;&#25506;&#32034;&#25968;&#25454;&#25110;&#20316;&#20026;&#31572;&#26696;&#21644;&#38750;&#31572;&#26696;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25139;&#25968;&#25454;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;LTL&#30340;&#27491;&#29255;&#27573;&#21046;&#23450;&#30340;&#26597;&#35810;&#30340;&#36825;&#20010;&#26597;&#35810;-by-example&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#35774;&#35745;&#21512;&#36866;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#20915;&#23450;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#26597;&#35810;&#22312;&#32473;&#23450;&#30340;&#35821;&#35328;&#20013;&#21487;&#20197;&#23558;&#32473;&#23450;&#30340;&#31572;&#26696;&#19982;&#38750;&#31572;&#26696;&#20998;&#24320;&#30340;&#32452;&#21512;&#21644;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;&#26222;&#36890;&#30340;LTL&#26597;&#35810;&#21644;&#37027;&#20123;&#36890;&#36807;LTL&#26412;&#20307;&#20171;&#23548;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reverse engineering of database queries, we aim to construct a query from a given set of answers and non-answers; it can then be used to explore the data further or as an explanation of the answers and non-answers. We investigate this query-by-example problem for queries formulated in positive fragments of linear temporal logic LTL over timestamped data, focusing on the design of suitable query languages and the combined and data complexity of deciding whether there exists a query in the given language that separates the given answers from non-answers. We consider both plain LTL queries and those mediated by LTL-ontologies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRPT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#19982;&#24490;&#29615;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#35760;&#21495;&#21442;&#25968;&#65292;&#20351;&#24471;&#22312;&#32452;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#25152;&#37319;&#29992;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#35782;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DRPT&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01239</link><description>&lt;p&gt;
DRPT: &#20998;&#31163;&#19982;&#24490;&#29615;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning. (arXiv:2305.01239v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRPT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#19982;&#24490;&#29615;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#35760;&#21495;&#21442;&#25968;&#65292;&#20351;&#24471;&#22312;&#32452;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#25152;&#37319;&#29992;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#35782;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DRPT&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#35782;&#21035;&#30001;&#24050;&#30693;&#30693;&#35782;&#32452;&#25104;&#30340;&#26032;&#27010;&#24565;&#65292;&#27809;&#26377;&#35757;&#32451;&#26679;&#26412;&#12290;&#26631;&#20934;&#30340;&#32452;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#35201;&#20040;&#35782;&#21035;&#35270;&#35273;&#21407;&#35821;&#65292;&#35201;&#20040;&#22686;&#24378;&#30475;&#19981;&#35265;&#30340;&#32452;&#21512;&#23454;&#20307;&#65292;&#23548;&#33268;&#29366;&#24577;&#21644;&#23545;&#35937;&#21407;&#35821;&#20043;&#38388;&#30340;&#32416;&#32544;&#26080;&#27861;&#23436;&#20840;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31163;&#21644;&#24490;&#29615;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;DRPT&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#20013;&#30340;&#35789;&#27719;&#23884;&#20837;&#29366;&#24577;&#21644;&#23545;&#35937;&#21407;&#35821;&#65292;&#24182;&#22312;&#30475;&#21040;&#30340;&#32452;&#21512;&#20013;&#35843;&#25972;&#23427;&#20204;&#65292;&#20248;&#21270;&#35760;&#21495;&#21442;&#25968;&#12290;&#30456;&#27604;&#20110;&#32852;&#21512;&#35843;&#25972;&#29366;&#24577;&#21644;&#23545;&#35937;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20998;&#31163;&#21644;&#24490;&#29615;&#30340;&#35843;&#25972;&#31574;&#30053;&#65292;&#25233;&#21046;&#20102;&#32416;&#32544;&#25152;&#24341;&#36215;&#30340;&#29301;&#24341;&#21147;&#65292;&#24182;&#36880;&#27493;&#20248;&#21270;&#20102;&#35760;&#21495;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#32452;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DRPT&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;CZSL&#24615;&#33021;&#65292;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional Zero-shot Learning (CZSL) aims to recognize novel concepts composed of known knowledge without training samples. Standard CZSL either identifies visual primitives or enhances unseen composed entities, and as a result, entanglement between state and object primitives cannot be fully utilized. Admittedly, vision- language models (VLMs) could naturally cope with CZSL through tuning prompts, while uneven entanglement leads prompts to be dragged into local optimum. In this paper, we take a further step to introduce a novel Disentangled and Recurrent Prompt Tuning framework termed DRPT to better tap the potential of VLMs in CZSL. Specifically, the state and object primitives are deemed as learnable tokens of vocabulary embedded in prompts and tuned on seen compositions. Instead of jointly tuning state and object, we devise a disentangled and recurrent tuning strategy to suppress the traction force caused by entanglement and gradually optimize the token parameters, leading to a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;MultiLegalSBD&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#35757;&#32451;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.01211</link><description>&lt;p&gt;
MultiLegalSBD&#65306;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;MultiLegalSBD&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#35757;&#32451;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#20043;&#19968;&#65292;&#19981;&#27491;&#30830;&#30340;&#20998;&#21106;&#20250;&#20005;&#37325;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#27861;&#24459;&#39046;&#22495;&#65292;&#22240;&#20026;&#20351;&#29992;&#30340;&#22797;&#26434;&#21477;&#23376;&#32467;&#26500;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#27861;&#24459;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#27861;&#24459;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#33889;&#33796;&#29273;&#35821;&#27979;&#35797;&#38598;&#30340;&#38646;-shot&#35774;&#32622;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;&#20026;&#20102;&#40723;&#21169;&#31038;&#21306;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.
&lt;/p&gt;</description></item><item><title>Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.01206</link><description>&lt;p&gt;
Chronosymbolic Learning: &#32467;&#21512;&#31526;&#21495;&#25512;&#29702;&#19982;&#24402;&#32435;&#23398;&#20064;&#30340;&#26377;&#25928;CHC&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning. (arXiv:2305.01206v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01206
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHC (Constrained Horn Clauses)&#30340;&#27714;&#35299;&#26159;&#35768;&#22810;&#39564;&#35777;&#21644;&#20998;&#26512;&#20219;&#21153;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#27861;&#22312;&#25552;&#39640;CHC&#27714;&#35299;&#25928;&#29575;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#21644;&#35843;&#25972;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32321;&#29712;&#24037;&#20316;&#12290;&#20294;&#25968;&#25454;&#39537;&#21160;&#30340;CHC&#27714;&#35299;&#22120;&#19982;&#22522;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;"Chronosymbolic Learning"&#65292;&#23427;&#23558;&#31526;&#21495;&#20449;&#24687;&#21644;&#25968;&#20540;&#25968;&#25454;&#28857;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;CHC&#31995;&#32479;&#39640;&#25928;&#22320;&#27714;&#35299;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Chronosymbolic Learning&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#22120;&#21644;&#19968;&#20010;BMC&#26679;&#24335;&#30340;&#25512;&#29702;&#22120;&#12290;&#23613;&#31649;&#35813;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#21147;&#21644;&#20581;&#22766;&#24615;&#12290;&#23427;&#22312;&#30001;288&#20010;&#22522;&#20934;&#27979;&#35797;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CHC&#27714;&#35299;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#21253;&#21547;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind a wide range of verification and analysis tasks. Data-driven approaches show great promise in improving CHC solving without the painstaking manual effort of creating and tuning various heuristics. However, a large performance gap exists between data-driven CHC solvers and symbolic reasoning-based solvers. In this work, we develop a simple but effective framework, "Chronosymbolic Learning", which unifies symbolic information and numerical data points to solve a CHC system efficiently. We also present a simple instance of Chronosymbolic Learning with a data-driven learner and a BMC-styled reasoner. Despite its great simplicity, experimental results show the efficacy and robustness of our tool. It outperforms state-of-the-art CHC solvers on a dataset consisting of 288 benchmarks, including many instances with non-linear integer arithmetics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;EVT&#29702;&#35770;&#24320;&#21457;&#20102;&#19968;&#20221;&#38382;&#21367;&#65292;&#35843;&#26597;&#20102;405&#21517;&#23398;&#29983;&#23545;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30475;&#27861;&#21644;&#24847;&#24895;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#29983;&#35748;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#20351;&#29992;&#20215;&#20540;&#21644;&#36739;&#23567;&#30340;&#24863;&#30693;&#25104;&#26412;&#65292;&#20294;&#22312;&#25512;&#24191;&#24212;&#29992;&#26102;&#38656;&#35201;&#32771;&#34385;&#28508;&#22312;&#30340;&#38271;&#26399;&#24433;&#21709;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01186</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#20215;&#20540;&#29702;&#35770;&#65288;EVT&#65289;&#20026;&#22522;&#30784;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#23398;&#29983;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deconstructing Student Perceptions of Generative AI (GenAI) through an Expectancy Value Theory (EVT)-based Instrument. (arXiv:2305.01186v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;EVT&#29702;&#35770;&#24320;&#21457;&#20102;&#19968;&#20221;&#38382;&#21367;&#65292;&#35843;&#26597;&#20102;405&#21517;&#23398;&#29983;&#23545;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30475;&#27861;&#21644;&#24847;&#24895;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#29983;&#35748;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#20351;&#29992;&#20215;&#20540;&#21644;&#36739;&#23567;&#30340;&#24863;&#30693;&#25104;&#26412;&#65292;&#20294;&#22312;&#25512;&#24191;&#24212;&#29992;&#26102;&#38656;&#35201;&#32771;&#34385;&#28508;&#22312;&#30340;&#38271;&#26399;&#24433;&#21709;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#23398;&#29983;&#23545;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30475;&#27861;&#19982;&#24847;&#24895;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#26399;&#26395;&#20215;&#20540;&#29702;&#35770;&#65288;EVT&#65289;&#24320;&#21457;&#20102;&#19968;&#20221;&#38382;&#21367;&#65292;&#29992;&#20110;&#27979;&#37327;&#23398;&#29983;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30693;&#35782;&#12289;&#24863;&#30693;&#20215;&#20540;&#21644;&#24863;&#30693;&#25104;&#26412;&#12290;&#20849;&#26377;405&#21517;&#23398;&#29983;&#21442;&#19982;&#20102;&#26412;&#30740;&#31350;&#65292;&#30830;&#35748;&#24615;&#22240;&#32032;&#20998;&#26512;&#34987;&#29992;&#26469;&#39564;&#35777;&#26500;&#24314;&#30340;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#29983;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;&#20215;&#20540;&#21644;&#20351;&#29992;&#24847;&#24895;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#27491;&#30456;&#20851;&#24615;&#65292;&#32780;&#24863;&#30693;&#25104;&#26412;&#19982;&#20351;&#29992;&#24847;&#24895;&#20043;&#38388;&#21017;&#23384;&#22312;&#24369;&#36127;&#30456;&#20851;&#24615;&#12290;&#22312;&#25105;&#20204;&#25345;&#32493;&#25506;&#32034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#24433;&#21709;&#26102;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#28508;&#22312;&#30340;&#38271;&#26399;&#24433;&#21709;&#21644;&#21487;&#33021;&#23548;&#33268;&#30340;&#20262;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the relationship between student perceptions and their intention to use generative AI in higher education. Drawing on Expectancy-Value Theory (EVT), a questionnaire was developed to measure students' knowledge of generative AI, perceived value, and perceived cost. A sample of 405 students participated in the study, and confirmatory factor analysis was used to validate the constructs. The results indicate a strong positive correlation between perceived value and intention to use generative AI, and a weak negative correlation between perceived cost and intention to use. As we continue to explore the implications of generative AI in education and other domains, it is crucial to carefully consider the potential long-term consequences and the ethical dilemmas that may arise from widespread adoption.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01157</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#30784;&#36923;&#36753;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#20960;&#20309;&#26469;&#23884;&#20837;&#23454;&#20307;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#25805;&#20316;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#22797;&#26434;&#26597;&#35810;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#30693;&#35782;&#22270;&#35889;&#25277;&#35937;&#25512;&#29702;&#65288;LARK&#65289;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#20197;&#20998;&#21035;&#21033;&#29992;&#22270;&#24418;&#25552;&#21462;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20915;&#31574;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;RKGCN&#65292;&#23427;&#21160;&#24577;&#20998;&#26512;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#23427;&#22312;&#29289;&#21697;&#21644;&#29992;&#25143;&#21452;&#26041;&#38754;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#20016;&#23500;&#23427;&#20204;&#30340;&#34920;&#31034;&#65292;&#26368;&#22823;&#21270;&#30693;&#35782;&#22270;&#35889;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290; RKGCN&#33021;&#22815;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20381;&#36182;&#32467;&#26500;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;TGN&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01128</link><description>&lt;p&gt;
&#19981;&#21516;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of different temporal graph neural network configurations on dynamic graphs. (arXiv:2305.01128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20381;&#36182;&#32467;&#26500;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;TGN&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#26512;&#21160;&#24577;&#22270;&#65288;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#22270;&#65289;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGN&#65289;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#21160;&#24577;&#22270;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;TGN&#27169;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25628;&#32034;&#20173;&#22312;&#36827;&#34892;&#20013;&#12290;&#36817;&#26399;&#65292;Pytorch Geometric Temporal&#25552;&#20379;&#20102;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#36824;&#27809;&#26377;&#29992;&#19981;&#21516;&#30340;TGN&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#20197;&#24314;&#31435;&#26368;&#20808;&#36827;&#30340;&#29366;&#20917;&#12290;&#22240;&#27492;&#65292;&#26412;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23545;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20381;&#36182;&#32467;&#26500;&#23398;&#20064;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#36873;&#23450;TGN&#27169;&#22411;&#22312;&#33410;&#28857;&#21644;&#36793;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#23545;&#26368;&#20339;&#34920;&#29616;TGN&#27169;&#22411;&#30340;&#19981;&#21516;&#21464;&#20307;&#36827;&#34892;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#20197;&#25506;&#32034;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increasing interest in the use of graph neural networks (GNNs) for analyzing dynamic graphs, which are graphs that evolve over time. However, there is still a lack of understanding of how different temporal graph neural network (TGNs) configurations can impact the accuracy of predictions on dynamic graphs. Moreover, the hunt for benchmark datasets for these TGNs models is still ongoing. Up until recently, Pytorch Geometric Temporal came up with a few benchmark datasets but most of these datasets have not been analyzed with different TGN models to establish the state-of-the-art. Therefore, this project aims to address this gap in the literature by performing a qualitative analysis of spatial-temporal dependence structure learning on dynamic graphs, as well as a comparative study of the effectiveness of selected TGNs on node and edge prediction tasks. Additionally, an extensive ablation study will be conducted on different variants of the best-performin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#20154;&#31867;&#36827;&#34892;&#21338;&#24328;&#65292;&#21457;&#29616;&#31639;&#27861;&#20250;&#25910;&#25947;&#20110;&#21338;&#24328;&#35770;&#22343;&#34913;&#65292;&#21516;&#26102;&#33258;&#36866;&#24212;&#26426;&#22120;&#30340;&#23384;&#22312;&#20250;&#23548;&#33268;&#20154;&#31867;&#34892;&#20026;&#21521;&#26356;&#21152;&#21093;&#21066;&#24615;&#31574;&#30053;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2305.01124</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#33258;&#36866;&#24212;&#26426;&#22120;&#30340;&#36866;&#24212;&#36235;&#21521;&#20110;&#21338;&#24328;&#35770;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human adaptation to adaptive machines converges to game-theoretic equilibria. (arXiv:2305.01124v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#20154;&#31867;&#36827;&#34892;&#21338;&#24328;&#65292;&#21457;&#29616;&#31639;&#27861;&#20250;&#25910;&#25947;&#20110;&#21338;&#24328;&#35770;&#22343;&#34913;&#65292;&#21516;&#26102;&#33258;&#36866;&#24212;&#26426;&#22120;&#30340;&#23384;&#22312;&#20250;&#23548;&#33268;&#20154;&#31867;&#34892;&#20026;&#21521;&#26356;&#21152;&#21093;&#21066;&#24615;&#31574;&#30053;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26426;&#22120;&#26377;&#28508;&#21147;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#24110;&#21161;&#25110;&#24178;&#25200;&#20154;&#31867;&#34892;&#20026;&#65292;&#20174;&#35748;&#30693;&#20915;&#31574;&#21040;&#29289;&#29702;&#35774;&#22791;&#25588;&#21161;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#23588;&#20026;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#30446;&#26631;&#19982;&#20154;&#31867;&#30446;&#26631;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#12290;&#30001;&#20110;&#20154;&#31867;&#20351;&#29992;&#26174;&#24335;&#21644;&#38544;&#21547;&#31574;&#30053;&#19981;&#26029;&#36866;&#24212;&#20182;&#20204;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#24403;&#29615;&#22659;&#20013;&#21253;&#21547;&#33258;&#36866;&#24212;&#26426;&#22120;&#26102;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23558;&#36827;&#34892;&#21338;&#24328;&#12290;&#21338;&#24328;&#35770;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#20004;&#20010;&#25110;&#22810;&#20010;&#20915;&#31574;&#32773;&#20043;&#38388;&#20132;&#20114;&#30340;&#25104;&#29087;&#26694;&#26550;&#65292;&#22312;&#32463;&#27982;&#24066;&#22330;&#21644;&#26426;&#22120;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#20010;&#20307;&#20154;&#31867;&#30340;&#36866;&#24212;&#22914;&#20309;&#21463;&#33258;&#36866;&#24212;&#26426;&#22120;&#20132;&#20114;&#24433;&#21709;&#36827;&#34892;&#20551;&#35774;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#19968;&#33324;&#21644;&#28216;&#25103;&#30340;&#29609;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25910;&#25947;&#20110;&#33879;&#21517;&#30340;&#21338;&#24328;&#35770;&#22343;&#34913;&#65292;&#23613;&#31649;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#33258;&#36866;&#24212;&#26426;&#22120;&#30340;&#23384;&#22312;&#21487;&#20197;&#23548;&#33268;&#20154;&#31867;&#34892;&#20026;&#21521;&#26356;&#21152;&#21093;&#21066;&#24615;&#30340;&#31574;&#30053;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#22312;&#33258;&#36866;&#24212;&#26426;&#22120;&#30340;&#35774;&#35745;&#20013;&#32771;&#34385;&#20154;&#26426;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24314;&#35758;&#21338;&#24328;&#35770;&#21487;&#20197;&#26159;&#39044;&#27979;&#36825;&#20123;&#24773;&#22659;&#19979;&#20154;&#31867;&#34892;&#20026;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive machines have the potential to assist or interfere with human behavior in a range of contexts, from cognitive decision-making to physical device assistance. Therefore it is critical to understand how machine learning algorithms can influence human actions, particularly in situations where machine goals are misaligned with those of people. Since humans continually adapt to their environment using a combination of explicit and implicit strategies, when the environment contains an adaptive machine, the human and machine play a game. Game theory is an established framework for modeling interactions between two or more decision-makers that has been applied extensively in economic markets and machine algorithms. However, existing approaches make assumptions about, rather than empirically test, how adaptation by individual humans is affected by interaction with an adaptive machine. Here we tested learning algorithms for machines playing general-sum games with human subjects. Our algo
&lt;/p&gt;</description></item><item><title>CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01118</link><description>&lt;p&gt;
CSP&#65306;&#38024;&#23545;&#22320;&#29702;&#31354;&#38388;&#35270;&#35273;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#31354;&#38388;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01118
&lt;/p&gt;
&lt;p&gt;
CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#20844;&#24320;&#21487;&#29992;&#65292;&#32780;&#23545;&#35937;&#31867;&#21035;&#31561;&#26631;&#31614;&#21017;&#30456;&#23545;&#31232;&#32570;&#19988;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20165;&#38656;&#24456;&#23569;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#26159;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#23545;&#35937;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#22312;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#38454;&#27573;&#30452;&#25509;&#21033;&#29992;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20016;&#23500;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; Contrastive Spatial Pre-Training&#65288;CSP&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#23545;&#27604;&#30446;&#26631;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#36716;&#31227;&#21040;&#19979;&#28216;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CSP&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#30340;&#34892;&#20154;&#24847;&#21521;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#34892;&#20154;&#21644;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01111</link><description>&lt;p&gt;
&#34892;&#20154;&#24847;&#21521;&#39044;&#27979;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Local and Global Contextual Features Fusion for Pedestrian Intention Prediction. (arXiv:2305.01111v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#30340;&#34892;&#20154;&#24847;&#21521;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#34892;&#20154;&#21644;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#27491;&#22312;&#25104;&#20026;&#26410;&#26469;&#20132;&#36890;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#20294;&#26159;&#65292;&#23433;&#20840;&#25361;&#25112;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#12290;&#19982;&#34892;&#20154;&#30340;&#20132;&#20114;&#21253;&#25324;&#8220;&#39044;&#27979;&#34892;&#20154;&#30340;&#31359;&#36234;&#24847;&#21521;&#8221;&#20540;&#24471;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#36947;&#36335;&#19978;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#21462;&#21644;&#20998;&#26512;&#34892;&#20154;&#21644;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#26102;&#31354;&#35270;&#35273;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#30340;&#24847;&#21521;&#39044;&#27979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles (AVs) are becoming an indispensable part of future transportation. However, safety challenges and lack of reliability limit their real-world deployment. Towards boosting the appearance of AVs on the roads, the interaction of AVs with pedestrians including "prediction of the pedestrian crossing intention" deserves extensive research. This is a highly challenging task as involves multiple non-linear parameters. In this direction, we extract and analyse spatio-temporal visual features of both pedestrian and traffic contexts. The pedestrian features include body pose and local context features that represent the pedestrian's behaviour. Additionally, to understand the global context, we utilise location, motion, and environmental information using scene parsing technology that represents the pedestrian's surroundings, and may affect the pedestrian's intention. Finally, these multi-modality features are intelligently fused for effective intention prediction learning. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#20013;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#38382;&#39064;&#12290;&#20351;&#29992;&#36710;&#21040;&#36710;&#36890;&#20449;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#36710;&#36742;&#30340;&#21152;&#36895;&#24230;&#20449;&#24687;&#65292;&#36890;&#36807;LSTM&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#24403;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.01096</link><description>&lt;p&gt;
&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Novel Model for Driver Lane Change Prediction in Cooperative Adaptive Cruise Control Systems. (arXiv:2305.01096v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#20013;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#38382;&#39064;&#12290;&#20351;&#29992;&#36710;&#21040;&#36710;&#36890;&#20449;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#36710;&#36742;&#30340;&#21152;&#36895;&#24230;&#20449;&#24687;&#65292;&#36890;&#36807;LSTM&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#24403;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21464;&#36947;&#39044;&#27979;&#21487;&#20197;&#20943;&#23569;&#28508;&#22312;&#20107;&#25925;&#65292;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;&#12290;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#65292;&#36710;&#36947;&#20559;&#31163;&#35686;&#21578;&#65288;LDA&#65289;&#21644;&#36710;&#36947;&#20445;&#25345;&#36741;&#21161;&#65288;LKA&#65289;&#26159;&#19968;&#20123;&#39640;&#32423;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#20256;&#32479;&#27169;&#22359;&#12290;&#26377;&#20102;&#36710;&#21040;&#36710;&#36890;&#20449;&#65288;V2V&#65289;&#65292;&#36710;&#36742;&#21487;&#20197;&#19982;&#21608;&#22260;&#36710;&#36742;&#20849;&#20139;&#20132;&#36890;&#20449;&#24687;&#65292;&#23454;&#29616;&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;CACC&#65289;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#25152;&#38656;&#30340;&#20449;&#24687;&#31867;&#22411;&#65288;&#20301;&#32622;&#12289;&#36895;&#24230;&#12289;&#21152;&#36895;&#24230;&#65289;&#21644;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;HighD&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;LSTM&#65288;&#38271;&#30701;&#26399;&#35760;&#24518;&#65289;&#27169;&#22411;&#26469;&#39044;&#27979;&#21464;&#36947;&#24847;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#24615;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate lane change prediction can reduce potential accidents and contribute to higher road safety. Adaptive cruise control (ACC), lane departure avoidance (LDA), and lane keeping assistance (LKA) are some conventional modules in advanced driver assistance systems (ADAS). Thanks to vehicle-to-vehicle communication (V2V), vehicles can share traffic information with surrounding vehicles, enabling cooperative adaptive cruise control (CACC). While ACC relies on the vehicle's sensors to obtain the position and velocity of the leading vehicle, CACC also has access to the acceleration of multiple vehicles through V2V communication. This paper compares the type of information (position, velocity, acceleration) and the number of surrounding vehicles for driver lane change prediction. We trained an LSTM (Long Short-Term Memory) on the HighD dataset to predict lane change intention. Results indicate a significant improvement in accuracy with an increase in the number of surrounding vehicles and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01095</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22120;&#65292;&#22312;&#36710;&#36947;&#21464;&#25442;&#26102;&#21487;&#20197;&#39044;&#27979;&#20808;&#21069;&#36710;&#36742;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#35843;&#33410;&#36710;&#36895;&#26469;&#30830;&#20445;&#19982;&#21069;&#36710;&#23433;&#20840;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ACC&#31995;&#32479;&#26080;&#27861;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39550;&#39542;&#26465;&#20214;&#21644;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20197;&#24448;&#30340;&#39550;&#39542;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#23454;&#26102;&#22320;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#30340;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#26159;&#22522;&#20110;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#39640;D&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;&#35013;&#22791;&#26377;&#25668;&#20687;&#22836;&#30340;&#26080;&#20154;&#26426;&#22312;&#24503;&#22269;&#39640;&#36895;&#20844;&#36335;&#19978;&#33719;&#21462;&#30340;&#12290;&#25105;&#20204;&#22312;&#20391;&#38754;&#36710;&#36947;&#21069;&#36710;&#21098;&#20999;&#24182;&#24378;&#21046;&#30446;&#26631;&#39550;&#39542;&#21592;&#20943;&#36895;&#30340;&#28608;&#36827;&#36710;&#36947;&#21464;&#21270;&#26102;&#35780;&#20272;&#20102;ACC&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#39304;&#36865;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#20013;&#26399;&#26395;&#30340;&#27169;&#20307;&#35745;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.01089</link><description>&lt;p&gt;
&#35745;&#31639;&#21487;&#20132;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#26399;&#26395;&#27169;&#20307;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Computing Expected Motif Counts for Exchangeable Graph Generative Models. (arXiv:2305.01089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#20013;&#26399;&#26395;&#30340;&#27169;&#20307;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#22270;&#24418;&#32479;&#35745;&#37327;&#30340;&#26399;&#26395;&#20540;&#26159;&#20351;&#29992;&#21644;&#23398;&#20064;&#22270;&#27169;&#22411;&#30340;&#37325;&#35201;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#29992;&#20110;&#26399;&#26395;&#27169;&#20307;&#35745;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#32479;&#35745;&#37327;&#31867;&#22411;&#12290;&#35813;&#31243;&#24207;&#36866;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#65292;&#36825;&#31181;&#27169;&#22411;&#24120;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#22270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the expected value of a graph statistic is an important inference task for using and learning graph models. This note presents a scalable estimation procedure for expected motif counts, a widely used type of graph statistic. The procedure applies for generative mixture models of the type used in neural and Bayesian approaches to graph data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;AI&#21644;&#21306;&#22359;&#38142;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23433;&#20840;&#35748;&#35777;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;AI&#21644;&#21306;&#22359;&#38142;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;&#25945;&#32946;&#23454;&#36341;&#20013;&#12290;&#24635;&#20043;&#65292;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#26377;&#26395;&#25104;&#20026;&#21487;&#25345;&#32493;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.01088</link><description>&lt;p&gt;
AI&#21644;&#21306;&#22359;&#38142;&#20316;&#20026;&#21487;&#25345;&#32493;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#26469;&#24212;&#23545;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
AI &amp; Blockchain as sustainable teaching and learning tools to cope with the 4IR. (arXiv:2305.01088v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;AI&#21644;&#21306;&#22359;&#38142;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23433;&#20840;&#35748;&#35777;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;AI&#21644;&#21306;&#22359;&#38142;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;&#25945;&#32946;&#23454;&#36341;&#20013;&#12290;&#24635;&#20043;&#65292;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#26377;&#26395;&#25104;&#20026;&#21487;&#25345;&#32493;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#27491;&#22312;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#27963;&#21644;&#24037;&#20316;&#65292;&#32780;&#25945;&#32946;&#20063;&#19981;&#20363;&#22806;&#12290;&#20026;&#20102;&#24212;&#23545;4IR&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#21019;&#26032;&#21644;&#21487;&#25345;&#32493;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#36825;&#26041;&#38754;&#25317;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22914;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23433;&#20840;&#35748;&#35777;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#32593;&#32476;&#12290;&#26412;&#25991;&#23545;AI&#21644;&#21306;&#22359;&#38142;&#22312;&#25945;&#32946;&#20013;&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20998;&#26512;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#28508;&#22312;&#21033;&#30410;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27169;&#22411;&#65292;&#23558;AI&#21644;&#21306;&#22359;&#38142;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#23454;&#36341;&#20013;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#38656;&#35201;&#26356;&#22810;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#25506;&#32034;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#26412;&#25991;&#35752;&#35770;&#30340;&#20851;&#38190;&#25688;&#35201;&#26159;&#65292;&#36890;&#36807;&#25552;&#39640;&#25945;&#32946;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;AI&#21644;&#21306;&#22359;&#38142;&#26377;&#28508;&#21147;&#25104;&#20026;&#21487;&#25345;&#32493;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fourth Industrial Revolution (4IR) is transforming the way we live and work, and education is no exception. To cope with the challenges of 4IR, there is a need for innovative and sustainable teaching and learning tools. AI and block chain technologies hold great promise in this regard, with potential benefits such as personalized learning, secure credentialing, and decentralized learning networks. This paper presents a review of existing research on AI and block chain in education, analyzing case studies and exploring the potential benefits and challenges of these technologies. The paper also suggests a unique model for integrating AI and block chain into sustainable teaching and learning practices. Future research directions are discussed, including the need for more empirical studies and the exploration of ethical and social implications. The key summary of this discussion is that, by enhancing accessibility, efficacy, and security in education, AI and blockchain have the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01063</link><description>&lt;p&gt;
&#19987;&#19994;&#30693;&#35782;&#26641;&#22312;&#38598;&#20307;&#20915;&#31574;&#20013;&#35299;&#20915;&#30693;&#35782;&#23616;&#38480;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#30340;&#19987;&#23478;&#24448;&#24448;&#20250;&#26174;&#31034;&#20986;&#38543;&#38382;&#39064;&#23454;&#20363;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#23569;&#25968;&#24773;&#20917;&#30340;&#27425;&#20248;&#25110;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#30693;&#35782;&#28145;&#24230;&#21644;&#24191;&#24230;&#30340;&#21464;&#21270;&#24314;&#27169;&#20026;&#23558;&#38382;&#39064;&#31354;&#38388;&#21010;&#20998;&#20026;&#19981;&#21516;&#19987;&#19994;&#30693;&#35782;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#31639;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#32771;&#34385;&#24182;&#36866;&#24212;&#38382;&#39064;&#23454;&#20363;&#19982;&#19987;&#23478;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#24378;&#35843;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#26597;&#35810;&#30340;&#22825;&#30495;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19987;&#19994;&#30693;&#35782;&#26641;&#65292;&#23427;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#29616;&#26377;&#26041;&#27861;&#34987;&#35777;&#26126;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Venn&#22270;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#22235;&#20010;&#31867;&#21035;&#32553;&#20943;&#20026;&#20004;&#20010;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01044</link><description>&lt;p&gt;
Venn&#22270;&#22810;&#26631;&#31614;&#20998;&#31867;&#35299;&#37322;&#19982;&#39068;&#33394;&#21644;&#28165;&#26224;&#24230;&#22686;&#24378;&#30340;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;
&lt;/p&gt;
&lt;p&gt;
Venn Diagram Multi-label Class Interpretation of Diabetic Foot Ulcer with Color and Sharpness Enhancement. (arXiv:2305.01044v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Venn&#22270;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#22235;&#20010;&#31867;&#21035;&#32553;&#20943;&#20026;&#20004;&#20010;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#26159;&#31958;&#23615;&#30149;&#30340;&#20005;&#37325;&#24182;&#21457;&#30151;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#27835;&#30103;&#65292;&#21487;&#33021;&#23548;&#33268;&#19979;&#32930;&#25130;&#32930;&#12290;&#21463;2021&#24180;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#22823;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;DFU&#30340;&#33258;&#21160;&#21270;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#21253;&#25324;&#24863;&#26579;&#12289;&#32570;&#34880;&#12289;&#36825;&#20004;&#31181;&#24773;&#20917;&#20197;&#21450;&#20197;&#19978;&#20004;&#31181;&#24773;&#20917;&#22343;&#19981;&#23646;&#20110;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#20998;&#31867;&#20934;&#30830;&#24230;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Venn &#22270;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#22522;&#20110; CNN &#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#22810;&#31867;&#21035; DFU &#20998;&#31867;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#22235;&#20010;&#31867;&#21035;&#20943;&#23569;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#22240;&#20026;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#20260;&#21475;&#21487;&#20197;&#35299;&#37322;&#20026;&#24863;&#26579;&#21644;&#32570;&#34880;&#30340;&#21516;&#26102;&#21457;&#29983;&#65292;&#32780;&#26080;&#31867;&#21035;&#20260;&#21475;&#21017;&#34920;&#31034;&#32570;&#20047;&#24863;&#26579;&#21644;&#32570;&#34880;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Venn &#22270;&#34920;&#31034;&#22359;&#65292;&#29992;&#20110;&#35299;&#37322;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#22235;&#20010;&#31867;&#12290;&#20026;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20855;&#24377;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#22686;&#24378;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
DFU is a severe complication of diabetes that can lead to amputation of the lower limb if not treated properly. Inspired by the 2021 Diabetic Foot Ulcer Grand Challenge, researchers designed automated multi-class classification of DFU, including infection, ischaemia, both of these conditions, and none of these conditions. However, it remains a challenge as classification accuracy is still not satisfactory. This paper proposes a Venn Diagram interpretation of multi-label CNN-based method, utilizing different image enhancement strategies, to improve the multi-class DFU classification. We propose to reduce the four classes into two since both class wounds can be interpreted as the simultaneous occurrence of infection and ischaemia and none class wounds as the absence of infection and ischaemia. We introduce a novel Venn Diagram representation block in the classifier to interpret all four classes from these two classes. To make our model more resilient, we propose enhancing the perceptual 
&lt;/p&gt;</description></item><item><title>CLIP-S$^4$&#26159;&#19968;&#31181;&#26080;&#38656;&#20687;&#32032;&#32423;&#26631;&#27880;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#35821;&#35328;&#24341;&#23548;&#33258;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#20687;&#32032;&#34920;&#31034;&#23398;&#20064;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#21508;&#31181;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.01040</link><description>&lt;p&gt;
CLIP-S$^4$: &#35821;&#35328;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation. (arXiv:2305.01040v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01040
&lt;/p&gt;
&lt;p&gt;
CLIP-S$^4$&#26159;&#19968;&#31181;&#26080;&#38656;&#20687;&#32032;&#32423;&#26631;&#27880;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#35821;&#35328;&#24341;&#23548;&#33258;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#20687;&#32032;&#34920;&#31034;&#23398;&#20064;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#21508;&#31181;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIP-S$^4$&#30340;&#35821;&#35328;&#24341;&#23548;&#33258;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#33258;&#30417;&#30563;&#20687;&#32032;&#34920;&#31034;&#23398;&#20064;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#21508;&#31181;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#21644;&#26410;&#30693;&#31867;&#21035;&#20449;&#24687;&#21363;&#21487;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined classes. In this work, we present CLIP-S$^4$ that leverages self-supervised pixel representation learning and vision-language models to enable various semantic segmentation tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different augmented views of images. To further improve the pixel embeddings and enable language-driven semantic segmentation, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP; and 2) semantic consistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.01034</link><description>&lt;p&gt;
&#26080;&#29305;&#23450;&#27169;&#22411;&#27867;&#21270;&#38590;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24230;&#37327;&#26159;&#20854;&#21487;&#20197;&#25191;&#34892;&#30340;&#20219;&#21153;&#38590;&#24230;&#65292;&#36275;&#22815;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#30340;&#27867;&#21270;&#38590;&#24230;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#20219;&#21153;&#22266;&#26377;&#27867;&#21270;&#38590;&#24230;&#30340;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#12290;&#36890;&#36807;&#27979;&#37327;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#20551;&#35774;&#22312;&#20219;&#21153;&#20013;&#27867;&#21270;&#30340;&#20998;&#25968;&#21344;&#25454;&#30340;&#23481;&#31215;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23427;&#19982;&#27169;&#22411;&#24517;&#39035;&#27867;&#21270;&#30340;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#25968;&#25104;&#25351;&#25968;&#27604;&#20363;&#65292;&#20294;&#20165;&#22312;&#27599;&#20010;&#32500;&#24230;&#30340;&#20998;&#36776;&#29575;&#19978;&#21576;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#34920;&#26126;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#30340;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35821;&#29992;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36890;&#36807;&#34920;&#36848;&#26377;&#20851;&#31561;&#32423;&#24418;&#23481;&#35789;&#8220;&#24378;&#8221;&#30340;&#38376;&#27099;&#20272;&#35745;&#26469;&#27979;&#35797; LLM &#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616; LLM &#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12289;&#31867;&#20154;&#30340;&#20998;&#24067;&#65292;&#20294;&#22312;&#32452;&#21512;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.01020</link><description>&lt;p&gt;
&#23558;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#29992;&#25512;&#29702;&#30340;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating statistical language models as pragmatic reasoners. (arXiv:2305.01020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35821;&#29992;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36890;&#36807;&#34920;&#36848;&#26377;&#20851;&#31561;&#32423;&#24418;&#23481;&#35789;&#8220;&#24378;&#8221;&#30340;&#38376;&#27099;&#20272;&#35745;&#26469;&#27979;&#35797; LLM &#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616; LLM &#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12289;&#31867;&#20154;&#30340;&#20998;&#24067;&#65292;&#20294;&#22312;&#32452;&#21512;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#35821;&#35328;&#21644;&#39044;&#26399;&#24847;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#26159;&#27010;&#29575;&#24615;&#30340;&#65292;&#32780;&#19988;&#23545;&#35821;&#22659;&#38750;&#24120;&#25935;&#24863;&#12290;&#35768;&#22810;&#31574;&#30053;&#35797;&#22270;&#20272;&#35745;&#36825;&#31181;&#26144;&#23556;&#65292;&#36890;&#24120;&#21033;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#36882;&#24402;&#27169;&#22411;&#30340;&#36890;&#20449;&#26041;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#65292;&#20219;&#21153;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#36923;&#36753;&#34920;&#31034;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340; LLM &#25506;&#32034;&#20027;&#35201;&#23616;&#38480;&#20110;&#23383;&#38754;&#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102; LLM &#25512;&#26029;&#35821;&#29992;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#8220;&#24378;&#8221;&#36825;&#20010;&#31561;&#32423;&#24418;&#23481;&#35789;&#30340;&#38376;&#27099;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#20174;&#20855;&#26377;&#24378;&#24230;&#20808;&#39564;&#26465;&#20214;&#30340;&#35821;&#22659;&#20986;&#21457;&#65292;&#28982;&#21518;&#25193;&#23637;&#21040;&#38480;&#23450;&#12289;&#21542;&#23450;&#12289;&#26497;&#24615;&#21453;&#36716;&#21644;&#31867;&#27604;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs &#21487;&#20197;&#25512;&#23548;&#20986;&#19982;&#35821;&#22659;&#30456;&#20851;&#30340;&#31867;&#20154;&#20998;&#24067;&#65292;&#28041;&#21450;&#21040;&#20960;&#20010;&#22797;&#26434;&#35821;&#29992;&#35805;&#35821;&#30340;&#35299;&#37322;&#65292;&#20294;&#22312;&#32452;&#21512;&#19978;&#36824;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between communicated language and intended meaning is often probabilistic and sensitive to context. Numerous strategies attempt to estimate such a mapping, often leveraging recursive Bayesian models of communication. In parallel, large language models (LLMs) have been increasingly applied to semantic parsing applications, tasked with inferring logical representations from natural language. While existing LLM explorations have been largely restricted to literal language use, in this work, we evaluate the capacity of LLMs to infer the meanings of pragmatic utterances. Specifically, we explore the case of threshold estimation on the gradable adjective ``strong'', contextually conditioned on a strength prior, then extended to composition with qualification, negation, polarity inversion, and class comparison. We find that LLMs can derive context-grounded, human-like distributions over the interpretations of several complex pragmatic utterances, yet struggle composing with n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#20803;&#36923;&#36753;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#30830;&#23450;&#30340;&#27604;&#29305;&#20540;&#21644;&#19981;&#23384;&#22312;&#30340;&#27604;&#29305;&#65288;&#30495;&#31354;&#24577;&#65289;&#65292;&#19982;&#26631;&#20934;&#30340;&#20108;&#20803;&#36923;&#36753;&#31995;&#32479;&#30456;&#27604;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#19988;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#20108;&#36827;&#21046;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.00984</link><description>&lt;p&gt;
&#19977;&#20803;&#30636;&#26102;&#22122;&#22768;&#36923;&#36753;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Ternary Instantaneous Noise-based Logic. (arXiv:2305.00984v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#20803;&#36923;&#36753;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#30830;&#23450;&#30340;&#27604;&#29305;&#20540;&#21644;&#19981;&#23384;&#22312;&#30340;&#27604;&#29305;&#65288;&#30495;&#31354;&#24577;&#65289;&#65292;&#19982;&#26631;&#20934;&#30340;&#20108;&#20803;&#36923;&#36753;&#31995;&#32479;&#30456;&#27604;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#19988;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#20108;&#36827;&#21046;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#20540;&#30636;&#26102;&#22122;&#22768;&#36923;&#36753;&#30340;&#21487;&#33021;&#34920;&#31034;&#26041;&#27861;&#12290;&#31532;&#19977;&#20010;&#20540;&#26159;&#19981;&#30830;&#23450;&#30340;&#27604;&#29305;&#20540;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#21516;&#26102;&#65292;&#36824;&#26377;&#31532;&#22235;&#20010;&#20540;&#65292;&#23427;&#21487;&#20197;&#34920;&#31034;&#19981;&#23384;&#22312;&#30340;&#27604;&#29305;&#65288;&#30495;&#31354;&#24577;&#65289;&#65292;&#23545;&#20110;&#25152;&#26377;&#27604;&#29305;&#26469;&#35828;&#65292;&#23427;&#26159;&#30456;&#21516;&#30340;&#65288;1&#20010;&#25968;&#20540;&#65289;&#65292;&#20294;&#21364;&#26159;&#25152;&#26377;&#27604;&#29305;&#30340;&#21387;&#32553;&#24577;&#12290;&#19968;&#20123;&#36923;&#36753;&#38376;&#20063;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#19977;&#20803;&#23431;&#23449;&#19982;&#26631;&#20934;&#30340;&#20108;&#20803;&#23431;&#23449;&#30456;&#27604;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#65306;&#22312;&#20219;&#20309;&#26102;&#38047;&#21608;&#26399;&#20869;&#65292;&#20854;&#24133;&#24230;&#27704;&#36828;&#19981;&#20026;&#38646;&#12290;&#25152;&#26377;&#24050;&#30693;&#30340;&#20108;&#36827;&#21046;&#36923;&#36753;&#38376;&#22312;&#20108;&#20803;&#27604;&#29305;&#20540;&#19978;&#30340;&#24037;&#20316;&#26041;&#24335;&#19982;&#20808;&#21069;&#30456;&#21516;&#65292;&#22240;&#27492;&#26087;&#30340;&#20108;&#36827;&#21046;&#31639;&#27861;&#21487;&#20197;&#22312;&#19977;&#20803;&#31995;&#32479;&#20013;&#36816;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#65292;&#24182;&#19988;&#19981;&#20250;&#20986;&#29616;&#23431;&#23449;&#38646;&#20540;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the possible representations of three-valued instantaneous noise-based logic is proposed. The third value is an uncertain bit value, which can be useful in artificial intelligence applications. There is a forth value, too, that can represent a non-existing bit (vacuum-state) that is the same (1 numeric value) for all bits, however that is a squeezed state common for all bits. Some logic gates are explored. The ternary Universe has a significant advantage compared to the standard binary one: its amplitude is never zero during any clock period. All the known binary logic gates work for the binary bit values in the same way as earlier therefore the former binary algorithms can be run in the ternary system with no change and without the problems posed by zero values of the Universe.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#38454;&#27573;&#22522;&#20110;&#21452;Copula&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#24037;&#25511;&#31995;&#32479;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00982</link><description>&lt;p&gt;
&#24037;&#25511;&#31995;&#32479;&#20013;&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#38454;&#27573;&#21452;Copula&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two-phase Dual COPOD Method for Anomaly Detection in Industrial Control System. (arXiv:2305.00982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#38454;&#27573;&#22522;&#20110;&#21452;Copula&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#24037;&#25511;&#31995;&#32479;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#22788;&#29702;&#35774;&#26045;&#21644;&#30005;&#31449;&#31561;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#20381;&#36182;&#20110;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#65288;ICS&#65289;&#36827;&#34892;&#30417;&#25511;&#21644;&#25511;&#21046;&#65292;&#36825;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#21644;&#31995;&#32479;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;ICS&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#23454;&#36341;&#32773;&#38590;&#20197;&#29702;&#35299;&#21644;&#20449;&#20219;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#38454;&#27573;&#30340;&#21452;Copula&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical infrastructures like water treatment facilities and power plants depend on industrial control systems (ICS) for monitoring and control, making them vulnerable to cyber attacks and system malfunctions. Traditional ICS anomaly detection methods lack transparency and interpretability, which make it difficult for practitioners to understand and trust the results. This paper proposes a two-phase dual Copula-based Outlier Detection (COPOD) method that addresses these challenges. The first phase removes unwanted outliers using an empirical cumulative distribution algorithm, and the second phase develops two parallel COPOD models based on the output data of phase 1. The method is based on empirical distribution functions, parameter-free, and provides interpretability by quantifying each feature's contribution to an anomaly. The method is also computationally and memory-efficient, suitable for low- and high-dimensional datasets. Experimental results demonstrate superior performance in 
&lt;/p&gt;</description></item><item><title>SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.00795</link><description>&lt;p&gt;
SelfDocSeg: &#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00795
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#25366;&#25496;&#12289;&#35782;&#21035;&#21040;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#12289;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#31561;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#24573;&#30053;&#20102;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#36825;&#19968;&#20851;&#38190;&#20107;&#23454;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#20013;&#29983;&#25104;&#20266;&#24067;&#23616;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30495;&#23454;&#26631;&#31614;&#25110;&#20854;&#23548;&#20986;&#29289;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00654</link><description>&lt;p&gt;
&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#29616;&#23398;&#20064;&#21644;&#25506;&#32034;&#26159;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#20445;&#30041;&#22495;&#20013;&#28508;&#22312;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36824;&#25429;&#25417;&#20102;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#65292;&#20174;&#32780;&#20813;&#36153;&#25552;&#20379;&#20102;&#20266;&#35745;&#25968;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#22495;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#24314;&#31435;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#20801;&#35768;&#23567;&#25209;&#37327;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#20013;&#21560;&#21462;&#28789;&#24863;&#65292;&#24182;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#35774;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#22312;DM-Lab-30&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.12985</link><description>&lt;p&gt;
Rubik&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#29289;&#29702;&#24863;&#30693;&#26059;&#36716;&#32467;&#26500;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12985
&lt;/p&gt;
&lt;p&gt;
RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#25512;&#36827;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;ONNs&#65289;&#65292;&#22312;&#21151;&#29575;&#25928;&#29575;&#65292;&#24182;&#34892;&#24615;&#21644;&#35745;&#31639;&#36895;&#24230;&#26041;&#38754;&#65292;ONNs&#24102;&#26469;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#36890;&#36807;&#23545;ResNet50&#21644;EfficientNetB0&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12289;JPEG&#36755;&#20837;&#21387;&#32553;&#21644;&#28784;&#24230;&#36755;&#20837;&#36716;&#25442;&#31561;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12486</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Adversarial Robustness on Document Image Classification. (arXiv:2304.12486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#36890;&#36807;&#23545;ResNet50&#21644;EfficientNetB0&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12289;JPEG&#36755;&#20837;&#21387;&#32553;&#21644;&#28784;&#24230;&#36755;&#20837;&#36716;&#25442;&#31561;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#19978;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#33267;&#20170;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#38480;&#20110;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23454;&#38469;&#19978;&#22788;&#29702;&#30340;&#26159;&#25991;&#26723;&#25968;&#25454;&#65292;&#36825;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#38750;&#24120;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#23558;&#23545;&#25239;&#25915;&#20987;&#21746;&#23398;&#24212;&#29992;&#20110;&#25991;&#29486;&#21644;&#33258;&#28982;&#25968;&#25454;&#65292;&#24182;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#26080;&#30446;&#26631;&#22522;&#20110;&#26799;&#24230;&#12289;&#22522;&#20110;&#36716;&#31227;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#25915;&#20987;&#19978;&#65292;&#24182;&#35780;&#20272;&#23545;&#25239;&#35757;&#32451;&#12289;JPEG&#36755;&#20837;&#21387;&#32553;&#21644;&#28784;&#24230;&#36755;&#20837;&#36716;&#25442;&#23545;ResNet50&#21644;EfficientNetB0&#27169;&#22411;&#26550;&#26500;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#31038;&#21306;&#27809;&#26377;&#36827;&#34892;&#36825;&#26679;&#30340;&#30740;&#31350;&#20197;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#23545;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks and defenses have gained increasing interest on computer vision systems in recent years, but as of today, most investigations are limited to images. However, many artificial intelligence models actually handle documentary data, which is very different from real world images. Hence, in this work, we try to apply the adversarial attack philosophy on documentary and natural data and to protect models against such attacks. We focus our work on untargeted gradient-based, transfer-based and score-based attacks and evaluate the impact of adversarial training, JPEG input compression and grey-scale input transformation on the robustness of ResNet50 and EfficientNetB0 model architectures. To the best of our knowledge, no such work has been conducted by the community in order to study the impact of these attacks on the document image classification task.
&lt;/p&gt;</description></item><item><title>DocParser&#26159;&#19968;&#31181;&#26080;OCR&#30340;&#31471;&#21040;&#31471;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#26356;&#22909;&#22320;&#25552;&#21462;&#21028;&#21035;&#24615;&#23383;&#31526;&#29305;&#24449;&#65292;&#24182;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12484</link><description>&lt;p&gt;
DocParser&#65306;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#26080;OCR&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents. (arXiv:2304.12484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12484
&lt;/p&gt;
&lt;p&gt;
DocParser&#26159;&#19968;&#31181;&#26080;OCR&#30340;&#31471;&#21040;&#31471;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#26356;&#22909;&#22320;&#25552;&#21462;&#21028;&#21035;&#24615;&#23383;&#31526;&#29305;&#24449;&#65292;&#24182;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#22312;&#20960;&#20010;&#22522;&#20110;&#25991;&#26723;&#25511;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#20854;&#24191;&#27867;&#30340;&#21830;&#19994;&#20215;&#20540;&#65292;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#36827;&#34892;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#24037;&#20316;&#37117;&#36981;&#24490;&#20004;&#27493;&#27861;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#24341;&#25806;&#35835;&#21462;&#25991;&#26412;&#65292;&#28982;&#21518;&#20174;&#25152;&#33719;&#24471;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#23383;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#22806;&#37096;OCR&#31995;&#32479;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#21644;&#35745;&#31639;&#36895;&#24230;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#26080;OCR&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#21463;&#21040;&#20854;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DocParser&#30340;OCR-free&#31471;&#21040;&#31471;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#20854;&#26356;&#22909;&#22320;&#25552;&#21462;&#21028;&#21035;&#24615;&#23383;&#31526;&#29305;&#24449;&#30340;&#33021;&#21147;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;DocParser&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#20381;&#36182;&#20110;OCR&#24341;&#25806;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Extraction from visually rich documents is a challenging task that has gained a lot of attention in recent years due to its importance in several document-control based applications and its widespread commercial value. The majority of the research work conducted on this topic to date follow a two-step pipeline. First, they read the text using an off-the-shelf Optical Character Recognition (OCR) engine, then, they extract the fields of interest from the obtained text. The main drawback of these approaches is their dependence on an external OCR system, which can negatively impact both performance and computational speed. Recent OCR-free methods were proposed to address the previous issues. Inspired by their promising results, we propose in this paper an OCR-free end-to-end information extraction model named DocParser. It differs from prior end-to-end approaches by its ability to better extract discriminative character features. DocParser achieves state-of-the-art results on v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21040;&#23494;&#38598;&#30340;&#20307;&#32032;&#21306;&#22495;&#21152;&#24378;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25237;&#24433;&#27599;&#20010;&#20307;&#32032;&#20869;&#37096;&#30340;&#31232;&#30095;&#23616;&#37096;&#28857;&#20113;&#33719;&#24471;&#20307;&#32032;&#21306;&#22495;&#65292;&#26356;&#22909;&#22320;&#23545;&#40784;&#21644;&#36991;&#20813;&#32972;&#26223;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#34701;&#21512;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08304</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#31232;&#30095;&#21040;&#23494;&#38598;&#20307;&#32032;&#21306;&#22495;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection. (arXiv:2304.08304v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21040;&#23494;&#38598;&#30340;&#20307;&#32032;&#21306;&#22495;&#21152;&#24378;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25237;&#24433;&#27599;&#20010;&#20307;&#32032;&#20869;&#37096;&#30340;&#31232;&#30095;&#23616;&#37096;&#28857;&#20113;&#33719;&#24471;&#20307;&#32032;&#21306;&#22495;&#65292;&#26356;&#22909;&#22320;&#23545;&#40784;&#21644;&#36991;&#20813;&#32972;&#26223;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#34701;&#21512;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#22810;&#27169;&#24577;&#26041;&#27861;&#25104;&#20026;&#36235;&#21183;&#65292;&#20027;&#35201;&#22240;&#20026;LiDAR&#28857;&#20113;&#21644;&#22270;&#20687;&#25968;&#25454;&#30340;&#34917;&#20805;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#28857;&#20113;&#31232;&#30095;&#25110;&#32773;LiDAR&#21644;&#30456;&#26426;&#20043;&#38388;&#20559;&#24046;&#23548;&#33268;&#30340;&#22122;&#22768;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;Voxel Region (VR)&#65292;&#36890;&#36807;&#21160;&#24577;&#25237;&#24433;&#27599;&#20010;&#20307;&#32032;&#20013;&#30340;&#31232;&#30095;&#23616;&#37096;&#28857;&#20113;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#31216;&#20026;Sparse-to-Dense Voxel Region Fusion (SDVRF)&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;VR&#20869;&#37096;&#36739;&#22810;&#30340;&#22270;&#20687;&#29305;&#24449;&#22270;&#20687;&#32032;&#25910;&#38598;&#36215;&#26469;&#65292;&#20197;&#34917;&#20805;&#20174;&#31232;&#30095;&#28857;&#20013;&#25552;&#21462;&#30340;&#20307;&#32032;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#23494;&#38598;&#30340;&#34701;&#21512;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#21306;&#22495;&#29983;&#25104;&#31574;&#30053;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#40784;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#22826;&#22810;&#30340;&#32972;&#26223;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34701;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19979;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the perception task of autonomous driving, multi-modal methods have become a trend due to the complementary characteristics of LiDAR point clouds and image data. However, the performance of previous methods is usually limited by the sparsity of the point cloud or the noise problem caused by the misalignment between LiDAR and the camera. To solve these two problems, we present a new concept, Voxel Region (VR), which is obtained by projecting the sparse local point clouds in each voxel dynamically. And we propose a novel fusion method, named Sparse-to-Dense Voxel Region Fusion (SDVRF). Specifically, more pixels of the image feature map inside the VR are gathered to supplement the voxel feature extracted from sparse points and achieve denser fusion. Meanwhile, different from prior methods, which project the size-fixed grids, our strategy of generating dynamic regions achieves better alignment and avoids introducing too much background noise. Furthermore, we propose a multi-scale fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#19978;&#28040;&#32791;&#30340;&#26102;&#38388;&#65292;&#36890;&#36807;&#31639;&#27861;&#22312;&#38024;&#23574;&#38382;&#39064;&#21644;&#26032;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#26368;&#20339;&#21464;&#24322;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;$(1+1)$&#36827;&#21270;&#31639;&#27861;&#30456;&#23545;&#20110;&#19968;&#33324;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#39640;&#21407;&#38382;&#39064;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.08021</link><description>&lt;p&gt;
Fourier&#20998;&#26512;&#19982;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#30456;&#36935;&#65306;&#20851;&#20110;&#8220;&#39640;&#21407;&#8221;&#38382;&#39064;&#30340;&#31934;&#20934;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fourier Analysis Meets Runtime Analysis: Precise Runtimes on Plateaus. (arXiv:2302.08021v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#19978;&#28040;&#32791;&#30340;&#26102;&#38388;&#65292;&#36890;&#36807;&#31639;&#27861;&#22312;&#38024;&#23574;&#38382;&#39064;&#21644;&#26032;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#26368;&#20339;&#21464;&#24322;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;$(1+1)$&#36827;&#21270;&#31639;&#27861;&#30456;&#23545;&#20110;&#19968;&#33324;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#39640;&#21407;&#38382;&#39064;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#19978;&#28040;&#32791;&#30340;&#26102;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#31435;&#21363;&#35777;&#26126;&#20102;&#30001;Garnier&#65292;Kallel&#65292;&#21644;Schoenauer&#65288;1999&#65289;&#25552;&#20986;&#30340;&#20851;&#20110;&#38024;&#23574;&#38382;&#39064;&#19978;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#30340;&#32463;&#20856;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#38382;&#39064;&#65306;&#30001;$n/\ell$&#20010;&#26377;&#25928;&#22823;&#23567;&#20026;$2^\ell-1$&#30340;&#39640;&#21407;&#32452;&#25104;&#65292;&#38656;&#35201;&#20197;LeadingOnes&#30340;&#26041;&#24335;&#39034;&#24207;&#22320;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38745;&#24577;&#21644;&#19982;&#36866;&#24212;&#24230;&#30456;&#20851;&#30340;&#21464;&#24322;&#29575;&#30340;&#31934;&#30830;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#28176;&#36827;&#26368;&#20248;&#30340;&#38745;&#24577;&#21644;&#19982;&#36866;&#24212;&#24230;&#30456;&#20851;&#30340;&#21464;&#24322;&#29575;&#12290;&#23545;&#20110;$\ell = o(n)$&#65292;&#26368;&#20248;&#30340;&#38745;&#24577;&#21464;&#24322;&#29575;&#36817;&#20284;&#20026;$1.59/n$&#12290;&#24403;&#25214;&#21040;&#21069;$k$&#20010;&#19982;&#36866;&#24212;&#24230;&#30456;&#20851;&#30340;&#20108;&#36827;&#21046;&#20301;&#26102;&#65292;&#26368;&#20248;&#30340;&#36866;&#24212;&#24230;&#30456;&#20851;&#21464;&#24322;&#29575;&#28176;&#36827;&#20026;$1/(k+1)$&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#32467;&#26524;&#20165;&#35777;&#26126;&#20102;&#39044;&#26399;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#34920;&#26126;&#35813;&#26032;&#22522;&#20934;&#38382;&#39064;&#38750;&#24120;&#22256;&#38590;&#65292;&#32780;$(1+1)$&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#38382;&#39064;&#19978;&#19982;&#19968;&#33324;&#30340;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#30456;&#27604;&#25928;&#29575;&#24778;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method based on discrete Fourier analysis to analyze the time evolutionary algorithms spend on plateaus. This immediately gives a concise proof of the classic estimate of the expected runtime of the $(1+1)$ evolutionary algorithm on the Needle problem due to Garnier, Kallel, and Schoenauer (1999).  We also use this method to analyze the runtime of the $(1+1)$ evolutionary algorithm on a new benchmark consisting of $n/\ell$ plateaus of effective size $2^\ell-1$ which have to be optimized sequentially in a LeadingOnes fashion.  Using our new method, we determine the precise expected runtime both for static and fitness-dependent mutation rates. We also determine the asymptotically optimal static and fitness-dependent mutation rates. For $\ell = o(n)$, the optimal static mutation rate is approximately $1.59/n$. The optimal fitness dependent mutation rate, when the first $k$ fitness-relevant bits have been found, is asymptotically $1/(k+1)$. These results, so far only prove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.06761</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#31350;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#20851;&#27880;&#20110;&#31616;&#21333;&#30340;&#19977;&#20803;&#32452;&#20851;&#31995;&#22411;&#30693;&#35782;&#24211;&#65292;&#24573;&#30053;&#20102;&#26356;&#20026;&#22797;&#26434;&#12289;&#36923;&#36753;&#20026;&#22522;&#30784;&#12289;&#27010;&#24565;&#21270;&#30340; OWL &#26412;&#20307;&#31561;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26412;&#20307;&#30340;&#20102;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986; OntoLAMA&#65292;&#23427;&#21253;&#21547;&#22522;&#20110;&#25512;&#29702;&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#20174;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#23376;&#31867;&#25512;&#26029;&#20844;&#29702;&#20986;&#21457;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35268;&#27169;&#30340;&#26412;&#20307;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#30340;&#32972;&#26223;&#30693;&#35782;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#26159;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23376;&#31867;&#25512;&#26029;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#28304;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#31639;&#27861;DP-PSAC&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#38750;&#21333;&#35843;&#33258;&#36866;&#24212;&#26435;&#37325;&#20989;&#25968;&#65292;&#26681;&#25454;&#26799;&#24230;&#30340;&#21382;&#21490;&#25935;&#24863;&#24615;&#33258;&#36866;&#24212;&#35009;&#21098;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#24133;&#24230;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#19988;&#20445;&#35777;&#20102;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2212.00328</link><description>&lt;p&gt;
&#24102;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Learning with Per-Sample Adaptive Clipping. (arXiv:2212.00328v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#31639;&#27861;DP-PSAC&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#38750;&#21333;&#35843;&#33258;&#36866;&#24212;&#26435;&#37325;&#20989;&#25968;&#65292;&#26681;&#25454;&#26799;&#24230;&#30340;&#21382;&#21490;&#25935;&#24863;&#24615;&#33258;&#36866;&#24212;&#35009;&#21098;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#24133;&#24230;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#19988;&#20445;&#35777;&#20102;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;AI&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#19968;&#30452;&#26159;&#21560;&#24341;&#30740;&#31350;&#32773;&#21644;&#20844;&#20247;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;AI&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20351;AI&#27169;&#22411;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#24120;&#37327;&#35009;&#21098;&#26469;&#38480;&#21046;&#26799;&#24230;&#24133;&#24230;&#20197;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#35009;&#21098;&#24120;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#26368;&#26032;&#30340;&#20316;&#21697;NSGD&#21644;Auto-S&#25552;&#20986;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270;&#26469;&#26367;&#20195;&#35009;&#21098;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;NSGD&#21644;Auto-S&#31561;&#22522;&#20110;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21333;&#35843;&#26435;&#37325;&#20989;&#25968;&#65292;&#23545;&#23567;&#26799;&#24230;&#26679;&#26412;&#26045;&#21152;&#36807;&#37327;&#26435;&#37325;&#24182;&#24341;&#20837;&#39069;&#22806;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21333;&#35843;&#33258;&#36866;&#24212;&#26435;&#37325;&#20989;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#65288;DP-PSAC&#65289;&#31639;&#27861;&#65292;&#23427;&#20445;&#35777;&#20102;&#38544;&#31169;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;DP-PSAC&#31639;&#27861;&#26681;&#25454;&#35813;&#26679;&#26412;&#30340;&#21382;&#21490;&#25935;&#24863;&#24615;&#33258;&#36866;&#24212;&#35009;&#21098;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#24133;&#24230;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35009;&#21098;&#26799;&#24230;&#30340;&#26041;&#24046;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;DP-PSAC&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.15613</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#24050;&#25104;&#20026;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#28041;&#21450;&#36328;&#24230;&#32423;&#21035;&#27880;&#37322;&#65288;&#20363;&#22914;&#20449;&#24687;&#25552;&#21462;&#25110;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#26631;&#31614;&#25237;&#24433;&#27493;&#39588;&#65292;&#23558;&#24050;&#27880;&#37322;&#30340;&#36328;&#24230;&#26144;&#23556;&#21040;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23545;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#65288;QA&#65292;NER&#21644;&#20107;&#20214;&#25552;&#21462;&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#27969;&#34892;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#35813;&#26041;&#27861;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.10873</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#22238;&#24402;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#21457;&#29616;&#65306;&#32508;&#36848;&#65288;arXiv:2211.10873v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretable Scientific Discovery with Symbolic Regression: A Review. (arXiv:2211.10873v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10873
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#27969;&#34892;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#35813;&#26041;&#27861;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#27969;&#34892;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#37319;&#29992;&#36951;&#20256;&#32534;&#31243;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#20174;&#22522;&#30784;&#31185;&#23398;&#21040;&#24212;&#29992;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#30340;&#32467;&#26500;&#21644;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery method, achieving significant advances in various application domains ranging from fundamental to applied sciences. This survey presents a structured and comprehensive overview of symbolic regression methods and discusses their strengths and limitations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#24037;&#20855;&#65288;Webvicob&#65289;&#65292;&#29992;&#20110;&#20174;Wikipedia HTML&#36716;&#20648;&#25991;&#20214;&#20013;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#21319;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03256</link><description>&lt;p&gt;
&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#29992;&#20110;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
On Web-based Visual Corpus Construction for Visual Document Understanding. (arXiv:2211.03256v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#24037;&#20855;&#65288;Webvicob&#65289;&#65292;&#29992;&#20110;&#20174;Wikipedia HTML&#36716;&#20648;&#25991;&#20214;&#20013;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#21319;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#30340;&#26377;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#25289;&#19969;&#35821;&#35328;&#25110;&#36164;&#28304;&#30701;&#32570;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#38598;&#21512;&#19982;&#35814;&#32454;&#25991;&#26412;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#24037;&#20855;&#65288;Webvicob&#65289;&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#20174;&#21407;&#22987;&#30340;Wikipedia HTML&#36716;&#20648;&#25991;&#20214;&#20013;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#35270;&#35273;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Webvicob&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;VDU&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;DocVQA&#21644;&#21518;OCR&#35299;&#26512;&#65289;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#30001;Webvicob&#29983;&#25104;&#30340;100&#19975;&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;DocVQA&#20219;&#21153;3&#19978;&#30456;&#23545;&#20110;COCO-Text&#25968;&#25454;&#38598;&#30340;1100&#19975;&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#26377;&#36229;&#36807;13&#65285;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research on visual document understanding (VDU) has grown significantly, with a particular emphasis on the development of self-supervised learning methods. However, one of the significant challenges faced in this field is the limited availability of publicly accessible visual corpora or extensive collections of images with detailed text annotations, particularly for non-Latin or resource-scarce languages. To address this challenge, we propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine capable of constructing large-scale, multilingual visual corpora from raw Wikipedia HTML dumps. Our experiments demonstrate that the data generated by Webvicob can be used to train robust VDU models that perform well on various downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using a dataset of 1 million images generated by Webvicob, we observed an improvement of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#21160;&#20316;&#35745;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#29615;&#22659;&#23545;&#35937;&#21644;&#23545;&#35937;&#20851;&#31995;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#35745;&#21010;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;LLM&#26041;&#27861;&#26377;&#26356;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.04964</link><description>&lt;p&gt;
&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#21160;&#20316;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#21160;&#20316;&#35745;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#29615;&#22659;&#23545;&#35937;&#21644;&#23545;&#35937;&#20851;&#31995;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#35745;&#21010;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;LLM&#26041;&#27861;&#26377;&#26356;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#39640;&#23618;&#27425;&#25991;&#26412;&#26597;&#35810;&#20013;&#29983;&#25104;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#21160;&#20316;&#35745;&#21010;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19981;&#32771;&#34385;&#26426;&#22120;&#20154;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#26080;&#27861;&#25191;&#34892;&#65292;&#22240;&#20026;&#35745;&#21010;&#20013;&#30340;&#21160;&#20316;&#27169;&#31946;&#19981;&#28165;&#25110;&#21463;&#21040;&#29615;&#22659;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#25191;&#34892;&#21160;&#20316;&#35745;&#21010;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#29615;&#22659;&#23545;&#35937;&#21644;&#23545;&#35937;&#20851;&#31995;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#38598;&#25104;&#21040;LLM&#21160;&#20316;&#35745;&#21010;&#29983;&#25104;&#20013;&#65292;&#20197;&#25552;&#20379;&#31995;&#32479;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#29289;&#20307;&#30456;&#23545;&#24212;&#30340;&#35745;&#21010;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#24110;&#21161;&#31995;&#32479;&#28040;&#38500;&#29289;&#20307;&#23454;&#20363;&#20043;&#38388;&#30340;&#27495;&#20041;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;LLM&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#39640;&#25104;&#21151;&#29575;&#30340;&#21487;&#25191;&#34892;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) trained using massive text datasets have recently shown promise in generating action plans for robotic agents from high level text queries. However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints. In this paper, we propose an approach to generate environmentally-aware action plans that agents are better able to execute. Our approach involves integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings, resulting in plans where each generated action is mapped to objects present in the scene. We also design a novel scoring function that, along with generating the action steps and associating them with objects, helps the system disambiguate among object instances and take into account their states. We ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#20013;&#30340;&#20004;&#31181;&#35823;&#23548;&#24615;&#34892;&#20026;&#8220;&#25968;&#25454;&#21024;&#38500;&#8221;&#21644;&#8220;&#22312;&#35757;&#32451;&#38598;&#19978;&#27979;&#35797;&#8221;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;NNWT&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#19981;&#24403;&#34892;&#20026;&#22312;&#20219;&#20309;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#38646;&#35823;&#24046;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11228</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#25968;&#25454;&#20026;&#20309;&#35823;&#23548;&#20154;
&lt;/p&gt;
&lt;p&gt;
Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#20013;&#30340;&#20004;&#31181;&#35823;&#23548;&#24615;&#34892;&#20026;&#8220;&#25968;&#25454;&#21024;&#38500;&#8221;&#21644;&#8220;&#22312;&#35757;&#32451;&#38598;&#19978;&#27979;&#35797;&#8221;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;NNWT&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#19981;&#24403;&#34892;&#20026;&#22312;&#20219;&#20309;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#38646;&#35823;&#24046;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;AIEE 2023&#24180;&#21516;&#21517;&#20250;&#35758;&#30340;&#20027;&#39064;&#28436;&#35762;&#30340;&#38468;&#24102;&#35770;&#25991;&#65292;&#23646;&#20110;&#19968;&#31687;&#29702;&#35770;&#24615;&#25991;&#31456;&#12290;&#25991;&#31456;&#35299;&#37322;&#20102;AI&#39033;&#30446;&#20013;&#25152;&#35859;&#30340;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#32473;&#20986;&#20102;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20294;&#36825;&#20123;&#34920;&#29616;&#25968;&#25454;&#23454;&#38469;&#19978;&#34987;&#38169;&#35823;&#22320;&#22840;&#22823;&#20102;&#12290;&#25991;&#31456;&#38416;&#26126;&#20102;&#8220;&#25968;&#25454;&#21024;&#38500;&#8221;&#21644;&#8220;&#22312;&#35757;&#32451;&#38598;&#19978;&#27979;&#35797;&#8221;&#30340;&#35823;&#23548;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#31867;&#26041;&#27861;&#8220;&#26368;&#36817;&#37051;&#21152;&#38408;&#20540;&#8221;&#65288;NNWT&#65289;&#12290;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;&#22312;&#20855;&#26377;&#26377;&#38480;&#20294;&#26080;&#38480;&#30340;&#23384;&#20648;&#31354;&#38388;&#21644;&#35757;&#32451;&#26102;&#38388;&#65288;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31867;&#20284;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;NNWT&#26041;&#27861;&#20351;&#29992;&#36825;&#20004;&#31181;&#19981;&#24403;&#34892;&#20026;&#21487;&#20197;&#22312;&#20219;&#20309;&#39564;&#35777;&#38598;&#21644;&#20219;&#20309;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#38646;&#35823;&#24046;&#65292;&#20294;&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;NNWT&#26041;&#27861;&#22312;&#20869;&#65292;&#24182;&#19981;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a theoretical paper, as a companion paper of the keynote talk at the same conference AIEE 2023. In contrast to conscious learning, many projects in AI have employed so-called "deep learning" many of which seemed to give impressive performance. This paper explains that such performance data are deceptively inflated due to two misconducts: "data deletion" and "test on training set". This paper clarifies "data deletion" and "test on training set" in deep learning and why they are misconducts. A simple classification method is defined, called Nearest Neighbor With Threshold (NNWT). A theorem is established that the NNWT method reaches a zero error on any validation set and any test set using the two misconducts, as long as the test set is in the possession of the author and both the amount of storage space and the time of training are finite but unbounded like with many deep learning methods. However, many deep learning methods, like the NNWT method, are all not generalizable since
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22312;&#32447;2&#38454;&#27573;&#30340;&#31283;&#23450;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#26368;&#20248;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.02057</link><description>&lt;p&gt;
&#22312;&#32447;2&#38454;&#27573;&#31283;&#23450;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Online 2-stage Stable Matching. (arXiv:2207.02057v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22312;&#32447;2&#38454;&#27573;&#30340;&#31283;&#23450;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#26368;&#20248;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#22312;&#32447;2&#38454;&#27573;&#38382;&#39064;&#65292;&#30001;&#20197;&#19979;&#24773;&#20917;&#28608;&#21457;&#65306;&#32771;&#34385;&#19968;&#20010;&#23398;&#29983;&#34987;&#20998;&#37197;&#21040;&#22823;&#23398;&#30340;&#31995;&#32479;&#12290;&#26377;&#31532;&#19968;&#36718;&#30003;&#35831;&#65292;&#38656;&#35201;&#35745;&#31639;&#31532;&#19968;&#20010;&#31283;&#23450;&#21305;&#37197;$M_1$&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#23398;&#29983;&#21487;&#33021;&#20250;&#20915;&#23450;&#31163;&#24320;&#36825;&#20010;&#31995;&#32479;&#65288;&#25913;&#21464;&#35745;&#21010;&#65292;&#21435;&#22806;&#22269;&#22823;&#23398;&#65292;&#25110;&#32773;&#21435;&#19968;&#20123;&#19981;&#22312;&#31995;&#32479;&#20013;&#30340;&#26426;&#26500;&#65289;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#20108;&#36718;&#65288;&#22312;&#36825;&#20123;&#21024;&#38500;&#20043;&#21518;&#65289;&#65292;&#25105;&#20204;&#24212;&#35813;&#35745;&#31639;&#31532;&#20108;&#20010;&#65288;&#26368;&#32456;&#65289;&#31283;&#23450;&#21305;&#37197;$M_2$&#12290;&#30001;&#20110;&#25913;&#21464;&#20998;&#37197;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#30446;&#26631;&#26159;&#23558;&#20004;&#20010;&#31283;&#23450;&#30340;&#21305;&#37197;$M_1$&#21644;$M_2$&#20043;&#38388;&#30340;&#31163;&#23130;/&#20462;&#25913;&#27425;&#25968;&#26368;&#23567;&#21270;&#12290;&#37027;&#20040;&#65292;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#36873;&#25321;$M_1$&#21644;$M_2$&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#26368;&#20339;&#30340;&#22312;&#32447;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#25903;&#37197;&#24615;&#36136;&#23637;&#31034;&#20102;&#21363;&#20351;&#19981;&#30693;&#36947;&#31163;&#24320;&#31995;&#32479;&#30340;&#23398;&#29983;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#26368;&#20248;&#22320;&#35745;&#31639;&#20986;$M_1$&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#36755;&#20837;&#25968;&#25454;&#20013;&#21487;&#33021;&#30340;&#20854;&#20182;&#19968;&#20123;&#20462;&#25913;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on an online 2-stage problem, motivated by the following situation: consider a system where students shall be assigned to universities. There is a first round where some students apply, and a first (stable) matching $M_1$ has to be computed. However, some students may decide to leave the system (change their plan, go to a foreign university, or to some institution not in the system). Then, in a second round (after these deletions), we shall compute a second (final) stable matching $M_2$. As it is undesirable to change assignments, the goal is to minimize the number of divorces/modifications between the two stable matchings $M_1$ and $M_2$. Then, how should we choose $M_1$ and $M_2$? We show that there is an {\it optimal online} algorithm to solve this problem. In particular, thanks to a dominance property, we show that we can optimally compute $M_1$ without knowing the students that will leave the system. We generalize the result to some other possible modifications in the inp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#33258;&#36866;&#24212;&#19981;&#21516;&#21548;&#20247;&#30340;&#30446;&#26631;&#20219;&#21153;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#36866;&#24212;&#21548;&#20247;&#30340;&#35821;&#35328;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.08349</link><description>&lt;p&gt;
&#20102;&#35299;&#20320;&#30340;&#21548;&#20247;&#65306;&#29992;&#21548;&#20247;&#20943;&#27861;&#19987;&#38376;&#21270;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Know your audience: specializing grounded language models with listener subtraction. (arXiv:2206.08349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#33258;&#36866;&#24212;&#19981;&#21516;&#21548;&#20247;&#30340;&#30446;&#26631;&#20219;&#21153;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#36866;&#24212;&#21548;&#20247;&#30340;&#35821;&#35328;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#27807;&#36890;&#38656;&#35201;&#36866;&#24212;&#27599;&#20010;&#20132;&#38469;&#24773;&#22659;&#30340;&#29305;&#27530;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#20132;&#20114;&#20249;&#20276;&#20998;&#20139;&#30340;&#20849;&#21516;&#35821;&#22659;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#23545;&#35805;&#28216;&#25103; Dixit &#30340;&#24605;&#24819;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#65292;&#35757;&#32451;&#19968;&#20010;&#35828;&#35805;&#32773;&#27169;&#22411;&#26469;&#25551;&#36848;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#20351;&#24471;&#19968;&#20010;&#21548;&#32773;&#33021;&#22815;&#22312;&#24178;&#25200;&#39033;&#20013;&#27491;&#30830;&#22320;&#35782;&#21035;&#20986;&#30446;&#26631;&#22270;&#20687;&#65292;&#32780;&#21478;&#19968;&#20010;&#21548;&#32773;&#21017;&#19981;&#33021;&#12290;&#36825;&#35201;&#27714;&#35828;&#35805;&#32773;&#21033;&#29992;&#23427;&#19982;&#19981;&#21516;&#21548;&#32773;&#30340;&#20849;&#21516;&#30693;&#35782;&#24046;&#24322;&#36827;&#34892;&#36866;&#24212;&#12290;&#26412;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#23545;&#27604;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#35821;&#22659;&#19979;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#36866;&#37197;&#22120;&#20250;&#33258;&#28982;&#22320;&#20135;&#29983;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#65292;&#19988;&#21482;&#38656;&#35201;&#36890;&#36807;&#22870;&#21169;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#29992;&#20004;&#20010;&#21548;&#32773;&#26469;&#35757;&#32451;&#35828;&#35805;&#32773;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive different
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31616;&#21333;&#31163;&#25955;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;Value Memory Graph&#65292;VMG&#65289;&#26469;&#25277;&#35937;&#21407;&#22987;&#22797;&#26434;&#29615;&#22659;&#65292;&#20174;&#32780;&#31616;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2206.04384</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#35760;&#24518;&#22270;
&lt;/p&gt;
&lt;p&gt;
Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. (arXiv:2206.04384v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31616;&#21333;&#31163;&#25955;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;Value Memory Graph&#65292;VMG&#65289;&#26469;&#25277;&#35937;&#21407;&#22987;&#22797;&#26434;&#29615;&#22659;&#65292;&#20174;&#32780;&#31616;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#36890;&#24120;&#30452;&#25509;&#24212;&#29992;&#20110;&#29615;&#22659;&#26469;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#19968;&#20123;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#12289;&#31232;&#30095;&#22870;&#21169;&#21644;/&#25110;&#38271;&#26102;&#38388;&#38388;&#38548;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#22312;&#21407;&#22987;&#29615;&#22659;&#20013;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#31163;&#32447;RL&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#31616;&#21333;&#19988;&#31163;&#25955;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#25277;&#35937;&#20986;&#21407;&#22987;&#29615;&#22659;&#12290;RL&#26041;&#27861;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#19990;&#30028;&#27169;&#22411;&#32780;&#38750;&#29615;&#22659;&#25968;&#25454;&#36827;&#34892;&#31616;&#21270;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#19990;&#30028;&#27169;&#22411;&#31216;&#20026;&#20215;&#20540;&#35760;&#24518;&#22270;&#65288;VMG&#65289;&#65292;&#23427;&#34987;&#35774;&#35745;&#20026;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#20013;&#39030;&#28857;&#21644;&#26377;&#21521;&#36793;&#20998;&#21035;&#20195;&#34920;&#22270;&#29366;&#24577;&#21644;&#22270;&#21160;&#20316;&#12290;&#30001;&#20110;VMG&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;&#21160;&#20316;&#31354;&#38388;&#30456;&#23545;&#20110;&#21407;&#22987;&#29615;&#22659;&#32780;&#35328;&#26159;&#26377;&#38480;&#19988;&#30456;&#23545;&#36739;&#23567;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;VMG&#19978;&#24212;&#29992;&#20215;&#20540;&#36845;&#20195;&#31639;&#27861;&#26469;&#20272;&#31639;&#22270;&#29366;&#24577;&#20540;&#24182;&#30830;&#23450;&#26368;&#20339;&#30340;&#22270;&#21160;&#20316;&#12290;VMG&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#21644;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#22270;&#20687;&#36741;&#21161;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#22823;&#37327;&#29616;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2109.14196</link><description>&lt;p&gt;
WEDGE&#65306;&#22522;&#20110;&#32593;&#32476;&#22270;&#20687;&#36741;&#21161;&#30340;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#22270;&#20687;&#36741;&#21161;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#22823;&#37327;&#29616;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#38754;&#20020;&#30528;&#35757;&#32451;&#25968;&#25454;&#26080;&#27861;&#35206;&#30422;&#21508;&#31181;&#21487;&#33021;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;WEb-image&#36741;&#21161;&#30340;Domain GEneralization&#65288;WEDGE&#65289;&#26041;&#26696;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#36827;&#34892;&#26222;&#36866;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#29616;&#23454;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#12289;&#32593;&#31449;&#12289;&#20809;&#29031;&#12289;&#30456;&#26426;&#39118;&#26684;&#31561;&#21508;&#31181;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#22320;&#23558;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#30340;&#39118;&#26684;&#34920;&#31034;&#27880;&#20837;&#28304;&#39046;&#22495;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#32463;&#21382;&#20855;&#26377;&#21487;&#38752;&#26631;&#31614;&#30340;&#19981;&#21516;&#39118;&#26684;&#30340;&#22270;&#20687;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#20266;&#26631;&#31614;&#26469;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#27169;&#22411;&#22312;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect a web-crawled dataset which presents large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects the style representation of the web-crawled data into the source domain on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled dataset with predicted pseudo labels for 
&lt;/p&gt;</description></item></channel></rss>