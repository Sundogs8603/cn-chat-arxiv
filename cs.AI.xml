<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.01855</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#21435;&#21738;&#37324;&#65306;&#22522;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01855
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25506;&#32034;&#21608;&#36793;&#29615;&#22659;&#30340;&#23453;&#36149;&#24314;&#35758;&#12290;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#20174;&#22823;&#35268;&#27169;&#29992;&#25143;&#31614;&#21040;&#25968;&#25454;&#26500;&#24314;&#25512;&#33616;&#27169;&#22411;&#65292;&#36825;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#38382;&#39064;&#26102;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#24212;&#25552;&#21462;&#29992;&#25143;&#30340;&#22320;&#29702;&#31227;&#21160;&#27169;&#24335;&#12290;&#34429;&#28982;&#26377;&#30740;&#31350;&#21033;&#29992;LLMs&#36827;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#22320;&#29702;&#24433;&#21709;&#21644;&#39034;&#24207;&#36716;&#25442;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01855v1 Announce Type: cross  Abstract: Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to ass
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.19012</link><description>&lt;p&gt;
ReflectSumm: &#19968;&#20010;&#29992;&#20110;&#35838;&#31243;&#21453;&#24605;&#25688;&#35201;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ReflectSumm: A Benchmark for Course Reflection Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19012
&lt;/p&gt;
&lt;p&gt;
ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ReflectSumm&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;ReflectSumm&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;%&#20855;&#26377;&#28508;&#22312;&#22312;&#24847;&#35265;&#24635;&#32467;&#39046;&#22495;&#21644;&#29305;&#21035;&#26159;&#25945;&#32946;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21253;&#25324;&#20840;&#38754;&#30340;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#20026;&#23637;&#31034;&#20854;&#25928;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19012v1 Announce Type: cross  Abstract: This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students' reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the opinion summarization domain in general and the educational domain in particular. The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide benchmarks for facilitating further research in this area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.16829</link><description>&lt;p&gt;
&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#24674;&#22797;&#19968;&#20010;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#22870;&#21169;&#65292;&#37319;&#29992;&#38543;&#26426;&#36719;&#31574;&#30053;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#20351;&#29992;$\mathcal{O}(1/\varepsilon^{2})$&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26679;&#26412;&#24674;&#22797;&#20986;&#19968;&#20010;&#20351;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;$\mathcal{O}(1/\varepsilon^{4})$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#24674;&#22797;&#22870;&#21169;&#23545;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#19982;&#19987;&#23478;&#31574;&#30053;$\varepsilon$-&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.
&lt;/p&gt;</description></item><item><title>WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16127</link><description>&lt;p&gt;
WangchanLion&#19982;WangchanX MRC&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
WangchanLion and WangchanX MRC Eval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16127
&lt;/p&gt;
&lt;p&gt;
WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;WangchanLion&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;SEA-LION&#21644;&#19968;&#31995;&#21015;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#30740;&#31350;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12289;&#20195;&#30721;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#65292;&#37319;&#29992;Apache-2&#35768;&#21487;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#27888;&#35821;MRC&#25968;&#25454;&#38598;XQuAD&#21644;Iapp_wiki_qa_squad&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;MRC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12289;&#24110;&#21161;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#25581;&#31034;&#20102;&#25105;&#20204;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16127v1 Announce Type: cross  Abstract: This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improv
&lt;/p&gt;</description></item><item><title>&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.12459</link><description>&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-negative Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12459
&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#22312;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#23545;&#20154;&#31867;&#29702;&#35299;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;&#65288;NCL&#65289;&#65292;&#36825;&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#22797;&#20852;&#65292;&#26088;&#22312;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;NCL&#30340;&#21147;&#37327;&#22312;&#20110;&#24378;&#21046;&#23558;&#38750;&#36127;&#32422;&#26463;&#24212;&#29992;&#20110;&#29305;&#24449;&#65292;&#36825;&#35753;&#20154;&#24819;&#36215;NMF&#33021;&#22815;&#25552;&#21462;&#19982;&#26679;&#26412;&#38598;&#32676;&#32039;&#23494;&#23545;&#40784;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;NCL&#19981;&#20165;&#22312;&#25968;&#23398;&#19978;&#19982;NMF&#30446;&#26631;&#24456;&#22909;&#22320;&#23545;&#40784;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20351;&#24471;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#21152;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;NCL&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19979;&#28216;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#34892;&#32467;&#26500;&#25915;&#20987;&#25152;&#38656;&#32771;&#34385;&#30340;&#38382;&#39064;&#31354;&#38388;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.11830</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#38382;&#39064;&#31354;&#38388;&#32467;&#26500;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#34892;&#32467;&#26500;&#25915;&#20987;&#25152;&#38656;&#32771;&#34385;&#30340;&#38382;&#39064;&#31354;&#38388;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#22320;&#29992;&#20110;&#25903;&#25345;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#26088;&#22312;&#21361;&#23475;&#20854;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25552;&#35758;&#26377;&#25928;&#22320;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#20135;&#29983;&#22522;&#20110;&#20837;&#20405;&#23637;&#31034;&#30340;&#32467;&#26500;&#27169;&#24335;&#30340;&#39044;&#27979;&#65292;&#20197;&#22686;&#24378;&#26816;&#27979;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#22522;&#20110;GNN&#30340;NIDS&#24341;&#20837;&#20102;&#26032;&#31867;&#22411;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19987;&#38376;&#38024;&#23545;GNN&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#24418;&#25104;&#23545;&#25239;&#25915;&#20987;&#30340;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#24182;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#38656;&#35201;&#32771;&#34385;&#30340;&#38382;&#39064;&#31354;&#38388;&#32422;&#26463;&#65292;&#20197;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#34892;&#30340;&#32467;&#26500;&#25915;&#20987;&#12290;&#20316;&#20026;&#26368;&#32456;&#36129;&#29486;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11830v1 Announce Type: cross  Abstract: Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive research has shown their vulnerability to adversarial attacks, which involve subtle perturbations to the inputs of the models aimed at compromising their performance. Recent proposals have effectively leveraged Graph Neural Networks (GNN) to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness. However, the adoption of GNN-based NIDS introduces new types of risks. In this paper, we propose the first formalization of adversarial attacks specifically tailored for GNN in network intrusion detection. Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios. As a final contribution, we conduct an extensive experimental campaign in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.11330</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#20010;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#26469;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#65288;&#21363;&#65292;&#23545;&#35805;&#32423;&#65289;&#22870;&#21169;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#21517;&#20026;GELI&#65289;&#36890;&#36807;&#23558;&#20154;&#31867;&#25552;&#20379;&#30340;&#20840;&#23616;&#26126;&#30830;&#65288;GE&#65289;&#20250;&#35805;&#32423;&#22870;&#21169;&#25286;&#20998;&#65292;&#21033;&#29992;&#26412;&#22320;&#38544;&#24335;&#65288;LI&#65289;&#22810;&#27169;&#24577;&#22870;&#21169;&#20449;&#21495;&#26469;&#36328;&#27169;&#24577;&#22320;&#22609;&#36896;&#22870;&#21169;&#20998;&#35299;&#27493;&#39588;&#12290;&#28982;&#21518;&#23558;&#36825;&#31181;&#20998;&#35299;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;RHLF&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26469;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;GELI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#65292;&#25552;&#39640;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#65292;&#22686;&#24378;&#20195;&#29702;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09567</link><description>&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#65306;&#19968;&#31181;&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09567
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#65292;&#25552;&#39640;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#65292;&#22686;&#24378;&#20195;&#29702;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#22312;&#28041;&#21450;&#20154;&#31867;&#20114;&#21160;&#30340;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#26085;&#30410;&#24341;&#36215;&#23433;&#20840;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#20107;&#20214;&#32972;&#21518;&#30340;&#24773;&#20917;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#21521;&#38750;&#19987;&#23478;&#29992;&#25143;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#35299;&#37322;&#22312;&#25552;&#39640;&#21487;&#20449;&#24230;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#20316;&#20026;&#38450;&#33539;&#22833;&#36133;&#12289;&#38169;&#35823;&#21644;&#35823;&#35299;&#30340;&#25514;&#26045;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#65292;&#24357;&#21512;&#20195;&#29702;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#30340;&#25928;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#22522;&#20110;ROS&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23454;&#26045;&#30340;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31867;&#20284;&#40657;&#30418;&#30340;&#20803;&#32032;&#29992;&#20110;&#25552;&#20379;&#38382;&#36131;&#21046;&#65292;&#20855;&#26377;&#36890;&#36807;&#21306;&#22359;&#38142;&#25216;&#26415;&#23454;&#29616;&#30340;&#38450;&#31713;&#25913;&#23646;&#24615;&#12290;&#20854;&#27425;&#65292;&#19968;&#20010;&#36127;&#36131;&#30340;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09567v1 Announce Type: cross  Abstract: The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.08974</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#31070;&#32463;&#22330;&#34920;&#31034;&#35299;&#21078;&#26641;
&lt;/p&gt;
&lt;p&gt;
Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#26641;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#21078;&#26641;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#24418;&#29366;&#22810;&#26679;&#19988;&#22797;&#26434;&#65292;&#20934;&#30830;&#34920;&#31034;&#35299;&#21078;&#26641;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#26469;&#34920;&#31034;&#35299;&#21078;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;INR&#31354;&#38388;&#20013;&#36827;&#34892;&#21435;&#22122;&#25193;&#25955;&#26469;&#25429;&#25417;&#19968;&#32452;&#26641;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#21487;&#20197;&#22312;&#20219;&#20309;&#25152;&#38656;&#20998;&#36776;&#29575;&#19979;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#30340;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08974v1 Announce Type: cross  Abstract: Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08755</link><description>&lt;p&gt;
DAM:&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DAM: Dynamic Adapter Merging for Continual Video QA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#65288;VidQA&#65289;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DAM&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26469;&#65288;i&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#65288;ii&#65289;&#23454;&#29616;&#23545;&#25345;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#65288;iii&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22788;&#29702;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#65292;&#65288;iv&#65289;&#23454;&#29616;&#36328;&#30456;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#22312;&#32473;&#23450;&#19968;&#32452;&#25345;&#32493;&#27969;&#24335;&#20256;&#36755;&#30340;VidQA&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#39034;&#24207;&#35757;&#32451;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#39592;&#24178;&#30340;&#21442;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#26469;&#33258;&#26410;&#30693;&#39046;&#22495;&#30340;&#35270;&#39057;&#38382;&#39064;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#36335;&#30001;&#22120;&#20989;&#25968;&#35745;&#31639;&#27599;&#20010;&#36866;&#37197;&#22120;&#30340;&#27010;&#29575;&#65292;&#21453;&#26144;&#20986;&#35813;&#36866;&#37197;&#22120;&#19982;&#24403;&#21069;&#35270;&#39057;&#38382;&#39064;&#36755;&#20837;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#26696;&#32858;&#21512;&#25152;&#26377;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07890</link><description>&lt;p&gt;
$\widetilde{O}(T^{-1})$ &#25910;&#25947;&#21040;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
No-regret&#23398;&#20064;&#19982;&#21338;&#24328;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#32806;&#21512;&#30340;&#26080;&#24724;&#23398;&#20064;&#21160;&#24577;&#65292;&#24403;&#25152;&#26377;&#29609;&#23478;&#22312;&#27491;&#21017;&#24418;&#24335;&#28216;&#25103;&#20013;&#37319;&#29992;&#26102;&#65292;&#20197;$\widetilde{O}(T^{-1})$&#30340;&#25509;&#36817;&#26368;&#20248;&#36895;&#29575;&#25910;&#25947;&#21040;&#21508;&#31181;&#22343;&#34913;&#35299;&#65292;&#36825;&#26174;&#30528;&#25913;&#36827;&#20102;&#32463;&#20856;&#26080;&#24724;&#23398;&#20064;&#32773;&#30340;$O(1/\sqrt{T})$&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#31867;&#20284;&#30340;&#25910;&#25947;&#32467;&#26524;&#24456;&#23569;&#35265;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#65292;&#36830;&#21516;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;$\widetilde{O}(T^{-1})$&#36817;&#20284;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#20063;&#21253;&#25324;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07890v1 Announce Type: cross  Abstract: No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15300</link><description>&lt;p&gt;
&#35265;&#35777;&#20026;&#20449;&#65306;&#36890;&#36807;CLIP&#24341;&#23548;&#35299;&#30721;&#32531;&#35299;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15300
&lt;/p&gt;
&lt;p&gt;
CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#23481;&#26131;&#20986;&#29616;&#23545;&#35937;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21477;&#23376;&#32423;LVLM&#24187;&#35273;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22270;&#20687;&#30340;CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#19968;&#20010;&#27604;&#21333;&#35789;&#21487;&#33021;&#24615;&#26356;&#24378;&#22823;&#12289;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#31034;&#22120;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35299;&#30721;&#26102;&#30340;&#23545;&#35937;&#24187;&#35273;&#12290;CGD&#21033;&#29992;CLIP&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32852;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CGD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09126</link><description>&lt;p&gt;
MPIrigen: &#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
MPIrigen: MPI Code Generation through Domain-Specific Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#20013;&#65292;&#39640;&#25928;&#30340;&#24182;&#34892;&#35745;&#31639;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#65288;MPI&#65289;&#38598;&#25104;&#39046;&#22495;&#12290;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#34892;&#32534;&#31243;&#20219;&#21153;&#65292;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;PolyCoder&#65288;&#19987;&#38376;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#65289;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#36890;&#29992;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;MPI&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;C&#21644;C++&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;MonoCoder&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;HPCorpusMPI&#19978;&#23545;MonoCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;MPI-based&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;SAM&#65292;&#24182;&#21457;&#29616;SAM&#22312;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.08671</link><description>&lt;p&gt;
&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#22312;&#21305;&#37197;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Semi-Dense Detector-Free Methods Good at Matching Local Features?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;SAM&#65292;&#24182;&#21457;&#29616;SAM&#22312;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#65292;&#22914;LoFTR&#65292;&#30446;&#21069;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#21305;&#37197;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;SDF&#26041;&#27861;&#34987;&#35757;&#32451;&#29992;&#20110;&#22312;&#20004;&#24133;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20960;&#20046;&#21482;&#20351;&#29992;&#30456;&#23545;&#20301;&#23039;&#20272;&#35745;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#22312;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#24471;&#21040;&#30340;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;&#65288;SAM&#65289;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;MegaDepth&#21644;HPatches&#65289;&#19978;&#23637;&#31034;&#19968;&#20010;&#36870;&#30452;&#35273;&#30340;&#32467;&#26524;&#65306;&#19968;&#26041;&#38754;&#65292;SAM&#22312;&#20301;&#23039;/&#21333;&#24212;&#24615;&#20272;&#35745;&#25351;&#26631;&#26041;&#38754;&#35201;&#20040;&#20248;&#20110;SDF&#26041;&#27861;&#65292;&#35201;&#20040;&#19982;&#20043;&#30456;&#24403;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;SAM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#39068;&#33394;&#31354;&#38388;&#26469;&#35299;&#32806;&#20142;&#24230;&#21644;&#39068;&#33394;&#65292;&#24182;&#35774;&#35745;&#20102;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#20197;&#25913;&#21892;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.05809</link><description>&lt;p&gt;
&#21482;&#38656;&#19968;&#20010;&#39068;&#33394;&#31354;&#38388;&#65306;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#39068;&#33394;&#31354;&#38388;&#26469;&#35299;&#32806;&#20142;&#24230;&#21644;&#39068;&#33394;&#65292;&#24182;&#35774;&#35745;&#20102;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#20197;&#25913;&#21892;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#65288;Low-Light Image Enhancement&#65292;LLIE&#65289;&#20219;&#21153;&#26088;&#22312;&#20174;&#21463;&#25439;&#30340;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#32454;&#33410;&#21644;&#35270;&#35273;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;sRGB&#21644;HSV&#39068;&#33394;&#31354;&#38388;&#19978;&#23398;&#20064;&#20302;/&#27491;&#24120;&#20809;&#22270;&#20687;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22686;&#24378;&#28041;&#21450;&#25918;&#22823;&#22270;&#20687;&#20449;&#21495;&#65292;&#24182;&#19988;&#23558;&#36825;&#20123;&#39068;&#33394;&#31354;&#38388;&#24212;&#29992;&#20110;&#20449;&#22122;&#27604;&#20302;&#30340;&#20302;&#20809;&#22270;&#20687;&#21487;&#33021;&#20250;&#24341;&#20837;&#28789;&#25935;&#24230;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22686;&#24378;&#22270;&#20687;&#20013;&#23384;&#22312;&#39068;&#33394;&#20266;&#24433;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35757;&#32451;&#39068;&#33394;&#31354;&#38388;&#65292;&#31216;&#20026;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#12290;&#23427;&#19981;&#20165;&#23558;&#20142;&#24230;&#21644;&#39068;&#33394;&#20174;RGB&#36890;&#36947;&#20998;&#31163;&#20986;&#26469;&#20197;&#20943;&#36731;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#32780;&#19988;&#30001;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#23427;&#36824;&#36866;&#24212;&#19981;&#21516;&#20809;&#29031;&#33539;&#22260;&#30340;&#20302;&#20809;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#65292;&#21547;&#26377;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.03494</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#23383;&#65306;&#36890;&#36807;&#35821;&#38899;&#32447;&#32034;&#25913;&#21892;LLM&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#34920;&#26126;&#20165;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#23545;&#35805;&#30340;&#27169;&#24577;&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#34429;&#28982;LLM&#22312;&#22788;&#29702;&#25991;&#26412;&#26041;&#38754;&#22312;&#36825;&#20123;&#20154;&#26426;&#23545;&#35805;&#20013;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#22312;&#31038;&#20132;&#23548;&#33322;&#31561;&#24773;&#22659;&#19979;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#21475;&#22836;&#25351;&#20196;&#30340;&#32454;&#24494;&#20043;&#22788;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#20854;&#20013;&#30340;&#27495;&#20041;&#21644;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36229;&#36234;&#25991;&#23383;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#36825;&#20123;&#38899;&#39057;&#22238;&#24212;&#30340;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#21475;&#22836;&#20132;&#27969;&#20013;&#19981;&#28041;&#21450;&#25991;&#23383;&#25514;&#36766;&#30340;&#26041;&#38754;&#65292;&#36890;&#36807;&#34920;&#36798;&#26041;&#24335;&#20256;&#36798;&#24847;&#20041;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36229;&#36234;&#25991;&#23383;&#8221;&#65307;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#38899;&#39057;&#36716;&#24405;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#30340;&#37096;&#20998;&#26469;&#25913;&#21892;LLM&#20915;&#31574;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#20391;&#37325;&#24773;&#24863;&#21644;&#26356;&#19982;&#20154;&#26426;&#23545;&#35805;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RMS&#30340;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#20248;&#21270;&#20102;3D&#28857;&#20113;&#30340;&#32763;&#35793;&#31354;&#38388;&#21487;&#35266;&#27979;&#24615;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#20013;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.07337</link><description>&lt;p&gt;
RMS&#65306;&#23454;&#26102;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#23567;&#20887;&#20313;&#28857;&#20113;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07337
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RMS&#30340;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#20248;&#21270;&#20102;3D&#28857;&#20113;&#30340;&#32763;&#35793;&#31354;&#38388;&#21487;&#35266;&#27979;&#24615;&#65292;&#35299;&#20915;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#20013;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#20013;&#20351;&#29992;&#30340;&#20856;&#22411;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#20445;&#30041;&#20102;&#39640;&#27700;&#24179;&#30340;&#28857;&#20887;&#20313;&#12290;&#36825;&#31181;&#20887;&#20313;&#19981;&#24517;&#35201;&#22320;&#20943;&#24930;&#20102;&#20272;&#35745;&#27969;&#31243;&#24182;&#21487;&#33021;&#22312;&#23454;&#26102;&#32422;&#26463;&#19979;&#23548;&#33268;&#28418;&#31227;&#12290;&#36825;&#31181;&#19981;&#24517;&#35201;&#30340;&#24310;&#36831;&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#65288;&#23588;&#20854;&#26159;&#26080;&#20154;&#26426;&#65289;&#30340;&#29942;&#39048;&#65292;&#38656;&#35201;&#26368;&#23567;&#30340;&#24310;&#36831;&#20197;&#36827;&#34892;&#25935;&#25463;&#21644;&#20934;&#30830;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RMS&#30340;&#26032;&#39062;&#12289;&#30830;&#23450;&#24615;&#12289;&#26410;&#30693;&#21644;&#21333;&#21442;&#25968;&#28857;&#20113;&#37319;&#26679;&#26041;&#27861;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;3D&#28857;&#20113;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07337v2 Announce Type: replace-cross  Abstract: The typical point cloud sampling methods used in state estimation for mobile robots preserve a high level of point redundancy. This redundancy unnecessarily slows down the estimation pipeline and may cause drift under real-time constraints. Such undue latency becomes a bottleneck for resource-constrained robots (especially UAVs), requiring minimal delay for agile and accurate operation. We propose a novel, deterministic, uninformed, and single-parameter point cloud sampling method named RMS that minimizes redundancy within a 3D point cloud. In contrast to the state of the art, RMS balances the translation-space observability by leveraging the fact that linear and planar surfaces inherently exhibit high redundancy propagated into iterative estimation pipelines. We define the concept of gradient flow, quantifying the local surface underlying a point. We also show that maximizing the entropy of the gradient flow minimizes point re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;VT-Former&#65292;&#22312;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.06623</link><description>&lt;p&gt;
VT-Former: &#22522;&#20110;Transformer&#30340;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;VT-Former&#65292;&#22312;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;&#36947;&#36335;&#23433;&#20840;&#21644;&#20132;&#36890;&#31649;&#29702;&#24050;&#25104;&#20026;&#29616;&#20195;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#37325;&#28857;&#39046;&#22495;&#12290;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#22312;&#20844;&#36335;&#21644;&#36947;&#36335;&#23433;&#20840;&#30340;&#20247;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#24212;&#29992;&#21253;&#25324;&#20132;&#36890;&#31649;&#29702;&#12289;&#20107;&#25925;&#39044;&#38450;&#12289;&#24037;&#22320;&#23433;&#20840;&#21644;&#33021;&#28304;&#20248;&#21270;&#31561;&#21508;&#31181;&#29992;&#20363;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#21450;&#30417;&#25511;&#25668;&#20687;&#22836;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#22686;&#21152;&#37096;&#32626;&#25512;&#21160;&#19979;&#65292;&#26234;&#33021;&#31649;&#29702;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;VT-Former&#12290;&#38500;&#20102;&#21033;&#29992;Transformer&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#27169;&#24335;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27880;&#24847;&#21147;&#20998;&#35789;&#65288;GAT&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing roadway safety and traffic management has become an essential focus area for a broad range of modern cyber-physical systems and intelligent transportation systems. Vehicle Trajectory Prediction is a pivotal element within numerous applications for highway and road safety. These applications encompass a wide range of use cases, spanning from traffic management and accident prevention to enhancing work-zone safety and optimizing energy conservation. The ability to implement intelligent management in this context has been greatly advanced by the developments in the field of Artificial Intelligence (AI), alongside the increasing deployment of surveillance cameras across road networks. In this paper, we introduce a novel transformer-based approach for vehicle trajectory prediction for highway safety and surveillance, denoted as VT-Former. In addition to utilizing transformers to capture long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module has been proposed
&lt;/p&gt;</description></item><item><title>PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09819</link><description>&lt;p&gt;
PPNet: &#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09819
&lt;/p&gt;
&lt;p&gt;
PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22914;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22312;&#21021;&#22987;&#35299;&#25935;&#24863;&#24615;&#21644;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#36895;&#24230;&#19978;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#20855;&#26377;&#26377;&#38480;&#21151;&#29575;/&#29123;&#26009;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#21363;&#36335;&#24452;&#31354;&#38388;&#20998;&#27573;&#21644;&#32473;&#23450;&#36335;&#24452;&#31354;&#38388;&#20013;&#30340;&#33322;&#28857;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36335;&#24452;&#35268;&#21010;&#32593;&#32476;&#65288;PPNet&#65289;&#30340;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#36848;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDaGe-PP&#30340;&#29992;&#20110;&#36335;&#24452;&#35268;&#21010;&#30340;&#39640;&#25928;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;PPNet&#35757;&#32451;&#38598;&#30001;EDaGe-PP&#29983;&#25104;&#30340;&#25104;&#21151;&#29575;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#21319;&#20102;$2\times$&#65292;&#24635;&#35745;&#31639;&#26102;&#38388;&#23569;&#20110;1/33&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;PPNet&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNe
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05176</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#32763;&#35793;&#20013;&#30340;&#31454;&#20105;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Rival Neural Machine Translation? A Comparative Study. (arXiv:2401.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20027;&#27969;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#22522;&#20110;&#38169;&#35823;&#31867;&#22411;&#21644;&#20845;&#20010;&#20998;&#26512;&#32454;&#21017;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#32771;&#23519;&#20102;ChatGPT&#21644;NMT&#24341;&#25806;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#20110;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;NMT&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#24471;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#24403;ChatGPT&#25552;&#20379;&#31034;&#20363;&#25110;&#32763;&#35793;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#20154;&#24037;&#35780;&#20272;&#32773;&#24448;&#24448;&#20250;&#32473;&#20104;&#26126;&#26174;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#32500;&#24230;&#20043;&#38388;&#30340;&#20004;&#20004;&#30456;&#20851;&#24615;&#32467;&#26524;&#36739;&#24369;&#19988;&#19981;&#26174;&#33879;&#65292;&#36825;&#34920;&#26126;&#20102;&#20004;&#31181;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.01627</link><description>&lt;p&gt;
VAL&#65306;&#24102;&#26377;GPT&#23545;&#35805;&#35299;&#26512;&#30340;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VAL: Interactive Task Learning with GPT Dialog Parsing. (arXiv:2310.01627v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01627
&lt;/p&gt;
&lt;p&gt;
VAL&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31526;&#21495;&#38598;&#25104;&#30340;&#29702;&#24565;&#65292;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#30340;&#33719;&#21462;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#35299;&#37322;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#22312;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#23454;&#39564;&#34920;&#26126;&#65292;VAL&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#19975;&#20010;&#26679;&#26412;&#26469;&#29983;&#25104;&#38745;&#24577;&#30340;&#40657;&#31665;&#27169;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#65288;ITL&#65289;&#24378;&#35843;&#20174;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#38480;&#25351;&#20196;&#20013;&#36880;&#27493;&#33719;&#24471;&#30693;&#35782;&#65292;&#36825;&#20123;&#25351;&#20196;&#20197;&#33258;&#28982;&#35821;&#35328;&#31561;&#24418;&#24335;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;ITL&#31995;&#32479;&#24448;&#24448;&#21463;&#21040;&#33030;&#24369;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#35821;&#35328;&#35299;&#26512;&#30340;&#22256;&#25200;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#33030;&#24369;&#24615;&#26377;&#19968;&#23450;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#20294;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VAL&#65292;&#19968;&#31181;&#20855;&#26377;&#26032;&#30340;LLM/&#31526;&#21495;&#38598;&#25104;&#29702;&#24565;&#30340;ITL&#31995;&#32479;&#12290;&#36890;&#36807;&#20165;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20351;&#29992;LLMs&#65288;&#20363;&#22914;&#35859;&#35789;&#21644;&#21442;&#25968;&#36873;&#25321;&#65289;&#65292;&#22312;&#31639;&#27861;&#26694;&#26550;&#20869;&#65292;VAL&#21033;&#29992;LLMs&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#20132;&#20114;&#24335;&#23398;&#20064;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#25903;&#25345;&#25191;&#34892;&#26032;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#29992;&#25143;&#19982;VAL&#30340;&#20132;&#20114;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#29992;&#25143;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#25351;&#20196;&#20013;&#25104;&#21151;&#23398;&#21040;&#26377;&#25928;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, in practice, ITL systems often suffers from brittle, error-prone language parsing. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks -- such as predicate and argument selection -- within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users' interactions with VAL in a video game setting, finding that most
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2309.15877</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#21644;&#22788;&#29702;&#26469;&#33258;&#22810;&#31181;&#20449;&#24687;&#28304;&#25110;&#27169;&#24577;&#23545;&#20110;&#33719;&#24471;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#35770;&#20998;&#23618;&#24863;&#30693;(ITHP)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27010;&#24565;&#12290;&#19982;&#22823;&#22810;&#25968;&#26088;&#22312;&#23558;&#25152;&#26377;&#27169;&#24577;&#32435;&#20837;&#36755;&#20837;&#30340;&#20256;&#32479;&#34701;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20027;&#35201;&#27169;&#24577;&#25351;&#23450;&#20026;&#36755;&#20837;&#65292;&#32780;&#20854;&#20313;&#27169;&#24577;&#21017;&#20316;&#20026;&#20449;&#24687;&#36335;&#24452;&#20013;&#30340;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#36755;&#20837;&#27169;&#24577;&#29366;&#24577;&#20043;&#38388;&#26368;&#23567;&#21270;&#30456;&#20114;&#20449;&#24687;&#24182;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#20854;&#20313;&#27169;&#24577;&#20043;&#38388;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#30340;&#24179;&#34913;&#65292;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#24182;&#26368;&#23567;&#21270;&#20887;&#20313;&#30340;&#32039;&#20945;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07136</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Transformer&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#20043;&#19968;&#12290;&#38543;&#30528;&#20808;&#36827;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;ECG&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;ECG&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#23454;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;MTECG&#65292;&#23427;&#23558;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;220,251&#20010;ECG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35786;&#26029;&#27880;&#37322;&#65292;&#20197;&#25506;&#32034;MTECG&#30340;&#29305;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#19979;&#65292;&#19968;&#20010;&#21482;&#26377;5.7M&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#65288;5%-75%&#65289;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#12290;&#28040;&#34701;&#30740;&#31350;&#31361;&#20986;&#20102;&#27874;&#21160;&#37325;&#26500;&#30446;&#26631;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;MTECG&#32791;&#26102;&#36739;&#23569;&#19988;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#21508;&#31181;&#24515;&#30005;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.12477</link><description>&lt;p&gt;
&#20851;&#20110;&#38745;&#24577;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
On Dynamic Program Decompositions of Static Risk Measures. (arXiv:2304.12477v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38745;&#24577;&#39118;&#38505;&#35268;&#36991;&#30446;&#26631;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23481;&#26131;&#25509;&#21463;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#20998;&#35299;&#26469;&#21046;&#23450;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#26412;&#25991;&#34920;&#26126;&#20960;&#31181;&#29616;&#26377;&#30340;&#20998;&#35299;&#26412;&#36136;&#19978;&#26159;&#19981;&#31934;&#30830;&#30340;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22768;&#26126;&#30456;&#30683;&#30462;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20030;&#20986;&#20102;&#19968;&#20123;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#30340;&#27969;&#34892;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;VaR&#30830;&#23454;&#23384;&#22312;&#31934;&#30830;&#30340;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#38416;&#26126;&#20102;VaR&#21644;CVaR&#21160;&#24577;&#35268;&#21010;&#23646;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental difference between VaR and CVaR dynamic programming properties.
&lt;/p&gt;</description></item><item><title>RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09207</link><description>&lt;p&gt;
RETVec&#65306;&#24377;&#24615;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09207
&lt;/p&gt;
&lt;p&gt;
RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RETVec&#65292;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#12290;RETVec&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#21487;&#36873;&#30340;&#23567;&#22411;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#35789;&#35821;&#23884;&#20837;&#21040;256&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;RETVec&#30340;&#23884;&#20837;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#24230;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#38024;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23383;&#31526;&#32423;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;RETVec&#22312;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20123;&#27604;&#36739;&#34920;&#26126;&#65292;RETVec&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25991;&#26412;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;RETVec&#22312;Apache 2&#35768;&#21487;&#19979;&#21487;&#22312;https://github.com/google-research/retvec&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.
&lt;/p&gt;</description></item></channel></rss>