<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.07869</link><description>&lt;p&gt;
TeleMoMa&#65306;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07869
&lt;/p&gt;
&lt;p&gt;
TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#38480;&#21046;&#27169;&#20223;&#23398;&#20064;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#25968;&#25454;&#30340;&#21294;&#20047;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#19982;&#38745;&#27490;&#25805;&#20316;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#36828;&#31243;&#25805;&#20316;&#30028;&#38754;&#65292;&#25910;&#38598;&#28436;&#31034;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TeleMoMa&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#31227;&#21160;&#25805;&#20316;&#22120;&#30340;&#36890;&#29992;&#21644;&#27169;&#22359;&#21270;&#30028;&#38754;&#12290;TeleMoMa&#23558;&#21253;&#25324;RGB&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#12289;&#34394;&#25311;&#29616;&#23454;&#25511;&#21046;&#22120;&#12289;&#38190;&#30424;&#12289;&#25805;&#32437;&#26438;&#31561;&#22810;&#20010;&#20154;&#26426;&#25509;&#21475;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#21450;&#36825;&#20123;&#25509;&#21475;&#30340;&#20219;&#20309;&#32452;&#21512;&#12290;&#22312;&#20854;&#26356;&#26131;&#35775;&#38382;&#30340;&#29256;&#26412;&#20013;&#65292; TeleMoMa&#21487;&#20197;&#20165;&#20351;&#29992;&#35270;&#35273;&#65288;&#22914;RGB-D&#30456;&#26426;&#65289;&#21363;&#21487;&#24037;&#20316;&#65292;&#38477;&#20302;&#20102;&#20154;&#31867;&#25552;&#20379;&#31227;&#21160;&#25805;&#20316;&#28436;&#31034;&#30340;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#36828;&#31243;&#25805;&#20316;&#20960;&#20010;&#29616;&#26377;&#30340;&#31227;&#21160;&#25805;&#20316;&#22120;&#8212;&#8212;PAL Tiago++, Toyota HSR&#21644;Fetch&#26469;&#23637;&#29616;TeleMoMa&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Module-wise Pruning Error&#65288;MoPE&#65289;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;CLIP&#27169;&#22359;&#37325;&#35201;&#24615;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21387;&#32553;&#38454;&#27573;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#21098;&#26525;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.07839</link><description>&lt;p&gt;
MoPE-CLIP: &#32467;&#26500;&#21270;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24102;&#26377;&#22522;&#20110;&#27169;&#22359;&#30340;&#21098;&#26525;&#35823;&#24046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Module-wise Pruning Error&#65288;MoPE&#65289;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;CLIP&#27169;&#22359;&#37325;&#35201;&#24615;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21387;&#32553;&#38454;&#27573;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#21098;&#26525;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24179;&#21488;&#19978;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;&#26356;&#23567;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23545;CLIP&#27169;&#22411;&#24212;&#29992;&#22522;&#20110;&#22823;&#23567;&#30340;&#21098;&#26525;&#20250;&#23548;&#33268;&#20725;&#21270;&#21644;&#24615;&#33021;&#36739;&#24046;&#12290;&#26368;&#36817;&#20851;&#20110;VLP&#21387;&#32553;&#30340;&#21162;&#21147;&#35201;&#20040;&#37319;&#29992;&#21333;&#27169;&#24577;&#21387;&#32553;&#25351;&#26631;&#23548;&#33268;&#24615;&#33021;&#26377;&#38480;&#65292;&#35201;&#20040;&#28041;&#21450;&#26114;&#36149;&#30340;&#25513;&#30721;&#25628;&#32034;&#36807;&#31243;&#21644;&#21487;&#23398;&#20064;&#25513;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#22359;&#30340;&#21098;&#26525;&#35823;&#24046;&#65288;MoPE&#65289;&#25351;&#26631;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#20934;&#30830;&#35780;&#20272;CLIP&#27169;&#22359;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;MoPE&#25351;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21098;&#26525;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21387;&#32553;&#38454;&#27573;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#65292;MoPE-CLIP&#26377;&#25928;&#21033;&#29992;&#20102;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07839v1 Announce Type: cross  Abstract: Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.07818</link><description>&lt;p&gt;
&#26631;&#31614;&#20002;&#22833;&#29575;&#65306;&#21033;&#29992;&#20855;&#26377;&#22495;&#36716;&#31227;&#21644;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;&#36229;&#22768;&#65289;&#26159;&#35780;&#20272;&#24515;&#33039;&#21151;&#33021;&#26102;&#20351;&#29992;&#30340;&#31532;&#19968;&#31181;&#25104;&#20687;&#26041;&#24335;&#12290;&#20174;&#36229;&#22768;&#20013;&#27979;&#37327;&#21151;&#33021;&#29983;&#29289;&#26631;&#24535;&#29289;&#20381;&#36182;&#20110;&#23545;&#24515;&#33039;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#24037;&#20855;&#36716;&#21270;&#20026;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#20998;&#21106;&#27169;&#22411;&#23545;&#21508;&#31181;&#22270;&#20687;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#19981;&#21516;&#25195;&#25551;&#20202;&#33719;&#24471;&#65292;&#30001;&#19981;&#21516;&#32423;&#21035;&#30340;&#19987;&#23478;&#25805;&#20316;&#21592;&#33719;&#24471;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#26377;&#24517;&#35201;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#26631;&#31614;&#23384;&#22312;&#30340;&#21464;&#21270;&#65292;&#21363;&#21512;&#24182;&#25968;&#25454;&#36890;&#24120;&#26159;&#37096;&#20998;&#26631;&#35760;&#30340;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#26469;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#30340;naively
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
&lt;/p&gt;</description></item><item><title>BTX&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20855;&#22791;&#22810;&#20010;&#19987;&#19994;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31181;&#23376;&#27169;&#22411;&#30340;&#20998;&#25903;&#22521;&#35757;&#25104;&#19987;&#23478;&#65292;&#28982;&#21518;&#22312;Mixture-of-Expert&#23618;&#20013;&#23558;&#23427;&#20204;&#27719;&#38598;&#20026;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;MoE&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#22522;&#20110;&#26631;&#35760;&#30340;&#36335;&#30001;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.07816</link><description>&lt;p&gt;
Branch-Train-MiX: &#23558;&#19987;&#23478;LLMs&#28151;&#21512;&#21040;&#28151;&#21512;&#19987;&#23478;LLM&#20013;
&lt;/p&gt;
&lt;p&gt;
Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07816
&lt;/p&gt;
&lt;p&gt;
BTX&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20855;&#22791;&#22810;&#20010;&#19987;&#19994;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31181;&#23376;&#27169;&#22411;&#30340;&#20998;&#25903;&#22521;&#35757;&#25104;&#19987;&#23478;&#65292;&#28982;&#21518;&#22312;Mixture-of-Expert&#23618;&#20013;&#23558;&#23427;&#20204;&#27719;&#38598;&#20026;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;MoE&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#22522;&#20110;&#26631;&#35760;&#30340;&#36335;&#30001;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20855;&#22791;&#22810;&#20010;&#19987;&#19994;&#39046;&#22495;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#20363;&#22914;&#32534;&#30721;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#19990;&#30028;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;Branch-Train-MiX&#65288;BTX&#65289;&#65292;&#20174;&#19968;&#20010;&#31181;&#23376;&#27169;&#22411;&#24320;&#22987;&#65292;&#23558;&#20854;&#20998;&#25903;&#35757;&#32451;&#25104;&#19987;&#23478;&#65292;&#20197;&#39640;&#21534;&#21520;&#37327;&#21644;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#23604;&#23596;&#24182;&#34892;&#26041;&#24335;&#12290;&#22312;&#21508;&#20010;&#19987;&#23478;&#24322;&#27493;&#35757;&#32451;&#21518;&#65292;BTX&#23558;&#23427;&#20204;&#20316;&#20026;&#19987;&#23478;&#22312;Mixture-of-Expert&#65288;MoE&#65289;&#23618;&#20013;&#27719;&#38598;&#20854;&#21069;&#39304;&#21442;&#25968;&#65292;&#24182;&#24179;&#22343;&#20854;&#20182;&#21442;&#25968;&#65292;&#38543;&#21518;&#26159;&#19968;&#20010;MoE&#24494;&#35843;&#38454;&#27573;&#26469;&#23398;&#20064;&#22522;&#20110;&#26631;&#35760;&#30340;&#36335;&#30001;&#12290;BTX&#27010;&#25324;&#20102;&#20004;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#21363;Branch-Train-Merge&#26041;&#27861;&#65292;&#23427;&#27809;&#26377;MoE&#24494;&#35843;&#38454;&#27573;&#26469;&#23398;&#20064;&#36335;&#30001;&#65292;&#20197;&#21450;&#31232;&#30095;&#21319;&#32423;&#65292;&#23427;&#30465;&#30053;&#20102;&#24322;&#27493;&#35757;&#32451;&#19987;&#23478;&#30340;&#38454;&#27573;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;BTX&#23454;&#29616;&#20102;&#26368;&#20339;&#31934;&#24230;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07816v1 Announce Type: cross  Abstract: We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.
&lt;/p&gt;</description></item><item><title>Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07815</link><description>&lt;p&gt;
Chronos: &#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Chronos: Learning the Language of Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07815
&lt;/p&gt;
&lt;p&gt;
Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Chronos&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26694;&#26550;&#12290;Chronos&#20351;&#29992;&#32553;&#25918;&#21644;&#37327;&#21270;&#23558;&#26102;&#38388;&#24207;&#21015;&#20540;&#26631;&#35760;&#21270;&#20026;&#22266;&#23450;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#36825;&#20123;&#26631;&#35760;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;T5&#31995;&#21015;&#65288;&#21442;&#25968;&#33539;&#22260;&#20174;20M&#21040;710M&#65289;&#23545;Chronos&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#21547;42&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Chronos&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;b&#65289;&#30456;&#23545;&#20110;&#19987;&#38376;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07805</link><description>&lt;p&gt;
&#36229;&#36234;&#27515;&#35760;&#30828;&#32972;&#65306;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: The Challenge of Random Memory Access in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#21442;&#25968;&#20869;&#37096;&#30340;&#30693;&#35782;&#23384;&#20648;&#21644;&#20869;&#23384;&#35775;&#38382;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#33021;&#22815;&#39034;&#24207;&#25110;&#38543;&#26426;&#22320;&#35775;&#38382;&#20854;&#20869;&#23384;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20840;&#38754;&#32972;&#35829;&#12289;&#36873;&#25321;&#24615;&#32972;&#35829;&#21644;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#33021;&#22815;&#39034;&#24207;&#35775;&#38382;&#20854;&#20869;&#23384;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35775;&#38382;&#24050;&#35760;&#24518;&#20869;&#23481;&#26102;&#36935;&#21040;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;LMs&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#36825;&#31181;&#24178;&#39044;&#24212;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#32972;&#35829;&#26469;&#22686;&#24378;&#38543;&#26426;&#35775;&#38382;&#25216;&#26415;&#23545;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
&lt;/p&gt;</description></item><item><title>jam-pgm&#26426;&#21046;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#33021;&#22815;&#32852;&#21512;&#36873;&#25321;&#20844;&#20849;&#25968;&#25454;&#21644;&#31169;&#23494;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.07797</link><description>&lt;p&gt;
&#32852;&#21512;&#36873;&#25321;&#65306;&#36866;&#24212;&#24615;&#22320;&#23558;&#20844;&#20849;&#20449;&#24687;&#32435;&#20837;&#31169;&#23494;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07797
&lt;/p&gt;
&lt;p&gt;
jam-pgm&#26426;&#21046;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#33021;&#22815;&#32852;&#21512;&#36873;&#25321;&#20844;&#20849;&#25968;&#25454;&#21644;&#31169;&#23494;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36793;&#38469;&#21644;&#22270;&#27169;&#22411;&#30340;&#19981;&#21516;ially&#31169;&#23494;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24050;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#26080;&#27861;&#25972;&#21512;&#20844;&#20849;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#21021;&#22987;&#21270;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20294;&#24403;&#27169;&#22411;&#32467;&#26500;&#26410;&#20107;&#20808;&#30830;&#23450;&#26102;&#65292;&#35813;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26426;&#21046;jam-pgm&#65292;&#23558;&#33258;&#36866;&#24212;&#27979;&#37327;&#26694;&#26550;&#25193;&#23637;&#21040;&#32852;&#21512;&#36873;&#25321;&#27979;&#37327;&#20844;&#20849;&#25968;&#25454;&#21644;&#31169;&#23494;&#25968;&#25454;&#20043;&#38388;&#12290;&#36825;&#19968;&#25216;&#26415;&#20801;&#35768;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#26426;&#21046;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#22312;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;jam-pgm&#33021;&#22815;&#32988;&#36807;&#20844;&#20849;&#36741;&#21161;&#21644;&#38750;&#20844;&#20849;&#36741;&#21161;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07797v1 Announce Type: cross  Abstract: Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.
&lt;/p&gt;</description></item><item><title>DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07788</link><description>&lt;p&gt;
DexCap&#65306;&#29992;&#20110;&#28789;&#24039;&#25805;&#20316;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#31227;&#26893;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07788
&lt;/p&gt;
&lt;p&gt;
DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#20026;&#26426;&#22120;&#20154;&#36171;&#20104;&#31867;&#20154;&#28789;&#24039;&#22312;&#29616;&#23454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#28982;&#32780;&#65292;&#29616;&#23384;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;&#20197;&#21450;&#23558;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#22256;&#38590;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DexCap&#65292;&#19968;&#20010;&#20415;&#25658;&#24335;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#20197;&#21450;DexIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#31639;&#27861;&#65292;&#21487;&#30452;&#25509;&#20174;&#20154;&#31867;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#35757;&#32451;&#28789;&#24039;&#26426;&#22120;&#20154;&#25216;&#33021;&#12290;DexCap&#22522;&#20110;SLAM&#21644;&#30005;&#30913;&#22330;&#20197;&#21450;&#29615;&#22659;&#30340;3D&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#23545;&#25163;&#33109;&#21644;&#25163;&#25351;&#36816;&#21160;&#30340;&#31934;&#30830;&#12289;&#25239;&#36974;&#25377;&#30340;&#36319;&#36394;&#12290;&#21033;&#29992;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;DexIL&#37319;&#29992;&#36870;&#36816;&#21160;&#23398;&#21644;&#22522;&#20110;&#28857;&#20113;&#30340;&#27169;&#20223;&#23398;&#20064;&#26469;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#19982;&#26426;&#22120;&#20154;&#25163;&#12290;&#38500;&#20102;&#20174;&#20154;&#31867;&#36816;&#21160;&#20013;&#23398;&#20064;&#22806;&#65292;DexCap&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07769</link><description>&lt;p&gt;
&#23558;&#31454;&#20105;&#36716;&#21270;&#20026;&#21512;&#20316;&#65306;&#22810;Agent&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#32452;&#32455;&#20013;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07769
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#65288;SMA&#65289;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#23454;&#20307;&#30340;&#21160;&#24577;&#24433;&#21709;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20316;&#20026;&#19968;&#31181;&#38761;&#26032;&#20154;&#31867;&#29992;&#25143;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#21033;&#29992;&#19987;&#38376;&#30340;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#20174;&#25805;&#20316;&#32452;&#32455;&#27969;&#31243;&#21040;&#22522;&#20110;&#24212;&#29992;&#30693;&#35782;&#21644;&#20154;&#30340;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#12290; &#20808;&#21069;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#22788;&#29702;&#26032;&#25361;&#25112;&#21644;&#23454;&#29992;&#20219;&#21153;&#65288;&#22914;&#24341;&#21457;&#36923;&#36753;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#65289;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#20195;&#29702;&#30340;&#33258;&#20027;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290; &#36824;&#32771;&#34385;&#21040;&#65292;&#20256;&#32479;&#25216;&#26415;&#65292;&#22914;&#28608;&#21457;&#24605;&#24819;&#38142;&#65292;&#38656;&#35201;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290; &#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VLM&#22312;&#22270;&#20687;&#23383;&#24149;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#20165;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.07750</link><description>&lt;p&gt;
Synth$^2$: &#29992;&#21512;&#25104;&#26631;&#39064;&#21644;&#22270;&#20687;&#23884;&#20837;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07750
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VLM&#22312;&#22270;&#20687;&#23383;&#24149;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#20165;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21457;&#23637;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20026;&#39640;&#25928;&#26377;&#25928;&#30340;VLM&#35757;&#32451;&#21019;&#24314;&#21512;&#25104;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20174;LLM&#29983;&#25104;&#30340;&#26631;&#39064;&#24320;&#22987;&#21512;&#25104;&#22270;&#20687;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#21512;&#25104;&#23545;&#29992;&#20110;&#35757;&#32451;VLM&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VLM&#22312;&#22270;&#20687;&#23383;&#24149;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#32780;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#20165;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#65292;&#25105;&#20204;&#36229;&#36807;&#22522;&#32447;17%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#22270;&#20687;&#23884;&#20837;&#31354;&#38388;&#21512;&#25104;&#27604;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#24555;25%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07750v1 Announce Type: cross  Abstract: The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel sp
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#25506;&#32034;&#21644;&#20250;&#21512;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#21035;&#33021;&#22312;&#22270;&#20013;$m$&#20010;&#21516;&#27493;&#26102;&#38388;&#27493;&#23454;&#29616;&#38598;&#20307;&#25506;&#32034;&#21644;$\frac{3}{2}m$&#26102;&#38388;&#27493;&#20869;&#23454;&#29616;&#20250;&#21512;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07748</link><description>&lt;p&gt;
&#38463;&#29790;&#38463;&#24503;&#28037;&#21644;&#24530;&#20462;&#26031;&#65306;&#22312;&#26410;&#30693;&#22270;&#20013;&#25506;&#32034;&#21644;&#20250;&#21512;&#30340;&#20004;&#20010;&#31227;&#21160;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07748
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#25506;&#32034;&#21644;&#20250;&#21512;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#21035;&#33021;&#22312;&#22270;&#20013;$m$&#20010;&#21516;&#27493;&#26102;&#38388;&#27493;&#23454;&#29616;&#38598;&#20307;&#25506;&#32034;&#21644;$\frac{3}{2}m$&#26102;&#38388;&#27493;&#20869;&#23454;&#29616;&#20250;&#21512;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#31227;&#21160;&#35745;&#31639;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#25506;&#32034;&#21644;&#20250;&#21512;&#65292;&#28041;&#21450;&#21040;&#19968;&#20010;&#26410;&#30693;&#22270;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#31227;&#21160;&#20195;&#29702;&#12290;&#36825;&#20004;&#20010;&#20195;&#29702;&#21487;&#20197;&#22312;&#25152;&#26377;&#33410;&#28857;&#19978;&#30340;&#30333;&#26495;&#19978;&#35835;&#20889;&#20449;&#24687;&#12290;&#23427;&#20204;&#27599;&#19968;&#27493;&#37117;&#27839;&#30528;&#19968;&#20010;&#30456;&#37051;&#30340;&#36793;&#31227;&#21160;&#12290;&#22312;&#25506;&#32034;&#38382;&#39064;&#20013;&#65292;&#20004;&#20010;&#20195;&#29702;&#20174;&#22270;&#20013;&#30456;&#21516;&#30340;&#33410;&#28857;&#20986;&#21457;&#65292;&#24517;&#39035;&#36941;&#21382;&#25152;&#26377;&#30340;&#36793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#19968;&#20010;&#31616;&#21333;&#21464;&#20307;&#21487;&#20197;&#22312;$m$&#20010;&#21516;&#27493;&#26102;&#38388;&#27493;&#20013;&#23454;&#29616;&#38598;&#20307;&#25506;&#32034;&#65292;&#20854;&#20013;$m$&#26159;&#22270;&#30340;&#36793;&#25968;&#12290;&#36825;&#25552;&#39640;&#20102;&#38598;&#20307;&#22270;&#25506;&#32034;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#22312;&#20250;&#21512;&#38382;&#39064;&#20013;&#65292;&#20195;&#29702;&#20174;&#22270;&#20013;&#19981;&#21516;&#30340;&#33410;&#28857;&#20986;&#21457;&#65292;&#24517;&#39035;&#23613;&#24555;&#30456;&#36935;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20445;&#35777;&#22312;&#33267;&#22810;$\frac{3}{2}m$&#20010;&#26102;&#38388;&#27493;&#20869;&#20250;&#21512;&#12290;&#36825;&#27604;&#25152;&#35859;&#30340;&#8220;&#31561;&#22920;&#22920;&#8221;&#31639;&#27861;&#38656;&#27714;&#30340;$2m$&#26102;&#38388;&#27493;&#26356;&#22909;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#20445;&#35777;&#37117;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07748v1 Announce Type: cross  Abstract: We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph. The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph. This improves the competitive ratio of collective graph exploration. In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps. All our guarantees are
&lt;/p&gt;</description></item><item><title>FineMath&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#38590;&#24230;&#32423;&#21035;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.07747</link><description>&lt;p&gt;
FineMath&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07747
&lt;/p&gt;
&lt;p&gt;
FineMath&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#38590;&#24230;&#32423;&#21035;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#31934;&#24515;&#31574;&#21010;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#21508;&#31181;&#25968;&#23398;&#27010;&#24565;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;FineMath&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;FineMath&#26088;&#22312;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#25945;&#25480;&#30340;&#20027;&#35201;&#25968;&#23398;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#28145;&#20837;&#20998;&#26512;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#25152;&#26377;17&#31867;&#25968;&#23398;&#38382;&#39064;&#22343;&#26681;&#25454;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25152;&#38656;&#30340;&#25512;&#29702;&#27493;&#39588;&#25968;&#37327;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#20854;&#38590;&#24230;&#32423;&#21035;&#12290;&#25105;&#20204;&#22312;FineMath&#19978;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07747v1 Announce Type: cross  Abstract: To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathem
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Probabilistic Easy Variational Causal Effect (PEACE)&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#27979;&#37327;X&#23545;Y&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#24773;&#20917;&#65292;&#36890;&#36807;&#31649;&#29702;&#27010;&#29575;&#23494;&#24230;&#20540;&#24378;&#24230;$d\ge 0$&#26469;&#23454;&#29616;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.07745</link><description>&lt;p&gt;
&#27010;&#29575;&#26131;&#21464;&#37327;&#22240;&#26524;&#25928;&#24212;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Easy Variational Causal Effect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07745
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Probabilistic Easy Variational Causal Effect (PEACE)&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#27979;&#37327;X&#23545;Y&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#24773;&#20917;&#65292;&#36890;&#36807;&#31649;&#29702;&#27010;&#29575;&#23494;&#24230;&#20540;&#24378;&#24230;$d\ge 0$&#26469;&#23454;&#29616;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#38543;&#26426;&#21521;&#37327;$X$&#21644;$Z$&#65292;&#20197;&#21450;$Y=g(X,Z)$&#30340;&#24773;&#20917;&#12290;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#36830;&#32493;&#30340;$X$&#21644;$Z$&#65292;&#36890;&#36807;&#20351;&#29992;&#24635;&#21464;&#24046;&#21644;$g$&#30340;&#36890;&#37327;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#22240;&#26524;&#38382;&#39064;&#39046;&#22495;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#31216;&#20026;Probabilistic Easy Variational Causal Effect (PEACE)&#30340;&#20989;&#25968;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;$X$&#23545;$Y$&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#32780;&#21516;&#26102;&#25913;&#21464;$X$&#30340;&#20540;&#65292;&#20294;&#20445;&#25345;$Z$&#30340;&#20540;&#19981;&#21464;&#12290;PEACE&#26159;&#20851;&#20110;$d\ge 0$&#30340;&#19968;&#20010;&#20989;&#25968;&#65292;&#31649;&#29702;&#30528;&#27010;&#29575;&#23494;&#24230;&#20540;$f(x|z)$&#30340;&#24378;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23558;&#19978;&#36848;&#24605;&#24819;&#25512;&#24191;&#21040;&#31163;&#25955;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#36830;&#32493;&#24773;&#20917;&#30340;&#20860;&#23481;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#27979;&#24230;&#35770;&#27010;&#24565;&#30740;&#31350;&#20102;PEACE&#30340;&#19968;&#20123;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#21487;&#36776;&#35782;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07745v1 Announce Type: cross  Abstract: Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07743</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#37197;&#22791;&#24037;&#20214;&#22788;&#29702;&#27969;&#27700;&#32447;&#65306;&#35745;&#31639;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26159;&#30284;&#30151;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#22312;&#26174;&#24494;&#38236;&#19979;&#36827;&#34892;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#22788;&#29702;&#36807;&#31243;&#20250;&#20135;&#29983;&#19968;&#20123;&#24037;&#20214;&#65292;&#26368;&#32456;&#20250;&#36716;&#31227;&#21040;&#29627;&#29827;&#36733;&#29627;&#29255;&#30340;&#25968;&#23383;&#21270;&#29256;&#26412;&#65292;&#21363;&#20840;&#29627;&#24187;&#28783;&#29255;&#12290;&#24037;&#20214;&#26159;&#35786;&#26029;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CPATH&#65289;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#24037;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#33258;&#21160;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#25324;&#25439;&#22351;&#32452;&#32455;&#12289;&#27169;&#31946;&#12289;&#35126;&#30385;&#32452;&#32455;&#12289;&#27668;&#27873;&#21644;&#22312;WSIs&#20013;&#30340;&#32452;&#32455;&#23398;&#26080;&#20851;&#34880;&#28082;&#31561;&#20116;&#31181;&#26174;&#33879;&#24037;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#29420;&#31435;&#30340;&#20108;&#20803;DL&#27169;&#22411;&#20316;&#20026;&#19987;&#23478;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#24037;&#20214;&#24418;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34701;&#21512;&#26426;&#21046;&#26469;&#38598;&#25104;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#26368;&#32456;&#30340;&#27010;&#29575;&#36827;&#34892;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26469;&#37327;&#21270;&#22810;&#38454;&#27573;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07741</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#36827;&#34892;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26469;&#37327;&#21270;&#22810;&#38454;&#27573;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
6D&#29289;&#20307;&#23039;&#24577;&#30340;&#20272;&#35745;&#26159;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#22312;&#20154;&#26426;&#20132;&#20114;&#12289;&#24037;&#19994;&#26816;&#39564;&#21644;&#33258;&#21160;&#21270;&#31561;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#21487;&#38752;&#30340;&#23039;&#24577;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#31934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;&#35768;&#22810;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#24182;&#38750;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#32780;&#26159;&#30001;&#22810;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#22312;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#38598;&#25104;&#34987;&#35748;&#20026;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#26657;&#20934;&#21644;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#38598;&#25104;&#21482;&#33021;&#24212;&#29992;&#20110;&#21487;&#20197;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26469;&#37327;&#21270;&#22810;&#38454;&#27573;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#36873;&#25321;SurfEmb&#20316;&#20026;&#20195;&#34920;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07741v1 Announce Type: cross  Abstract: The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07733</link><description>&lt;p&gt;
DSEG-LIME -- &#36890;&#36807;&#23618;&#27425;&#21270;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#25552;&#21319;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31034;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;LIME (Local Interpretable Model-agnostic Explanations) &#26159;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;XAI&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#26469;&#21019;&#24314;&#29305;&#24449;&#20197;&#35782;&#21035;&#30456;&#20851;&#30340;&#20998;&#31867;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36739;&#24046;&#30340;&#20998;&#21106;&#21487;&#33021;&#20250;&#24433;&#21709;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#24182;&#21066;&#24369;&#21508;&#20010;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#25972;&#20307;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSEG-LIME (Data-Driven Segmentation LIME)&#65292;&#20855;&#26377;: i) &#29992;&#20110;&#29983;&#25104;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;, &#21644; ii) &#36890;&#36807;&#32452;&#21512;&#23454;&#29616;&#30340;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#20351;&#29992;&#26469;&#33258;ImageNet&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#23545;DSEG-LIME&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;-&#36825;&#20123;&#24773;&#26223;&#19981;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;XAI&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07733v1 Announce Type: cross  Abstract: Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30452;&#25509;&#20998;&#26512;&#26368;&#20248;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#20197;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07724</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#38480;&#21046;&#30340;&#20108;&#20998;&#31867;&#20013;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Balancing Fairness and Accuracy in Data-Restricted Binary Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07724
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30452;&#25509;&#20998;&#26512;&#26368;&#20248;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#20197;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#25935;&#24863;&#20449;&#24687;&#30340;&#24212;&#29992;&#21487;&#33021;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#21487;&#29992;&#25968;&#25454;&#35774;&#32622;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#27169;&#25311;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#22312;&#22235;&#31181;&#23454;&#38469;&#24773;&#26223;&#19979;&#25506;&#35752;&#21487;&#29992;&#20110;&#20998;&#26512;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#19982;&#20808;&#21069;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32463;&#35757;&#32451;&#20197;&#38544;&#24335;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21521;&#37327;&#12289;&#31867;&#21035;&#26631;&#31614;&#21644;&#25935;&#24863;&#23646;&#24615;&#30340;&#28508;&#22312;&#20998;&#24067;&#30340;&#35780;&#20998;&#20989;&#25968;&#30340;&#36755;&#20986;&#26469;&#32771;&#34385;&#36825;&#31181;&#26435;&#34913;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30452;&#25509;&#36890;&#36807;&#20174;&#25968;&#25454;&#38598;&#26412;&#36523;&#26500;&#24314;&#30340;&#31163;&#25955;&#36817;&#20284;&#26469;&#20998;&#26512;&#26368;&#20248;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#36825;&#20010;&#28508;&#22312;&#20998;&#24067;&#19978;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#22810;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07724v1 Announce Type: cross  Abstract: Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25104;&#21151;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#19978;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07720</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35789;&#36827;&#34892;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Auto-regressive Modeling via Visual Words
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25104;&#21151;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#19978;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#22238;&#24402;&#24314;&#27169;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#22330;&#26223;&#20197;&#26500;&#24314;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#26102;&#65292;&#23384;&#22312;&#19968;&#20010;&#24040;&#22823;&#22256;&#38590;&#65292;&#21363;&#22270;&#20687;&#20449;&#24687;&#22312;LMM&#20013;&#20197;&#36830;&#32493;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#22788;&#29702;&#65292;&#26080;&#27861;&#33719;&#24471;&#31163;&#25955;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#26412;&#25991;&#39318;&#27425;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;LLM&#35789;&#27719;&#34920;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LMM&#20013;&#35821;&#20041;&#31354;&#38388;&#20869;&#35270;&#35273;&#29305;&#24449;&#30340;&#20998;&#24067;&#20197;&#21450;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to repre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07711</link><description>&lt;p&gt;
SSM&#36935;&#19978;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;: &#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#19979;&#30340;&#39640;&#25928;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#20687;&#29983;&#25104;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#30028;&#23545;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#35270;&#39057;&#29983;&#25104;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#26469;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#20869;&#23384;&#28040;&#32791;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#36825;&#31181;&#38480;&#21046;&#22312;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26356;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#30001;&#20110;&#30456;&#23545;&#20110;&#24207;&#21015;&#38271;&#24230;&#65292;SSMs&#20855;&#26377;&#32447;&#24615;&#20869;&#23384;&#28040;&#32791;&#65292;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;UCF101&#36825;&#19968;&#35270;&#39057;&#29983;&#25104;&#30340;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#25506;&#35752;SSMs&#22312;&#26356;&#38271;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.07708</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#22870;&#21169;&#25913;&#21892;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07708
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26159;&#29992;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;RLHF&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#26469;&#28304;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#20154;&#31867;&#26631;&#27880;&#38169;&#35823;&#65292;&#20351;&#24471;&#27969;&#31243;&#33030;&#24369;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#22870;&#21169;&#30340;&#24809;&#32602;&#39033;&#65292;&#21629;&#21517;&#20026;&#8220;&#23545;&#27604;&#22870;&#21169;&#8221;&#65292;&#26469;&#25552;&#39640;&#22870;&#21169;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#31163;&#32447;&#25277;&#26679;&#27493;&#39588;&#65292;&#33719;&#21462;&#29992;&#20316;&#22522;&#20934;&#35745;&#31639;&#30340;&#25552;&#31034;&#21709;&#24212;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#22522;&#20934;&#21709;&#24212;&#35745;&#31639;&#23545;&#27604;&#22870;&#21169;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;Proximal Policy Optimization&#65288;PPO&#65289;&#27493;&#39588;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#22870;&#21169;&#20351;&#24471;LLM&#33021;&#22815;&#24809;&#32602;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#20248;&#20110;&#22522;&#32447;&#30340;&#25913;&#36827;&#65292;&#26681;&#25454;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#19988;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
&lt;/p&gt;</description></item><item><title>&#23545;&#31216; Q-learning&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#22122;&#22768;&#26469;&#20943;&#23569;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#20559;&#26012;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07704</link><description>&lt;p&gt;
&#23545;&#31216; Q-learning&#65306;&#20943;&#23569;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#20559;&#26012;
&lt;/p&gt;
&lt;p&gt;
Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07704
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216; Q-learning&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#22122;&#22768;&#26469;&#20943;&#23569;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#20559;&#26012;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20272;&#35745;&#20540;&#20989;&#25968;&#20197;&#35780;&#20272;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#36136;&#37327;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35813;&#20540;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#38544;&#21547;&#22320;&#20551;&#23450;&#19968;&#20010;&#39640;&#26031;&#35823;&#24046;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#29305;&#24615;&#65292;&#29992;&#20110;&#35757;&#32451;&#20540;&#20989;&#25968;&#30340;&#35823;&#24046;&#20998;&#24067;&#36890;&#24120;&#26159;&#20542;&#26012;&#30340;&#65292;&#36829;&#21453;&#20102;&#26368;&#23567;&#20108;&#20056;&#27861;&#20013;&#23545;&#27491;&#24577;&#35823;&#24046;&#20998;&#24067;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216; Q-learning &#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20174;&#38646;&#22343;&#20540;&#20998;&#24067;&#29983;&#25104;&#30340;&#21512;&#25104;&#22122;&#22768;&#34987;&#28155;&#21152;&#21040;&#30446;&#26631;&#20540;&#20013;&#65292;&#20197;&#29983;&#25104;&#39640;&#26031;&#35823;&#24046;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;MuJoCo&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#20998;&#24067;&#30340;&#20559;&#26012;&#65292;&#23427;&#25552;&#39640;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07704v1 Announce Type: cross  Abstract: In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35780;&#35770;&#26469;&#23454;&#29616;&#35266;&#28857;&#25688;&#35201;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#36991;&#20813;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#39640;&#26114;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.07693</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;&#35265;&#35266;&#28857;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07693
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35780;&#35770;&#26469;&#23454;&#29616;&#35266;&#28857;&#25688;&#35201;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#36991;&#20813;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#39640;&#26114;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#35266;&#28857;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;70&#65285;&#30340;&#35780;&#35770;&#26159;&#31215;&#26497;&#30340;&#65292;&#24403;&#21069;&#30340;&#35266;&#28857;&#25688;&#35201;&#26041;&#27861;&#22312;&#32473;&#23450;&#36127;&#38754;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#19981;&#24895;&#29983;&#25104;&#36127;&#38754;&#25688;&#35201;&#65292;&#36896;&#25104;&#24773;&#24863;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#24863;&#20559;&#35265;&#65292;&#19968;&#20010;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#32780;&#19981;&#36807;&#20998;&#20381;&#36182;&#29305;&#23450;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#38754;&#20020;&#20004;&#20010;&#32570;&#28857;&#65306;1&#65289;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#38382;&#39064;&#25110;&#27602;&#24615;&#65307;2&#65289;&#26114;&#36149;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#35265;&#35266;&#28857;&#25688;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#27491;&#38754;&#25991;&#26412;&#33719;&#24471;&#20102;&#23567;&#35268;&#27169;&#21512;&#25104;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#29983;&#25104;&#30340;&#20869;&#23481;&#35757;&#32451;&#19968;&#20010;&#35299;&#32806;&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;</title><link>https://arxiv.org/abs/2403.07691</link><description>&lt;p&gt;
&#20855;&#26377;&#36180;&#29575;&#27604;&#30340;&#26080;&#21442;&#32771;&#21333;&#20307;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reference-free Monolithic Preference Optimization with Odds Ratio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#20559;&#22909;&#23545;&#40784;&#31639;&#27861;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20173;&#28982;&#23545;&#20110;&#25104;&#21151;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20559;&#22909;&#23545;&#40784;&#30340;&#29615;&#22659;&#20013;SFT&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#23545;&#20110;&#20559;&#22909;&#23545;&#40784;&#30340;SFT&#26469;&#35828;&#65292;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#26045;&#21152;&#36731;&#24494;&#24809;&#32602;&#23601;&#36275;&#22815;&#20102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21019;&#26032;&#30340;&#26080;&#21442;&#32771;&#27169;&#22411;&#30340;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#25163;&#27573;&#35777;&#26126;&#65292;&#36180;&#29575;&#27604;&#26159;&#22312;125M&#33267;7B&#19981;&#21516;&#35268;&#27169;&#19979;&#36827;&#34892;SFT&#26102;&#23545;&#27604;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#39118;&#26684;&#30340;&#26126;&#26234;&#36873;&#25321;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;ORPO&#22312;&#20165;UltraFeedback&#19978;&#23545;Phi-2&#65288;2.7B&#65289;&#12289;Llama-2&#65288;7B&#65289;&#21644;Mistral&#65288;7B&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36229;&#36234;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27515;&#20129;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;Demon Pruning&#65288;DemP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#20135;&#29983;&#65292;&#21160;&#24577;&#23454;&#29616;&#32593;&#32476;&#31232;&#30095;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07688</link><description>&lt;p&gt;
Maxwell&#30340;&#24694;&#39764;&#20043;&#24037;&#20316;&#65306;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#39281;&#21644;&#23454;&#29616;&#26377;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07688
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27515;&#20129;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;Demon Pruning&#65288;DemP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#20135;&#29983;&#65292;&#21160;&#24577;&#23454;&#29616;&#32593;&#32476;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;$\textit{&#27515;&#20129;&#31070;&#32463;&#20803;}$&#29616;&#35937;&#8212;&#8212;&#22312;&#35757;&#32451;&#26399;&#38388;&#21464;&#24471;&#19981;&#27963;&#36291;&#25110;&#39281;&#21644;&#65292;&#36755;&#20986;&#20026;&#38646;&#30340;&#21333;&#20803;&#8212;&#20256;&#32479;&#19978;&#34987;&#35270;&#20026;&#19981;&#21487;&#21462;&#30340;&#65292;&#19982;&#20248;&#21270;&#25361;&#25112;&#26377;&#20851;&#65292;&#24182;&#23548;&#33268;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20007;&#22833;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#19987;&#27880;&#20110;&#31232;&#30095;&#24615;&#21644;&#20462;&#21098;&#12290;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#32034;&#21508;&#31181;&#36229;&#21442;&#25968;&#37197;&#32622;&#23545;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#26377;&#21161;&#20110;&#20419;&#36827;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Demon Pruning}$&#65288;DemP&#65289;&#65292;&#19968;&#31181;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#25193;&#24352;&#65292;&#21160;&#24577;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27963;&#36291;&#21333;&#20803;&#19978;&#27880;&#20837;&#22122;&#22768;&#21644;&#37319;&#29992;&#21333;&#21608;&#26399;&#35843;&#24230;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;DemP&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;CIFAR10&#19978;&#30340;&#23454;&#39564;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07688v1 Announce Type: cross  Abstract: When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.07687</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#27880;&#37322;&#65306;&#21033;&#29992;&#22320;&#29702;&#25968;&#25454;&#30456;&#20284;&#24615;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#22312;&#22320;&#29702;&#21644;&#32463;&#27982;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23545;&#27599;&#20010;&#20154;&#37117;&#26377;&#25928;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#26469;&#33258;&#35199;&#26041;&#22269;&#23478;&#65292;&#23548;&#33268;&#23545;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#22269;&#23478;&#30340;&#32467;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#20174;&#36825;&#20123;&#22269;&#23478;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#65292;&#20294;&#27880;&#37322;&#25104;&#26412;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#38656;&#35201;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#21644;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#28041;&#21450;&#25214;&#21040;&#20855;&#26377;&#26368;&#22823;&#35270;&#35273;&#24046;&#24322;&#30340;&#20027;&#39064;&#65288;&#29289;&#20307;&#21644;&#21160;&#20316;&#65289;&#22270;&#20687;&#30340;&#22269;&#23478;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23545;&#20110;&#36825;&#20123;&#20027;&#39064;&#22312;&#35270;&#35273;&#19978;&#26356;&#30456;&#20284;&#30340;&#22269;&#23478;&#65292;&#24182;&#34920;&#26126;&#21033;&#29992;&#36825;&#20123;&#22269;&#23478;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#27880;&#37322;&#25104;&#26412;&#26041;&#38754;&#33410;&#30465;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07687v1 Announce Type: cross  Abstract: Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that usin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07657</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Spatiotemporal Prediction with Bayesian Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#38598;&#30001;&#31354;&#38388;&#21442;&#32771;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#21830;&#19994;&#26234;&#33021;&#39046;&#22495;&#65292;&#20363;&#22914;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#65292;&#30142;&#30149;&#36319;&#36394;&#21644;&#20113;&#38656;&#27714;&#39044;&#27979;&#12290;&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#39044;&#27979;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Bayesian Neural Field (BayesNF)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#26102;&#31354;&#22495;&#19978;&#20016;&#23500;&#27010;&#29575;&#20998;&#24067;&#30340;&#36890;&#29992;&#39046;&#22495;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#21464;&#24322;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;BayesNF&#23558;&#29992;&#20110;&#39640;&#23481;&#37327;&#20989;&#25968;&#20272;&#35745;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#29992;&#20110;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#26041;&#38754;&#36827;&#34892;&#24207;&#21015;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#19978;&#19979;&#25991;&#21407;&#22411;&#24863;&#30693;&#23398;&#20064;&#31574;&#30053;&#65288;CPAL&#65289;&#65292;&#36890;&#36807;&#20943;&#23569;&#30693;&#35782;&#20559;&#24046;&#20197;&#22686;&#24378;&#21407;&#22411;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25429;&#33719;&#23454;&#20363;&#35821;&#20041;&#30340;&#22810;&#26679;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07630</link><description>&lt;p&gt;
&#29417;&#29454;&#23646;&#24615;&#65306;&#38754;&#21521;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#19978;&#19979;&#25991;&#21407;&#22411;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#19978;&#19979;&#25991;&#21407;&#22411;&#24863;&#30693;&#23398;&#20064;&#31574;&#30053;&#65288;CPAL&#65289;&#65292;&#36890;&#36807;&#20943;&#23569;&#30693;&#35782;&#20559;&#24046;&#20197;&#22686;&#24378;&#21407;&#22411;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25429;&#33719;&#23454;&#20363;&#35821;&#20041;&#30340;&#22810;&#26679;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26041;&#27861;&#33268;&#21147;&#20110;&#25972;&#21512;&#19978;&#19979;&#25991;&#30693;&#35782;&#20197;&#25552;&#39640;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;CAM&#65289;&#30340;&#23436;&#25972;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23454;&#20363;&#19982;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#30693;&#35782;&#20559;&#24046;&#24433;&#21709;&#21407;&#22411;&#20805;&#20998;&#29702;&#35299;&#23454;&#20363;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#21463;&#21407;&#22411;&#23398;&#20064;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21407;&#22411;&#24863;&#30693;&#26469;&#25429;&#33719;&#23454;&#20363;&#30340;&#22810;&#26679;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#30001;&#20110;&#36825;&#31181;&#30693;&#35782;&#20559;&#24046;&#65292;&#19978;&#19979;&#25991;&#21407;&#22411;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#28608;&#27963;&#30456;&#20284;&#21644;&#39057;&#32321;&#20849;&#21516;&#20986;&#29616;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20943;&#23569;&#20559;&#24046;&#26469;&#22686;&#24378;&#21407;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#23545;&#35937;&#21306;&#22495;&#30340;&#31354;&#38388;&#35206;&#30422;&#12290;&#22522;&#20110;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#19978;&#19979;&#25991;&#21407;&#22411;&#24863;&#30693;&#23398;&#20064;&#65288;CPAL&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#20041;&#19978;&#19979;&#25991;&#26469;&#20016;&#23500;&#23454;&#20363;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07630v1 Announce Type: cross  Abstract: Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#39062;&#28508;&#22312;&#31354;&#38388;&#26144;&#23556;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22686;&#24378;&#21387;&#32553;&#26263;&#22270;&#20687;&#26102;&#36991;&#20813;&#21387;&#32553;&#20266;&#24433;&#25918;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07622</link><description>&lt;p&gt;
&#21387;&#32553;&#26263;&#22270;&#20687;&#22686;&#24378;&#30340;&#22810;&#20010;&#28508;&#22312;&#31354;&#38388;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Multiple Latent Space Mapping for Compressed Dark Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#39062;&#28508;&#22312;&#31354;&#38388;&#26144;&#23556;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22686;&#24378;&#21387;&#32553;&#26263;&#22270;&#20687;&#26102;&#36991;&#20813;&#21387;&#32553;&#20266;&#24433;&#25918;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26263;&#22270;&#20687;&#22686;&#24378;&#26088;&#22312;&#23558;&#26263;&#22270;&#20687;&#36716;&#25442;&#20026;&#27491;&#24120;&#20142;&#24230;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#26263;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#20197;&#26410;&#32463;&#21387;&#32553;&#30340;&#26263;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26263;&#22270;&#20687;&#32463;&#24120;&#22312;&#23384;&#20648;&#25110;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#20043;&#21069;&#34987;&#21387;&#32553;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21387;&#32553;&#26263;&#22270;&#20687;&#26102;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#12290;&#24403;&#21069;&#26041;&#27861;&#25918;&#22823;&#20102;&#38544;&#34255;&#22312;&#26263;&#21306;&#22495;&#20013;&#30340;&#20266;&#24433;&#65292;&#36825;&#23548;&#33268;&#35266;&#23519;&#32773;&#24863;&#21040;&#19981;&#36866;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22686;&#24378;&#21387;&#32553;&#26263;&#22270;&#20687;&#65292;&#21516;&#26102;&#36991;&#20813;&#21387;&#32553;&#20266;&#24433;&#30340;&#25918;&#22823;&#12290;&#30001;&#20110;&#32441;&#29702;&#32454;&#33410;&#19982;&#21387;&#32553;&#20266;&#24433;&#22312;&#21387;&#32553;&#26263;&#22270;&#20687;&#20013;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#32454;&#33410;&#22686;&#24378;&#19982;&#38459;&#22622;&#20266;&#24433;&#25233;&#21046;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#30456;&#20114;&#30683;&#30462;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22788;&#29702;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#39062;&#28508;&#22312;&#26144;&#23556;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07622v1 Announce Type: cross  Abstract: Dark image enhancement aims at converting dark images to normal-light images. Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance. However, in practice, dark images are often compressed before storage or transmission over the Internet. Current methods get poor performance when processing compressed dark images. Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers. Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification. Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space. Therefore, we handle the task in latent space. To this end, we propose a novel latent mapping network based on variational auto-encoder 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#37096;&#20998;&#22833;&#24518;&#24335;&#36951;&#24536;&#21644;&#36880;&#23618;&#37096;&#20998;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.07611</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#23618;&#37096;&#20998;&#26426;&#22120;&#36951;&#24536;&#23454;&#29616;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#30693;&#35782;&#21024;&#38500;
&lt;/p&gt;
&lt;p&gt;
Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#37096;&#20998;&#22833;&#24518;&#24335;&#36951;&#24536;&#21644;&#36880;&#23618;&#37096;&#20998;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07611v1 &#21457;&#34920;&#31867;&#22411;&#65306;cross  &#25688;&#35201;&#65306;&#26426;&#22120;&#36951;&#24536;&#22240;&#20854;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#25830;&#38500;&#24050;&#32463;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20174;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#33719;&#24471;&#30340;&#30693;&#35782;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31181;&#33021;&#21147;&#20351;&#25968;&#25454;&#25345;&#26377;&#32773;&#33021;&#22815;&#20005;&#26684;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36951;&#24536;&#25216;&#26415;&#38754;&#20020;&#23454;&#38469;&#32422;&#26463;&#65292;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#36951;&#24536;&#21518;&#36827;&#34892;&#31616;&#30701;&#30340;&#24494;&#35843;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#37096;&#20998;&#22833;&#24518;&#24335;&#36951;&#24536;&#65292;&#23558;&#36880;&#23618;&#20462;&#21098;&#19982;&#22833;&#24518;&#24335;&#36951;&#24536;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#30340;&#26356;&#26032;&#34987;&#20462;&#21098;&#24182;&#23384;&#20648;&#65292;&#38543;&#21518;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#36951;&#24536;&#29305;&#23450;&#25968;&#25454;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#23558;&#36880;&#23618;&#37096;&#20998;&#26356;&#26032;&#38598;&#25104;&#21040;&#26631;&#31614;&#32763;&#36716;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#36951;&#24536;&#20013;&#65292;&#20197;&#20943;&#36731;&#30001;&#20110;&#36951;&#24536;&#32780;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07611v1 Announce Type: cross  Abstract: Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of da
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;Couler&#31995;&#32479;&#65292;&#29992;&#20110;&#20113;&#20013;&#32479;&#19968;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20248;&#21270;&#65292;&#20027;&#35201;&#35265;&#35299;&#22312;&#20110;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#29983;&#25104;ML&#24037;&#20316;&#27969;</title><link>https://arxiv.org/abs/2403.07608</link><description>&lt;p&gt;
Couler: &#20113;&#20013;&#32479;&#19968;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Couler: Unified Machine Learning Workflow Optimization in Cloud
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07608
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;Couler&#31995;&#32479;&#65292;&#29992;&#20110;&#20113;&#20013;&#32479;&#19968;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20248;&#21270;&#65292;&#20027;&#35201;&#35265;&#35299;&#22312;&#20110;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#29983;&#25104;ML&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#25512;&#21160;&#30528;&#21508;&#31181;&#32452;&#32455;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#35266;&#24565;&#20013;&#30740;&#31350;&#20013;&#30340;ML&#30456;&#21453;&#65292;ML&#24037;&#20316;&#27969;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65292;&#36164;&#28304;&#23494;&#38598;&#30340;&#65292;&#24182;&#19988;&#32791;&#26102;&#30340;&#12290;&#25193;&#23637;ML&#24037;&#20316;&#27969;&#20197;&#21253;&#21547;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#21644;&#25968;&#25454;&#31867;&#22411;&#21487;&#33021;&#23548;&#33268;&#26356;&#22823;&#30340;&#24037;&#20316;&#37327;&#21644;&#22686;&#21152;&#30340;&#37096;&#32626;&#25104;&#26412;&#12290;&#30446;&#21069;&#65292;&#26377;&#35768;&#22810;&#24037;&#20316;&#27969;&#24341;&#25806;&#21487;&#29992;&#65288;&#20854;&#20013;&#36229;&#36807;&#21313;&#20010;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#65289;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#23545;&#20110;&#26368;&#32456;&#29992;&#25143;&#26469;&#35828;&#26500;&#25104;&#20102;&#25484;&#25569;&#19981;&#21516;&#24341;&#25806;API&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#29305;&#23450;&#24037;&#20316;&#27969;&#24341;&#25806;&#20248;&#21270;ML&#25805;&#20316;&#65288;MLOps&#65289;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#22312;&#36328;&#19981;&#21516;&#24341;&#25806;&#36827;&#34892;&#24037;&#20316;&#27969;&#20248;&#21270;&#26041;&#38754;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07608v1 Announce Type: cross  Abstract: Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20351;&#29992;&#26465;&#27454;&#65288;DToU&#65289;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#20197;&#21450;&#19968;&#20010; DToU &#25512;&#29702;&#22120;&#65292;&#20351;&#29992;&#25143;&#21644;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20351;&#29992;&#26412;&#22320;&#30693;&#35782;&#25351;&#23450;&#20854;DToU&#25919;&#31574;&#30340;&#37096;&#20998;&#65292;&#20174;&#32780;&#39564;&#35777;&#21512;&#35268;&#24615;&#65292;&#24182;&#38450;&#27490;&#25968;&#25454;&#28389;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.07587</link><description>&lt;p&gt;
&#20998;&#24067;&#24335; Web &#30340;&#38271;&#26399;&#35821;&#20041;&#25968;&#25454;&#20351;&#29992;&#26465;&#27454;
&lt;/p&gt;
&lt;p&gt;
Perennial Semantic Data Terms of Use for Decentralized Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20351;&#29992;&#26465;&#27454;&#65288;DToU&#65289;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#20197;&#21450;&#19968;&#20010; DToU &#25512;&#29702;&#22120;&#65292;&#20351;&#29992;&#25143;&#21644;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20351;&#29992;&#26412;&#22320;&#30693;&#35782;&#25351;&#23450;&#20854;DToU&#25919;&#31574;&#30340;&#37096;&#20998;&#65292;&#20174;&#32780;&#39564;&#35777;&#21512;&#35268;&#24615;&#65292;&#24182;&#38450;&#27490;&#25968;&#25454;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#32593;&#32476;&#26085;&#30410;&#20013;&#24515;&#21270;&#65292;&#24341;&#21457;&#23545;&#29992;&#25143;&#38544;&#31169;&#20405;&#29359;&#30340;&#25285;&#24551;&#12290;&#20998;&#24067;&#24335; Web &#26550;&#26500;&#65288;&#22914; Solid&#65289;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22909;&#25484;&#25511;&#20010;&#20154;&#8220;Pods&#8221;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#29992;&#25143;&#24517;&#39035;&#22312;&#20247;&#22810;&#24212;&#29992;&#31243;&#24207;&#38388;&#36827;&#34892;&#25225;&#25321;&#65292;&#20915;&#23450;&#21738;&#20010;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#34987;&#20449;&#20219;&#35775;&#38382;&#20182;&#20204;&#30340;&#25968;&#25454; Pods&#12290;&#36825;&#36890;&#24120;&#28041;&#21450;&#38405;&#35835;&#20887;&#38271;&#22797;&#26434;&#30340;&#20351;&#29992;&#26465;&#27454;&#21327;&#35758;&#65292;&#29992;&#25143;&#24448;&#24448;&#35273;&#24471;&#33392;&#38590;&#25110;&#24178;&#33030;&#24573;&#35270;&#12290;&#36825;&#23041;&#32961;&#20102;&#29992;&#25143;&#33258;&#20027;&#26435;&#65292;&#24182;&#38459;&#30861;&#20102;&#23545;&#25968;&#25454;&#28389;&#29992;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20351;&#29992;&#26465;&#27454;&#65288;DToU&#65289;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#20197;&#21450;&#19968;&#20010; DToU &#25512;&#29702;&#22120;&#12290;&#29992;&#25143;&#21644;&#24212;&#29992;&#31243;&#24207;&#20250;&#20351;&#29992;&#26412;&#22320;&#30693;&#35782;&#25351;&#23450;&#20182;&#20204;DToU&#25919;&#31574;&#30340;&#37096;&#20998;&#65292;&#28085;&#30422;&#26435;&#38480;&#65292;&#35201;&#27714;&#65292;&#31105;&#27490;&#21644;&#20041;&#21153;&#12290;&#33258;&#21160;&#25512;&#29702;&#20250;&#39564;&#35777;&#21512;&#35268;&#24615;&#65292;&#21516;&#26102;&#25512;&#23548;&#25919;&#31574;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07587v1 Announce Type: new  Abstract: In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives poli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#36866;&#23452;&#24615;&#65292;&#24182;&#32467;&#21512;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#20174;&#24444;&#27492;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#31038;&#20250;&#35268;&#33539;&#12290;</title><link>https://arxiv.org/abs/2403.07586</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#31038;&#20250;&#36866;&#23452;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#36866;&#23452;&#24615;&#65292;&#24182;&#32467;&#21512;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#20174;&#24444;&#27492;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#31038;&#20250;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#34892;&#20026;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24191;&#27867;&#24212;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#65292;&#25506;&#32034;&#20010;&#20307;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#23398;&#20064;&#20854;&#29420;&#29305;&#29615;&#22659;&#30340;&#21516;&#26102;&#20063;&#20174;&#24444;&#27492;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35774;&#32622;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;FL&#22522;&#20934;&#65292;&#35780;&#20272;&#19981;&#21516;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#26631;&#31614;&#22238;&#24402;&#30446;&#26631;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#21035;&#23398;&#20064;&#39044;&#27979;&#19981;&#21516;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#36866;&#23452;&#24615;&#65292;&#21516;&#26102;&#19982;&#20182;&#20154;&#20849;&#20139;&#20854;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#25353;&#19981;&#21516;&#19978;&#19979;&#25991;&#25286;&#20998;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#36880;&#28176;&#36328;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#65288;FCL&#65289;&#22522;&#20934;&#65292;&#23558;FL&#26041;&#27861;&#35843;&#25972;&#20026;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#20197;&#25345;&#32493;&#23398;&#20064;&#31038;&#20250;&#36866;&#23452;&#30340;&#20195;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07586v1 Announce Type: cross  Abstract: As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.07573</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#21487;&#36866;&#24212;&#24615;&#35745;&#31639;&#21644;&#32593;&#32476;&#34701;&#21512;&#30340;&#21160;&#24577;&#26410;&#26469;&#65288;ACNC&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#36827;6G&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#35745;&#20250;&#20986;&#29616;&#23454;&#36136;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#31361;&#20986;&#20102;&#30001;&#22823;&#37327;&#36830;&#25509;&#21644;&#20005;&#26684;&#36981;&#23432;&#26381;&#21153;&#36136;&#37327;/&#20307;&#39564;&#65288;QoS/E&#65289;&#20808;&#20915;&#26465;&#20214;&#25152;&#29305;&#24449;&#21270;&#30340;&#20840;&#38754;&#30340;&#19968;&#20999;&#23545;&#19968;&#20999;&#20132;&#20114;&#12290;&#21363;&#23558;&#38754;&#20020;&#30340;&#25361;&#25112;&#28304;&#20110;&#36164;&#28304;&#31232;&#32570;&#65292;&#20419;&#20351;&#26377;&#24847;&#35782;&#22320;&#21521;&#35745;&#31639;-&#32593;&#32476;&#34701;&#21512;&#65288;CNC&#65289;&#36807;&#28193;&#65292;&#20316;&#20026;&#32852;&#21512;&#36164;&#28304;&#32534;&#25490;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;CNC&#30340;&#26426;&#21046;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#29616;&#26410;&#26469;&#26381;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;Metaverse&#30340;&#20351;&#29992;&#24773;&#26223;&#20013;&#65292;&#21487;&#33021;&#20250;&#30001;&#20110;&#29992;&#25143;&#12289;&#26381;&#21153;&#21644;&#36164;&#28304;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#34880;&#31958;&#25511;&#21046;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36716;&#25442;BG&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#32771;&#34385;&#33647;&#29289;&#20316;&#29992;&#30340;&#24310;&#36831;&#21644;&#25345;&#20037;&#24615;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07566</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#34880;&#31958;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#34880;&#31958;&#25511;&#21046;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36716;&#25442;BG&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#32771;&#34385;&#33647;&#29289;&#20316;&#29992;&#30340;&#24310;&#36831;&#21644;&#25345;&#20037;&#24615;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#31958;&#65288;BG&#65289;&#25511;&#21046;&#28041;&#21450;&#36890;&#36807;&#20307;&#22806;&#33008;&#23707;&#32032;&#27880;&#23556;&#23558;&#20010;&#20307;&#30340;&#34880;&#31958;&#32500;&#25345;&#22312;&#20581;&#24247;&#33539;&#22260;&#20869;&#65292;&#36825;&#23545;&#20110;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24739;&#32773;&#33258;&#25105;&#31649;&#29702;&#26041;&#24335;&#32321;&#29712;&#19988;&#21361;&#38505;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#21644;&#33258;&#21160;&#21270;&#30340;BG&#25511;&#21046;&#26041;&#27861;&#65292;&#20854;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26174;&#31034;&#20986;&#20316;&#20026;&#26032;&#20852;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33647;&#29289;&#27987;&#24230;&#30340;&#25351;&#25968;&#34928;&#20943;&#27169;&#22411;&#23558;BG&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#36716;&#25442;&#20026;MDP&#65292;&#32771;&#34385;&#21040;&#33647;&#29289;&#20316;&#29992;&#30340;&#24310;&#36831;&#21644;&#25345;&#20037;&#24615;&#65292;&#20174;PAE-POMDP&#65288;&#25345;&#32493;&#34892;&#21160;&#25928;&#26524;-&#37096;&#20998;&#21487;&#35265;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#21040;MDP&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#27493;DRL&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20854;&#20013;&#36824;&#20351;&#29992;&#20102;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#65288;PER&#65289;&#37319;&#26679;&#26041;&#27861;&#12290;&#19982;&#21333;&#27493;&#33258;&#20030;&#26356;&#26032;&#30456;&#27604;&#65292;&#22810;&#27493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07566v1 Announce Type: new  Abstract: Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07559</link><description>&lt;p&gt;
&#20026;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38598;&#25104;&#20248;&#20808;&#32423;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#36817;&#26469;&#22240;&#20854;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Ensembling Prioritized Hybrid Policies (EPH)&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#36890;&#20449;&#30340;MARL-MAPF&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Q-learning&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Donut&#21644;OpenAI GPT-3.5 Turbo&#20004;&#31181;&#21069;&#27839;AI&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#25991;&#20214;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24314;&#31569;&#35268;&#26684;&#25991;&#20214;&#30340;&#30446;&#24405;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#20195;&#34920;&#20102;&#25991;&#26723;&#32034;&#24341;&#39046;&#22495;&#21521;&#33258;&#21160;&#21270;&#20449;&#24687;&#25552;&#21462;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.07553</link><description>&lt;p&gt;
&#25991;&#26723;&#32034;&#24341;&#30340;&#26410;&#26469;&#65306;GPT&#21644;Donut&#38761;&#26032;&#30446;&#24405;&#20869;&#23481;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
The future of document indexing: GPT and Donut revolutionize table of content processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Donut&#21644;OpenAI GPT-3.5 Turbo&#20004;&#31181;&#21069;&#27839;AI&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#25991;&#20214;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24314;&#31569;&#35268;&#26684;&#25991;&#20214;&#30340;&#30446;&#24405;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#20195;&#34920;&#20102;&#25991;&#26723;&#32034;&#24341;&#39046;&#22495;&#21521;&#33258;&#21160;&#21270;&#20449;&#24687;&#25552;&#21462;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#39033;&#30446;&#20005;&#37325;&#20381;&#36182;&#20887;&#38271;&#12289;&#22797;&#26434;&#30340;&#35268;&#26684;&#25991;&#20214;&#65292;&#25163;&#24037;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#32321;&#29712;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#21033;&#29992;&#20004;&#31181;&#21069;&#27839;AI&#27169;&#22411;&#30340;&#33021;&#21147;&#65306;Donut&#65292;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20174;&#25195;&#25551;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#32780;&#26080;&#38656;OCR&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;OpenAI GPT-3.5 Turbo&#65292;&#19968;&#20010;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#33719;&#21462;&#24314;&#31569;&#35268;&#26684;&#25991;&#20214;&#30340;&#30446;&#24405;&#65288;ToCs&#65289;&#65292;&#28982;&#21518;&#23558;ToCs&#25991;&#26412;&#32467;&#26500;&#21270;&#20026;JSON&#25968;&#25454;&#12290;&#23588;&#20026;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#34987;&#23454;&#29616;&#65292;Donut&#22312;&#26377;&#25928;&#32452;&#32455;ToCs&#26041;&#38754;&#36798;&#21040;85%&#65292;GPT-3.5 Turbo&#36798;&#21040;89%&#12290;&#36825;&#19968;&#37324;&#31243;&#30865;&#24335;&#30340;&#25104;&#23601;&#20195;&#34920;&#20102;&#25991;&#26723;&#32034;&#24341;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#23637;&#31034;&#20102;AI&#33258;&#21160;&#21270;&#20449;&#24687;&#25552;&#21462;&#22312;&#21508;&#31181;&#25991;&#26723;&#31867;&#22411;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#25552;&#21319;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07553v1 Announce Type: cross  Abstract: Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effectively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting effic
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07548</link><description>&lt;p&gt;
&#20114;&#21160;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning For Interactive Instruction Following Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07548
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25991;&#29486;&#22823;&#37117;&#20551;&#23450;&#20195;&#29702;&#22312;&#24320;&#22987;&#26102;&#23601;&#23398;&#20064;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#23398;&#20064;&#22330;&#26223;&#36739;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#20195;&#29702;&#24212;&#35813;&#22312;&#25506;&#32034;&#21644;&#24863;&#30693;&#19990;&#30028;&#30340;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#23398;&#20064;&#12290;&#20026;&#20102;&#26397;&#30528;&#26356;&#30495;&#23454;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#22330;&#26223;&#36808;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20379;&#20855;&#36523;&#20195;&#29702;&#20351;&#29992;&#65307;&#23398;&#20064;&#26032;&#34892;&#20026;&#65288;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#65292;Behavior-IL&#65289;&#21644;&#26032;&#29615;&#22659;&#65288;&#29615;&#22659;&#22686;&#37327;&#23398;&#20064;&#65292;Environment-IL&#65289;&#12290;&#22312;&#20219;&#21153;&#20013;&#65292;&#20808;&#21069;&#22522;&#20110;&#8220;&#25968;&#25454;&#20808;&#39564;&#8221;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#32500;&#25252;&#36807;&#21435;&#20219;&#21153;&#30340;logits&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#30340;&#20449;&#24687;&#24448;&#24448;&#26159;&#19981;&#20805;&#20998;&#23398;&#20064;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#65292;&#32780;&#36825;&#31181;&#20449;&#24687;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#22522;&#20110;&#33258;&#20449;&#24230;&#24471;&#20998;&#32780;&#26080;&#38656;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#26469;&#26356;&#26032;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#27169;&#25311;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#23384;&#20648;I/O&#30165;&#36857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#65292;&#20026;&#21457;&#23637;&#36127;&#36131;&#20219;&#30340;&#32593;&#32476;&#23433;&#20840;&#24037;&#20855;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.07540</link><description>&lt;p&gt;
WannaLaugh: &#19968;&#31181;&#21487;&#37197;&#32622;&#30340;&#21202;&#32034;&#36719;&#20214;&#27169;&#25311;&#22120; -- &#23398;&#20064;&#27169;&#20223;&#24694;&#24847;&#23384;&#20648;&#30165;&#36857;
&lt;/p&gt;
&lt;p&gt;
WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#27169;&#25311;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#23384;&#20648;I/O&#30165;&#36857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#65292;&#20026;&#21457;&#23637;&#36127;&#36131;&#20219;&#30340;&#32593;&#32476;&#23433;&#20840;&#24037;&#20855;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21202;&#32034;&#36719;&#20214;&#20316;&#20026;&#19968;&#31181;&#21487;&#24597;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#65292;&#25345;&#32493;&#23545;&#20840;&#29699;&#20010;&#20154;&#21644;&#32452;&#32455;&#36896;&#25104;&#20005;&#37325;&#21518;&#26524;&#12290;&#20256;&#32479;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#38745;&#24577;&#31614;&#21517;&#21644;&#24212;&#29992;&#31243;&#24207;&#34892;&#20026;&#27169;&#24335;&#65292;&#21463;&#21040;&#36825;&#20123;&#23041;&#32961;&#21160;&#24577;&#26412;&#36136;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21202;&#32034;&#36719;&#20214;&#27169;&#25311;&#22120;&#12290;&#35813;&#24037;&#20855;&#26088;&#22312;&#23433;&#20840;&#22320;&#27169;&#20223;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#65292;&#32780;&#19981;&#24341;&#36215;&#23454;&#38469;&#20260;&#23475;&#25110;&#20256;&#25773;&#24694;&#24847;&#36719;&#20214;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#21202;&#32034;&#36719;&#20214;&#34892;&#20026;&#30340;&#29420;&#29305;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#27169;&#25311;&#22120;&#21019;&#24314;&#23384;&#20648;I/O&#30165;&#36857;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#22312;&#24320;&#21457;&#36127;&#36131;&#20219;&#30340;&#32593;&#32476;&#23433;&#20840;&#24037;&#20855;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07540v1 Announce Type: cross  Abstract: Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a ransomware emulator. This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#30456;&#20851;&#24615;&#20998;&#25968;&#8221;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#22312;&#22823;&#22810;&#25968;&#20294;&#19981;&#26159;&#25152;&#26377;&#35745;&#21010;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#25110;&#34892;&#21160;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#22320;&#26631;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#22320;&#26631;&#30340;&#38382;&#39064;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07510</link><description>&lt;p&gt;
&#30456;&#20851;&#24615;&#20998;&#25968;&#65306;&#35268;&#21010;&#20013;&#30340;&#19968;&#31181;&#31867;&#20284;&#22320;&#26631;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Relevance Score: A Landmark-Like Heuristic for Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#30456;&#20851;&#24615;&#20998;&#25968;&#8221;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#22312;&#22823;&#22810;&#25968;&#20294;&#19981;&#26159;&#25152;&#26377;&#35745;&#21010;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#25110;&#34892;&#21160;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#22320;&#26631;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#22320;&#26631;&#30340;&#38382;&#39064;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#26631;&#26159;&#35745;&#21010;&#38382;&#39064;&#25152;&#26377;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#20013;&#37117;&#20986;&#29616;&#30340;&#20107;&#23454;&#25110;&#34892;&#21160;&#12290;&#23427;&#20204;&#24050;&#25104;&#21151;&#29992;&#20110;&#35745;&#31639;&#24341;&#23548;&#35745;&#21010;&#25628;&#32034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#30456;&#20851;&#24615;&#20998;&#25968;&#8221;&#26469;&#25193;&#23637;&#36825;&#19968;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#20294;&#19981;&#26159;&#25152;&#26377;&#35745;&#21010;&#20013;&#20197;&#23454;&#29616;&#20219;&#20309;&#32473;&#23450;&#30446;&#26631;&#30340;&#20107;&#23454;&#25110;&#34892;&#21160;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35745;&#31639;&#36825;&#31181;&#30456;&#20851;&#24615;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#35745;&#21010;&#25628;&#32034;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20934;&#35268;&#21010;&#38382;&#39064;&#23454;&#39564;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#22320;&#26631;&#30340;&#26368;&#20808;&#36827;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21407;&#22987;&#30340;&#22522;&#20110;&#22320;&#26631;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#22320;&#26631;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32570;&#20047;&#38750;&#24179;&#20961;&#22320;&#26631;&#30340;&#38382;&#39064;&#19978;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07510v1 Announce Type: new  Abstract: Landmarks are facts or actions that appear in all valid solutions of a planning problem. They have been used successfully to calculate heuristics that guide the search for a plan. We investigate an extension to this concept by defining a novel "relevance score" that helps identify facts or actions that appear in most but not all plans to achieve any given goal. We describe an approach to compute this relevance score and use it as a heuristic in the search for a plan. We experimentally compare the performance of our approach with that of a state of the art landmark-based heuristic planning approach using benchmark planning problems. While the original landmark-based heuristic leads to better performance on problems with well-defined landmarks, our approach substantially improves performance on problems that lack non-trivial landmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20998;&#22359;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#21644;&#39118;&#26684;&#21270;&#32454;&#31890;&#24230;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.07500</link><description>&lt;p&gt;
&#20998;&#22359;LoRA&#65306;&#37325;&#26032;&#23457;&#35270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32454;&#31890;&#24230;LoRA&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#21644;&#39118;&#26684;&#21270;
&lt;/p&gt;
&lt;p&gt;
Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07500
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20998;&#22359;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#21644;&#39118;&#26684;&#21270;&#32454;&#31890;&#24230;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21644;&#39118;&#26684;&#21270;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26159;&#25351;&#23548;&#19968;&#20010;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#24341;&#20837;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#39044;&#26399;&#30340;&#39118;&#26684;&#20013;&#12290; &#26368;&#36817;&#65292;&#24191;&#27867;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#24182;&#26497;&#22823;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290; &#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#29616;&#26377;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#20010;&#24615;&#21270;&#21644;&#39118;&#26684;&#21270;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#22359;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20197;&#23545;SD&#30340;&#19981;&#21516;&#22359;&#25191;&#34892;&#32454;&#31890;&#24230;&#24494;&#35843;&#65292;&#21487;&#20197;&#29983;&#25104;&#24544;&#23454;&#20110;&#36755;&#20837;&#25552;&#31034;&#21644;&#30446;&#26631;&#36523;&#20221;&#19988;&#20855;&#26377;&#25152;&#38656;&#26679;&#24335;&#30340;&#22270;&#20687;&#12290; &#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07500v1 Announce Type: cross  Abstract: The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.07483</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Diabetes Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07483
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#30001;&#33008;&#23707;&#32032;&#20135;&#29983;&#25110;&#21033;&#29992;&#19981;&#36275;&#23548;&#33268;&#30340;&#65292;&#23545;&#36523;&#20307;&#36896;&#25104;&#20102;&#24191;&#27867;&#30340;&#21361;&#23475;&#12290;&#29616;&#26377;&#30340;&#35786;&#26029;&#26041;&#27861;&#36890;&#24120;&#26159;&#20405;&#20837;&#24615;&#30340;&#65292;&#24182;&#20276;&#26377;&#35832;&#22810;&#32570;&#28857;&#65292;&#27604;&#22914;&#25104;&#26412;&#38480;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#20687;&#31867;&#38388;k&#26368;&#36817;&#37051;(CkNN)&#21644;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;(GRNN)&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#21033;&#29992;&#20256;&#24863;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;&#25209;&#37327;&#26631;&#20934;&#21270;&#30340;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;(BPNN)&#36827;&#34892;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#37325;&#37319;&#26679;&#21644;&#24402;&#19968;&#21270;&#20197;&#23454;&#29616;&#31867;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20851;&#30340;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07483v1 Announce Type: cross  Abstract: Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07440</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#65306;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LPLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#25511;&#21046;LPLMs&#30340;&#36755;&#20986;&#34892;&#20026;&#12290;&#26412;&#25991;&#21463;&#22823;&#33041;&#21151;&#33021;&#21463;&#20854;&#20960;&#20309;&#32467;&#26500;&#22609;&#36896;&#30340;&#21551;&#21457;&#65292;&#23558;&#36825;&#19968;&#24605;&#24819;&#34701;&#20837;LoRA&#25216;&#26415;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>&#23558;&#39135;&#29289;&#35782;&#21035;&#20174;&#39135;&#22530;&#22330;&#26223;&#27867;&#21270;&#21040;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;DailyFood-172&#21644;DailyFood-16&#65292;&#26088;&#22312;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07403</link><description>&lt;p&gt;
&#20174;&#39135;&#22530;&#39135;&#29289;&#21040;&#26085;&#24120;&#39184;&#28857;&#65306;&#23558;&#39135;&#29289;&#35782;&#21035;&#27867;&#21270;&#21040;&#26356;&#23454;&#38469;&#30340;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07403
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39135;&#29289;&#35782;&#21035;&#20174;&#39135;&#22530;&#22330;&#26223;&#27867;&#21270;&#21040;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;DailyFood-172&#21644;DailyFood-16&#65292;&#26088;&#22312;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#31867;&#21035;&#30340;&#31934;&#30830;&#35782;&#21035;&#23545;&#26234;&#33021;&#20581;&#24247;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#33879;&#21517;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22914;Food-101&#21644;VIREO Food-172&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#39135;&#29289;&#22270;&#20687;&#36164;&#28304;&#65292;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#30340;&#32321;&#33635;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#37117;&#26159;&#20174;&#39135;&#22530;&#22330;&#26223;&#31934;&#24515;&#31574;&#21010;&#30340;&#65292;&#19982;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#39135;&#29289;&#22806;&#35266;&#26377;&#25152;&#20559;&#24046;&#12290;&#36825;&#31181;&#24046;&#24322;&#32473;&#22312;&#36825;&#20123;&#39135;&#22530;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#20154;&#31867;&#25152;&#36935;&#21040;&#30340;&#26356;&#24191;&#27867;&#30340;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21363;DailyFood-172&#21644;DailyFood-16&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#31574;&#21010;&#26469;&#33258;&#26085;&#24120;&#39184;&#28857;&#30340;&#39135;&#29289;&#22270;&#20687;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#35780;&#20272;&#26041;&#27861;&#20174;&#31934;&#24515;&#31574;&#21010;&#30340;&#39135;&#29289;&#22270;&#20687;&#39046;&#22495;&#21521;&#26085;&#24120;&#29983;&#27963;&#39135;&#29289;&#22270;&#20687;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07403v1 Announce Type: cross  Abstract: The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field. Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life. This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans. Toward this end, we present two new benchmarks, namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals. These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain. In addition, we also propose a simple ye
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COM2&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#24182;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07398</link><description>&lt;p&gt;
&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#30340;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COM2&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#24182;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24120;&#35782;&#25512;&#29702;&#38656;&#35201;&#20855;&#26377;&#25512;&#29702;&#20107;&#20214;&#20043;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25512;&#26029;&#22312;&#36825;&#31181;&#20851;&#31995;&#20043;&#19979;&#30340;&#38544;&#21547;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#23398;&#20250;&#20026;&#28041;&#21450;&#22797;&#26434;&#20107;&#20214;&#30456;&#20114;&#20316;&#29992;&#30340;&#32972;&#26223;&#21644;&#38382;&#39064;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COM2&#65288;COMplex COMmonsense&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20174;&#29616;&#26377;&#24120;&#35782;&#30693;&#35782;&#22270;&#65288;CSKG&#65289;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;A&#21644;B&#30340;&#32852;&#21512;&#25928;&#26524;&#25110;&#22240;&#26524;&#20851;&#31995;&#65292;&#25110;&#20107;&#20214;C&#30340;&#25928;&#26524;&#30340;&#25928;&#26524;&#65289;&#65292;&#24182;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20854;&#29992;&#22810;&#36873;&#21644;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#30340;&#24418;&#24335;&#34920;&#36798;&#20986;&#26469;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;COM2&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#38646;-shot&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#39046;&#22495;&#20869;&#36824;&#26159;&#39046;&#22495;&#22806;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#20174;&#32780;&#21033;&#29992;&#36741;&#21161;&#30340;&#20813;&#30123;&#33639;&#20809;&#22270;&#20687;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21452;&#21521;&#21040;&#21333;&#21521;IHC&#22270;&#20687;&#30340;&#20219;&#21153;&#24863;&#30693;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07389</link><description>&lt;p&gt;
&#36741;&#21161;CycleGAN&#24341;&#23548;&#19979;&#30340;&#20174;&#21452;&#21521;&#21040;&#21333;&#21521;IHC&#22270;&#20687;&#30340;&#20219;&#21153;&#24863;&#30693;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07389
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#20174;&#32780;&#21033;&#29992;&#36741;&#21161;&#30340;&#20813;&#30123;&#33639;&#20809;&#22270;&#20687;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21452;&#21521;&#21040;&#21333;&#21521;IHC&#22270;&#20687;&#30340;&#20219;&#21153;&#24863;&#30693;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#20174;&#19968;&#20010;&#28304;&#22270;&#20687;&#22495;&#21040;&#19968;&#20010;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22495;&#30340;&#36716;&#25442;&#25104;&#20026;&#21487;&#33021;&#12290;&#34429;&#28982;Cycle&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#20013;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#32422;&#26463;&#20381;&#36182;&#20110;&#20004;&#20010;&#22495;&#20043;&#38388;&#23384;&#22312;&#21487;&#36870;&#26144;&#23556;&#30340;&#24773;&#20917;&#65292;&#32780;&#22312;&#26579;&#33394;&#21333;&#21521;&#21644;&#21452;&#21521;&#20813;&#30123;&#32452;&#21270;&#65288;IHC&#65289;&#26816;&#27979;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#36716;&#25442;&#19981;&#26159;&#36825;&#26679;&#30340;&#12290;&#38024;&#23545;&#20174;&#21518;&#32773;&#21040;&#21069;&#32773;&#30340;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#65292;&#21033;&#29992;&#19968;&#32452;&#20813;&#30123;&#33639;&#20809;&#65288;IF&#65289;&#22270;&#20687;&#20316;&#20026;&#36741;&#21161;&#30340;&#19981;&#37197;&#23545;&#22270;&#20687;&#22495;&#12290;&#22312;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07389v1 Announce Type: cross  Abstract: Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches.
&lt;/p&gt;</description></item><item><title>S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07384</link><description>&lt;p&gt;
SmallToLarge (S2L): &#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25552;&#20379;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07384
&lt;/p&gt;
&lt;p&gt;
S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#36873;&#25321;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20013;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#24494;&#35843;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;S2L&#65288;SmallToLarge&#65289;&#65292;&#23427;&#21033;&#29992;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#26469;&#25351;&#23548;&#26356;&#22823;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;SFT&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#32553;&#20943;&#21040;&#21407;&#22987;MathInstruct&#25968;&#25454;&#38598;&#65288;Yue&#31561;&#20154;&#65292;2023&#65289;&#30340;&#20165;11%&#65292;&#20197;&#36798;&#21040;&#20840;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;6&#20010;&#39046;&#22495;&#20869;&#22806;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#24179;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;4.7%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#36873;&#25321;50K&#25968;&#25454;&#36827;&#34892;SFT&#65292;S2L&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gabor&#24341;&#23548;&#30340;&#21464;&#21387;&#22120;&#65288;Gabformer&#65289;&#29992;&#20110;&#21333;&#24133;&#22270;&#20687;&#21435;&#38632;&#65292;&#36890;&#36807;&#34701;&#20837;Gabor&#28388;&#27874;&#22120;&#22788;&#29702;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#23545;&#23616;&#37096;&#32441;&#29702;&#29305;&#24449;&#30340;&#20851;&#27880;&#65292;&#20197;&#21450;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07380</link><description>&lt;p&gt;
&#22522;&#20110;Gabor&#24341;&#23548;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#21333;&#24133;&#22270;&#20687;&#21435;&#38632;
&lt;/p&gt;
&lt;p&gt;
Gabor-guided transformer for single image deraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gabor&#24341;&#23548;&#30340;&#21464;&#21387;&#22120;&#65288;Gabformer&#65289;&#29992;&#20110;&#21333;&#24133;&#22270;&#20687;&#21435;&#38632;&#65292;&#36890;&#36807;&#34701;&#20837;Gabor&#28388;&#27874;&#22120;&#22788;&#29702;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#23545;&#23616;&#37096;&#32441;&#29702;&#29305;&#24449;&#30340;&#20851;&#27880;&#65292;&#20197;&#21450;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21435;&#38632;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24212;&#23545;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#23545;&#35270;&#35273;&#20219;&#21153;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gabor&#24341;&#23548;&#30340;&#21464;&#21387;&#22120;&#65288;Gabformer&#65289;&#29992;&#20110;&#21333;&#24133;&#22270;&#20687;&#21435;&#38632;&#65292;&#36890;&#36807;&#23558;Gabor&#28388;&#27874;&#22120;&#22788;&#29702;&#30340;&#20449;&#24687;&#34701;&#20837;&#26597;&#35810;&#21521;&#37327;&#26469;&#22686;&#24378;&#23545;&#23616;&#37096;&#32441;&#29702;&#29305;&#24449;&#30340;&#20851;&#27880;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07380v1 Announce Type: cross  Abstract: Image deraining have have gained a great deal of attention in order to address the challenges posed by the effects of harsh weather conditions on visual tasks. While convolutional neural networks (CNNs) are popular, their limitations in capturing global information may result in ineffective rain removal. Transformer-based methods with self-attention mechanisms have improved, but they tend to distort high-frequency details that are crucial for image fidelity. To solve this problem, we propose the Gabor-guided tranformer (Gabformer) for single image deraining. The focus on local texture features is enhanced by incorporating the information processed by the Gabor filter into the query vector, which also improves the robustness of the model to noise due to the properties of the filter. Extensive experiments on the benchmarks demonstrate that our method outperforms state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07376</link><description>&lt;p&gt;
NavCoT: &#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#25552;&#21319;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;(VLN)&#20316;&#20026;&#20855;&#26377;&#37325;&#35201;&#30740;&#31350;&#20215;&#20540;&#30340;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#36523;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#31034;&#31359;&#36234;&#22797;&#26434;&#30340;3D&#29615;&#22659;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;VLN&#20013;&#25552;&#39640;&#23548;&#33322;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#31163;&#32447;&#26041;&#24335;&#19979;&#30340;&#20351;&#29992;&#36890;&#24120;&#22312;VLN&#20219;&#21153;&#21644;LLM&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#36973;&#21463;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23548;&#33322;&#24605;&#32500;&#38142;(NavCoT)&#30340;&#26032;&#22411;&#31574;&#30053;&#65292;&#25105;&#20204;&#36890;&#36807;&#23436;&#25104;&#39046;&#22495;&#20869;&#39640;&#25928;&#21442;&#25968;&#35757;&#32451;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#39046;&#22495;&#24046;&#36317;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;LLM&#34987;&#25552;&#31034;&#36890;&#36807;&#20316;&#20026;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#23548;&#33322;&#24605;&#32500;&#38142;&#65306;1)&#26681;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07376v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#38543;&#26426;&#24615;&#12289;&#27169;&#31946;&#36923;&#36753;&#21644;&#27169;&#31946;&#38598;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2403.07363</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07363
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#38543;&#26426;&#24615;&#12289;&#27169;&#31946;&#36923;&#36753;&#21644;&#27169;&#31946;&#38598;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#23545;&#20110;&#25968;&#25454;&#25366;&#25496;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#25925;&#38556;&#26816;&#27979;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24320;&#21457;&#20934;&#30830;&#12289;&#36866;&#29992;&#19988;&#39640;&#25928;&#30340;&#20998;&#31867;&#26041;&#27861;&#21644;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#24378;&#28872;&#38656;&#27714;&#12290;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#24120;&#29992;&#20110;&#22797;&#26434;&#26465;&#20214;&#19979;&#20998;&#31867;&#30340;&#36890;&#29992;&#31639;&#27861;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#20854;&#19982;&#19981;&#21516;&#27169;&#31946;&#29702;&#35770;&#30340;&#32467;&#21512;&#20173;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#30452;&#35273;&#27169;&#31946;&#38543;&#26426;&#26862;&#26519;&#65288;IFRF&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#30001;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#65288;IFDT&#65289;&#32452;&#25104;&#30340;&#26032;&#30340;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#12290;&#26862;&#26519;&#20013;&#30340;&#36825;&#20123;&#26641;&#20351;&#29992;&#30452;&#35273;&#27169;&#31946;&#20449;&#24687;&#22686;&#30410;&#26469;&#36873;&#25321;&#29305;&#24449;&#65292;&#24182;&#32771;&#34385;&#20449;&#24687;&#20256;&#36755;&#20013;&#30340;&#29369;&#35947;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34701;&#21512;&#20102;&#26469;&#33258;&#33258;&#21161;&#25277;&#26679;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#38543;&#26426;&#24615;&#12289;&#27169;&#31946;&#36923;&#36753;&#21644;&#27169;&#31946;&#38598;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07363v1 Announce Type: new  Abstract: Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection. There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability. Random forest is a general algorithm that is often used for classification under complex conditions. Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring. In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT). Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission. The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier syste
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;CSI&#21453;&#39304;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#30340;&#36716;&#25442;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#30340;&#30721;&#26412;&#35774;&#35745;&#31574;&#30053;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;CSI&#37325;&#24314;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07355</link><description>&lt;p&gt;
&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#29992;&#20110;CSI&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;CSI&#21453;&#39304;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#30340;&#36716;&#25442;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#30340;&#30721;&#26412;&#35774;&#35745;&#31574;&#30053;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;CSI&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#36895;&#29575;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21453;&#39304;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#22312;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#26694;&#26550;&#19979;&#25552;&#20379;&#20102;&#28508;&#22312;&#30690;&#37327;&#30340;&#26377;&#38480;&#20301;&#34920;&#31034;&#65292;&#21516;&#26102;&#22522;&#20110;&#24418;&#29366;&#22686;&#30410;&#30690;&#37327;&#37327;&#21270;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#28508;&#22312;&#30690;&#37327;&#30340;&#24133;&#24230;&#20351;&#29992;&#21512;&#36866;&#30340;&#36716;&#25442;&#20989;&#25968;&#21644;&#38750;&#22343;&#21248;&#26631;&#37327;&#30721;&#26412;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28508;&#22312;&#30690;&#37327;&#30340;&#26041;&#21521;&#21017;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;Grassmann&#30721;&#26412;&#36827;&#34892;&#37327;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#20110;&#23884;&#22871;&#30721;&#26412;&#30340;&#30721;&#23383;&#36873;&#25321;&#35268;&#21017;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#36895;&#29575;&#30721;&#26412;&#35774;&#35745;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38477;&#20302;&#20102;&#19982;VQ-VAE&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;CSI&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07355v1 Announce Type: cross  Abstract: This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction pe
&lt;/p&gt;</description></item><item><title>KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07350</link><description>&lt;p&gt;
KEBench: &#29992;&#20110;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07350
&lt;/p&gt;
&lt;p&gt;
KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30446;&#21069;&#65292;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#30693;&#35782;&#32534;&#36753;&#30740;&#31350;&#24456;&#23569;&#12290;&#32534;&#36753;LVLMs&#38754;&#20020;&#30528;&#26377;&#25928;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30830;&#20445;&#20462;&#25913;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#38752;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#19968;&#33324;&#24615;&#65289;&#29992;&#20110;&#34913;&#37327;LVLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#65292;&#24182;&#19988;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#22320;&#21033;&#29992;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;$\textbf{KEBench}$&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#24230;&#37327;&#26631;&#20934;(&#21487;&#31227;&#26893;&#24615;)&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20511;&#21161;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#25968;&#25454;&#21576;&#29616;&#20986;&#26126;&#30830;&#30340;&#32473;&#23454;&#20307;&#26041;&#21521;&#24615;&#12290;&#36825;&#31181;&#26041;&#21521;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25552;&#21462;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#36827;&#34892;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;ASTE:&#19968;&#31181;&#26497;&#31616;&#30340;&#26631;&#35760;&#26041;&#26696;&#19982;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) &#26159;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#26032;&#20852;&#23376;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#24773;&#24863;&#19977;&#20803;&#32452;&#12290;&#29616;&#26377;&#30340;ASTE&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#39069;&#22806;&#30340;&#32467;&#26500;&#25110;&#22806;&#37096;&#25968;&#25454;&#26469;&#22797;&#26434;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#21487;&#27604;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26102;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#20110;GPT 3.5&#21644;GPT 4&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#36824;&#20026;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#33539;&#24335;&#19979;&#25512;&#36827;ASTE&#25216;&#26415;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07342v1 Announce Type: cross  Abstract: Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba UNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#23616;&#37096;&#31354;&#38388;&#24314;&#27169;&#26041;&#38754;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#20840;&#23616;&#24314;&#27169;&#26041;&#38754;&#20445;&#25345;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07332</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba UNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#36229;&#36234;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba UNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#23616;&#37096;&#31354;&#38388;&#24314;&#27169;&#26041;&#38754;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#20840;&#23616;&#24314;&#27169;&#26041;&#38754;&#20445;&#25345;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25552;&#20379;&#20102;&#26377;&#20851;&#30446;&#26631;&#22120;&#23448;&#25110;&#32452;&#32455;&#36718;&#24275;&#21644;&#23610;&#23544;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#35786;&#26029;&#12289;&#20998;&#26512;&#21644;&#27835;&#30103;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#22312;&#36825;&#19968;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38382;&#39064;&#65292;&#22914;&#26377;&#38480;&#30340;&#24863;&#30693;&#33539;&#22260;&#25110;&#26114;&#36149;&#30340;&#36828;&#31243;&#24314;&#27169;&#12290;Mamba&#65292;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#24314;&#27169;&#30340;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#26368;&#36817;&#20986;&#29616;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;2D&#21644;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba U-&#24418;&#32593;&#32476;&#65292;&#21363;LMa-UNet&#12290;&#25105;&#20204;LMa-UNet&#30340;&#19968;&#20010;&#31361;&#20986;&#29305;&#28857;&#26159;&#21033;&#29992;&#22823;&#31383;&#21475;&#65292;&#22312;&#23616;&#37096;&#31354;&#38388;&#24314;&#27169;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#23567;&#26680;&#30340;CNN&#21644;&#22522;&#20110;&#23567;&#31383;&#21475;&#30340;Transformer&#65292;&#21516;&#26102;&#19982;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#30340;&#33258;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;&#22312;&#20840;&#23616;&#24314;&#27169;&#26041;&#38754;&#20445;&#25345;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07332v1 Announce Type: cross  Abstract: In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;POSNEGDM&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#21453;&#39304;&#24378;&#21270;&#22120;&#65292;&#22312;&#36133;&#34880;&#30151;&#27835;&#30103;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;&#24739;&#32773;&#29983;&#23384;&#29575;&#25552;&#39640;&#33267;97.39&#65285;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07309</link><description>&lt;p&gt;
&#38024;&#23545;&#36133;&#34880;&#30151;&#27835;&#30103;&#30340;&#24378;&#21270;&#24207;&#36143;&#20915;&#31574;&#65306;&#20855;&#26377;&#27515;&#20129;&#20998;&#31867;&#22120;&#21644;&#21464;&#21387;&#22120;&#30340;POSNEGDM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;POSNEGDM&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#21453;&#39304;&#24378;&#21270;&#22120;&#65292;&#22312;&#36133;&#34880;&#30151;&#27835;&#30103;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;&#24739;&#32773;&#29983;&#23384;&#29575;&#25552;&#39640;&#33267;97.39&#65285;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36133;&#34880;&#30151;&#26159;&#30001;&#26426;&#20307;&#23545;&#24863;&#26579;&#20135;&#29983;&#22840;&#24352;&#21453;&#24212;&#24341;&#21457;&#30340;&#19968;&#31181;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#65292;&#35201;&#27714;&#32039;&#24613;&#24178;&#39044;&#20197;&#38450;&#27490;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#22788;&#29702;&#36133;&#34880;&#30151;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31163;&#32447;&#22330;&#26223;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#23384;&#27963;&#29575;&#20302;&#20110;50&#65285;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;POSNEGDM&#26694;&#26550;&#65292;&#21363;&#8220;&#29992;&#20110;&#24207;&#36143;&#20915;&#31574;&#30340;&#20855;&#26377;&#27491;&#36127;&#31034;&#33539;&#30340;&#24378;&#21270;&#23398;&#20064;&#8221;&#65292;&#21033;&#29992;&#21019;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#21644;&#21453;&#39304;&#24378;&#21270;&#22120;&#22797;&#21046;&#19987;&#23478;&#34892;&#20026;&#65292;&#21516;&#26102;&#32771;&#34385;&#20010;&#20307;&#24739;&#32773;&#29305;&#24449;&#12290;&#20855;&#26377;96.7&#65285;&#20934;&#30830;&#24230;&#30340;&#27515;&#20129;&#20998;&#31867;&#22120;&#25351;&#23548;&#27835;&#30103;&#20915;&#31574;&#21462;&#24471;&#31215;&#26497;&#32467;&#26524;&#12290;POSNEGDM&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#65292;&#25405;&#25937;&#20102;97.39&#65285;&#30340;&#24739;&#32773;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#20915;&#31574;&#21464;&#21387;&#22120;&#21644;&#34892;&#20026;&#20811;&#38534;&#65289;&#30340;&#23384;&#27963;&#29575;&#20998;&#21035;&#20026;33.4&#65285;&#21644;43.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07309v1 Announce Type: cross  Abstract: Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7\% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respective
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23631;&#38556;&#20989;&#25968;&#30340;&#32456;&#27490;&#20445;&#35777;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07308</link><description>&lt;p&gt;
&#20351;&#29992;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23631;&#38556;&#20989;&#25968;&#24182;&#20855;&#26377;&#32456;&#27490;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07308
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23631;&#38556;&#20989;&#25968;&#30340;&#32456;&#27490;&#20445;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#38556;&#20989;&#25968;&#26159;&#20026;&#31995;&#32479;&#24314;&#31435;&#23433;&#20840;&#20445;&#35777;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26080;&#36890;&#29992;&#26041;&#27861;&#25214;&#21040;&#36825;&#20123;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#30001;&#39564;&#35777;&#31243;&#24207;&#21608;&#26399;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#36825;&#20123;&#20989;&#25968;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#20855;&#26377;&#39564;&#35777;&#36741;&#21161;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#23613;&#31649;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#26694;&#26550;&#22312;&#33258;&#21160;&#21512;&#25104;&#23631;&#38556;&#20989;&#25968;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#26694;&#26550;&#32570;&#20047;&#32456;&#27490;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#22312;&#25214;&#21040;&#26377;&#25928;&#23631;&#38556;&#20989;&#25968;&#30340;&#25104;&#21151;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#23631;&#38556;&#20989;&#25968;&#21512;&#25104;&#30340;&#20984;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#23398;&#20064;&#19968;&#20010;&#32463;&#39564;&#33391;&#22909;&#30340;NN&#22522;&#20989;&#25968;&#65292;&#28982;&#21518;&#24212;&#29992;&#19968;&#31181;&#21033;&#29992;&#20984;&#24615;&#21644;&#39564;&#35777;&#20013;&#30340;&#21453;&#20363;&#30340;&#24494;&#35843;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07308v1 Announce Type: cross  Abstract: Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07294</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Graph Data Condensation via Self-expressive Graph Structure Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#22270;&#25968;&#25454;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#22312;&#35757;&#32451;&#38454;&#27573;&#20943;&#36731;&#23384;&#20648;&#21644;&#26102;&#38388;&#25104;&#26412;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#23558;&#21407;&#22987;&#22823;&#35268;&#27169;&#22270;&#21387;&#32553;&#20026;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#35757;&#32451;&#19979;&#28216;GNN&#25152;&#38656;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#20165;&#20248;&#21270;&#33410;&#28857;&#29305;&#24449;&#65292;&#35201;&#20040;&#21162;&#21147;&#29420;&#31435;&#23398;&#20064;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#22120;&#12290;&#23427;&#20204;&#26080;&#27861;&#26126;&#30830;&#21033;&#29992;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#24182;&#26410;&#33021;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31361;&#20986;&#20043;&#22788;&#22312;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32479;&#19968;&#32593;&#32476;&#32467;&#26500;&#19978;&#36827;&#34892;&#25345;&#32493;&#20840;&#26041;&#20301;&#24694;&#21155;&#22825;&#27668;&#21435;&#38500;&#20219;&#21153;&#65292;&#20197;&#36866;&#24212;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65292;&#32780;&#38750;&#20256;&#32479;&#38745;&#24577;&#23398;&#20064;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07292</link><description>&lt;p&gt;
&#22312;&#32479;&#19968;&#32593;&#32476;&#32467;&#26500;&#19978;&#36890;&#36807;&#30693;&#35782;&#37325;&#25773;&#36827;&#34892;&#25345;&#32493;&#20840;&#26041;&#20301;&#24694;&#21155;&#22825;&#27668;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32479;&#19968;&#32593;&#32476;&#32467;&#26500;&#19978;&#36827;&#34892;&#25345;&#32493;&#20840;&#26041;&#20301;&#24694;&#21155;&#22825;&#27668;&#21435;&#38500;&#20219;&#21153;&#65292;&#20197;&#36866;&#24212;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65292;&#32780;&#38750;&#20256;&#32479;&#38745;&#24577;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#30001;&#24694;&#21155;&#22825;&#27668;&#24341;&#36215;&#30340;&#22270;&#20687;&#36864;&#21270;&#24635;&#26159;&#22797;&#26434;&#30340;&#65292;&#24182;&#19988;&#38543;&#30528;&#19981;&#21516;&#22825;&#27668;&#26465;&#20214;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#31995;&#32479;&#19981;&#26029;&#36935;&#21040;&#20197;&#21069;&#26410;&#26366;&#35266;&#23519;&#21040;&#30340;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#24694;&#21155;&#22825;&#27668;&#21435;&#38500;&#27169;&#22411;&#23454;&#38469;&#19978;&#38656;&#35201;&#19981;&#26029;&#23398;&#20064;&#22686;&#37327;&#25910;&#38598;&#30340;&#21453;&#26144;&#21508;&#31181;&#36864;&#21270;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#24694;&#21155;&#22825;&#27668;&#21435;&#38500;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#38024;&#23545;&#21333;&#20010;&#36824;&#26159;&#22810;&#20010;&#24694;&#21155;&#22825;&#27668;&#65292;&#20027;&#35201;&#35774;&#35745;&#20026;&#38745;&#24577;&#23398;&#20064;&#33539;&#24335;&#65292;&#20551;&#35774;&#22312;&#21333;&#19968;&#38454;&#27573;&#23398;&#20064;&#36807;&#31243;&#20043;&#21069;&#21487;&#20197;&#31934;&#32454;&#25910;&#38598;&#22788;&#29702;&#25152;&#26377;&#31867;&#22411;&#30340;&#36864;&#21270;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#22686;&#37327;&#23398;&#20064;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26368;&#26089;&#21162;&#21147;&#30740;&#31350;&#20102;&#25345;&#32493;&#20840;&#26041;&#20301;&#24694;&#21155;&#22825;&#27668;&#21435;&#38500;&#20219;&#21153;&#65292;&#22788;&#20110;&#26356;&#25509;&#36817;&#23454;&#38469;&#30340;&#29615;&#22659;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07292v1 Announce Type: cross  Abstract: In real-world applications, image degeneration caused by adverse weather is always complex and changes with different weather conditions from days and seasons. Systems in real-world environments constantly encounter adverse weather conditions that are not previously observed. Therefore, it practically requires adverse weather removal models to continually learn from incrementally collected data reflecting various degeneration types. Existing adverse weather removal approaches, for either single or multiple adverse weathers, are mainly designed for a static learning paradigm, which assumes that the data of all types of degenerations to handle can be finely collected at one time before a single-phase learning process. They thus cannot directly handle the incremental learning requirements. To address this issue, we made the earliest effort to investigate the continual all-in-one adverse weather removal task, in a setting closer to real-wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;von Mises-Fisher&#26680;&#26469;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;OOD&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07277</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to OOD Robustness in Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;von Mises-Fisher&#26680;&#26469;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;OOD&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#30830;&#20445;&#31639;&#27861;&#23545;&#22270;&#20687;&#39046;&#22495;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#22788;&#29702;&#27492;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#27809;&#26377;&#27880;&#37322;&#30340;&#22270;&#20687;&#12290;&#22312;&#38754;&#20020;&#30495;&#23454;&#19990;&#30028;&#30340;&#22495;&#20043;&#22806;&#65288;OOD&#65289;&#24178;&#25200;&#21644;&#36974;&#25377;&#30340;OOD-CV&#22522;&#20934;&#25361;&#25112;&#30340;&#28608;&#21169;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#23454;&#29616;&#29289;&#20307;&#20998;&#31867;&#30340;OOD&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24050;&#34987;&#35777;&#26126;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#20294;&#22312;OOD&#25968;&#25454;&#27979;&#35797;&#26102;&#20005;&#37325;&#38477;&#32423;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;CompNets&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;CompNets&#21253;&#21547;&#30340;&#22312;von Mises-Fisher&#65288;vMF&#65289;&#26680;&#34920;&#31034;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#23450;&#20041;&#30340;&#29983;&#25104;&#22836;&#65292;&#36825;&#20123;&#26680;&#22823;&#33268;&#23545;&#24212;&#20110;&#23545;&#35937;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#26576;&#20123;vMF&#26680;&#26159;&#30456;&#20284;&#30340;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#19981;&#26159;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;transiti
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07277v1 Announce Type: cross  Abstract: An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transiti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Anderson&#21152;&#36895;&#30340;IRL1&#31639;&#27861;&#65292;&#23558;&#20854;&#25910;&#25947;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#20809;&#28369;&#22330;&#26223;&#65292;&#19981;&#20381;&#36182;&#20110;Kurdyka-Lojasiewicz&#26465;&#20214;</title><link>https://arxiv.org/abs/2403.07271</link><description>&lt;p&gt;
Anderson&#21152;&#36895;&#29992;&#20110;&#36845;&#20195;&#37325;&#26032;&#21152;&#26435;&#30340;$\ell_1$&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anderson acceleration for iteratively reweighted $\ell_1$ algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07271
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Anderson&#21152;&#36895;&#30340;IRL1&#31639;&#27861;&#65292;&#23558;&#20854;&#25910;&#25947;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#20809;&#28369;&#22330;&#26223;&#65292;&#19981;&#20381;&#36182;&#20110;Kurdyka-Lojasiewicz&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#37325;&#26032;&#21152;&#26435;L1&#65288;IRL1&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24120;&#35265;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#21152;&#36895;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#36890;&#24120;&#37319;&#29992;Nesterov&#21152;&#36895;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21152;&#36895;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#22797;&#26434;&#24615;&#20998;&#26512;&#19968;&#30452;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;Anderson&#21152;&#36895;&#22240;&#20854;&#22312;&#21152;&#36895;&#22266;&#23450;&#28857;&#36845;&#20195;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22791;&#21463;&#30633;&#30446;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#12290;&#21463;&#21040;Anderson&#21152;&#36895;&#24378;&#22823;&#24433;&#21709;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Anderson&#21152;&#36895;&#30340;IRL1&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#36890;&#24120;&#22312;&#24179;&#28369;&#35774;&#32622;&#20013;&#35266;&#23519;&#21040;&#30340;&#25910;&#25947;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#20809;&#28369;&#22330;&#26223;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#19981;&#20381;&#36182;&#20110;Kurdyka-Lojasiewicz&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07271v1 Announce Type: cross  Abstract: Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20174;&#26377;&#38480;&#25968;&#37327;&#31574;&#30053;&#20013;&#23398;&#20064;&#20219;&#21153;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#31574;&#30053;&#19982;&#31163;&#32447;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.07261</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#31574;&#30053;&#20174;&#31163;&#32447;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#35299;&#34261;
&lt;/p&gt;
&lt;p&gt;
Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07261
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20174;&#26377;&#38480;&#25968;&#37327;&#31574;&#30053;&#20013;&#23398;&#20064;&#20219;&#21153;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#31574;&#30053;&#19982;&#31163;&#32447;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#39640;&#25928;&#22320;&#20801;&#35768;&#19968;&#20010;&#20195;&#29702;&#22312;&#20165;&#20381;&#36182;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#26032;&#39062;&#20219;&#21153;&#12290;&#20026;&#20102;&#31934;&#20934;&#39640;&#25928;&#22320;&#35782;&#21035;&#20219;&#21153;&#65292;&#29616;&#26377;&#30340;OMRL&#30740;&#31350;&#24314;&#35758;&#23398;&#20064;&#21333;&#29420;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#31574;&#30053;&#36755;&#20837;&#32467;&#21512;&#65292;&#20174;&#32780;&#24418;&#25104;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20803;&#31574;&#30053;&#12290;&#35757;&#32451;&#20219;&#21153;&#34920;&#31034;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#37319;&#29992;&#20351;&#29992;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#25968;&#25454;&#38598;&#36890;&#24120;&#28085;&#30422;&#26469;&#33258;&#21508;&#31181;&#31574;&#30053;&#65288;&#21363;&#34892;&#20026;&#31574;&#30053;&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#35774;&#32622;&#20013;&#65292;&#25910;&#38598;&#26469;&#33258;&#22823;&#37327;&#31574;&#30053;&#30340;&#25968;&#25454;&#19981;&#20165;&#19981;&#20999;&#23454;&#38469;&#65292;&#32780;&#19988;&#36890;&#24120;&#38590;&#20197;&#23454;&#29616;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36716;&#32780;&#37319;&#21462;&#26356;&#21463;&#38480;&#21046;&#20294;&#26356;&#23454;&#38469;&#30340;&#24773;&#24418;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31574;&#30053;&#36827;&#34892;&#22810;&#20219;&#21153;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07261v1 Announce Type: cross  Abstract: Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representatio
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25480;&#26435;NOMA&#31995;&#32479;&#20013;&#32852;&#21512;&#22788;&#29702;&#27963;&#21160;&#26816;&#27979;&#12289;&#20449;&#36947;&#20272;&#35745;&#21644;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07255</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#26426;&#22120;&#31867;&#22411;&#36890;&#20449;&#20013;&#26080;&#25480;&#26435;NOMA&#30340;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07255
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25480;&#26435;NOMA&#31995;&#32479;&#20013;&#32852;&#21512;&#22788;&#29702;&#27963;&#21160;&#26816;&#27979;&#12289;&#20449;&#36947;&#20272;&#35745;&#21644;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19978;&#34892;&#26080;&#25480;&#26435;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#65288;NOMA&#65289;&#31995;&#32479;&#20013;&#32852;&#21512;&#36827;&#34892;&#27963;&#21160;&#26816;&#27979;&#65288;AD&#65289;&#12289;&#20449;&#36947;&#20272;&#35745;&#65288;CE&#65289;&#21644;&#25968;&#25454;&#26816;&#27979;&#65288;DD&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#21463;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;&#65288;PIC&#65289;&#21551;&#21457;&#30340;&#36845;&#20195;&#21644;&#24182;&#34892;&#24178;&#25200;&#31227;&#38500;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#26469;&#20849;&#21516;&#35299;&#20915;AD&#12289;CE&#21644;DD&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;PIC&#26694;&#26550;&#65292;&#27599;&#31181;&#26694;&#26550;&#37117;&#35774;&#35745;&#29992;&#20110;&#30456;&#24178;&#25110;&#38750;&#19968;&#33268;&#26041;&#26696;&#12290;&#31532;&#19968;&#20010;&#26694;&#26550;&#22312;&#30456;&#24178;&#26041;&#26696;&#20013;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#23548;&#39057;&#20449;&#21495;&#36827;&#34892;&#32852;&#21512;AD&#21644;CE&#12290;&#22312;&#27492;&#26694;&#26550;&#22522;&#30784;&#19978;&#65292;&#31532;&#20108;&#20010;&#26694;&#26550;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#23548;&#39057;&#21644;&#25968;&#25454;&#20449;&#21495;&#36827;&#34892;CE&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30456;&#24178;&#26041;&#26696;&#20013;AD&#12289;CE&#21644;DD&#30340;&#24615;&#33021;&#12290;&#31532;&#19977;&#20010;&#26694;&#26550;&#35774;&#35745;&#29992;&#20110;&#36866;&#24212;&#21253;&#21547;&#23569;&#37327;&#25968;&#25454;&#20301;&#30340;&#38750;&#30456;&#24178;&#26041;&#26696;&#65292;&#21516;&#26102;&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07255v1 Announce Type: cross  Abstract: In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously perf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07230</link><description>&lt;p&gt;
Curry-DPO&#65306;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#25490;&#21517;&#20559;&#22909;&#22686;&#24378;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Curry-DPO: Enhancing Alignment using Curriculum Learning &amp; Ranked Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;(&#36890;&#24120;&#26159;&#27599;&#20010;&#29992;&#25143;&#25552;&#31034;&#36873;&#25321;&#21644;&#25298;&#32477;&#30340;&#21709;&#24212;&#23545;)&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#21487;&#33021;&#20250;&#23384;&#22312;&#22810;&#20010;&#21709;&#24212;&#65292;&#36825;&#20123;&#21709;&#24212;&#30340;&#36136;&#37327;&#30456;&#23545;&#20110;&#24444;&#27492;&#32780;&#35328;&#26377;&#25152;&#19981;&#21516;&#12290;&#26377;&#20102;&#36825;&#20123;&#22810;&#20010;&#21709;&#24212;&#30340;&#36136;&#37327;&#35780;&#32423;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36825;&#20123;&#21709;&#24212;&#20026;&#32473;&#23450;&#25552;&#31034;&#21019;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#31995;&#32479;&#22320;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#36827;&#34892;DPO&#35757;&#32451;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#23558;&#36825;&#20123;&#22810;&#20010;&#20559;&#22909;&#25968;&#25454;&#23545;&#20174;&#26131;&#21040;&#38590;(&#27169;&#25311;&#35838;&#31243;&#35757;&#32451;)&#25490;&#24207;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Curry-DPO&#65292;&#22312;MTbench&#12289;Vicuna&#12289;Wiz&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24320;&#21457;&#20102;&#29992;&#20110;ICU&#30149;&#20154;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21160;&#24577;&#39044;&#27979;&#35893;&#22916;&#12289;&#26127;&#36855;&#21644;&#27515;&#20129;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.07201</link><description>&lt;p&gt;
&#20351;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#39044;&#27979;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#29366;&#24577;&#30340;&#22810;&#38431;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A multi-cohort study on prediction of acute brain dysfunction states using selective state space models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24320;&#21457;&#20102;&#29992;&#20110;ICU&#30149;&#20154;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21160;&#24577;&#39044;&#27979;&#35893;&#22916;&#12289;&#26127;&#36855;&#21644;&#27515;&#20129;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#65288;&#21253;&#25324;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20013;&#30340;&#35893;&#22916;&#21644;&#26127;&#36855;&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#35786;&#26029;&#26041;&#27861;&#20381;&#36182;&#20110;&#19981;&#32463;&#24120;&#30340;&#20020;&#24202;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07201v1 Announce Type: cross  Abstract: Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals thr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#65292;&#21487;&#20197;&#25913;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23545;&#23398;&#29983;&#34920;&#29616;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07194</link><description>&lt;p&gt;
&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#30340;&#38598;&#25104;&#25913;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07194
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#65292;&#21487;&#20197;&#25913;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23545;&#23398;&#29983;&#34920;&#29616;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#26469;&#39044;&#27979;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#34920;&#29616;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;40&#21517;&#23398;&#29983;&#37319;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#26469;&#33258;&#19981;&#21516;&#22810;&#27169;&#24577;&#26469;&#28304;&#30340;&#25968;&#25454;&#65306;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#38754;&#37096;&#24405;&#20687;&#20013;&#30340;&#24773;&#32490;&#65292;&#30524;&#21160;&#36861;&#36394;&#20013;&#30340;&#20132;&#20114;&#21306;&#22495;&#65292;&#20197;&#21450;&#26368;&#32456;&#30693;&#35782;&#35780;&#20272;&#30340;&#27979;&#35797;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#20998;&#31867;&#38598;&#25104;&#26469;&#27979;&#35797;&#26159;&#21542;&#21487;&#20197;&#25913;&#36827;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20845;&#31181;&#20998;&#31867;&#31639;&#27861;&#24212;&#29992;&#20110;&#25968;&#20540;&#21270;&#21644;&#31163;&#25955;&#21270;&#39044;&#22788;&#29702;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38598;&#25104;&#21644;&#36873;&#25321;&#26368;&#20339;&#23646;&#24615;&#30340;&#26041;&#27861;&#32467;&#21512;&#25968;&#20540;&#25968;&#25454;&#26102;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07194v1 Announce Type: cross  Abstract: The aim of this study was to predict university students' learning performance using different sources of data from an Intelligent Tutoring System. We collected and preprocessed data from 40 students from different multimodal sources: learning strategies from system logs, emotions from face recording videos, interaction zones from eye tracking, and test performance from final knowledge evaluation. Our objective was to test whether the prediction could be improved by using attribute selection and classification ensembles. We carried out three experiments by applying six classification algorithms to numerical and discretized preprocessed multimodal data. The results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.
&lt;/p&gt;</description></item><item><title>CuentosIE&#25552;&#20379;&#20102;&#19968;&#22871;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23507;&#35328;&#25925;&#20107;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24037;&#20855;&#65292;&#26088;&#22312;&#25945;&#32946;&#29992;&#25143;&#24773;&#24863;&#30693;&#35782;&#65292;&#24182;&#30417;&#27979;&#20182;&#20204;&#30340;&#24773;&#24863;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.07193</link><description>&lt;p&gt;
CuentosIE&#65306;&#19968;&#20010;&#20851;&#20110;&#8220;&#23507;&#35328;&#23507;&#24847;&#8221;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#26377;&#21161;&#20110;&#25945;&#25480;&#24773;&#21830;&#65311;
&lt;/p&gt;
&lt;p&gt;
CuentosIE: can a chatbot about "tales with a message" help to teach emotional intelligence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07193
&lt;/p&gt;
&lt;p&gt;
CuentosIE&#25552;&#20379;&#20102;&#19968;&#22871;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23507;&#35328;&#25925;&#20107;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24037;&#20855;&#65292;&#26088;&#22312;&#25945;&#32946;&#29992;&#25143;&#24773;&#24863;&#30693;&#35782;&#65292;&#24182;&#30417;&#27979;&#20182;&#20204;&#30340;&#24773;&#24863;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuentosIE&#65288;TalesEI&#65306;&#19968;&#20010;&#20851;&#20110;&#24773;&#21830;&#21457;&#23637;&#30340;&#23507;&#35328;&#23507;&#24847;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24773;&#24863;&#30340;&#25945;&#32946;&#24615;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20063;&#20026;&#25945;&#24072;&#21644;&#24515;&#29702;&#23398;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;CuentosIE&#32534;&#21046;&#30340;&#25351;&#26631;&#21644;&#25968;&#25454;&#26469;&#30417;&#27979;&#20182;&#20204;&#30340;&#23398;&#29983;/&#24739;&#32773;&#12290;&#36873;&#25321;&#8220;&#23507;&#35328;&#23507;&#24847;&#8221;&#30340;&#29702;&#30001;&#22312;&#20110;&#23427;&#20204;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#29702;&#35299;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#36947;&#24503;&#25110;&#30456;&#20851;&#30340;&#38544;&#21947;&#12290; CuentosIE&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#36873;&#25321;&#12289;&#25910;&#38598;&#21644;&#20998;&#31867;&#19968;&#32452;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#25925;&#20107;&#65292;&#20197;&#21450;&#25552;&#20379;&#24037;&#20855;&#65288;&#25628;&#32034;&#12289;&#38405;&#35835;&#29702;&#35299;&#12289;&#32842;&#22825;&#12289;&#25512;&#33616;&#21644;&#20998;&#31867;&#65289;&#23545;&#25945;&#32946;&#29992;&#25143;&#24773;&#24863;&#24182;&#30417;&#27979;&#20182;&#20204;&#30340;&#24773;&#24863;&#21457;&#23637;&#37117;&#24456;&#26377;&#29992;&#12290;&#35813;&#24037;&#20855;&#30340;&#21021;&#27493;&#35780;&#20272;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#36825;&#32943;&#23450;&#20102;&#25991;&#31456;&#26631;&#39064;&#20013;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07193v1 Announce Type: cross  Abstract: In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of "tales with a message" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;$(N,K)$-Puzzle&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07191</link><description>&lt;p&gt;
$\mathbf{(N,K)}$-Puzzle&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#26412;&#25928;&#30410;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07191
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;$(N,K)$-Puzzle&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#26088;&#22312;&#25552;&#39640;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#19988;&#26631;&#20934;&#21270;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#20123;&#31639;&#27861;&#12290; &#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;24-Puzzle&#30340;&#19968;&#33324;&#21270;&#29256;&#26412;&#65306;$(N, K)$-Puzzle&#65292;&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#20197;&#20351;&#29992;$N$&#20010;&#25972;&#25968;&#36798;&#21040;&#30446;&#26631;&#20540;$K$&#12290; &#25105;&#20204;&#35780;&#20272;&#20102;&#24050;&#24314;&#31435;&#30340;RL&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65289;&#65292;&#20197;&#21450;&#26032;&#39062;&#26041;&#27861;&#65288;&#22914;Identity Policy Optimization&#65288;IPO&#65289;&#21644;Direct Policy Optimization&#65288;DPO&#65289;&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07191v1 Announce Type: cross  Abstract: Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07183</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#30417;&#27979;AI&#20462;&#25913;&#30340;&#20869;&#23481;&#65306;AI&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;ChatGPT&#24433;&#21709;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#25991;&#26412;&#21487;&#33021;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22823;&#24133;&#20462;&#25913;&#25110;&#29983;&#25104;&#30340;&#37096;&#20998;&#27604;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#22411;&#21033;&#29992;&#19987;&#23478;&#25776;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#20934;&#30830;&#39640;&#25928;&#22320;&#26816;&#26597;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#30495;&#23454;&#19990;&#30028;LLM&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#20250;&#35758;&#19978;&#31185;&#23398;&#21516;&#34892;&#35780;&#23457;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29983;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#65292;&#21253;&#25324;ICLR 2024&#12289;NeurIPS 2023&#12289;CoRL 2023&#21644;EMNLP 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20250;&#35758;&#25552;&#20132;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#30001;LLMs&#22823;&#24133;&#20462;&#25913;&#30340;&#65292;&#21363;&#36229;&#20986;&#25340;&#20889;&#26816;&#26597;&#25110;&#23567;&#24133;&#26356;&#26032;&#30340;&#33539;&#22260;&#12290;&#29983;&#25104;&#25991;&#26412;&#20986;&#29616;&#30340;&#24773;&#20917;&#20026;&#29992;&#25143;&#34892;&#20026;&#25552;&#20379;&#20102;&#35265;&#35299;&#65306;&#22312;&#25253;&#21578;&#20449;&#24515;&#36739;&#20302;&#12289;&#22312;&#25130;&#27490;&#26085;&#26399;&#21069;&#25552;&#20132;&#30340;&#35780;&#35770;&#20197;&#21450;&#20174;&#35780;&#35770;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07175</link><description>&lt;p&gt;
&#37325;&#24314;ROME: &#35299;&#20915;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;Rank-One Model Editing (ROME)&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#19968;&#20123;&#20107;&#23454;&#34920;&#26126;&#35813;&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#32534;&#36753;&#32780;&#19981;&#30772;&#22351;&#27169;&#22411;&#12290;&#36825;&#20123;&#32534;&#36753;&#20197;&#21069;&#34987;&#31216;&#20026;&#31105;&#29992;&#32534;&#36753;&#12290;&#36825;&#20123;&#31105;&#29992;&#32534;&#36753;&#20250;&#23548;&#33268;&#31435;&#21363;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#38480;&#21046;&#20102;ROME&#29992;&#20110;&#39034;&#24207;&#32534;&#36753;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;CounterFact&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#36753;&#26102;&#65292;ROME&#20165;&#22312;&#27492;&#26102;&#21457;&#29983;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#22312;&#20351;&#29992;zsRE&#25968;&#25454;&#38598;&#26102;&#19981;&#20250;&#21457;&#29983;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#31105;&#29992;&#32534;&#36753;&#26159;ROME&#21407;&#22987;&#23454;&#29616;&#30340;&#20135;&#29289;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#31283;&#23450;&#30340;&#23454;&#29616;ROME&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;r-ROME&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#22312;&#20351;&#29992;ROME&#36827;&#34892;&#22823;&#35268;&#27169;&#39034;&#24207;&#32534;&#36753;&#26102;&#19981;&#20877;&#35266;&#23519;&#21040;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21382;&#21490;&#24863;&#30693;&#30340;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;FLContrib&#65292;&#29992;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.07151</link><description>&lt;p&gt;
&#19981;&#35201;&#24536;&#35760;&#25105;&#20570;&#30340;&#20107;&#65306;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Don't Forget What I did?: Assessing Client Contributions in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21382;&#21490;&#24863;&#30693;&#30340;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;FLContrib&#65292;&#29992;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#22810;&#20010;&#23458;&#25143;&#21442;&#19982;&#35757;&#32451;ML&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#31169;&#20154;&#25968;&#25454;&#12290;&#20844;&#24179;&#20934;&#30830;&#35780;&#20272;&#23458;&#25143;&#36129;&#29486;&#22312;FL&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#28608;&#21169;&#20998;&#37197;&#24182;&#40723;&#21169;&#22810;&#26679;&#21270;&#23458;&#25143;&#21442;&#19982;&#32479;&#19968;&#27169;&#22411;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21382;&#21490;&#24863;&#30693;&#30340;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;FLContrib&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#27599;&#20010;FL&#35757;&#32451;&#26102;&#26399;&#20013;&#30340;&#65288;&#28508;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#23458;&#25143;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07151v1 Announce Type: cross  Abstract: Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client cont
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#34920;&#24449;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20302;&#25928;&#29575;&#65292;&#36825;&#25581;&#31034;&#20102;&#20215;&#20540;&#20989;&#25968;&#21644;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.07136</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#30340;&#26377;&#38480;&#34920;&#24449;&#33021;&#21147;&#21450;&#20854;&#19982;&#32479;&#35745;(&#19981;)&#25928;&#29575;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07136
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#34920;&#24449;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20302;&#25928;&#29575;&#65292;&#36825;&#25581;&#31034;&#20102;&#20215;&#20540;&#20989;&#25968;&#21644;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35782;&#21035;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290; &#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#32479;&#35745;&#19978;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19968;&#26679;&#26377;&#25928;&#12290; &#28982;&#32780;&#65292;&#24403;&#20851;&#27880;&#31574;&#30053;&#35780;&#20272;&#30340;&#26680;&#24515;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20851;&#20110;&#36716;&#31227;&#21160;&#24577;&#30340;&#20449;&#24687;&#21487;&#33021;&#26080;&#27861;&#22312;&#20215;&#20540;&#20989;&#25968;&#31354;&#38388;&#20013;&#34920;&#31034;&#12290; &#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30528;&#37325;&#20110;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#32467;&#26500;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#25506;&#31350;&#36825;&#19968;&#28857;&#12290; &#22312;&#20854;&#20013;&#20960;&#31181;&#24773;&#20917;&#20013;&#65292;&#27809;&#26377;&#20449;&#24687;&#20002;&#22833;&#65292;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#25928;&#29575;&#19978;&#30456;&#24403;&#12290; &#22312;&#20854;&#20182;&#30456;&#20851;&#31034;&#20363;&#20013;&#65292;&#20449;&#24687;&#20002;&#22833;&#20005;&#37325;&#65292;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#24615;&#33021;&#20005;&#37325;&#19981;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290; &#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#25351;&#20986;&#20102;&#34920;&#24449;&#33021;&#21147;&#30340;&#38480;&#21046;&#20316;&#20026;&#20302;&#25928;&#24615;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#32780;&#38750;&#31639;&#27861;&#35774;&#35745;&#19978;&#30340;&#22833;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07136v1 Announce Type: cross  Abstract: Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning. Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#28608;&#21169;&#20989;&#25968;&#30340;&#22823;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#37197;&#65292;&#36890;&#36807;&#24320;&#21457;&#22270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#20108;&#37096;&#22270;&#21305;&#37197;&#26041;&#27861;MRTA&#20013;&#30340;&#21551;&#21457;&#24335;&#25110;&#28608;&#21169;&#12290;</title><link>https://arxiv.org/abs/2403.07131</link><description>&lt;p&gt;
&#24102;&#26377;&#23398;&#20064;&#28608;&#21169;&#20989;&#25968;&#30340;&#22823;&#22270;&#21305;&#37197;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#28608;&#21169;&#20989;&#25968;&#30340;&#22823;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#37197;&#65292;&#36890;&#36807;&#24320;&#21457;&#22270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#20108;&#37096;&#22270;&#21305;&#37197;&#26041;&#27861;MRTA&#20013;&#30340;&#21551;&#21457;&#24335;&#25110;&#28608;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20998;&#37197;&#65288;MRTA&#65289;&#38382;&#39064;&#38656;&#35201;&#24555;&#36895;&#39640;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#36741;&#21161;&#26041;&#27861;&#65288;&#22914;&#36951;&#20256;&#31639;&#27861;&#12289;&#22522;&#20110;&#25293;&#21334;&#30340;&#26041;&#27861;&#21644;&#20108;&#37096;&#22270;&#21305;&#37197;&#26041;&#27861;&#65289;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#24418;&#24335;&#65292;&#30456;&#27604;&#20110;MRTA&#30340;&#31471;&#21040;&#31471;&#65288;&#23398;&#20064;&#65289;&#31070;&#32463;&#32593;&#32476;&#25919;&#31574;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36866;&#24403;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#33021;&#24456;&#32321;&#29712;&#12289;&#39118;&#38505;&#36739;&#39640;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22914;&#26524;&#38382;&#39064;&#36807;&#20110;&#22797;&#26434;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#33021;&#21542;&#34987;&#23398;&#20064;&#65311;&#20026;&#27492;&#65292;&#26412;&#25991;&#29305;&#21035;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20108;&#37096;&#22270;&#21305;&#37197;&#26041;&#27861;MRTA&#20013;&#30340;&#21551;&#21457;&#24335;&#25110;&#28608;&#21169;&#12290;&#20855;&#20307;&#22320;&#65292;&#20351;&#29992;&#20102;&#33014;&#22218;&#27880;&#24847;&#21147;&#31574;&#30053;&#27169;&#22411;&#26469;&#23398;&#20064;&#22914;&#20309;&#20026;&#36830;&#25509;&#20219;&#21153;&#38598;&#21644;&#26426;&#22120;&#20154;&#38598;&#30340;&#20108;&#37096;&#22270;&#20013;&#30340;&#20219;&#21153;/&#26426;&#22120;&#20154;&#37197;&#23545;&#65288;&#36793;&#65289;&#36827;&#34892;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07131v1 Announce Type: new  Abstract: Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule att
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25506;&#32034;GAB&#21644;Telegram&#36825;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#21465;&#20107;&#28436;&#21464;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#65292;&#20197;&#25552;&#21462;&#21487;&#33021;&#34987;&#25513;&#30422;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07090</link><description>&lt;p&gt;
&#21453;&#26144;&#22797;&#26434;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#27969;&#20013;&#20851;&#38190;&#31038;&#20250;&#20107;&#20214;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07090
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25506;&#32034;GAB&#21644;Telegram&#36825;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#21465;&#20107;&#28436;&#21464;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#65292;&#20197;&#25552;&#21462;&#21487;&#33021;&#34987;&#25513;&#30422;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07090v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#34164;&#34255;&#30528;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#28982;&#32780;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#25429;&#25417;&#24555;&#36895;&#21464;&#21270;&#20107;&#20214;&#20013;&#30340;&#20851;&#38190;&#20449;&#21495;&#12290;&#38543;&#30528;&#20840;&#29699;&#20107;&#20214;&#36805;&#36895;&#28436;&#21464;&#65292;&#21253;&#25324;&#34394;&#20551;&#20449;&#24687;&#30340;&#31038;&#20132;&#23186;&#20307;&#21465;&#20107;&#25104;&#20026;&#37325;&#35201;&#30340;&#35265;&#35299;&#26469;&#28304;&#12290;&#20026;&#20102;&#28385;&#36275;&#24402;&#32435;&#31574;&#30053;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#23567;&#20247;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;GAB&#21644;&#19968;&#20010;&#25104;&#29087;&#30340;&#28040;&#24687;&#26381;&#21153;Telegram&#65292;&#20197;&#24320;&#21457;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35805;&#35821;&#20998;&#26512;&#25216;&#26415;&#26469;&#30740;&#31350;&#36825;&#20123;&#24179;&#21488;&#19978;&#30340;&#21465;&#20107;&#28436;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#20197;&#25552;&#28860;&#21487;&#33021;&#34987;&#25513;&#30422;&#30340;&#20851;&#38190;&#20449;&#24687;&#30340;&#26032;&#39062;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#29992;&#19988;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;GAB&#21644;Telegram&#25968;&#25454;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#35770;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07090v1 Announce Type: cross  Abstract: Social media platforms hold valuable insights, yet extracting essential information can be challenging. Traditional top-down approaches often struggle to capture critical signals in rapidly changing events. As global events evolve swiftly, social media narratives, including instances of disinformation, become significant sources of insights. To address the need for an inductive strategy, we explore a niche social media platform GAB and an established messaging service Telegram, to develop methodologies applicable on a broader scale. This study investigates narrative evolution on these platforms using quantitative corpus-based discourse analysis techniques. Our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise, allowing for useful and actionable insights. The paper details the technical and methodological aspects of gathering and preprocessing GAB and Telegram data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#29983;&#25104;&#22312;&#21382;&#21490;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#35328;&#20016;&#23500;&#12289;&#35821;&#22659;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07087</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#21382;&#21490;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LSTM-Based Text Generation: A Study on Historical Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#29983;&#25104;&#22312;&#21382;&#21490;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#35328;&#20016;&#23500;&#12289;&#35821;&#22659;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#33678;&#22763;&#27604;&#20122;&#21644;&#23612;&#37319;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#12290;LSTMs&#20197;&#20854;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#32780;&#38395;&#21517;&#65292;&#36825;&#37324;&#24212;&#29992;&#23427;&#20204;&#26469;&#24314;&#27169;&#21382;&#21490;&#25991;&#26412;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;LSTM&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#35821;&#35328;&#20016;&#23500;&#19988;&#35821;&#22659;&#30456;&#20851;&#30340;&#25991;&#26412;&#65292;&#36824;&#33021;&#25552;&#20379;&#20851;&#20110;&#35821;&#35328;&#27169;&#24335;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#35265;&#35299;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#23612;&#37319;&#20316;&#21697;&#25991;&#26412;&#26102;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#25439;&#22833;&#20540;&#21644;100&#27425;&#36845;&#20195;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20026;0.9521&#65292;&#34920;&#26126;&#20854;&#39640;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#30340;&#25439;&#22833;&#20026;0.2518&#65292;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07087v1 Announce Type: cross  Abstract: This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#20511;&#21161;&#20808;&#39564;&#30693;&#35782;&#21644;&#35748;&#30693;&#27169;&#22411;&#26469;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#23545;&#25239;&#38450;&#24481;&#12289;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24357;&#34917;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39046;&#22495;&#30693;&#35782;&#21033;&#29992;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#38450;&#24481;&#12289;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#24320;&#25918;&#29615;&#22659;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.07078</link><description>&lt;p&gt;
&#20511;&#21161;&#20808;&#39564;&#30693;&#35782;&#21644;&#35748;&#30693;&#27169;&#22411;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#20511;&#21161;&#20808;&#39564;&#30693;&#35782;&#21644;&#35748;&#30693;&#27169;&#22411;&#26469;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#23545;&#25239;&#38450;&#24481;&#12289;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24357;&#34917;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39046;&#22495;&#30693;&#35782;&#21033;&#29992;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#38450;&#24481;&#12289;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#24320;&#25918;&#29615;&#22659;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;&#21644;&#26032;&#20852;&#30340;&#30693;&#35782;&#39537;&#21160;&#21644;&#33041;&#21551;&#21457;&#30340;&#35748;&#30693;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#25239;&#24615;&#38450;&#24481;&#12289;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20197;&#21450;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#36229;&#36234;&#20154;&#31867;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#26080;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#20005;&#37325;&#24615;&#33021;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23427;&#20204;&#20570;&#20986;&#26126;&#26174;&#38169;&#35823;&#30340;&#20915;&#23450;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#21363;&#23427;&#20204;&#30340;&#20915;&#31574;&#26080;&#27861;&#34987;&#20154;&#31867;&#20027;&#20307;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#20855;&#26377;&#23553;&#38381;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#65292;&#23427;&#20204;&#24456;&#38590;&#25512;&#24191;&#21040;&#26410;&#35265;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07078v1 Announce Type: cross  Abstract: We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#19982;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21551;&#21457;&#33258;NLP&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2403.07040</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#65288;&#25193;&#23637;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#19982;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21551;&#21457;&#33258;NLP&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#25105;&#20204;&#22312;KDD23&#20013;&#33719;&#24471;&#26368;&#20339;&#30740;&#31350;&#35770;&#25991;&#22870;&#30340;&#21407;&#22987;&#24037;&#20316;&#30340;&#25193;&#23637;&#25688;&#35201;&#65292;&#20854;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#21644;&#23427;&#20204;&#24212;&#29992;&#20110;&#30340;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;NLP&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#65288;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#65289;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36127;&#36801;&#31227;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32479;&#19968;&#22270;&#21644;&#35821;&#35328;&#25552;&#31034;&#26684;&#24335;&#65292;&#20351;NLP&#30340;&#25552;&#31034;&#31574;&#30053;&#33021;&#22815;&#36866;&#29992;&#20110;&#22270;&#20219;&#21153;&#12290;&#36890;&#36807;&#20998;&#26512;&#22270;&#24212;&#29992;&#30340;&#20219;&#21153;&#31354;&#38388;&#65292;&#25105;&#20204;&#37325;&#26032;&#21046;&#23450;&#38382;&#39064;&#20197;&#36866;&#24212;&#22270;&#32423;&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#20803;&#23398;&#20064;&#26469;&#25913;&#36827;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07040v1 Announce Type: cross  Abstract: This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initializ
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#25913;&#21464;&#20102;ASIC&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#20195;&#30721;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36890;&#36807;&#24494;&#35843;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#37325;&#32452;HDL&#20195;&#30721;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07039</link><description>&lt;p&gt;
&#20174;&#33521;&#35821;&#21040;ASIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30828;&#20214;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
From English to ASIC: Hardware Implementation with Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07039
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#25913;&#21464;&#20102;ASIC&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#20195;&#30721;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36890;&#36807;&#24494;&#35843;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#37325;&#32452;HDL&#20195;&#30721;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ASIC&#24037;&#31243;&#39046;&#22495;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29616;&#20195;&#25968;&#23383;&#30005;&#36335;&#30340;&#22797;&#26434;&#24615;&#20063;&#22312;&#22686;&#21152;&#65292;&#36825;&#21152;&#21095;&#20102;&#23545;HDL&#32534;&#30721;&#30340;&#35201;&#27714;&#65292;&#38656;&#35201;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#20195;&#30721;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#21152;&#20043;&#30456;&#24212;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25361;&#25112;&#19981;&#26029;&#12290;&#36825;&#20123;&#25361;&#25112;&#20984;&#26174;&#20102;LLM&#28508;&#21147;&#38761;&#26032;&#25968;&#23383;&#30005;&#36335;&#35774;&#35745;&#19982;&#24403;&#21069;&#33021;&#21147;&#20934;&#30830;&#35299;&#37322;&#21644;&#23454;&#26045;&#30828;&#20214;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;&#39046;&#20808;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;HDL&#20195;&#30721;&#25968;&#25454;&#38598;&#37325;&#32452;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07039v1 Announce Type: cross  Abstract: In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21407;&#22411;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#65292;&#32467;&#21512;&#20154;&#31867;&#22266;&#26377;&#21407;&#22411;&#21305;&#37197;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#65292;&#36890;&#36807;&#21305;&#37197;&#29305;&#24449;&#19982;&#21407;&#22411;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#26469;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.07033</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22411;&#21305;&#37197;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Interpreting What Typical Fault Signals Look Like via Prototype-matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21407;&#22411;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#65292;&#32467;&#21512;&#20154;&#31867;&#22266;&#26377;&#21407;&#22411;&#21305;&#37197;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#65292;&#36890;&#36807;&#21305;&#37197;&#29305;&#24449;&#19982;&#21407;&#22411;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#26469;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24378;&#22823;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#21644;&#20998;&#31867;&#33021;&#21147;&#65292;&#22312;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#20197;&#30830;&#20445;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20856;&#22411;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#23427;&#20204;&#22312;&#39640;&#21487;&#38752;&#24615;&#35201;&#27714;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#29702;&#35299;&#20998;&#31867;&#36923;&#36753;&#24182;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20154;&#31867;&#22266;&#26377;&#21407;&#22411;&#21305;&#37197;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#30340;&#21407;&#22411;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#12290;PMN&#23558;AE&#25552;&#21462;&#30340;&#29305;&#24449;&#19982;&#27599;&#20010;&#21407;&#22411;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#20316;&#20026;&#39044;&#27979;&#32467;&#26524;&#12290;&#23427;&#22312;&#20998;&#31867;&#36923;&#36753;&#12289;&#25925;&#38556;&#21407;&#22411;&#21644;&#21305;&#37197;&#36129;&#29486;&#26041;&#38754;&#26377;&#19977;&#26465;&#35299;&#37322;&#36335;&#24452;&#12290;&#20256;&#32479;&#35786;&#26029;&#21644;&#39046;&#22495;&#27867;&#21270;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#35786;&#26029;&#24615;&#33021;&#20197;&#21450;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#65288;&#21363;&#26679;&#26412;&#32423;&#21407;&#22411;&#65289;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07033v1 Announce Type: cross  Abstract: Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#27880;&#24847;&#21147;&#27969;&#23884;&#20837;&#21644;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#27169;&#22359;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#27969;&#39044;&#27979;&#20013;&#30340;&#23616;&#37096;&#20381;&#36182;&#21305;&#37197;&#21644;&#38750;&#21018;&#24615;&#29289;&#20307;&#21464;&#24418;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07032</link><description>&lt;p&gt;
STARFlow: &#20855;&#26377;&#27880;&#24847;&#21147;&#23398;&#20064;&#30340;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#27880;&#24847;&#21147;&#27969;&#23884;&#20837;&#21644;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#27169;&#22359;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#27969;&#39044;&#27979;&#20013;&#30340;&#23616;&#37096;&#20381;&#36182;&#21305;&#37197;&#21644;&#38750;&#21018;&#24615;&#29289;&#20307;&#21464;&#24418;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#39044;&#27979;&#26159;&#29702;&#35299;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#22522;&#26412;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#22330;&#26223;&#27969;&#26041;&#27861;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20165;&#22522;&#20110;&#23616;&#37096;&#24863;&#21463;&#37326;&#30340;&#27969;&#20272;&#35745;&#32570;&#20047;&#28857;&#23545;&#30340;&#38271;&#20381;&#36182;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#27880;&#24847;&#21147;&#27969;&#23884;&#20837;&#65292;&#20197;&#21305;&#37197;&#29305;&#24449;&#31354;&#38388;&#21644;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#25152;&#26377;&#28857;&#23545;&#65292;&#25552;&#20379;&#23616;&#37096;&#32454;&#21270;&#20043;&#21069;&#30340;&#20840;&#23616;&#21021;&#22987;&#21270;&#12290;&#20854;&#27425;&#65292;&#22312;&#21464;&#24418;&#21518;&#23384;&#22312;&#38750;&#21018;&#24615;&#29289;&#20307;&#30340;&#21464;&#24418;&#65292;&#23548;&#33268;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#26102;&#31354;&#20851;&#31995;&#21464;&#21270;&#12290;&#20026;&#20102;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27531;&#20313;&#27969;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#22312;&#21464;&#24418;&#21518;&#33719;&#21462;&#24207;&#21015;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#26174;&#33879;&#22495;&#24046;&#24322;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07032v1 Announce Type: cross  Abstract: Scene flow prediction is a crucial underlying task in understanding dynamic scenes as it offers fundamental motion information. However, contemporary scene flow methods encounter three major challenges. Firstly, flow estimation solely based on local receptive fields lacks long-dependency matching of point pairs. To address this issue, we propose global attentive flow embedding to match all-to-all point pairs in both feature space and Euclidean space, providing global initialization before local refinement. Secondly, there are deformations existing in non-rigid objects after warping, which leads to variations in the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow, a spatial temporal feature re-embedding module is devised to acquire the sequence features after deformation. Furthermore, previous methods perform poor generalization due to the significant domain gap between the synthesi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26041;&#21521;&#24863;&#30693;&#25216;&#26415;&#21644;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07028</link><description>&lt;p&gt;
&#19968;&#31181;&#19982;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#27604;&#30340;&#39640;&#25928;&#23398;&#20064;&#22411;&#35299;&#20915;&#22120;&#65292;&#29992;&#20110;&#23481;&#37327;&#24359;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26041;&#21521;&#24863;&#30693;&#25216;&#26415;&#21644;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#32452;&#21512;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#23481;&#37327;&#24359;&#36335;&#30001;&#38382;&#39064;&#65288;CARP&#65289;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;CARP&#26159;&#25351;&#22312;&#22270;&#19978;&#25214;&#21040;&#35206;&#30422;&#25152;&#26377;&#24517;&#38656;&#36793;&#30340;&#26368;&#23567;&#25104;&#26412;&#36335;&#24452;&#65292;&#21516;&#26102;&#22312;&#23481;&#37327;&#32422;&#26463;&#20869;&#12290;&#22312;&#35299;&#20915;CARP&#26041;&#38754;&#65292;&#22522;&#20110;NN&#30340;&#26041;&#27861;&#24448;&#24448;&#33853;&#21518;&#20110;&#20808;&#36827;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#38024;&#23545;&#22797;&#26434;CARP&#23450;&#21046;&#30340;&#23450;&#21521;&#24359;&#24314;&#27169;&#21644;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;NN&#30340;&#35299;&#20915;&#22120;&#65292;&#20197;&#22823;&#22823;&#32553;&#23567;&#19982;&#20808;&#36827;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#21521;&#24863;&#30693;&#27880;&#24847;&#27169;&#22411;&#65288;DaAM&#65289;&#65292;&#23558;&#26041;&#21521;&#24615;&#24341;&#20837;&#23884;&#20837;&#36807;&#31243;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#19968;&#38454;&#27573;&#20915;&#31574;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#28041;&#21450;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20026;&#38543;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#30340;&#21021;&#22987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07028v1 Announce Type: cross  Abstract: Recently, neural networks (NN) have made great strides in combinatorial optimization. However, they face challenges when solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour covering all required edges on a graph, while within capacity constraints. In tackling CARP, NN-based approaches tend to lag behind advanced metaheuristics, since they lack directed arc modeling and efficient learning methods tailored for complex CARP. In this paper, we introduce an NN-based solver to significantly narrow the gap with advanced metaheuristics while exhibiting superior efficiency. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a supervised reinforcement learning scheme that involves supervised pre-training to establish a robust initial policy for subsequent reinforcement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;One4All-ST&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20026;&#20219;&#24847;&#21487;&#20462;&#25913;&#21306;&#22495;&#21333;&#20301;&#36827;&#34892;ST&#39044;&#27979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22810;&#23610;&#24230;&#39044;&#27979;&#30340;&#25104;&#26412;&#38382;&#39064;&#21644;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07022</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#29992;&#20110;&#20855;&#26377;&#20219;&#24847;&#21487;&#20462;&#25913;&#21306;&#22495;&#21333;&#20301;&#30340;&#26102;&#31354;&#39044;&#27979;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;One4All-ST&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20026;&#20219;&#24847;&#21487;&#20462;&#25913;&#21306;&#22495;&#21333;&#20301;&#36827;&#34892;ST&#39044;&#27979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22810;&#23610;&#24230;&#39044;&#27979;&#30340;&#25104;&#26412;&#38382;&#39064;&#21644;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#65288;ST&#65289;&#39044;&#27979;&#23545;&#20110;&#22312;&#22478;&#24066;&#22522;&#20110;&#20301;&#32622;&#30340;&#24212;&#29992;&#20013;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65288;&#22914;&#39034;&#39118;&#36710;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ST&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#21306;&#22495;&#21010;&#20998;&#20316;&#20026;&#20808;&#20915;&#26465;&#20214;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#38656;&#35201;&#20026;&#19981;&#21516;&#30446;&#30340;&#32780;&#23450;&#20041;&#20020;&#26102;&#21306;&#22495;&#65292;&#38656;&#35201;&#25903;&#25345;&#25104;&#26412;&#39640;&#26114;&#30340;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#21644;&#21306;&#22495;&#30340;ST&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#19981;&#21516;&#30340;ST&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#20914;&#31361;&#30340;&#36755;&#20986;&#65292;&#23548;&#33268;&#28151;&#20081;&#30340;&#39044;&#27979;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;One4All-ST&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#26469;&#20026;&#20219;&#24847;&#21487;&#20462;&#25913;&#30340;&#21306;&#22495;&#21333;&#20803;&#36827;&#34892;ST&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#23569;&#33719;&#21462;&#22810;&#23610;&#24230;&#39044;&#27979;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#20998;&#23618;&#31354;&#38388;&#24314;&#27169;&#21644;&#35268;&#27169;&#24402;&#19968;&#21270;&#27169;&#22359;&#30340;ST&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#19988;&#24179;&#31561;&#22320;&#23398;&#20064;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#23610;&#24230;&#30340;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;sch
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07022v1 Announce Type: cross  Abstract: Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming sch
&lt;/p&gt;</description></item><item><title>&#25968;&#23398;&#22312;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#31995;&#32479;&#20851;&#27880;&#36827;&#21270;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25506;&#35752;&#20102;&#20010;&#20307;&#31574;&#30053;&#22312;&#20154;&#32676;&#20013;&#28436;&#21270;&#30340;&#24773;&#20917;&#65292;&#24182;&#30740;&#31350;&#20102;&#26234;&#33021;&#20307;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#26681;&#25454;&#21453;&#39304;&#21644;&#32463;&#39564;&#35843;&#25972;&#31574;&#30053;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.07017</link><description>&lt;p&gt;
&#25968;&#23398;&#22312;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07017
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#22312;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#31995;&#32479;&#20851;&#27880;&#36827;&#21270;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25506;&#35752;&#20102;&#20010;&#20307;&#31574;&#30053;&#22312;&#20154;&#32676;&#20013;&#28436;&#21270;&#30340;&#24773;&#20917;&#65292;&#24182;&#30740;&#31350;&#20102;&#26234;&#33021;&#20307;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#26681;&#25454;&#21453;&#39304;&#21644;&#32463;&#39564;&#35843;&#25972;&#31574;&#30053;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#21338;&#24328;&#35770;(EGT)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#26159;&#20004;&#20010;&#20045;&#19968;&#30475;&#21487;&#33021;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#20294;&#23427;&#20204;&#26377;&#26174;&#33879;&#30340;&#32852;&#31995;&#21644;&#20132;&#27719;&#12290;&#21069;&#32773;&#19987;&#27880;&#20110;&#20154;&#32676;&#20013;&#34892;&#20026;(&#25110;&#31574;&#30053;)&#30340;&#28436;&#21270;&#65292;&#20010;&#20307;&#19982;&#20854;&#20182;&#20154;&#20114;&#21160;&#65292;&#24182;&#26681;&#25454;&#27169;&#20223;(&#25110;&#31038;&#20250;&#23398;&#20064;)&#26356;&#26032;&#20182;&#20204;&#30340;&#31574;&#30053;&#12290;&#19968;&#20010;&#31574;&#30053;&#36234;&#25104;&#21151;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23427;&#23601;&#20250;&#21464;&#24471;&#36234;&#26222;&#36941;&#12290;&#21518;&#32773;&#21017;&#38598;&#20013;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;(&#28145;&#24230;)&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#24120;&#26159;&#20174;&#21333;&#19968;&#26234;&#33021;&#20307;&#30340;&#35282;&#24230;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#22320;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#21453;&#39304;&#21644;&#32463;&#39564;&#35843;&#25972;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#26377;&#28857;&#31867;&#20284;&#20110;&#28436;&#21270;&#36807;&#31243;&#65292;&#20294;&#22312;&#20854;&#33258;&#25105;&#23398;&#20064;&#33021;&#21147;&#19978;&#21364;&#26377;&#25152;&#19981;&#21516;&#12290;&#37492;&#20110;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#25152;&#38656;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;(i)&#23398;&#20064;&#21644;&#36866;&#24212;&#24615;&#65292;(ii)&#21512;&#20316;&#21644;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07017v1 Announce Type: cross  Abstract: Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two fields that, at first glance, might seem distinct, but they have notable connections and intersections. The former focuses on the evolution of behaviors (or strategies) in a population, where individuals interact with others and update their strategies based on imitation (or social learning). The more successful a strategy is, the more prevalent it becomes over time. The latter, meanwhile, is centered on machine learning algorithms and (deep) neural networks. It is often from a single-agent perspective but increasingly involves multi-agent environments, in which intelligent agents adjust their strategies based on feedback and experience, somewhat akin to the evolutionary process yet distinct in their self-learning capacities. In light of the key components necessary to address real-world problems, including (i) learning and adaptation, (ii) cooperation and competit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29699;&#24418;T-&#29699;&#24418;&#27169;&#31946;&#65288;G-TSF&#65289;&#38598;&#21512;&#20316;&#20026;T-&#29699;&#24418;&#27169;&#31946;&#38598;&#21512;&#21644;&#22278;&#24418;&#29699;&#24418;&#27169;&#31946;&#38598;&#21512;&#30340;&#21019;&#26032;&#25193;&#23637;&#27010;&#24565;&#65292;&#21033;&#29992;&#29699;&#24418;/&#29699;&#20307;&#36793;&#30028;&#34920;&#31034;&#38582;&#23646;&#24230;&#12289;&#19981;&#30830;&#23450;&#24230;&#21644;&#38750;&#38582;&#23646;&#24230;&#31243;&#24230;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20449;&#24687;&#25551;&#32472;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25968;&#25454;&#28857;&#35780;&#20272;&#20915;&#31574;&#23545;&#35937;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.07010</link><description>&lt;p&gt;
&#20851;&#20110;&#29699;&#24418;T-&#29699;&#24418;&#27169;&#31946;&#65288;G-TSF&#65289;&#38598;&#21512;&#21450;&#20854;&#22312;G-TSF&#22810;&#26631;&#20934;&#32676;&#20307;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF Multi-Criteria Group Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29699;&#24418;T-&#29699;&#24418;&#27169;&#31946;&#65288;G-TSF&#65289;&#38598;&#21512;&#20316;&#20026;T-&#29699;&#24418;&#27169;&#31946;&#38598;&#21512;&#21644;&#22278;&#24418;&#29699;&#24418;&#27169;&#31946;&#38598;&#21512;&#30340;&#21019;&#26032;&#25193;&#23637;&#27010;&#24565;&#65292;&#21033;&#29992;&#29699;&#24418;/&#29699;&#20307;&#36793;&#30028;&#34920;&#31034;&#38582;&#23646;&#24230;&#12289;&#19981;&#30830;&#23450;&#24230;&#21644;&#38750;&#38582;&#23646;&#24230;&#31243;&#24230;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20449;&#24687;&#25551;&#32472;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25968;&#25454;&#28857;&#35780;&#20272;&#20915;&#31574;&#23545;&#35937;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29699;&#24418;T-&#29699;&#24418;&#27169;&#31946;&#65288;G-TSF&#65289;&#38598;&#21512;&#20316;&#20026;T-&#29699;&#24418;&#27169;&#31946;&#38598;&#21512;&#65288;TSFSs&#65289;&#21644;&#22278;&#24418;&#29699;&#24418;&#27169;&#31946;&#38598;&#21512;&#65288;C-SFSs&#65289;&#30340;&#21019;&#26032;&#25193;&#23637;&#27010;&#24565;&#12290;G-TSF&#38598;&#21512;&#21033;&#29992;&#19968;&#20010;&#29699;&#24418;/&#29699;&#20307;&#36793;&#30028;&#26469;&#34920;&#31034;&#38582;&#23646;&#24230;&#12289;&#19981;&#30830;&#23450;&#24230;&#21644;&#38750;&#38582;&#23646;&#24230;&#31243;&#24230;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25551;&#32472;&#27169;&#31946;&#12289;&#21547;&#31946;&#21644;&#19981;&#31934;&#30830;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#29305;&#23450;&#20013;&#24515;&#21644;&#21322;&#24452;&#30340;&#29699;&#20307;&#19978;&#32467;&#26500;&#21270;&#34920;&#31034;&#25968;&#25454;&#28857;&#65292;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#22312;&#19968;&#20010;&#28789;&#27963;&#21306;&#22495;&#20869;&#23545;&#23545;&#35937;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#26032;&#23450;&#20041;&#30340;G-TSF&#38598;&#21512;&#20043;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#38598;&#21512;&#36816;&#31639;&#65292;&#24182;&#24341;&#20837;&#20102;G-TSF&#20540;&#65288;G-TSFVs&#65289;&#30340;&#22522;&#26412;&#20195;&#25968;&#36816;&#31639;&#12290;&#36825;&#20123;&#36816;&#31639;&#25193;&#23637;&#20102;&#20915;&#31574;&#32773;&#30340;&#35780;&#20272;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#22312;&#26356;&#24191;&#27867;&#30340;&#21306;&#22495;&#20869;&#36827;&#34892;&#26356;&#25935;&#24863;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#37327;&#21270;&#30456;&#20284;&#24230;&#27979;&#37327;&#65288;SM&#65289;b
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07010v1 Announce Type: new  Abstract: In this paper, we give the concept of Globular T-Spherical Fuzzy (G-TSF) Sets (G-TSFSs) as an innovative extension of T-Spherical Fuzzy Sets (TSFSs) and Circular Spherical Fuzzy Sets (C-SFSs). G-TSFSs represent membership, indeterminacy, and non-membership degrees using a globular/sphere bound that can offer a more accurate portrayal of vague, ambiguous, and imprecise information. By employing a structured representation of data points on a sphere with a specific center and radius, this model enhances decision-making processes by enabling a more comprehensive evaluation of objects within a flexible region. Following the newly defined G-TSFSs, we establish some basic set operations and introduce fundamental algebraic operations for G-TSF Values (G-TSFVs). These operations expand the evaluative capabilities of decision-makers, facilitating more sensitive decision-making processes in a broader region. To quantify a similarity measure (SM) b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;</title><link>https://arxiv.org/abs/2403.07008</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20215;&#27491;&#30830;: &#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoEval Done Right: Using Synthetic Data for Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#20351;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21487;&#20197;&#20351;&#29992;AI&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#27492;&#31867;&#30446;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#37327;&#65292;&#36825;&#19968;&#36807;&#31243;&#31216;&#20026;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20559;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#22870;&#21169;&#26426;&#22120;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MAHRM&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#23376;&#20219;&#21153;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07005</link><description>&lt;p&gt;
&#20855;&#26377;&#22870;&#21169;&#26426;&#22120;&#23618;&#27425;&#32467;&#26500;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07005
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#22870;&#21169;&#26426;&#22120;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MAHRM&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#23376;&#20219;&#21153;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RMs&#65289;&#26469;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#21033;&#29992;&#20219;&#21153;&#20013;&#39640;&#32423;&#20107;&#20214;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#20419;&#36827;&#23398;&#20064;&#25928;&#29575;&#30340;&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#22870;&#21169;&#26426;&#22120;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MAHRM&#65289;&#65292;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20107;&#20214;&#21487;&#20197;&#21516;&#26102;&#21457;&#29983;&#19988;&#20114;&#30456;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07005v1 Announce Type: new  Abstract: In this paper, we study the cooperative Multi-Agent Reinforcement Learning (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent.   MAHRM exploits the relationship of high-level events to decompose a task into a hierarchy of simpler subtasks that are assigned to a small group of agents, so as to reduce the overall computational complexity.   Experimental results in three cooperative MAR
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.07004</link><description>&lt;p&gt;
&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;
&lt;/p&gt;
&lt;p&gt;
Convergence of Some Convex Message Passing Algorithms to a Fixed Point
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07004
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#27169;&#22411;&#20013;&#35299;&#20915;MAP&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#36890;&#36807;&#65288;&#22359;&#29366;&#65289;&#22352;&#26631;&#19979;&#38477;&#26368;&#23567;&#21270;&#20174;&#23545;&#20598;&#32447;&#24615;&#35268;&#21010;&#25110;Lagrange&#26494;&#24347;&#20013;&#33719;&#24471;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#36825;&#26679;&#30340;&#31639;&#27861;&#21253;&#25324;&#26368;&#22823;&#21644;&#25193;&#25955;&#20197;&#21450;&#39034;&#24207;&#26641;&#37325;&#26032;&#21152;&#26435;&#28040;&#24687;&#20256;&#36882;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#30446;&#21069;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#20250;&#25910;&#25947;&#21040;&#30001;&#27963;&#36291;&#32422;&#26463;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#25152;&#34920;&#24449;&#30340;&#38598;&#21512;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26410;&#30693;&#65307;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#36845;&#20195;&#26159;&#21542;&#20250;&#25910;&#25947;&#65288;&#21040;&#20219;&#20309;&#19968;&#20010;&#21333;&#19968;&#28857;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#32467;&#26524;&#65288;&#20043;&#21069;&#26377;&#29468;&#24819;&#20294;&#20174;&#26410;&#35777;&#26126;&#36807;&#65289;&#65306;&#36845;&#20195;&#20250;&#25910;&#25947;&#21040;&#31639;&#27861;&#30340;&#19968;&#20010;&#22266;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#23427;&#20204;&#22312;$\mathcal{O}(1/\varepsilon)$&#27425;&#36845;&#20195;&#20013;&#36798;&#21040;&#20102;&#31934;&#24230;$\varepsilon&gt;0$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07004v1 Announce Type: new  Abstract: A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon&gt;0$ in $\mathcal{O}(1/\varepsilon)$ iterations.   We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#22478;&#24066;&#20840;&#35206;&#30422;&#26234;&#33021;&#24212;&#24613;&#20114;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#30095;&#25955;&#31649;&#29702;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#30340;&#24212;&#24613;&#21709;&#24212;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#23621;&#27665;&#30340;&#20844;&#20849;&#26381;&#21153;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07003</link><description>&lt;p&gt;
&#38754;&#21521;&#26234;&#33021;&#22478;&#24066;&#20840;&#35206;&#30422;&#26234;&#33021;&#24212;&#24613;&#20114;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#30095;&#25955;&#31649;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#22478;&#24066;&#20840;&#35206;&#30422;&#26234;&#33021;&#24212;&#24613;&#20114;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#30095;&#25955;&#31649;&#29702;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#30340;&#24212;&#24613;&#21709;&#24212;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#23621;&#27665;&#30340;&#20844;&#20849;&#26381;&#21153;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;6G&#32593;&#32476;&#37096;&#32626;&#30340;&#26234;&#33021;&#22478;&#24066;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#20013;&#23567;&#20225;&#19994;&#12289;&#34892;&#19994;&#21644;&#25919;&#24220;&#26426;&#26500;&#19982;&#22522;&#30784;&#35774;&#26045;&#36830;&#25509;&#65292;&#24182;&#22312;&#25552;&#39640;&#24212;&#24613;&#20934;&#22791;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#22871;&#21327;&#35843;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#29616;&#26377;&#30340;&#24212;&#24613;&#21709;&#24212;&#31995;&#32479;&#36716;&#21464;&#20026;&#26234;&#33021;&#20114;&#21160;&#31995;&#32479;&#65292;&#20174;&#32780;&#25913;&#21892;&#23621;&#27665;&#22312;&#23478;&#20013;&#12289;&#22312;&#36947;&#36335;&#19978;&#12289;&#22312;&#21307;&#38498;&#12289;&#20132;&#36890;&#26530;&#32445;&#31561;&#22320;&#30340;&#20844;&#20849;&#26381;&#21153;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20174;&#19982;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#23494;&#20999;&#30456;&#20851;&#30340;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#30528;&#25163;&#32771;&#34385;&#22478;&#24066;&#20840;&#26223;&#35270;&#35282;&#65292;&#20197;&#20248;&#21270;&#30456;&#20851;&#37096;&#38376;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26469;&#23454;&#29616;&#19979;&#19968;&#20195;&#20114;&#32852;&#36710;&#36742;&#20307;&#39564;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21457;&#29983;&#22312;&#24037;&#21378;&#30340;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07003v1 Announce Type: new  Abstract: A smart city solution toward future 6G network deployment allows small and medium sized enterprises (SMEs), industry, and government entities to connect with the infrastructures and play a crucial role in enhancing emergency preparedness with advanced sensors. The objective of this work is to propose a set of coordinated technological solutions to transform an existing emergency response system into an intelligent interactive system, thereby improving the public services and the quality of life for residents at home, on road, in hospitals, transport hubs, etc. In this context, we consider a city wide view from three different application scenes that are closely related to peoples daily life, to optimize the actions taken at relevant departments. Therefore, using artificial intelligence (AI) and machine learning (ML) techniques to enable the next generation connected vehicle experiences, we specifically focus on accidents happening in ind
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36827;&#34892;&#20102;&#20351;&#29992;&#22810;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#20837;&#38498;&#21518;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06999</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#29983;&#23384;&#24314;&#27169;&#65306;&#39044;&#27979;&#20837;&#38498;&#21518;&#27515;&#20129;&#29575;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36827;&#34892;&#20102;&#20351;&#29992;&#22810;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#20837;&#38498;&#21518;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#23545;&#20110;&#30740;&#31350;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#33021;&#22815;&#21160;&#24577;&#22320;&#29702;&#35299;&#20107;&#20214;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#21508;&#31181;&#29983;&#23384;&#20998;&#26512;&#25216;&#26415;&#65292;&#20174;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#21040;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25903;&#25345;&#21307;&#30103;&#24178;&#39044;&#21644;&#25919;&#31574;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27604;&#36739;&#24615;&#33021;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65288;CoxPH&#65289;&#12289;&#36880;&#27493;CoxPH&#12289;&#24377;&#24615;&#32593;&#24809;&#32602;Cox&#27169;&#22411;&#12289;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#65288;RSF&#65289;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#65288;GBM&#65289;&#23398;&#20064;&#12289;AutoScore-Survival&#12289;DeepSurv&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#30456;&#20851;Cox&#27169;&#22411;&#65288;CoxTime&#65289;&#20197;&#21450;DeepHit&#29983;&#23384;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#33268;&#24615;&#25351;&#25968;&#65288;C&#25351;&#25968;&#65289;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#24230;&#35780;&#20272;&#65292;&#29992;&#31215;&#20998;Brier&#20998;&#25968;&#65288;IBS&#65289;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06999v1 Announce Type: cross  Abstract: Survival analysis is essential for studying time-to-event outcomes and providing a dynamic understanding of the probability of an event occurring over time. Various survival analysis techniques, from traditional statistical models to state-of-the-art machine learning algorithms, support healthcare intervention and policy decisions. However, there remains ongoing discussion about their comparative performance. We conducted a comparative study of several survival analysis methods, including Cox proportional hazards (CoxPH), stepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF), Gradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv, time-dependent Cox model based on neural network (CoxTime), and DeepHit survival neural network. We applied the concordance index (C-index) for model goodness-of-fit, and integral Brier scores (IBS) for calibration, and considered the model interpretability. As a case 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#32479;&#35745;&#34920;&#24449;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#32467;&#21512;&#22810;&#20010;&#39046;&#22495;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#31361;&#20986;&#20154;&#31867;&#21019;&#36896;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#34920;&#24449;&#35780;&#20272;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#21147;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.06996</link><description>&lt;p&gt;
&#35770;&#20154;&#31867;&#21644;&#20154;&#24037;&#21019;&#36896;&#21147;&#30340;&#38543;&#26426;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the stochastics of human and artificial creativity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06996
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#32479;&#35745;&#34920;&#24449;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#32467;&#21512;&#22810;&#20010;&#39046;&#22495;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#31361;&#20986;&#20154;&#31867;&#21019;&#36896;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#34920;&#24449;&#35780;&#20272;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#21147;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#26500;&#25104;&#20154;&#31867;&#21019;&#36896;&#21147;&#65292;&#35745;&#31639;&#26426;&#26159;&#21542;&#33021;&#22815;&#23637;&#29616;&#30495;&#27491;&#30340;&#21019;&#36896;&#21147;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#35201;&#23454;&#29616;&#35745;&#31639;&#26426;&#30340;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#65292;&#25110;&#32773;&#25152;&#35859;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65292;&#24517;&#39035;&#21516;&#26102;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#34920;&#24449;&#65292;&#32467;&#21512;&#38543;&#26426;&#29702;&#35770;&#12289;&#24515;&#29702;&#23398;&#12289;&#21746;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#28151;&#27788;&#29702;&#35770;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#20026;&#36825;&#19968;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#12290;&#36825;&#31361;&#26174;&#20102;&#20154;&#31867;&#21019;&#36896;&#36807;&#31243;&#30340;&#38543;&#26426;&#24615;&#36136;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26377;&#20559;&#21521;&#23548;&#21521;&#30340;&#38543;&#26426;&#25552;&#26696;&#27493;&#39588;&#65292;&#20197;&#21450;&#20381;&#36182;&#20110;&#28789;&#27963;&#25110;&#21487;&#21464;&#20559;&#21521;&#32467;&#26500;&#30340;&#35780;&#20272;&#27493;&#39588;&#12290;&#33719;&#24471;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#34920;&#24449;&#38543;&#21518;&#34987;&#29992;&#26469;&#35780;&#20272;&#21508;&#31181;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#21147;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#12289;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06996v1 Announce Type: new  Abstract: What constitutes human creativity, and is it possible for computers to exhibit genuine creativity? We argue that achieving human-level intelligence in computers, or so-called Artificial General Intelligence, necessitates attaining also human-level creativity. We contribute to this discussion by developing a statistical representation of human creativity, incorporating prior insights from stochastic theory, psychology, philosophy, neuroscience, and chaos theory. This highlights the stochastic nature of the human creative process, which includes both a bias guided, random proposal step, and an evaluation step depending on a flexible or transformable bias structure. The acquired representation of human creativity is subsequently used to assess the creativity levels of various contemporary AI systems. Our analysis includes modern AI algorithms such as reinforcement learning, diffusion models, and large language models, addressing to what ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23481;&#37327;&#35206;&#30422;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;CCSP&#65289;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ILP&#21644;BRKGA&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06995</link><description>&lt;p&gt;
&#23481;&#37327;&#35206;&#30422;&#25512;&#38144;&#21592;&#38382;&#39064;&#30340;&#31934;&#30830;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exact algorithms and heuristics for capacitated covering salesman problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23481;&#37327;&#35206;&#30422;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;CCSP&#65289;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ILP&#21644;BRKGA&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23481;&#37327;&#35206;&#30422;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;CCSP&#65289;&#65292;&#25506;&#35752;&#20102;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#26381;&#21153;&#35206;&#30422;&#30340;&#27010;&#24565;&#12290;&#22312;CCSP&#20013;&#65292;&#25552;&#20379;&#20102;&#36710;&#36742;&#21487;&#20197;&#32463;&#36807;&#30340;&#20301;&#32622;&#65292;&#20854;&#20013;&#19968;&#20123;&#20301;&#32622;&#26377;&#38656;&#27714;&#30340;&#23458;&#25143;&#12290;&#30446;&#26631;&#26159;&#36890;&#36807;&#39547;&#25166;&#22312;&#26576;&#19968;&#28857;&#30340;&#36710;&#38431;&#20026;&#23458;&#25143;&#25552;&#20379;&#26381;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#36710;&#36742;&#34892;&#39542;&#30340;&#24635;&#36317;&#31163;&#12290;CCSP&#22312;&#23458;&#25143;&#26080;&#38656;&#34987;&#36710;&#36742;&#35775;&#38382;&#21363;&#21487;&#25552;&#20379;&#26381;&#21153;&#30340;&#24847;&#20041;&#19978;&#26159;&#29420;&#29305;&#30340;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#23458;&#25143;&#20301;&#20110;&#36710;&#36742;&#30340;&#35206;&#30422;&#21306;&#22495;&#20869;&#65292;&#23427;&#20204;&#23601;&#21487;&#20197;&#24471;&#21040;&#26381;&#21153;&#12290;&#35813;&#20551;&#35774;&#30340;&#21160;&#26426;&#26159;&#26576;&#20123;&#23458;&#25143;&#26080;&#27861;&#21040;&#36798;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#35775;&#38382;&#36710;&#36742;&#65289;&#65292;&#25110;&#32773;&#35775;&#38382;&#27599;&#20010;&#23458;&#25143;&#37117;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;ILP&#65288;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65289;&#21644;BRKGA&#65288;&#20559;&#21521;&#38543;&#26426;&#38190;&#36951;&#20256;&#31639;&#27861;&#65289;&#20803;&#21551;&#21457;&#24335;&#65292;&#20026;CCSP&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06995v1 Announce Type: new  Abstract: This paper introduces the Capacitated Covering Salesman Problem (CCSP), approaching the notion of service by coverage in capacitated vehicle routing problems. In CCSP, locations where vehicles can transit are provided, some of which have customers with demands. The objective is to service customers through a fleet of vehicles based in a depot, minimizing the total distance traversed by the vehicles. CCSP is unique in the sense that customers, to be serviced, do not need to be visited by a vehicle. Instead, they can be serviced if they are within a coverage area of the vehicle. This assumption is motivated by applications in which some customers are unreachable (e.g., forbidden access to vehicles) or visiting every customer is impractical. In this work, optimization methodologies are proposed for the CCSP based on ILP (Integer Linear Programming) and BRKGA (Biased Random-Key Genetic Algorithm) metaheuristic. Computational experiments cond
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;TSFallDetect&#65292;&#21033;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#36300;&#20498;&#21160;&#20316;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06994</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physics Sensor Based Deep Learning Fall Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;TSFallDetect&#65292;&#21033;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#36300;&#20498;&#21160;&#20316;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#30340;&#36300;&#20498;&#26816;&#27979;&#26159;&#36817;&#24180;&#26469;&#30340;&#19968;&#20010;&#23454;&#29992;&#19988;&#27969;&#34892;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TSFallDetect&#30340;&#23436;&#25972;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#25509;&#25910;&#35774;&#22791;&#12289;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#24179;&#21488;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#30340;&#26381;&#21153;&#22120;&#65292;&#29992;&#20110;&#25910;&#38598;&#27169;&#22411;&#21644;&#25968;&#25454;&#20197;&#36827;&#34892;&#26410;&#26469;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#24815;&#24615;&#21644;&#34180;&#33180;&#21387;&#21147;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#36300;&#20498;&#21160;&#20316;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06994v1 Announce Type: cross  Abstract: Fall detection based on embedded sensor is a practical and popular research direction in recent years. In terms of a specific application: fall detection methods based upon physics sensors such as [gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them in machine learning models like Markov chain or just threshold based classification methods. In this paper, we build a complete system named TSFallDetect including data receiving device based on embedded sensor, mobile deep-learning model deploying platform, and a simple server, which will be used to gather models and data for future expansion. On the other hand, we exploit the sequential deep-learning methods to address this falling motion prediction problem based on data collected by inertial and film pressure sensors. We make a empirical study based on existing datasets and our datasets collected from our system separately, which shows tha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#25935;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#21464;&#26356;&#23433;&#20840;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06993</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#21464;&#26356;&#23433;&#20840;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automatic driving lane change safety prediction model based on LSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06993
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#25935;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#21464;&#26356;&#23433;&#20840;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#65292;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#39640;&#20102;&#20132;&#36890;&#27969;&#37327;&#65292;&#20943;&#23569;&#25317;&#22581;&#65292;&#33410;&#32422;&#33021;&#28304;&#24182;&#25552;&#39640;&#20986;&#34892;&#25928;&#29575;&#12290;&#22312;&#30456;&#23545;&#25104;&#29087;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20013;&#65292;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#20998;&#20026;&#20960;&#20010;&#27169;&#22359;&#65306;&#24863;&#30693;&#12289;&#20915;&#31574;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#21512;&#29702;&#30340;&#20998;&#24037;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#20855;&#22791;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#30340;&#33021;&#21147;&#65292;&#20197;&#20570;&#20986;&#21512;&#29702;&#30340;&#20915;&#31574;&#35268;&#21010;&#21644;&#23433;&#20840;&#25514;&#26045;&#65292;&#25552;&#39640;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#12289;&#20197;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#23433;&#20840;&#25935;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#32531;&#35299;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#35268;&#21010;&#30340;&#32570;&#28857;&#65292;&#36755;&#20986;&#36712;&#36857;&#19981;&#20165;&#20445;&#35777;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#21319;&#20102;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06993v1 Announce Type: cross  Abstract: Autonomous driving technology can improve traffic safety and reduce traffic accidents. In addition, it improves traffic flow, reduces congestion, saves energy and increases travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. Therefore, autonomous vehicles need to have the ability to predict the trajectory of surrounding vehicles in order to make reasonable decision planning and safety measures to improve driving safety. By using deep learning method, a safety-sensitive deep learning model based on short term memory (LSTM) network is proposed. This model can alleviate the shortcomings of current automatic driving trajectory planning, and the output trajectory not only ensures high accuracy but also improves safety. The cell sta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06914</link><description>&lt;p&gt;
MEND&#65306;&#20803;&#28436;&#31034;&#33976;&#39311;&#29992;&#20110;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#65292;&#20854;&#20013;LLM&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#36755;&#20837;&#21644;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#23545;(&#28436;&#31034;)&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#28436;&#31034;&#30340;&#21152;&#20837;&#23548;&#33268;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#24320;&#38144;&#21576;&#20108;&#27425;&#22686;&#21152;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23581;&#35797;&#23558;&#20887;&#38271;&#30340;&#28436;&#31034;&#33976;&#39311;&#25104;&#32039;&#20945;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#29306;&#29298;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#23558;&#20219;&#20309;&#20887;&#38271;&#28436;&#31034;&#33976;&#39311;&#20026;&#21521;&#37327;&#65292;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;MEND&#20855;&#26377;&#33976;&#39311;&#28436;&#31034;&#30340;&#20803;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06914v1 Announce Type: cross  Abstract: Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrat
&lt;/p&gt;</description></item><item><title>CEAT&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#31034;&#33539;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21442;&#25968;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#21644;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06670</link><description>&lt;p&gt;
CEAT&#65306;&#29992;&#20110;&#38750;&#31034;&#33539;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06670
&lt;/p&gt;
&lt;p&gt;
CEAT&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#31034;&#33539;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21442;&#25968;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#21644;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21160;&#24577;&#22330;&#26223;&#35201;&#27714;&#27169;&#22411;&#20855;&#22791;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#26087;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#32463;&#39564;&#37325;&#25918;&#26041;&#27861;&#23384;&#20648;&#19968;&#37096;&#20998;&#26087;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#22312;&#26356;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#65292;&#23384;&#20648;&#26087;&#22270;&#20687;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36825;&#23548;&#33268;&#20102;&#26356;&#20026;&#20005;&#37325;&#30340;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#21644;&#20998;&#31867;&#22120;&#20559;&#24046;&#12290;&#20026;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21464;&#21387;&#22120;&#65288;CEAT&#65289;&#12290;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#25193;&#23637;-&#34701;&#21512;&#23618;&#19982;&#20923;&#32467;&#21069;&#26399;&#21442;&#25968;&#24182;&#34892;&#25193;&#23637;&#26469;&#23398;&#20064;&#26032;&#30693;&#35782;&#12290;&#20219;&#21153;&#32467;&#26463;&#21518;&#65292;&#25105;&#20204;&#26080;&#25439;&#22320;&#21560;&#25910;&#25193;&#23637;&#30340;&#21442;&#25968;&#21040;&#20027;&#24178;&#65292;&#20197;&#30830;&#20445;&#21442;&#25968;&#25968;&#37327;&#20445;&#25345;&#24658;&#23450;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21407;&#22411;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#26087;&#31867;&#21644;&#26032;&#31867;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06670v1 Announce Type: cross  Abstract: In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes 
&lt;/p&gt;</description></item><item><title>DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06397</link><description>&lt;p&gt;
DeepSafeMPC: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06397
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;safe MARL&#65289;&#22312;&#26368;&#36817;&#20960;&#24180;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#24378;&#35843;&#20102;&#26234;&#20307;&#19981;&#20165;&#38656;&#35201;&#20248;&#21270;&#20840;&#23616;&#22238;&#25253;&#65292;&#36824;&#38656;&#35201;&#36890;&#36807;&#34892;&#20026;&#32422;&#26463;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#30340;&#24517;&#35201;&#24615;&#12290;&#36817;&#26399;&#19968;&#20123;&#24037;&#20316;&#23558;&#25511;&#21046;&#29702;&#35770;&#19982;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24212;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#26234;&#20307;&#29615;&#22659;&#20013;&#22797;&#26434;&#19988;&#38544;&#24335;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;DeepSafeMPC&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DeepSafeMPC &#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24456;&#22909;&#22320;&#39044;&#27979;&#29615;&#22659;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;MARL&#21407;&#21017;&#26469;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#39033;&#30446;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.06025</link><description>&lt;p&gt;
CarbonNet: &#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#27668;&#20505;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#26159;&#20160;&#20040;&#65311; &#24212;&#29992;&#65306;&#23398;&#20064;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#24418;&#29366;&#20013;&#20943;&#32531;&#20840;&#29699;&#21464;&#26262;&#30340;&#22320;&#36136;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#39033;&#30446;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#65292;&#20197;&#24212;&#29992;&#20110;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#12290;CCS&#24050;&#34987;&#35777;&#26126;&#26159;&#30899;&#20013;&#21644;&#31038;&#20250;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#23478;&#21457;&#29616;&#23384;&#22312;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#22823;&#27169;&#22411;&#23610;&#24230;&#32780;&#23548;&#33268;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#38590;&#20197;&#27867;&#21270;&#20855;&#26377;&#22797;&#26434;&#29289;&#29702;&#23398;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#30001;&#30899;&#27880;&#20837;&#23548;&#33268;&#30340;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#21709;&#24212;&#65292;&#24182;&#21033;&#29992;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#20026;CCS&#39033;&#30446;&#30340;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06025v1 Announce Type: cross  Abstract: We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface geometry images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNe
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05918</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05918
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#26080;&#27861;&#26377;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#24179;&#34913;&#27169;&#22411;&#35757;&#32451;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36890;&#24120;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#20026;&#23569;&#25968;&#31867;&#29983;&#25104;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#20998;&#31867;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;SMOTE&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20165;&#20851;&#27880;&#25968;&#25454;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#19981;&#22815;&#36924;&#30495;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#24403;&#21069;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#65292;&#20294;&#35757;&#32451;&#20013;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65307;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;U-Net&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#31070;&#32463;&#32593;&#32476;&#19981;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04964</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#23454;&#35805;&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tell me the truth: A system to measure the trustworthiness of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#20174;2023&#24180;11&#26376;ChatGPT&#25512;&#20986;&#20197;&#26469;&#65292;&#22312;&#22823;&#22810;&#25968;&#26032;&#38395;&#20013;&#21344;&#25454;&#20102;&#37325;&#35201;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#19968;&#24180;&#22810;&#36807;&#21435;&#20102;&#65292;&#20844;&#21496;&#25269;&#35302;&#37319;&#29992;&#23427;&#20204;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20182;&#20204;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#32570;&#20047;&#20449;&#24515;&#12290;&#19968;&#39033;&#30001;Baymard&#65288;2023&#65289;&#36827;&#34892;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT-4 &#22312;&#35782;&#21035;&#32593;&#31449;&#21487;&#29992;&#24615;&#38382;&#39064;&#26102;&#26377;80.1%&#30340;&#20551;&#38451;&#24615;&#38169;&#35823;&#29575;&#12290;&#32780;&#12298;JAMA&#20799;&#31185;&#23398;&#12299;&#26434;&#24535;&#65288;JAMA Pediatrics&#65289;&#20110;2024&#24180;1&#26376;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT &#22312;&#35786;&#26029;&#20799;&#31185;&#21307;&#30103;&#26696;&#20363;&#26102;&#30340;&#20934;&#30830;&#29575;&#20026;17%&#65288;Barile et al., 2024&#65289;&#12290;&#37027;&#20040;&#65292;&#20309;&#20026;&#8220;&#20449;&#20219;&#8221;&#65311;&#20449;&#20219;&#26159;&#19968;&#20010;&#30456;&#23545;&#30340;&#12289;&#20027;&#35266;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#21270;&#12289;&#39046;&#22495;&#21644;&#20010;&#20307;&#32780;&#21464;&#21270;&#12290;&#37027;&#20040;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#34913;&#37327;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#21602;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#34920;&#31034;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.04558</link><description>&lt;p&gt;
&#20943;&#23569;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#25913;&#21892;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20174;&#24120;&#35268;&#21487;&#29992;&#30340;&#32452;&#32455;&#23398;&#25968;&#25454;&#20013;&#25552;&#21462;&#20020;&#24202;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#30340;&#26631;&#27880;&#65292;&#36825;&#31181;&#26631;&#27880;&#31232;&#32570;&#19988;&#26114;&#36149;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#28040;&#38500;&#20102;&#36825;&#19968;&#38556;&#30861;&#65292;&#20801;&#35768;&#23545;&#38750;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;SSL&#26041;&#27861;&#37319;&#29992;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#25968;&#25454;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#30828;&#20214;&#35201;&#27714;&#21644;&#25972;&#20307;&#25104;&#26412;&#22686;&#21152;&#65292;&#20351;&#24471;&#24456;&#23569;&#26426;&#26500;&#33021;&#22815;&#33719;&#24471;&#36825;&#20123;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#22797;&#26434;&#24615;&#19982;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#37327;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#30340;&#35843;&#25972;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#23545;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#65292;&#31361;&#30772;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02760</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#22312;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#20013;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02760
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#23545;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#65292;&#31361;&#30772;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#21830;&#21697;&#20043;&#38388;&#30340;&#20114;&#21160;&#24182;&#34701;&#20837;&#20854;&#25991;&#26412;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;DNN&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#38590;&#20197;&#26377;&#25928;&#29702;&#35299;&#29992;&#25143;&#30340;&#20852;&#36259;&#21644;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22914;ChatGPT&#21644;GPT-4&#65292;&#30001;&#20110;&#22312;&#35821;&#35328;&#29702;&#35299;&#31561;&#22522;&#26412;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#33021;&#21147;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02760v1 Announce Type: new  Abstract: With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions. At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understandin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;</title><link>https://arxiv.org/abs/2403.02624</link><description>&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#19987;&#27880;&#20110;&#21457;&#23637;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#26469;&#33258;&#30701;&#26399;&#21644;&#38271;&#26399;&#25928;&#26524;&#30340;&#24635;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290; &#20363;&#22914;&#65292;&#33647;&#29289;&#21058;&#37327;&#30340;&#22686;&#21152;&#21487;&#33021;&#20250;&#25552;&#39640;&#24739;&#32773;&#24247;&#22797;&#36895;&#24230;&#65288;&#30701;&#26399;&#65289;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#38271;&#26399;&#21103;&#20316;&#29992;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#26377;&#20851;&#30701;&#26399;&#25110;&#38271;&#26399;&#25928;&#24212;&#25110;&#20004;&#32773;&#30340;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20197;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#20256;&#32479;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30452;&#25509;&#20272;&#35745;&#22810;&#20010;&#30446;&#26631;&#26102;&#65292;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#20248;&#21270;&#26041;&#21521;&#20063;&#21487;&#33021;&#21457;&#29983;&#20914;&#31361;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24085;&#32047;&#25176;&#26377;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#65288;POE&#65289;&#21644;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65288;POPL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#25299;&#25169;&#19978;&#26356;&#20026;&#22797;&#26434;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10011</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Simplicial Message Passing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#25299;&#25169;&#19978;&#26356;&#20026;&#22797;&#26434;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21333;&#20307;&#22797;&#21512;&#20307;&#19978;&#36827;&#34892;&#21487;&#25511;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#21518;&#32773;&#22312;&#25299;&#25169;&#19978;&#27604;&#24120;&#35268;&#22270;&#28040;&#24687;&#20256;&#36882;&#26356;&#21152;&#22797;&#26434;&#12290;Clifford&#20195;&#25968;&#21253;&#25324;&#39640;&#38454;&#23545;&#35937;&#65292;&#22914;&#21452;&#21521;&#37327;&#21644;&#19977;&#21521;&#37327;&#65292;&#36825;&#20123;&#23545;&#35937;&#36890;&#36807;&#21521;&#37327;&#34893;&#29983;&#20986;&#20960;&#20309;&#29305;&#24449;&#65288;&#20363;&#22914;&#38754;&#31215;&#65292;&#20307;&#31215;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#39030;&#28857;&#30340;&#20960;&#20309;&#20056;&#31215;&#34920;&#31034;&#31616;&#21333;&#24418;&#24335;&#29305;&#24449;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#20849;&#20139;&#28040;&#24687;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#30340;&#28040;&#24687;&#38480;&#21046;&#20026;&#26469;&#33258;&#19981;&#21516;&#32500;&#24230;&#30340;&#20256;&#20837;&#28040;&#24687;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#20849;&#20139;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36755;&#20986;&#36866;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10011v1 Announce Type: new  Abstract: We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to ou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;TENG&#20316;&#20026;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#31616;&#21333;&#32467;&#26500;&#21644;&#39640;&#30636;&#26102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;TENG&#20316;&#20026;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#31616;&#21333;&#32467;&#26500;&#21644;&#39640;&#30636;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#22914;&#20309;&#24320;&#21457;&#20855;&#26377;&#21487;&#25345;&#32493;&#30005;&#28304;&#20379;&#24212;&#12289;&#26131;&#20110;&#37096;&#32626;&#21644;&#28789;&#27963;&#20351;&#29992;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#31995;&#32479;&#24050;&#25104;&#20026;&#19968;&#20010;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#30005;&#28304;&#20379;&#24212;&#23384;&#22312;&#39057;&#32321;&#26356;&#25442;&#25110;&#20351;&#29992;&#26102;&#20805;&#30005;&#31561;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32858;&#22235;&#27679;&#20057;&#28911;&#65288;PTFE&#65289;&#21644;&#38109;&#31636;&#65288;AI&#65289;&#21046;&#22791;&#25509;&#35302;-&#20998;&#31163;&#25705;&#25830;&#32435;&#31859;&#21457;&#30005;&#26426;&#65288;TENG&#65289;&#26469;&#25910;&#38598;&#20154;&#20307;&#36816;&#21160;&#33021;&#37327;&#65292;&#26681;&#25454;&#36755;&#20986;&#30005;&#20449;&#21495;&#30340;&#21464;&#21270;&#26469;&#30417;&#27979;&#20154;&#20307;&#36816;&#21160;&#23039;&#21183;&#12290; 2012&#24180;&#65292;&#29579;&#20013;&#26519;&#38498;&#22763;&#21450;&#20854;&#22242;&#38431;&#21457;&#26126;&#20102;&#25705;&#25830;&#30005;&#32435;&#31859;&#21457;&#30005;&#26426;&#65288;TENG&#65289;&#65292;&#23427;&#21033;&#29992;&#26368;&#22823;&#20301;&#31227;&#30005;&#27969;&#20316;&#20026;&#39537;&#21160;&#21147;&#65292;&#23558;&#26426;&#26800;&#21050;&#28608;&#30452;&#25509;&#36716;&#25442;&#20026;&#30005;&#20449;&#21495;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20316;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#12290;TENG&#20256;&#24863;&#22120;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#21644;&#30636;&#26102;&#24615;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09442v1 Announce Type: cross  Abstract: In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instant
&lt;/p&gt;</description></item><item><title>WiMANS&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#21644;&#21516;&#27493;&#35270;&#39057;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#30740;&#31350;</title><link>https://arxiv.org/abs/2402.09430</link><description>&lt;p&gt;
WiMANS: WiFi-based&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09430
&lt;/p&gt;
&lt;p&gt;
WiMANS&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#21644;&#21516;&#27493;&#35270;&#39057;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi-based human sensing&#34920;&#29616;&#20986;&#20102;&#22312;&#19981;&#20405;&#20837;&#21644;&#26080;&#38656;&#35774;&#22791;&#30340;&#24773;&#20917;&#19979;&#20998;&#26512;&#29992;&#25143;&#34892;&#20026;&#30340;&#26174;&#30528;&#28508;&#21147;&#65292;&#20351;&#24471;&#26234;&#33021;&#23478;&#23621;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#24212;&#29992;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#29992;&#25143;&#24863;&#30693;&#19978;&#65292;&#22312;&#28041;&#21450;&#22810;&#29992;&#25143;&#22330;&#26223;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25506;&#35752;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;WiMANS&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;WiFi&#30340;&#22810;&#29992;&#25143;&#27963;&#21160;&#24863;&#30693;&#25968;&#25454;&#38598;&#12290;WiMANS&#21253;&#21547;&#36229;&#36807;9.4&#23567;&#26102;&#30340;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#65292;&#30417;&#27979;&#22810;&#20010;&#29992;&#25143;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#21516;&#26102;&#36827;&#34892;&#30340;&#27963;&#21160;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;WiMANS&#19981;&#20165;&#25910;&#38598;&#20102;&#21452;WiFi&#39057;&#27573;&#30340;CSI&#65292;&#36824;&#21253;&#25324;&#20102;&#21516;&#27493;&#35270;&#39057;&#12290;&#25105;&#20204;&#21033;&#29992;WiMANS&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09430v1 Announce Type: cross  Abstract: WiFi-based human sensing has exhibited remarkable potential to analyze user behaviors in a non-intrusive and device-free manner, benefiting applications as diverse as smart homes and healthcare. However, most previous works focus on single-user sensing, which has limited practicability in scenarios involving multiple users. Although recent studies have begun to investigate WiFi-based multi-user activity sensing, there remains a lack of benchmark datasets to facilitate reproducible and comparable research. To bridge this gap, we present WiMANS, to our knowledge, the first dataset for multi-user activity sensing based on WiFi. WiMANS contains over 9.4 hours of WiFi Channel State Information (CSI), monitoring simultaneous activities performed by multiple users in various environments. Compared to existing datasets, WiMANS not only collects the CSI of dual WiFi bands but also includes synchronized videos. We exploit WiMANS to benchmark the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#19968;&#32500;&#32467;&#26500;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25830;&#38500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#38750;&#20405;&#20837;&#24615;&#12289;&#31934;&#30830;&#24615;&#12289;&#21487;&#23450;&#21046;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.16145</link><description>&lt;p&gt;
&#19968;&#32500;&#36866;&#37197;&#22120;&#26469;&#32479;&#27835;&#23427;&#20204;&#25152;&#26377;&#65306;&#27010;&#24565;&#12289;&#25193;&#25955;&#27169;&#22411;&#21644;&#28040;&#38500;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16145
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19968;&#32500;&#32467;&#26500;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25830;&#38500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#38750;&#20405;&#20837;&#24615;&#12289;&#31934;&#30830;&#24615;&#12289;&#21487;&#23450;&#21046;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21830;&#19994;&#21644;&#24320;&#28304;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#38450;&#27490;&#19981;&#33391;&#34892;&#20026;&#65292;&#29616;&#26377;&#30340;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#22522;&#20110;&#23436;&#20840;&#21442;&#25968;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35266;&#23519;&#21040;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#19981;&#26029;&#20405;&#34432;&#26397;&#21521;&#30340;&#29983;&#25104;&#65306;&#30446;&#26631;&#28040;&#38500;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#28418;&#31227;&#23548;&#33268;&#25152;&#26377;&#29983;&#25104;&#30340;&#21464;&#21270;&#21644;&#28508;&#22312;&#21464;&#24418;&#65292;&#29978;&#33267;&#22312;&#22810;&#27010;&#24565;&#28040;&#38500;&#26102;&#20405;&#34432;&#20854;&#20182;&#27010;&#24565;&#65292;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#26356;&#21152;&#26126;&#26174;&#65307;2&#65289;&#36716;&#31227;&#33021;&#21147;&#21644;&#37096;&#32626;&#25928;&#29575;&#65306;&#20043;&#21069;&#22522;&#20110;&#27169;&#22411;&#30340;&#25830;&#38500;&#38459;&#30861;&#20102;&#27010;&#24565;&#30340;&#28789;&#27963;&#32452;&#21512;&#21644;&#35757;&#32451;&#20813;&#36153;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#23548;&#33268;&#37096;&#32626;&#22330;&#26223;&#22686;&#21152;&#26102;&#25104;&#26412;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#20405;&#20837;&#12289;&#31934;&#30830;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#36716;&#31227;&#30340;&#28040;&#38500;&#65292;&#25105;&#20204;&#23558;&#25830;&#38500;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#32500;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16145v2 Announce Type: replace-cross  Abstract: The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability &amp; deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimension
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#20854;&#22312;&#25216;&#26415;&#20538;&#21153;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.15020</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Automated Approaches to Detect Self-Admitted Technical Debt: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15020
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#20854;&#22312;&#25216;&#26415;&#20538;&#21153;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#20538;&#21153;&#26159;&#36719;&#20214;&#24320;&#21457;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#28304;&#33258;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#26435;&#34913;&#65292;&#22312;&#24433;&#21709;&#36719;&#20214;&#21487;&#32500;&#25252;&#24615;&#21644;&#38459;&#30861;&#26410;&#26469;&#24320;&#21457;&#24037;&#20316;&#26041;&#38754;&#36215;&#21040;&#20316;&#29992;&#12290;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#25351;&#30340;&#26159;&#24320;&#21457;&#20154;&#21592;&#26126;&#30830;&#25215;&#35748;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#30340;&#20195;&#30721;&#36136;&#37327;&#25110;&#35774;&#35745;&#32570;&#38519;&#12290;&#33258;&#21160;&#26816;&#27979;SATD&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;&#25216;&#26415;&#20538;&#21153;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;NLP&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31639;&#27861;&#31181;&#31867;&#22810;&#26679;&#21270;&#24120;&#24120;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#20854;&#30446;&#30340;&#26159;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#25152;&#32771;&#23519;&#30740;&#31350;&#20013;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15020v2 Announce Type: replace-cross  Abstract: Technical debt is a pervasive issue in software development, often arising from trade-offs made during development, which can impede software maintainability and hinder future development efforts. Self-admitted technical debt (SATD) refers to instances where developers explicitly acknowledge suboptimal code quality or design flaws in the codebase. Automated detection of SATD has emerged as a critical area of research, aiming to assist developers in identifying and addressing technical debt efficiently. However, the enormous variety of feature extraction approaches of NLP and algorithms employed in the literature often hinder researchers from trying to improve their performance. In light of this, this systematic literature review proposes a taxonomy of feature extraction techniques and ML/DL algorithms used in technical debt detection: its objective is to compare and benchmark their performance in the examined studies. We select
&lt;/p&gt;</description></item><item><title>HyperATLS$^*_S&#25193;&#23637;&#20102;ATL$^*$&#65292;&#20801;&#35768;&#27604;&#36739;&#22810;&#20010;&#25112;&#30053;&#20114;&#21160;&#32467;&#26524;&#19982;&#36229;&#24615;&#36136;&#65292;&#24182;&#35201;&#27714;&#19968;&#20123;Agent&#20849;&#20139;&#30456;&#21516;&#31574;&#30053;&#65292;&#25429;&#25417;&#37325;&#35201;AI&#30456;&#20851;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.12403</link><description>&lt;p&gt;
&#35770;&#20132;&#26367;&#26102;&#38388;&#26102;&#24577;&#36923;&#36753;&#65292;&#36229;&#24615;&#36136;&#21644;&#31574;&#30053;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
On Alternating-Time Temporal Logic, Hyperproperties, and Strategy Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12403
&lt;/p&gt;
&lt;p&gt;
HyperATLS$^*_S&#25193;&#23637;&#20102;ATL$^*$&#65292;&#20801;&#35768;&#27604;&#36739;&#22810;&#20010;&#25112;&#30053;&#20114;&#21160;&#32467;&#26524;&#19982;&#36229;&#24615;&#36136;&#65292;&#24182;&#35201;&#27714;&#19968;&#20123;Agent&#20849;&#20139;&#30456;&#21516;&#31574;&#30053;&#65292;&#25429;&#25417;&#37325;&#35201;AI&#30456;&#20851;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#26367;&#26102;&#38388;&#26102;&#24577;&#36923;&#36753;&#65288;ATL$^*$&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#24418;&#24335;&#25512;&#29702;&#22810;Agent&#31995;&#32479;&#30340;&#33391;&#22909;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;ATL$^*$&#21487;&#20197;&#25512;&#29702;Agent&#30340;&#25112;&#30053;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#26576;&#20010;&#32852;&#30431;$A$&#21487;&#20197;&#30830;&#20445;&#26368;&#32456;&#23454;&#29616;&#30446;&#26631;&#65289;&#65292;&#25105;&#20204;&#21364;&#26080;&#27861;&#27604;&#36739;&#22810;&#20010;&#25112;&#30053;&#20114;&#21160;&#65292;&#20063;&#19981;&#33021;&#35201;&#27714;&#22810;&#20010;Agent&#37319;&#29992;&#30456;&#21516;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperATLS$^*_S&#65292;&#36825;&#26159;ATL$^*$&#30340;&#25193;&#23637;&#65292;&#25105;&#20204;&#21487;&#20197;&#65288;1&#65289;&#27604;&#36739;&#22810;&#20010;&#25112;&#30053;&#20114;&#21160;&#30340;&#32467;&#26524;&#65292;2&#65289;&#24378;&#21046;&#19968;&#20123;Agent&#20849;&#20139;&#30456;&#21516;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HyperATL$^*_S&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#35268;&#33539;&#35821;&#35328;&#65292;&#25429;&#25417;&#20102;&#29616;&#26377;&#36923;&#36753;&#26080;&#27861;&#36798;&#21040;&#30340;&#37325;&#35201;AI&#30456;&#20851;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12403v2 Announce Type: replace  Abstract: Alternating-time temporal logic (ATL$^*$) is a well-established framework for formal reasoning about multi-agent systems. However, while ATL$^*$ can reason about the strategic ability of agents (e.g., some coalition $A$ can ensure that a goal is reached eventually), we cannot compare multiple strategic interactions, nor can we require multiple agents to follow the same strategy. For example, we cannot state that coalition $A$ can reach a goal sooner (or more often) than some other coalition $A'$. In this paper, we propose HyperATLS$^*_S$, an extension of ATL$^*$ in which we can (1) compare the outcome of multiple strategic interactions w.r.t. a hyperproperty, i.e., a property that refers to multiple paths at the same time, and (2) enforce that some agents share the same strategy. We show that HyperATL$^*_S$ is a rich specification language that captures important AI-related properties that were out of reach of existing logics. We pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.09982</link><description>&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#65306;AI-Enabled Compiler-driven Program Optimization&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;LLVM&#25552;&#20379;&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#19981;&#21516;&#30340;&#20248;&#21270;&#36890;&#36335;&#20013;&#33719;&#30410;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;ACPO&#30340;&#39640;&#23618;&#35270;&#22270;&#12289;&#31867;&#23618;&#27425;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#24490;&#29615;&#23637;&#24320;&#21644;&#20989;&#25968;&#20869;&#32852;&#20256;&#36882;&#30340;ML&#20351;&#33021;&#21270;&#65292;&#23637;&#31034;&#20102;ACPO&#30340;&#19968;&#20123;&#29992;&#20363;&#65292;&#25551;&#36848;&#20102;ACPO&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
&lt;/p&gt;</description></item><item><title>Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.03781</link><description>&lt;p&gt;
Lite-Mind: &#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lite-Mind: Towards Efficient and Robust Brain Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03781
&lt;/p&gt;
&lt;p&gt;
Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30340;fMRI&#26041;&#27861;&#35299;&#30721;&#22823;&#33041;&#20013;&#30340;&#35270;&#35273;&#20449;&#24687;&#30340;&#30740;&#31350;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#25361;&#25112;&#22312;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;fMRI&#20449;&#21495;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#23548;&#33268;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#30340;&#20302;&#31934;&#24230;&#12290;MindEye&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#39640;&#21442;&#25968;&#35745;&#25968;&#30340;&#28145;&#24230;MLP&#65288;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;996M MLP&#20027;&#24178;&#65289;&#23558;fMRI&#23884;&#20837;&#23545;&#40784;&#21040;CLIP&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26368;&#32456;&#38544;&#34255;&#23618;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#20869;&#65292;&#21463;&#35797;&#32773;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#30340;&#20010;&#20307;&#24046;&#24322;&#65292;&#38656;&#35201;&#35757;&#32451;&#29305;&#23450;&#20110;&#21463;&#35797;&#32773;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#22823;&#37327;&#30340;&#21442;&#25968;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#37096;&#32626;fMRI&#35299;&#30721;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#25552;&#20379;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03781v2 Announce Type: replace-cross  Abstract: Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRA&#30340;&#32852;&#37030;&#35843;&#20248;&#31639;&#27861;&#65292;&#21487;&#20197;&#38543;&#26426;&#29983;&#25104;&#20998;&#37197;&#30697;&#38453;&#26469;&#24212;&#23545;&#25317;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#65292;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2311.11227</link><description>&lt;p&gt;
FedRA:&#19968;&#31181;&#29992;&#20110;&#37322;&#25918;&#24322;&#26500;&#23458;&#25143;&#31471;&#24378;&#22823;&#28508;&#21147;&#30340;&#38543;&#26426;&#20998;&#37197;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRA&#30340;&#32852;&#37030;&#35843;&#20248;&#31639;&#27861;&#65292;&#21487;&#20197;&#38543;&#26426;&#29983;&#25104;&#20998;&#37197;&#30697;&#38453;&#26469;&#24212;&#23545;&#25317;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#65292;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#26085;&#30410;&#21487;&#29992;&#65292;&#32852;&#37030;&#35843;&#20248;&#22312;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#21033;&#29992;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#20849;&#21516;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32852;&#37030;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#65292;&#23548;&#33268;&#23427;&#20204;&#26080;&#27861;&#25903;&#25345;&#25972;&#20010;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#35843;&#20248;&#31639;&#27861;FedRA&#12290;FedRA&#30340;&#23454;&#26045;&#31616;&#21333;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#26080;&#38656;&#23545;&#21407;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#20462;&#25913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#19968;&#36718;&#36890;&#20449;&#20013;&#65292;FedRA&#20250;&#38543;&#26426;&#29983;&#25104;&#19968;&#20010;&#20998;&#37197;&#30697;&#38453;&#12290;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#65292;&#23427;&#20250;&#26681;&#25454;&#20998;&#37197;&#24773;&#20917;&#37325;&#26032;&#32452;&#32455;&#21407;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11227v2 Announce Type: replace-cross  Abstract: With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the alloc
&lt;/p&gt;</description></item><item><title>3DCoMPaT$^{++}$&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#20159;&#20010;&#28210;&#26579;&#35270;&#22270;&#30340;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#65292;&#24102;&#26377;&#35814;&#32454;&#30340;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#26631;&#27880;&#65292;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2310.18511</link><description>&lt;p&gt;
3DCoMPaT$^{++}$&#65306;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#30340;&#25913;&#36827;&#22411;&#22823;&#35268;&#27169;&#19977;&#32500;&#35270;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18511
&lt;/p&gt;
&lt;p&gt;
3DCoMPaT$^{++}$&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#20159;&#20010;&#28210;&#26579;&#35270;&#22270;&#30340;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#65292;&#24102;&#26377;&#35814;&#32454;&#30340;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#26631;&#27880;&#65292;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3DCoMPaT$^{++}$&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1.6&#20159;&#20010;&#20197;&#19978;10&#30334;&#19975;&#20010;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#30340;&#28210;&#26579;&#35270;&#22270;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#24418;&#29366;&#22312;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#31934;&#24515;&#27880;&#37322;&#65292;&#24182;&#37197;&#26377;&#21305;&#37197;&#30340;RGB&#28857;&#20113;&#12289;3D&#32441;&#29702;&#32593;&#26684;&#12289;&#28145;&#24230;&#22270;&#21644;&#20998;&#21106;&#33945;&#29256;&#12290;3DCoMPaT$^{++}$&#28085;&#30422;&#20102;41&#20010;&#24418;&#29366;&#31867;&#21035;&#12289;275&#20010;&#32454;&#31890;&#24230;&#37096;&#20998;&#31867;&#21035;&#21644;293&#20010;&#32454;&#31890;&#24230;&#26448;&#26009;&#31867;&#21035;&#65292;&#36825;&#20123;&#31867;&#21035;&#21487;&#20197;&#32452;&#21512;&#24212;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#30340;&#21508;&#37096;&#20998;&#12290;&#25105;&#20204;&#20174;&#22235;&#20010;&#31561;&#38388;&#36317;&#35270;&#22270;&#21644;&#22235;&#20010;&#38543;&#26426;&#35270;&#22270;&#20013;&#28210;&#26579;&#20102;&#19968;&#30334;&#19975;&#20010;&#39118;&#26684;&#21270;&#24418;&#29366;&#30340;&#23376;&#38598;&#65292;&#20849;&#35745;1.6&#20159;&#20010;&#28210;&#26579;&#12290;&#37096;&#20214;&#22312;&#23454;&#20363;&#32423;&#21035;&#12289;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Grounded CoMPaT Recognition (GCR)&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#20849;&#21516;&#35782;&#21035;&#21644;&#22522;&#20110;&#29289;&#20307;&#37096;&#20998;&#30340;&#26448;&#26009;&#32452;&#21512;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#27963;&#21160;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18511v2 Announce Type: replace-cross  Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#30452;&#26041;&#22270;&#21305;&#37197;&#26469;&#36716;&#25442;&#39069;&#22806;&#25968;&#25454;&#22312;&#22788;&#29702;&#22495;&#20559;&#31227;&#26102;&#27604;&#31616;&#21333;&#24402;&#19968;&#21270;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2309.02001</link><description>&lt;p&gt;
&#20998;&#26512;&#22312;MICCAI KiTS23&#25361;&#25112;&#20013;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#26102;&#30340;&#22495;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02001
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30452;&#26041;&#22270;&#21305;&#37197;&#26469;&#36716;&#25442;&#39069;&#22806;&#25968;&#25454;&#22312;&#22788;&#29702;&#22495;&#20559;&#31227;&#26102;&#27604;&#31616;&#21333;&#24402;&#19968;&#21270;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#24050;&#30693;&#21487;&#20197;&#25913;&#21892;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;3D&#20998;&#21106;&#65292;&#22312;&#37027;&#37324;&#32570;&#20047;&#35757;&#32451;&#36164;&#26009;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#21487;&#33021;&#26159;&#20351;&#29992;&#20854;&#20182;&#20202;&#22120;&#33719;&#21462;&#24182;&#32463;&#36807;&#39044;&#22788;&#29702;&#65292;&#20351;&#24471;&#20854;&#20998;&#24067;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21892;&#22495;&#20559;&#31227;&#30340;&#25216;&#26415;&#65292;&#20351;&#24471;&#39069;&#22806;&#25968;&#25454;&#21487;&#20197;&#26356;&#22909;&#22320;&#29992;&#20110;&#39044;&#22788;&#29702;&#21644;&#19982;&#21407;&#22987;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#30452;&#26041;&#22270;&#21305;&#37197;&#36716;&#25442;&#39069;&#22806;&#25968;&#25454;&#30340;&#25928;&#26524;&#20248;&#20110;&#31616;&#21333;&#30340;&#24402;&#19968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02001v2 Announce Type: replace-cross  Abstract: Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2309.00770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#65292;&#36880;&#28176;&#34701;&#20837;&#35302;&#21450;&#25105;&#20204;&#31038;&#20132;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#12289;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#12289;&#24418;&#24335;&#21270;&#21644;&#25193;&#23637;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#20260;&#23475;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#20960;&#20010;&#23454;&#29616;LLMs&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#30452;&#35266;&#30340;&#20998;&#31867;&#20307;&#31995;&#32479;&#19968;&#20102;&#25991;&#29486;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#20559;&#35265;&#35780;&#20272;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21363;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32531;&#35299;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#31227;&#21160;AI&#29983;&#24577;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#20316;&#31649;&#29702;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;NPU&#20869;&#37096;&#25918;&#32622;&#19981;&#21463;&#24212;&#29992;&#25110;&#25805;&#20316;&#31995;&#32479;&#20462;&#35746;&#24433;&#21709;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#24212;&#29992;&#36129;&#29486;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#20026;&#24191;&#27867;&#31227;&#21160;AI&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2308.14363</link><description>&lt;p&gt;
&#22312;LLM&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#31227;&#21160;AI&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Rethinking Mobile AI Ecosystem in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14363
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31227;&#21160;AI&#29983;&#24577;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#20316;&#31649;&#29702;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;NPU&#20869;&#37096;&#25918;&#32622;&#19981;&#21463;&#24212;&#29992;&#25110;&#25805;&#20316;&#31995;&#32479;&#20462;&#35746;&#24433;&#21709;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#24212;&#29992;&#36129;&#29486;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#20026;&#24191;&#27867;&#31227;&#21160;AI&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20170;&#22825;&#30340;&#32972;&#26223;&#19979;&#65292;&#26234;&#33021;&#25163;&#26426;&#24050;&#32463;&#28436;&#21464;&#25104;&#20102;&#25176;&#31649;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20013;&#24515;&#65292;&#26088;&#22312;&#36827;&#34892;&#26412;&#22320;&#25191;&#34892;&#12290;&#25512;&#21160;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#24847;&#35782;&#26159;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#20998;&#25955;&#24615;&#65292;&#20854;&#29305;&#28857;&#26159;&#19981;&#21516;&#30340;&#26550;&#26500;&#12289;&#36816;&#31639;&#31526;&#21644;&#23454;&#29616;&#12290;&#36825;&#31181;&#20998;&#25955;&#24615;&#32473;&#30828;&#20214;&#12289;&#31995;&#32479;&#35774;&#32622;&#21644;&#31639;&#27861;&#30340;&#20840;&#38754;&#20248;&#21270;&#24102;&#26469;&#20102;&#37325;&#22823;&#36127;&#25285;&#12290;&#22312;&#26368;&#36817;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#30340;&#25512;&#21160;&#19979;&#65292;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31227;&#21160;AI&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65306;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#21327;&#20316;&#31649;&#29702;&#26041;&#27861;&#65292;&#30417;&#30563;&#20855;&#26377;&#20026;&#24191;&#27867;&#31227;&#21160;AI&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21363;&#20351;&#36824;&#19981;&#33021;&#20026;&#25152;&#26377;&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;&#36825;&#20010;&#22522;&#30784;&#27169;&#22411;&#39547;&#30041;&#22312;NPU&#20869;&#37096;&#65292;&#31867;&#20284;&#20110;&#22266;&#20214;&#65292;&#19981;&#21463;&#24212;&#29992;&#25110;&#25805;&#20316;&#31995;&#32479;&#30340;&#20462;&#35746;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#37117;&#20250;&#36129;&#29486;&#19968;&#20010;&#31616;&#27905;&#30340;&#12289;&#31163;&#32447;&#24494;&#35843;&#30340;&#8220;&#36866;&#37197;&#22120;&#8221;&#65292;&#29992;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14363v2 Announce Type: replace  Abstract: In today's landscape, smartphones have evolved into hubs for hosting a multitude of deep learning models aimed at local execution. A key realization driving this work is the notable fragmentation among these models, characterized by varied architectures, operators, and implementations. This fragmentation imposes a significant burden on the comprehensive optimization of hardware, system settings, and algorithms.   Buoyed by the recent strides in large foundation models, this work introduces a pioneering paradigm for mobile AI: a collaborative management approach between the mobile OS and hardware, overseeing a foundational model capable of serving a broad spectrum of mobile AI tasks, if not all. This foundational model resides within the NPU and remains impervious to app or OS revisions, akin to firmware. Concurrently, each app contributes a concise, offline fine-tuned "adapter" tailored to distinct downstream tasks. From this concept
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2307.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Defending Against Malicious Behaviors in Federated Learning with Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#32852;&#37030;&#23398;&#20064;(FL)&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#23478;&#26426;&#26500;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20381;&#36182;&#20110;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#65292;&#23548;&#33268;&#21333;&#28857;&#25925;&#38556;&#12290;&#36825;&#20351;&#31995;&#32479;&#22312;&#22788;&#29702;&#19981;&#35802;&#23454;&#30340;&#23458;&#25143;&#26102;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21487;&#38752;FL&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32467;&#21512;&#20102;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#30001;&#38142;&#19978;&#26234;&#33021;&#21512;&#32422;&#25552;&#20379;&#21160;&#21147;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#24694;&#24847;&#23458;&#25143;&#26159;&#24378;&#22823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2306.08543</link><description>&lt;p&gt;
MiniLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniLLM: Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;KD&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#30333;&#30418;&#20998;&#31867;&#27169;&#22411;&#25110;&#35757;&#32451;&#23567;&#27169;&#22411;&#26469;&#27169;&#20223;&#22914;ChatGPT&#20043;&#31867;&#30340;&#40657;&#30418;&#27169;&#22411;API&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#30333;&#30418;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#23567;&#27169;&#22411;&#20013;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38543;&#30528;&#24320;&#28304;LLMs&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;KD&#26041;&#27861;&#65292;&#23558;LLMs&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#24378;&#35843;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21160;&#24577;&#24615;&#65292;&#25552;&#20986;&#20845;&#39033;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#25351;&#26126;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2211.14611</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Principles of Data-Centric AI (DCAI)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.14611
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#24378;&#35843;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21160;&#24577;&#24615;&#65292;&#25552;&#20986;&#20845;&#39033;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#25351;&#26126;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#65292;&#20197;&#25968;&#25454;&#36136;&#37327;&#20026;&#20195;&#20215;&#12290;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#24433;&#21709;&#20102;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#19979;&#28216;&#37096;&#32626;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#36890;&#36807;&#36845;&#20195;&#21644;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#23558;&#25968;&#25454;&#12289;&#20854;&#36136;&#37327;&#21644;&#21160;&#24577;&#24615;&#32622;&#20110;AI&#31995;&#32479;&#32771;&#34385;&#30340;&#21069;&#27839;&#12290;&#20316;&#20026;&#39318;&#27425;&#27010;&#36848;&#20043;&#19968;&#65292;&#26412;&#25991;&#27719;&#38598;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;&#35270;&#35282;&#21644;&#27010;&#24565;&#65292;&#21246;&#21202;&#20102;DCAI&#30340;&#22522;&#30784;&#12290;&#23427;&#29305;&#21035;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#21046;&#23450;&#20102;&#20845;&#39033;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#20026;DCAI&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.14611v2 Announce Type: replace-cross  Abstract: Data is a crucial infrastructure to how artificial intelligence (AI) systems learn. However, these systems to date have been largely model-centric, putting a premium on the model at the expense of the data quality. Data quality issues beset the performance of AI systems, particularly in downstream deployments and in real-world applications. Data-centric AI (DCAI) as an emerging concept brings data, its quality and its dynamism to the forefront in considerations of AI systems through an iterative and systematic approach. As one of the first overviews, this article brings together data-centric perspectives and concepts to outline the foundations of DCAI. It specifically formulates six guiding principles for researchers and practitioners and gives direction for future advancement of DCAI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340;Consprompt&#32467;&#21512;&#20102;&#25552;&#31034;&#32534;&#30721;&#32593;&#32476;&#12289;&#23545;&#27604;&#37319;&#26679;&#27169;&#22359;&#21644;&#23545;&#27604;&#35780;&#20998;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24046;&#24322;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#23454;&#20102;&#21033;&#29992;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.04118</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#26679;&#26412;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340;Consprompt&#32467;&#21512;&#20102;&#25552;&#31034;&#32534;&#30721;&#32593;&#32476;&#12289;&#23545;&#27604;&#37319;&#26679;&#27169;&#22359;&#21644;&#23545;&#27604;&#35780;&#20998;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24046;&#24322;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#23454;&#20102;&#21033;&#29992;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#25104;&#20026;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#35821;&#35328;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;&#25552;&#31034;&#35774;&#35745;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#24635;&#26159;&#23548;&#33268;&#32467;&#26524;&#24046;&#24322;&#24456;&#22823;&#65292;&#24182;&#19988;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20063;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#26377;&#38480;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#21512;&#36866;&#30340;&#23545;&#27604;&#26679;&#26412;&#21644;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#25552;&#31034;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#25552;&#20986;&#30340;Consprompt&#19982;&#25552;&#31034;&#32534;&#30721;&#32593;&#32476;&#12289;&#23545;&#27604;&#37319;&#26679;&#27169;&#22359;&#21644;&#23545;&#27604;&#35780;&#20998;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24046;&#24322;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#19981;&#21516;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#28040;&#34701;&#23454;&#39564;&#20063;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04118v3 Announce Type: replace-cross  Abstract: The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.
&lt;/p&gt;</description></item><item><title>WaveNets&#21033;&#29992;&#23567;&#27874;&#21464;&#25442;&#21387;&#32553;&#20316;&#20026;&#36890;&#36947;&#34920;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21517;&#20026;WaveNet&#12290;</title><link>https://arxiv.org/abs/2211.02695</link><description>&lt;p&gt;
WaveNets&#65306;&#23567;&#27874;&#36890;&#36947;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
WaveNets: Wavelet Channel Attention Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.02695
&lt;/p&gt;
&lt;p&gt;
WaveNets&#21033;&#29992;&#23567;&#27874;&#21464;&#25442;&#21387;&#32553;&#20316;&#20026;&#36890;&#36947;&#34920;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21517;&#20026;WaveNet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#27880;&#24847;&#21147;&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#33267;&#39640;&#26080;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SENet&#25552;&#20986;&#30340;&#36890;&#36947;&#27880;&#24847;&#21147;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#23384;&#22312;&#20449;&#24687;&#25439;&#22833;&#65292;&#23548;&#33268;&#20351;&#29992;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#34920;&#31034;&#36890;&#36947;&#20026;&#26631;&#37327;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#26377;&#25928;&#30340;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#38656;&#35201;&#25214;&#21040;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#22312;&#24314;&#27169;&#36890;&#36947;&#30456;&#20114;&#20381;&#36182;&#24615;&#20013;&#30340;&#20445;&#30041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23567;&#27874;&#21464;&#25442;&#21387;&#32553;&#20316;&#20026;&#35299;&#20915;&#36890;&#36947;&#34920;&#31034;&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;&#23567;&#27874;&#21464;&#25442;&#20316;&#20026;&#37197;&#22791;&#20256;&#32479;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#27979;&#35797;&#23567;&#27874;&#21464;&#25442;&#20316;&#20026;&#29420;&#31435;&#30340;&#36890;&#36947;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#31561;&#25928;&#20110;&#36882;&#24402;&#36817;&#20284;&#30340;Haar&#23567;&#27874;&#21464;&#25442;&#12290;&#26377;&#20102;&#36825;&#20010;&#35777;&#26126;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#20351;&#29992;&#23567;&#27874;&#21387;&#32553;&#30340;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#23558;&#20854;&#21629;&#21517;&#20026;WaveNet&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.02695v2 Announce Type: replace-cross  Abstract: Channel Attention reigns supreme as an effective technique in the field of computer vision. However, the proposed channel attention by SENet suffers from information loss in feature learning caused by the use of Global Average Pooling (GAP) to represent channels as scalars. Thus, designing effective channel attention mechanisms requires finding a solution to enhance features preservation in modeling channel inter-dependencies. In this work, we utilize Wavelet transform compression as a solution to the channel representation problem. We first test wavelet transform as an Auto-Encoder model equipped with conventional channel attention module. Next, we test wavelet transform as a standalone channel compression method. We prove that global average pooling is equivalent to the recursive approximate Haar wavelet transform. With this proof, we generalize channel attention using Wavelet compression and name it WaveNet. Implementation o
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#23481;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#22823;&#37327;&#35745;&#31639;&#24320;&#38144;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#25512;&#24191;&#12290;</title><link>https://arxiv.org/abs/2210.09292</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Diffusion Models for Vision: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.09292
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#23481;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#22823;&#37327;&#35745;&#31639;&#24320;&#38144;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20004;&#27493;&#35757;&#32451;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#36880;&#28176;&#21521;&#25968;&#25454;&#65288;&#36890;&#24120;&#26159;&#22270;&#20687;&#65289;&#28155;&#21152;&#22122;&#22768;&#12290;&#28982;&#21518;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#36880;&#28176;&#21435;&#38500;&#22122;&#22768;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#34987;&#24314;&#27169;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;DMs&#21463;&#38750;&#24179;&#34913;&#28909;&#21147;&#23398;&#21551;&#21457;&#65292;&#24182;&#20855;&#26377;&#22825;&#28982;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#39057;&#32321;&#36827;&#34892;&#20989;&#25968;&#35780;&#20272;&#21644;&#26799;&#24230;&#35745;&#31639;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20250;&#20135;&#29983;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#36825;&#19981;&#20165;&#21487;&#33021;&#38459;&#30861;&#22522;&#20110;&#25193;&#25955;&#30340;&#24314;&#27169;&#30340;&#27665;&#20027;&#21270;&#65292;&#20063;&#20250;&#38459;&#30861;&#25193;&#25955;&#27169;&#22411;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#26356;&#19981;&#29992;&#35828;&#65292;&#35745;&#31639;&#27169;&#22411;&#30340;&#25928;&#29575;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#19968;&#31181;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.09292v3 Announce Type: replace-cross  Abstract: Diffusion Models (DMs) have demonstrated state-of-the-art performance in content generation without requiring adversarial training. These models are trained using a two-step process. First, a forward - diffusion - process gradually adds noise to a datum (usually an image). Then, a backward - reverse diffusion - process gradually removes the noise to turn it into a sample of the target distribution being modelled. DMs are inspired by non-equilibrium thermodynamics and have inherent high computational complexity. Due to the frequent function evaluations and gradient calculations in high-dimensional spaces, these models incur considerable computational overhead during both training and inference stages. This can not only preclude the democratization of diffusion-based modelling, but also hinder the adaption of diffusion models in real-life applications. Not to mention, the efficiency of computational models is fast becoming a sign
&lt;/p&gt;</description></item><item><title>SATformer&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#19981;&#21487;&#28385;&#36275;&#24615;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#24335;&#65292;&#20197;&#35782;&#21035;&#19981;&#21487;&#28385;&#36275;&#30340;&#23376;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#20248;&#20110;NeuroSAT&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.00953</link><description>&lt;p&gt;
SATformer: &#22522;&#20110;Transformer&#30340;UNSAT&#26680;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SATformer: Transformer-Based UNSAT Core Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00953
&lt;/p&gt;
&lt;p&gt;
SATformer&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#19981;&#21487;&#28385;&#36275;&#24615;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#24335;&#65292;&#20197;&#35782;&#21035;&#19981;&#21487;&#28385;&#36275;&#30340;&#23376;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#20248;&#20110;NeuroSAT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SATformer&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#30340;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290; SATformer&#24182;&#38750;&#30452;&#25509;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#26159;&#20174;&#30456;&#21453;&#30340;&#26041;&#21521;&#20837;&#25163;&#65292;&#30528;&#37325;&#20110;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#23376;&#21477;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#35782;&#21035;&#20219;&#20309;&#19981;&#21487;&#28385;&#36275;&#30340;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#23376;&#21477;&#36716;&#25442;&#20026;&#23376;&#21477;&#23884;&#20837;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;Transformer&#27169;&#22411;&#26469;&#29702;&#35299;&#23376;&#21477;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; SATformer&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#21333;&#27604;&#29305;&#21487;&#28385;&#36275;&#24615;&#32467;&#26524;&#20197;&#21450;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#26680;&#24515;&#65288;MUC&#65289;&#20316;&#20026;&#23376;&#21477;&#30417;&#30563;&#26469;&#22788;&#29702;UNSAT&#38382;&#39064;&#12290;&#20316;&#20026;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#21487;&#28385;&#36275;&#24615;&#20998;&#31867;&#22120;&#65292;SATformer&#30340;&#24615;&#33021;&#26174;&#33879;&#36229;&#36234;&#20102;NeuroSAT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;SATformer&#20570;&#20986;&#30340;&#23376;&#21477;&#39044;&#27979;&#38598;&#25104;&#21040;&#29616;&#20195;&#21551;&#21457;&#24335;SAT&#27714;&#35299;&#22120;&#20013;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00953v2 Announce Type: replace  Abstract: This paper introduces SATformer, a novel Transformer-based approach for the Boolean Satisfiability (SAT) problem. Rather than solving the problem directly, SATformer approaches the problem from the opposite direction by focusing on unsatisfiability. Specifically, it models clause interactions to identify any unsatisfiable sub-problems. Using a graph neural network, we convert clauses into clause embeddings and employ a hierarchical Transformer-based model to understand clause correlation. SATformer is trained through a multi-task learning approach, using the single-bit satisfiability result and the minimal unsatisfiable core (MUC) for UNSAT problems as clause supervision. As an end-to-end learning-based satisfiability classifier, the performance of SATformer surpasses that of NeuroSAT significantly. Furthermore, we integrate the clause predictions made by SATformer into modern heuristic-based SAT solvers and validate our approach wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;LIC&#35299;&#37322;&#20026;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#21495;&#35843;&#21046;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#21464;&#25442;&#26041;&#27861;&#30340;&#25968;&#23398;&#31616;&#21270;&#21644;&#25193;&#23637;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2203.02158</link><description>&lt;p&gt;
&#20174;&#35843;&#21046;&#30340;&#35282;&#24230;&#25506;&#35752;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Transformations in Learned Image Compression from a Modulation Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.02158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;LIC&#35299;&#37322;&#20026;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#21495;&#35843;&#21046;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#21464;&#25442;&#26041;&#27861;&#30340;&#25968;&#23398;&#31616;&#21270;&#21644;&#25193;&#23637;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#35843;&#21046;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;(LIC)&#21464;&#25442;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;LIC&#20013;&#30340;&#37327;&#21270;&#35270;&#20026;&#24102;&#26377;&#21152;&#24615;&#22343;&#21248;&#22122;&#22768;&#30340;&#24191;&#20041;&#20449;&#36947;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#32467;&#26500;&#21644;&#20248;&#21270;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#65292;&#23558;LIC&#35299;&#37322;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#24212;&#29992;&#36890;&#20449;&#31995;&#32479;&#25216;&#26415;&#26469;&#25351;&#23548;LIC&#20013;&#27169;&#22359;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#35843;&#21046;&#30340;&#32479;&#19968;&#21464;&#25442;&#26041;&#27861;&#65288;TSM&#65289;&#12290;&#20174;TSM&#30340;&#35266;&#28857;&#26469;&#30475;&#65292;&#29616;&#26377;&#30340;&#21464;&#25442;&#26041;&#27861;&#22312;&#25968;&#23398;&#19978;&#34987;&#31616;&#21270;&#20026;&#32447;&#24615;&#35843;&#21046;&#12290;&#36890;&#36807;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#35843;&#21046;&#65292;&#21487;&#20197;&#24471;&#21040;&#19968;&#31995;&#21015;&#21464;&#25442;&#26041;&#27861;&#65292;&#22914;TPM&#21644;TJM&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39592;&#24178;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.02158v3 Announce Type: replace-cross  Abstract: In this paper, a unified transformation method in learned image compression(LIC) is proposed from the perspective of modulation. Firstly, the quantization in LIC is considered as a generalized channel with additive uniform noise. Moreover, the LIC is interpreted as a particular communication system according to the consistency in structures and optimization objectives. Thus, the technology of communication systems can be applied to guide the design of modules in LIC. Furthermore, a unified transform method based on signal modulation (TSM) is defined. In the view of TSM, the existing transformation methods are mathematically reduced to a linear modulation. A series of transformation methods, e.g. TPM and TJM, are obtained by extending to nonlinear modulation. The experimental results on various datasets and backbone architectures verify that the effectiveness and robustness of the proposed method. More importantly, it further co
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#36924;&#36817;&#35889;&#22270;&#21367;&#31215;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;ChebNet&#24615;&#33021;&#36739;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#23398;&#20064;&#21040;&#30340;&#38750;&#27861;&#31995;&#25968;&#36817;&#20284;&#35299;&#26512;&#28388;&#27874;&#22120;&#20989;&#25968;</title><link>https://arxiv.org/abs/2202.03580</link><description>&lt;p&gt;
&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#36924;&#36817;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#37325;&#26032;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03580
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#36924;&#36817;&#35889;&#22270;&#21367;&#31215;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;ChebNet&#24615;&#33021;&#36739;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#23398;&#20064;&#21040;&#30340;&#38750;&#27861;&#31995;&#25968;&#36817;&#20284;&#35299;&#26512;&#28388;&#27874;&#22120;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#35889;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;ChebNet&#26159;&#26089;&#26399;&#23581;&#35797;&#20043;&#19968;&#65292;&#23427;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#36817;&#20284;&#35889;&#22270;&#21367;&#31215;&#12290;GCN&#31616;&#21270;&#20102;ChebNet&#65292;&#20165;&#21033;&#29992;&#21069;&#20004;&#20010;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#65292;&#21516;&#26102;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#20248;&#20110;&#20854;&#12290;GPR-GNN&#21644;BernNet&#34920;&#26126;&#65292;&#21333;&#39033;&#24335;&#21644;&#20271;&#24681;&#26031;&#22374;&#22522;&#20063;&#22312;&#23398;&#20064;&#35889;&#22270;&#21367;&#31215;&#26041;&#38754;&#20248;&#20110;&#20999;&#27604;&#38634;&#22827;&#22522;&#12290;&#36825;&#26679;&#30340;&#32467;&#35770;&#22312;&#36924;&#36817;&#29702;&#35770;&#39046;&#22495;&#26159;&#21453;&#30452;&#35273;&#30340;&#65292;&#36924;&#36817;&#20989;&#25968;&#26102;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.03580v5 Announce Type: replace-cross  Abstract: Designing spectral convolutional networks is a challenging problem in graph learning. ChebNet, one of the early attempts, approximates the spectral graph convolutions using Chebyshev polynomials. GCN simplifies ChebNet by utilizing only the first two Chebyshev polynomials while still outperforming it on real-world datasets. GPR-GNN and BernNet demonstrate that the Monomial and Bernstein bases also outperform the Chebyshev basis in terms of learning the spectral graph convolutions. Such conclusions are counter-intuitive in the field of approximation theory, where it is established that the Chebyshev polynomial achieves the optimum convergent rate for approximating a function.   In this paper, we revisit the problem of approximating the spectral graph convolutions with Chebyshev polynomials. We show that ChebNet's inferior performance is primarily due to illegal coefficients learnt by ChebNet approximating analytic filter functio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#19968;&#20010;&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#36879;&#26126;&#24230;&#22686;&#21152;&#30340;&#26041;&#27861;&#35770;&#65292;&#39318;&#27425;&#23558;&#35813;&#29992;&#25143;&#20013;&#24515;&#26041;&#27861;&#35770;&#24212;&#29992;&#20110;&#23454;&#36341;&#65292;&#25253;&#21578;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22242;&#38431;&#30340;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2201.13224</link><description>&lt;p&gt;
&#35780;&#20272;&#22686;&#21152;&#20154;&#24037;&#26234;&#33021;&#36879;&#26126;&#24230;&#30340;&#26041;&#27861;&#35770;&#65306;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating a Methodology for Increasing AI Transparency: A Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.13224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#19968;&#20010;&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#36879;&#26126;&#24230;&#22686;&#21152;&#30340;&#26041;&#27861;&#35770;&#65292;&#39318;&#27425;&#23558;&#35813;&#29992;&#25143;&#20013;&#24515;&#26041;&#27861;&#35770;&#24212;&#29992;&#20110;&#23454;&#36341;&#65292;&#25253;&#21578;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22242;&#38431;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#28508;&#22312;&#21361;&#23475;&#30340;&#22686;&#38271;&#20851;&#20999;&#65292;&#31038;&#20250;&#24050;&#24320;&#22987;&#35201;&#27714;&#26356;&#22810;&#20851;&#20110;AI&#27169;&#22411;&#21644;&#31995;&#32479;&#21019;&#24314;&#21644;&#20351;&#29992;&#26041;&#24335;&#30340;&#36879;&#26126;&#24230;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#20123;&#21162;&#21147;&#25552;&#20986;&#20102;&#21253;&#21547;&#27169;&#22411;&#24320;&#21457;&#32773;&#38656;&#35201;&#22238;&#31572;&#30340;&#38382;&#39064;&#30340;&#25991;&#26723;&#27169;&#26495;&#12290;&#36825;&#20123;&#27169;&#26495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#36215;&#28857;&#65292;&#20294;&#26159;&#27809;&#26377;&#21333;&#19968;&#27169;&#26495;&#21487;&#20197;&#28085;&#30422;&#21508;&#31181;&#25991;&#26723;&#20351;&#29992;&#32773;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21407;&#21017;&#19978;&#21487;&#20197;&#21019;&#24314;&#19968;&#31181;&#21487;&#37325;&#22797;&#30340;&#26041;&#27861;&#35770;&#26469;&#29983;&#25104;&#30495;&#27491;&#26377;&#29992;&#30340;&#25991;&#26723;&#12290;Richards&#31561;&#20154;[25]&#25552;&#20986;&#20102;&#36825;&#26679;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#20855;&#20307;&#30340;&#25991;&#26723;&#38656;&#27714;&#65292;&#24182;&#21019;&#24314;&#27169;&#26495;&#26469;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#34429;&#28982;&#36825;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25552;&#35758;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#35780;&#20272;&#12290; &#26412;&#25991;&#39318;&#27425;&#22312;&#23454;&#36341;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#25253;&#21578;&#20102;&#19968;&#20010;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22242;&#38431;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.13224v2 Announce Type: replace-cross  Abstract: In reaction to growing concerns about the potential harms of artificial intelligence (AI), societies have begun to demand more transparency about how AI models and systems are created and used. To address these concerns, several efforts have proposed documentation templates containing questions to be answered by model developers. These templates provide a useful starting point, but no single template can cover the needs of diverse documentation consumers. It is possible in principle, however, to create a repeatable methodology to generate truly useful documentation. Richards et al. [25] proposed such a methodology for identifying specific documentation needs and creating templates to address those needs. Although this is a promising proposal, it has not been evaluated.   This paper presents the first evaluation of this user-centered methodology in practice, reporting on the experiences of a team in the domain of AI for healthca
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Outcome Adaptive Elastic Net&#65288;OAENet&#65289;&#30340;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2111.13800</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22312;&#39640;&#32500;&#35266;&#23519;&#25968;&#25454;&#20013;&#31283;&#20581;&#35780;&#20272;&#27835;&#30103;&#25928;&#26524;&#30340;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.13800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Outcome Adaptive Elastic Net&#65288;OAENet&#65289;&#30340;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#34987;&#35748;&#20026;&#26159;&#35780;&#20272;&#20219;&#20309;&#24178;&#39044;&#25110;&#27835;&#30103;&#25928;&#26524;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20262;&#29702;&#12289;&#32463;&#27982;&#21644;&#27861;&#24459;&#32771;&#34385;&#65292;&#20854;&#21487;&#34892;&#24615;&#32463;&#24120;&#21463;&#38459;&#65292;&#20351;&#24471;&#35266;&#23519;&#25968;&#25454;&#25104;&#20026;&#32472;&#21046;&#22240;&#26524;&#32467;&#35770;&#30340;&#23453;&#36149;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#30103;&#35266;&#23519;&#25968;&#25454;&#20855;&#26377;&#39640;&#32500;&#24615;&#65292;&#36825;&#24102;&#26469;&#20102;&#22256;&#38590;&#25361;&#25112;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#20197;&#30830;&#20445;&#26080;&#20559;&#12289;&#21487;&#38752;&#21644;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Outcome Adaptive Elastic Net&#65288;OAENet&#65289;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20351;&#29992;&#21305;&#37197;&#25216;&#26415;&#20570;&#20986;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#20915;&#31574;&#12290;OAENet&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#22312;&#30456;&#20851;&#21644;&#39640;&#32500;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36873;&#25321;&#29305;&#23450;&#21464;&#37327;&#38598;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.13800v2 Announce Type: replace-cross  Abstract: A Randomized Control Trial (RCT) is considered as the gold standard for evaluating the effect of any intervention or treatment. However, its feasibility is often hindered by ethical, economical, and legal considerations, making observational data a valuable alternative for drawing causal conclusions. Nevertheless, healthcare observational data presents a difficult challenge due to its high dimensionality, requiring careful consideration to ensure unbiased, reliable, and robust causal inferences. To overcome this challenge, in this study, we propose a novel two-stage feature selection technique called, Outcome Adaptive Elastic Net (OAENet), explicitly designed for making robust causal inference decisions using matching techniques. OAENet offers several key advantages over existing methods: superior performance on correlated and high-dimensional data compared to the existing methods and the ability to select specific sets of vari
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32534;&#30721;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;DNA&#23384;&#20648;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#31283;&#20581;&#30340;DNA&#23384;&#20648;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#39640;3200&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;40%&#12290;</title><link>https://arxiv.org/abs/2109.00031</link><description>&lt;p&gt;
&#28145;&#24230;DNA&#23384;&#20648;&#65306;&#22522;&#20110;&#32534;&#30721;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#21644;&#31283;&#20581;DNA&#23384;&#20648;
&lt;/p&gt;
&lt;p&gt;
Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.00031
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32534;&#30721;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;DNA&#23384;&#20648;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#31283;&#20581;&#30340;DNA&#23384;&#20648;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#39640;3200&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#23384;&#20648;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#25968;&#23383;&#20449;&#24687;&#23384;&#26723;&#22312;DNA&#20998;&#23376;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#30913;&#24615;&#21644;&#20809;&#23398;&#23384;&#20648;&#26041;&#26696;&#20855;&#26377;&#20027;&#35201;&#20248;&#21183;&#65292;&#22914;&#20986;&#33394;&#30340;&#20449;&#24687;&#23494;&#24230;&#12289;&#22686;&#24378;&#30340;&#25968;&#25454;&#32784;&#20037;&#24615;&#20197;&#21450;&#32500;&#25345;&#25968;&#25454;&#23436;&#25972;&#24615;&#25152;&#38656;&#30340;&#21487;&#24573;&#30053;&#21151;&#32791;&#12290;&#20026;&#20102;&#35775;&#38382;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#29942;&#39048;&#26159;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#21644;&#25972;&#20307;&#24615;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#12289;&#22522;&#20110;&#24352;&#37327;&#31215;&#65288;TP&#65289;&#30340;&#32416;&#38169;&#30721;&#65288;ECC&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#23433;&#20840;&#35029;&#24230;&#26426;&#21046;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27969;&#27700;&#32447;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#27979;&#24207;&#25216;&#26415;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;3.1MB &#20449;&#24687;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#36895;&#24230;&#19978;&#27604;&#24403;&#21069;&#39046;&#20808;&#35299;&#20915;&#26041;&#26696;&#25552;&#39640;&#20102;&#26368;&#39640;3200&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.00031v3 Announce Type: replace-cross  Abstract: DNA-based storage is an emerging technology that enables digital information to be archived in DNA molecules. This method enjoys major advantages over magnetic and optical storage solutions such as exceptional information density, enhanced data durability, and negligible power consumption to maintain data integrity. To access the data, an information retrieval process is employed, where some of the main bottlenecks are the scalability and accuracy, which have a natural tradeoff between the two. Here we show a modular and holistic approach that combines Deep Neural Networks (DNN) trained on simulated data, Tensor-Product (TP) based Error-Correcting Codes (ECC), and a safety margin mechanism into a single coherent pipeline. We demonstrated our solution on 3.1MB of information using two different sequencing technologies. Our work improves upon the current leading solutions by up to x3200 increase in speed, 40% improvement in accur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#23618;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2108.00408</link><description>&lt;p&gt;
CSC-Unet&#65306;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#31574;&#30053;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural Network for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.00408
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#23618;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#30495;&#23454;&#22270;&#20687;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;&#35768;&#22810;&#22522;&#20110;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#25429;&#25417;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#22806;&#35266;&#20449;&#24687;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23558;&#24120;&#29992;&#30340;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#23618;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#22359;&#65292;&#20197;&#32531;&#35299;&#21069;&#36848;&#32570;&#38519;&#12290;&#35813;&#31574;&#30053;&#21487;&#33021;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#20219;&#20309;&#28041;&#21450;&#21367;&#31215;&#25805;&#20316;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#24819;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;U-Net&#27169;&#22411;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#22522;&#20110;U-Net&#35774;&#35745;&#20102;CSC-Unet&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20449;&#30340;&#35777;&#25454;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.00408v2 Announce Type: replace-cross  Abstract: It is a challenging task to accurately perform semantic segmentation due to the complexity of real picture scenes. Many semantic segmentation methods based on traditional deep learning insufficiently captured the semantic and appearance information of images, which put limit on their generality and robustness for various application scenes. In this paper, we proposed a novel strategy that reformulated the popularly-used convolution operation to multi-layer convolutional sparse coding block to ease the aforementioned deficiency. This strategy can be possibly used to significantly improve the segmentation performance of any semantic segmentation model that involves convolutional operations. To prove the effectiveness of our idea, we chose the widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet model series based on U-Net. Through extensive analysis and experiments, we provided credible evidence showing
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27491;&#24120;&#20107;&#25925;&#29702;&#35770;&#12289;&#39640;&#21487;&#38752;&#24615;&#29702;&#35770;&#21644;&#24320;&#25918;&#31995;&#32479;&#29702;&#35770;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#39118;&#38505;&#65292;&#24378;&#35843;&#20102;&#20851;&#27880;&#24403;&#21069;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2104.12582</link><description>&lt;p&gt;
&#29702;&#35299;&#24182;&#36991;&#20813;&#20154;&#24037;&#26234;&#33021;&#22833;&#36133;&#65306;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Understanding and Avoiding AI Failures: A Practical Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.12582
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27491;&#24120;&#20107;&#25925;&#29702;&#35770;&#12289;&#39640;&#21487;&#38752;&#24615;&#29702;&#35770;&#21644;&#24320;&#25918;&#31995;&#32479;&#29702;&#35770;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#39118;&#38505;&#65292;&#24378;&#35843;&#20102;&#20851;&#27880;&#24403;&#21069;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#33021;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#19978;&#30340;&#22686;&#21152;&#65292;&#20154;&#24037;&#26234;&#33021;&#20107;&#25925;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22522;&#20110;&#27491;&#24120;&#20107;&#25925;&#29702;&#35770;&#12289;&#39640;&#21487;&#38752;&#24615;&#29702;&#35770;&#21644;&#24320;&#25918;&#31995;&#32479;&#29702;&#35770;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#19982;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21407;&#21017;&#26469;&#37327;&#21270;&#22686;&#21152;&#26234;&#33021;&#21644;&#20154;&#31867;&#29305;&#36136;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24102;&#26469;&#30340;&#29420;&#29305;&#39118;&#38505;&#12290;&#20004;&#20010;&#39046;&#22495;&#20849;&#21516;&#25551;&#32472;&#20102;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#26356;&#23436;&#25972;&#30340;&#22270;&#26223;&#12290;&#36890;&#36807;&#32858;&#28966;&#20107;&#25925;&#38468;&#36817;&#30340;&#31995;&#32479;&#23646;&#24615;&#32780;&#19981;&#26159;&#23547;&#25214;&#20107;&#25925;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24212;&#35813;&#20851;&#27880;&#24403;&#21069;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.12582v4 Announce Type: replace-cross  Abstract: As AI technologies increase in capability and ubiquity, AI accidents are becoming more common. Based on normal accident theory, high reliability theory, and open systems theory, we create a framework for understanding the risks associated with AI applications. In addition, we also use AI safety principles to quantify the unique risks of increased intelligence and human-like qualities in AI. Together, these two fields give a more complete picture of the risks of contemporary AI. By focusing on system properties near accidents instead of seeking a root cause of accidents, we identify where attention should be paid to safety for current generation AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>CivRealm&#26159;&#19968;&#20010;&#21463;&#25991;&#26126;&#28216;&#25103;&#21551;&#21457;&#30340;&#29615;&#22659;&#65292;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#22797;&#26434;&#30340;&#24773;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#20197;&#24212;&#23545;&#21464;&#21270;&#30340;&#28216;&#25103;&#35268;&#21017;&#21644;&#38543;&#26426;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2401.10568</link><description>&lt;p&gt;
CivRealm: &#19968;&#20010;&#22312;&#25991;&#26126;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#19982;&#25512;&#29702;&#22855;&#24187;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents. (arXiv:2401.10568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10568
&lt;/p&gt;
&lt;p&gt;
CivRealm&#26159;&#19968;&#20010;&#21463;&#25991;&#26126;&#28216;&#25103;&#21551;&#21457;&#30340;&#29615;&#22659;&#65292;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#22797;&#26434;&#30340;&#24773;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#20197;&#24212;&#23545;&#21464;&#21270;&#30340;&#28216;&#25103;&#35268;&#21017;&#21644;&#38543;&#26426;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#20195;&#29702;&#30340;&#27867;&#21270;&#21253;&#25324;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#65306;&#20174;&#36807;&#21435;&#32463;&#39564;&#20013;&#23398;&#20064;&#21644;&#22312;&#26032;&#24773;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#20132;&#20114;&#29615;&#22659;&#20013;&#65292;&#23545;&#23398;&#20064;&#30340;&#37325;&#35270;&#24448;&#24448;&#20197;&#29306;&#29298;&#25512;&#29702;&#22797;&#26434;&#24615;&#20026;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CivRealm&#65292;&#19968;&#20010;&#21463;&#21040;&#25991;&#26126;&#28216;&#25103;&#21551;&#21457;&#30340;&#29615;&#22659;&#12290;&#25991;&#26126;&#28216;&#25103;&#19982;&#20154;&#31867;&#21382;&#21490;&#21644;&#31038;&#20250;&#30340;&#28145;&#21051;&#22865;&#21512;&#38656;&#35201;&#22797;&#26434;&#30340;&#23398;&#20064;&#65292;&#32780;&#20854;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#22659;&#21017;&#35201;&#27714;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;CivRealm&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#30528;&#19981;&#23436;&#20840;&#20449;&#24687;&#21644;&#21464;&#21270;&#20154;&#25968;&#30340;&#24635;&#21644;&#28216;&#25103;&#65307;&#23427;&#21576;&#29616;&#20102;&#20247;&#22810;&#22797;&#26434;&#30340;&#29305;&#24449;&#65292;&#25361;&#25112;&#20195;&#29702;&#20154;&#22788;&#29702;&#24320;&#25918;&#30340;&#12289;&#38543;&#26426;&#30340;&#29615;&#22659;&#65292;&#38656;&#35201;&#22806;&#20132;&#21644;&#35848;&#21028;&#25216;&#24039;&#12290;&#22312;CivRealm&#20013;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#20856;&#22411;&#30340;&#20195;&#29702;&#31867;&#22411;&#25552;&#20379;&#20102;&#25509;&#21475;&#65306;&#22522;&#20110;&#24352;&#37327;&#30340;&#23398;&#20064;&#22411;&#20195;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#25512;&#29702;&#22411;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further rese
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27604;&#36739;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#26435;&#37325;&#20272;&#35745;&#65292;&#20351;&#24471;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#28385;&#36275;&#24230;&#39640;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#22312;&#20154;&#20307;&#35797;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02099</link><description>&lt;p&gt;
&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles. (arXiv:2311.02099v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27604;&#36739;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#26435;&#37325;&#20272;&#35745;&#65292;&#20351;&#24471;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#28385;&#36275;&#24230;&#39640;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#22312;&#20154;&#20307;&#35797;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#30830;&#20445;&#31526;&#21512;&#32473;&#23450;&#35268;&#33539;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25551;&#36848;&#20132;&#36890;&#35268;&#21017;&#30340;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(STL)&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#32435;&#20837;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#21152;&#26435;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(PWSTL)&#65292;&#25105;&#20204;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#23433;&#20840;&#20445;&#35777;&#30340;&#20559;&#22909;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#32473;&#23450;PWSTL&#20844;&#24335;&#26435;&#37325;&#30340;&#21487;&#34892;&#20272;&#35745;&#65292;&#20351;&#24471;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#26102;&#65292;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#23450;&#37327;&#28385;&#36275;&#24230;&#22823;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#26435;&#37325;&#30340;&#21487;&#34892;&#20272;&#35745;&#23548;&#33268;&#20102;&#19968;&#20010;&#21152;&#26435;STL&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#30830;&#24615;&#21644;&#23450;&#21046;&#21512;&#25104;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#25311;&#39550;&#39542;&#22330;&#26223;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20307;&#35797;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a preference learning method that ensures adherence to given specifications, with an application to autonomous vehicles. Our approach incorporates the priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a learning framework. By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), we formulate the problem of safety-guaranteed preference learning based on pairwise comparisons and propose an approach to solve this learning problem. Our approach finds a feasible valuation for the weights of the given PWSTL formula such that, with these weights, preferred signals have weighted quantitative satisfaction measures greater than their non-preferred counterparts. The feasible valuation of weights given by our approach leads to a weighted STL formula that can be used in correct-and-custom-by-construction controller synthesis. We demonstrate the performance of our method with a pilot human subject study in two different simulated dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.06117</link><description>&lt;p&gt;
&#36864;&#21518;&#19968;&#27493;&#65306;&#36890;&#36807;&#25277;&#35937;&#21796;&#36215;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#36864;&#21518;&#25552;&#31034;&#8221;&#30340;&#31616;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#21253;&#21547;&#20855;&#20307;&#32454;&#33410;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#25277;&#35937;&#65292;&#24471;&#20986;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#21644;&#21407;&#29702;&#26469;&#25351;&#23548;&#25512;&#29702;&#27493;&#39588;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#25512;&#29702;&#36335;&#24452;&#19978;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;PaLM-2L&#27169;&#22411;&#36827;&#34892;&#20102;&#36864;&#21518;&#25552;&#31034;&#23454;&#39564;&#65292;&#22312;&#21253;&#25324;STEM&#12289;&#30693;&#35782;&#38382;&#31572;&#21644;&#22810;&#36339;&#25512;&#29702;&#22312;&#20869;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20363;&#22914;&#65292;&#22312;MMLU&#29289;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#65292;&#36864;&#21518;&#25552;&#31034;&#21487;&#20197;&#23558;PaLM-2L&#30340;&#24615;&#33021;&#25552;&#21319;7%&#21644;11%&#65292;&#22312;TimeQA&#20219;&#21153;&#19978;&#25552;&#21319;27%&#65292;&#22312;MuSiQue&#20219;&#21153;&#19978;&#25552;&#21319;7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
&lt;/p&gt;</description></item><item><title>ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16319</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#32452;&#21512;&#22810;&#31890;&#24230;&#34920;&#31034;&#30340;Transformer&#22686;&#24378;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16319
&lt;/p&gt;
&lt;p&gt;
ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReCAT&#30340;&#36882;&#24402;&#32452;&#21512;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#26126;&#30830;&#24314;&#27169;&#21407;&#22987;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#12290;&#29616;&#26377;&#30740;&#31350;&#38480;&#21046;&#25968;&#25454;&#36981;&#24490;&#23618;&#32423;&#26641;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#36328;&#36317;&#36890;&#20449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20869;&#22806;(CIO)&#23618;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#39030;&#21521;&#19979;&#30340;&#20256;&#36882;&#23398;&#20064;&#36328;&#24230;&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#33258;&#24213;&#21521;&#19978;&#20256;&#36882;&#36890;&#36807;&#32452;&#21512;&#20302;&#32423;&#36328;&#24230;&#24418;&#25104;&#39640;&#32423;&#36328;&#24230;&#30340;&#34920;&#31034;&#65292;&#32780;&#33258;&#39030;&#21521;&#19979;&#20256;&#36882;&#21017;&#32467;&#21512;&#20102;&#36328;&#24230;&#20869;&#37096;&#21644;&#22806;&#37096;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#20043;&#38388;&#21472;&#21152;&#22810;&#20010;CIO&#23618;&#65292;ReCAT&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#36328;&#36317;&#20869;&#37096;&#21644;&#36328;&#36317;&#38388;&#30340;&#28145;&#23618;&#20132;&#20114;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#23436;&#20840;&#19978;&#19979;&#25991;&#21270;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CIO&#23618;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained
&lt;/p&gt;</description></item><item><title>MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10691</link><description>&lt;p&gt;
MINT: &#35780;&#20272;&#22312;&#19982;&#24037;&#20855;&#21644;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10691
&lt;/p&gt;
&lt;p&gt;
MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#38656;&#35201;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#26377;&#26102;&#20505;&#36741;&#20197;&#22806;&#37096;&#24037;&#20855;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24378;&#35843;&#29992;&#21333;&#36718;&#20132;&#27969;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#12289;LLMs&#21644;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#32454;&#33268;&#20114;&#21160;&#65292;&#24182;&#20302;&#20272;&#20102;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30095;&#24573;&#23548;&#33268;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#19982;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MINT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#35780;&#20272;LLMs&#35299;&#20915;&#22810;&#36718;&#20132;&#20114;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;Python&#20195;&#30721;&#26469;&#35775;&#38382;&#24037;&#20855;&#65292;&#24182;&#25509;&#25910;&#30001;GPT-4&#27169;&#25311;&#30340;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#30340;&#24050;&#24314;&#31435;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#20915;&#31574;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13212</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#29992;&#20110;&#38271;&#26399;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20855;&#26377;&#31561;&#21464;&#24615;&#36136;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23545;&#29289;&#29702;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20182;&#20204;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#31163;&#25955;&#29366;&#24577;&#65292;&#24182;&#23398;&#20064;&#25152;&#26377;&#30456;&#37051;&#29366;&#24577;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#25509;&#26144;&#23556;&#24573;&#30053;&#20102;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#24050;&#32463;&#39564;&#35777;&#20102;&#22312;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#30452;&#25509;&#26144;&#23556;&#27169;&#22411;&#20013;&#65292;&#20004;&#20010;&#31163;&#25955;&#21160;&#24577;&#29366;&#24577;&#20043;&#38388;&#23384;&#22312;&#26080;&#25968;&#21487;&#33021;&#30340;&#36712;&#36857;&#12290;&#36825;&#20010;&#38382;&#39064;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#38271;&#26399;&#27169;&#25311;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#36890;&#36807;&#31163;&#25955;&#30417;&#30563;&#20449;&#21495;&#24314;&#27169;&#28508;&#22312;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE(PINGO)&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04522</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#26159;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#30340;&#20004;&#20010;&#30456;&#20851;&#26041;&#38754;&#12290;&#38544;&#20889;&#26415;&#26088;&#22312;&#38544;&#34255;&#36890;&#20449;&#65292;&#32780;&#38544;&#20889;&#20998;&#26512;&#21017;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#38544;&#34255;&#20449;&#24687;&#65292;&#29978;&#33267;&#23581;&#35797;&#24674;&#22797;&#20854;&#25152;&#21253;&#21547;&#30340;&#25968;&#25454;&#12290;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#21463;&#21040;&#25191;&#27861;&#37096;&#38376;&#30340;&#20851;&#27880;&#12290;&#38544;&#20889;&#26415;&#24120;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29978;&#33267;&#24656;&#24598;&#20998;&#23376;&#29992;&#26469;&#36991;&#20813;&#22312;&#25317;&#26377;&#35777;&#25454;&#26102;&#34987;&#25429;&#65292;&#21363;&#20351;&#21152;&#23494;&#20063;&#19968;&#26679;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#22269;&#23478;&#31105;&#27490;&#25110;&#38480;&#21046;&#20351;&#29992;&#23494;&#30721;&#23398;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25581;&#31034;&#38544;&#34255;&#20449;&#24687;&#30340;&#23574;&#31471;&#25216;&#26415;&#23545;&#25581;&#38706;&#38750;&#27861;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#24378;&#22823;&#21487;&#38752;&#30340;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;</title><link>http://arxiv.org/abs/2308.02409</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#22810;&#31354;&#38388;&#28145;&#24230;&#27169;&#22411;&#30340;&#33041;&#30005;&#20449;&#21495;&#26469;&#20272;&#35745;&#24515;&#29702;&#36127;&#33655;
&lt;/p&gt;
&lt;p&gt;
Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#22312;&#24037;&#20316;&#21644;&#20241;&#24687;&#26102;&#37117;&#22788;&#20110;&#25345;&#32493;&#27963;&#21160;&#30340;&#29366;&#24577;&#12290;&#24515;&#29702;&#27963;&#21160;&#26159;&#26085;&#24120;&#36807;&#31243;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#24403;&#22823;&#33041;&#36807;&#24230;&#21171;&#32047;&#26102;&#65292;&#20250;&#23545;&#20154;&#20307;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#37325;&#35270;&#36880;&#28176;&#22686;&#21152;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#12290;&#22810;&#31181;&#20449;&#21495;&#34987;&#29992;&#20110;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#20449;&#24687;&#30340;&#29305;&#28857;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#34987;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#24515;&#29702;&#36127;&#33655;&#20998;&#20026;&#19977;&#31181;&#29366;&#24577;&#24182;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#24515;&#29702;&#20272;&#35745;&#32467;&#26524;&#12290;&#22312;&#26102;&#38388;&#22495;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#27531;&#24046;&#22359;&#30340;&#26032;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#36890;&#36807;&#34701;&#21512;$3\times3$&#21644;$1\times1$&#21367;&#31215;&#26680;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#24341;&#20837;&#26080;&#21442;&#25968;&#30340;&#31354;&#38388;&#24179;&#31227;&#25805;&#20316;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.16140</link><description>&lt;p&gt;
&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully $1\times1$ Convolutional Network for Lightweight Image Super-Resolution. (arXiv:2307.16140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#36890;&#36807;&#34701;&#21512;$3\times3$&#21644;$1\times1$&#21367;&#31215;&#26680;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#24341;&#20837;&#26080;&#21442;&#25968;&#30340;&#31354;&#38388;&#24179;&#31227;&#25805;&#20316;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#22312;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20102;&#22823;&#21367;&#31215;&#26680;&#65288;$3\times3$&#25110;&#26356;&#22823;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#26102;&#12289;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#12290;&#30456;&#21453;&#65292;$1\times1$&#21367;&#31215;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#22312;&#32858;&#21512;&#23616;&#37096;&#31354;&#38388;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;SISR&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#23545;&#31435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34701;&#21512;$3\times3$&#21644;$1\times1$&#21367;&#31215;&#26680;&#20248;&#28857;&#30340;&#36731;&#37327;&#32423;SISR&#32593;&#32476;&#65292;&#21517;&#20026;Shift-Conv-based Network (SCNet)&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26080;&#21442;&#25968;&#30340;&#31354;&#38388;&#24179;&#31227;&#25805;&#20316;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36171;&#20104;&#20102;&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#32593;&#32476;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models have achieved significant process on single image super-resolution (SISR) tasks, in particular large models with large kernel ($3\times3$ or more). However, the heavy computational footprint of such models prevents their deployment in real-time, resource-constrained environments. Conversely, $1\times1$ convolutions bring substantial computational efficiency, but struggle with aggregating local spatial representations, an essential capability to SISR models. In response to this dichotomy, we propose to harmonize the merits of both $3\times3$ and $1\times1$ kernels, and exploit a great potential for lightweight SISR tasks. Specifically, we propose a simple yet effective fully $1\times1$ convolutional network, named Shift-Conv-based Network (SCNet). By incorporating a parameter-free spatial-shift operation, it equips the fully $1\times1$ convolutional network with powerful representation capability while impressive computational efficiency. Extensive experiments demonstrate th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;</title><link>http://arxiv.org/abs/2306.14114</link><description>&lt;p&gt;
TNPAR: &#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20107;&#20214;&#24207;&#21015;Granger&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;Granger&#22240;&#26524;&#20851;&#31995;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20107;&#20214;&#24207;&#21015;&#29420;&#31435;&#21516;&#20998;&#24067; (i.i.d.) &#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20107;&#20214;&#24207;&#21015;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#19968; i.i.d. &#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#34987;&#24314;&#27169;&#25104;&#19968;&#20010;&#25299;&#25169;&#32593;&#32476;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23558;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#24341;&#20837;Granger&#22240;&#26524;&#21457;&#29616;&#26469;&#35299;&#20915;&#38750; i.i.d. &#38382;&#39064;&#12290;&#36825;&#19968;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;1) &#22914;&#20309;&#22312;&#27169;&#22411;&#20107;&#20214;&#24207;&#21015;&#26102;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#65307;2) &#22914;&#20309;&#23398;&#20064;Granger&#22240;&#26524;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#32479;&#19968;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#27850;&#26494;&#36807;&#31243;&#30340;&#19968;&#31181;&#21464;&#20307;&#26469;&#24314;&#27169;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#21051;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#20851;&#31995;&#21644;&#29616;&#26377;&#20107;&#20214;&#24207;&#21015;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08762</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;POMDP&#30340;&#29702;&#35770;&#38590;&#24230;&#21644;&#21487;&#35745;&#31639;&#24615;
&lt;/p&gt;
&lt;p&gt;
Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25429;&#25417;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#30340;POMDP&#20013;&#23398;&#20064;&#21487;&#33021;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#28508;&#22312;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#26377;&#22810;&#23569;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65288;OSI&#65289;&#36275;&#20197;&#23454;&#29616;&#21487;&#35745;&#31639;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#25105;&#20204;&#20855;&#26377;&#23436;&#25972;&#30340;OSI&#65292;&#21542;&#21017;&#25105;&#20204;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#37319;&#26679;&#22797;&#26434;&#24230;&#25165;&#33021;&#33719;&#24471;POMDP&#30340;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21463;&#21040;&#25105;&#20204;&#19979;&#30028;&#35774;&#35745;&#30340;&#20851;&#38190;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#21482;&#26377;&#37096;&#20998;OSI&#65292;&#20063;&#23384;&#22312;&#37325;&#35201;&#30340;&#21487;&#35745;&#31639;&#30340;POMDP&#31867;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20855;&#26377;&#37096;&#20998;OSI&#30340;&#20004;&#20010;&#26032;&#39062;&#30340;POMDP&#31867;&#21035;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#26032;&#30340;&#36951;&#25022;&#19978;&#19979;&#30028;&#35777;&#26126;&#20102;&#26032;&#30340;&#31639;&#27861;&#26159;&#25509;&#36817;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03403</link><description>&lt;p&gt;
SGAT4PASS&#65306;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer
&lt;/p&gt;
&lt;p&gt;
SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;&#21487;&#20197;&#26681;&#25454;&#36229;&#24191;&#35282;&#35266;&#23519;&#21040;&#30340;&#23436;&#25972;&#22330;&#26223;&#26469;&#36827;&#34892;&#24863;&#30693;&#12290;&#20256;&#32479;&#30340;&#38024;&#23545;2D&#20840;&#26223;&#22270;&#20687;&#30340;PASS&#26041;&#27861;&#20391;&#37325;&#20110;&#35299;&#20915;&#22270;&#20687;&#30072;&#21464;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#23545;&#21407;&#22987;360&#176;&#25968;&#25454;&#30340;3D&#23646;&#24615;&#30340;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#24403;&#36755;&#20837;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#20840;&#26223;&#22270;&#20687;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24212;&#23545;3D&#25200;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#21363;SGAT4PASS&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#21363;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#22270;&#20687;&#25237;&#24433;&#65292;&#29699;&#38754;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21644;&#20840;&#26223;&#24863;&#30693;&#25439;&#22833;&#65292;&#23427;&#23545;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#23545;&#24050;&#26377;&#30340;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21152;&#20837;&#20102;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.14550</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14550
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#26368;&#22823;&#21270;&#25910;&#30410;&#31574;&#30053;&#12290;&#31163;&#32447;RL&#30340;&#19977;&#22823;&#33539;&#24335;&#26159;Q-Learning&#12289;Imitation Learning&#21644;Sequence Modeling&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#65292;&#21738;&#31181;&#33539;&#24335;&#34987;&#20248;&#20808;&#36873;&#25321;&#65311;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#20195;&#34920;&#24615;&#31639;&#27861;&#8212;&#8212;&#20445;&#23432;Q-Learning(CQL)&#12289;&#34892;&#20026;&#20811;&#38534; (BC)&#21644;&#20915;&#31574;Transformer (DT)&#8212;&#8212;&#22312;&#24120;&#29992;&#30340;D4RL&#21644;Robomimic&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#26469;&#29702;&#35299;&#23427;&#20204;&#22312;&#25968;&#25454;&#23376;&#20248;&#24615;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1)&#24207;&#21015;&#24314;&#27169;&#38656;&#35201;&#27604;Q-Learning&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31454;&#20105;&#24615;&#31574;&#30053;&#65292;&#20294;&#26356;&#21152;&#31283;&#20581;&#65307;(2)&#24207;&#21015;&#24314;&#27169;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#37117;&#35201;&#22909;&#24471;&#22810;&#65307;(3)&#38543;&#30528;&#20219;&#21153;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.09582</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#23545;&#24773;&#32490;&#25512;&#26029;&#30340;&#22240;&#26524;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#31185;&#23398;&#20013;&#65292;&#22914;&#20309;&#29702;&#35299;&#35821;&#35328;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#35843;&#26597;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#20351;&#29992;&#25552;&#31034;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;14&#20010;&#24773;&#32490;&#27010;&#24565;&#30340;&#23646;&#24615;&#30001;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#32676;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#23646;&#24615;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#19982;&#38543;&#26426;&#25805;&#32437;&#30456;&#27604;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#34920;&#29616;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#23646;&#24615;&#29305;&#23450;&#30340;&#34920;&#29616;&#19979;&#38477;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#30340;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
&lt;/p&gt;</description></item></channel></rss>