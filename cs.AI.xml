<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;TPUs&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#33455;&#29255;&#26550;&#26500;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TPUs&#21487;&#20197;&#22312;&#20113;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08918</link><description>&lt;p&gt;
&#25506;&#32034;TPUs&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploration of TPUs for AI Applications. (arXiv:2309.08918v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;TPUs&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#33455;&#29255;&#26550;&#26500;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TPUs&#21487;&#20197;&#22312;&#20113;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tensor Processing Units&#65288;TPUs&#65289;&#26159;&#30001;Google&#24320;&#21457;&#30340;&#19987;&#38376;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;TPU&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24615;&#33021;&#21644;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#23454;&#29616;&#12290;&#39318;&#20808;&#27010;&#36848;&#20102;TPU&#30340;&#35774;&#35745;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20854;&#24635;&#20307;&#26550;&#26500;&#12289;&#32534;&#35793;&#25216;&#26415;&#21644;&#25903;&#25345;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20113;TPU&#21644;&#36793;&#32536;TPU&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#33455;&#29255;&#26550;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;TPU&#21152;&#36895;AI&#24037;&#20316;&#36127;&#36733;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TPU&#22312;&#20113;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#22343;&#33021;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#22312;&#36793;&#32536;TPU&#20013;&#37096;&#32626;&#26356;&#22810;&#26550;&#26500;&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#36827;&#34892;&#26356;&#21487;&#38752;&#27604;&#36739;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor Processing Units (TPUs) are specialized hardware accelerators for deep learning developed by Google. This paper explores the performance of TPU with a focus on AI and its implementation in edge computing. It first provides an overview of TPUs, specifically their design in relation to neural networks, their general architecture, compilation techniques and supporting frameworks. Furthermore, we provide a comparative analysis of Cloud and Edge TPU performance against other counterpart chip architectures. It is then discussed how TPUs can be used to speed up AI workloads. The results show that TPUs can provide significant performance improvements both in cloud and edge computing. Additionally, we address the need for further research for the deployment of more architectures in the Edge TPU, as well as the need for the development of more robust comparisons in edge computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08916</link><description>&lt;p&gt;
&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65306;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#33041;&#30142;&#30149;&#30340;&#21457;&#30149;&#26426;&#21046;&#65292;&#21253;&#25324;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#65292;&#33041;&#32467;&#26500;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23558;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#26144;&#23556;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#26469;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;InnerGCN&#65289;&#27169;&#22359;&#65292;BGGAN&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#21033;&#29992;&#30452;&#25509;&#21644;&#38388;&#25509;&#33041;&#21306;&#22495;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Balancer&#30340;&#26032;&#27169;&#22359;&#26469;&#24179;&#34913;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;Balancer&#24341;&#20837;&#21040;BGGAN&#20013;&#65292;&#32467;&#26500;&#29983;&#25104;&#22120;&#21644;&#21151;&#33021;&#29983;&#25104;&#22120;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;AD&#20013;&#20934;&#30830;&#22320;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#32473;&#23450;&#35780;&#20272;&#29615;&#22659;&#19979;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08913</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Statistical Turing Test for Generative Models. (arXiv:2309.08913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#32473;&#23450;&#35780;&#20272;&#29615;&#22659;&#19979;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#20869;&#23481;&#29983;&#25104;&#33021;&#21147;&#30340;&#20986;&#29616;&#20652;&#29983;&#20102;&#29992;&#20110;&#21306;&#20998;&#20869;&#23481;&#26469;&#28304;&#20110;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#20998;&#31867;&#22120;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24037;&#20316;&#30340;&#38544;&#21547;&#20551;&#35774;&#26159;&#20154;&#31867;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#26426;&#22120;&#30340;&#29983;&#25104;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32479;&#35745;&#27169;&#24335;&#35782;&#21035;&#35821;&#35328;&#20013;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#24046;&#24322;&#30340;&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#26694;&#26550;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#21521;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20998;&#26512;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of human-like abilities of AI systems for content generation in domains such as text, audio, and vision has prompted the development of classifiers to determine whether content originated from a human or a machine. Implicit in these efforts is an assumption that the generation properties of a human are different from that of the machine. In this work, we provide a framework in the language of statistical pattern recognition that quantifies the difference between the distributions of human and machine-generated content conditioned on an evaluation context. We describe current methods in the context of the framework and demonstrate how to use the framework to evaluate the progression of generative models towards human-like capabilities, among many axes of analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35270;&#39057;&#21040;&#20107;&#20214;&#27969;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;DVS&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#29983;&#25104;&#30340;&#20107;&#20214;&#20307;&#32032;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#21160;&#24577;&#24863;&#30693;&#26102;&#38388;&#25139;&#25512;&#26029;&#31574;&#30053;&#65292;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#20107;&#20214;&#26102;&#38388;&#25139;&#12290;</title><link>http://arxiv.org/abs/2309.08891</link><description>&lt;p&gt;
V2CE: &#35270;&#39057;&#21040;&#36830;&#32493;&#20107;&#20214;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
V2CE: Video to Continuous Events Simulator. (arXiv:2309.08891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35270;&#39057;&#21040;&#20107;&#20214;&#27969;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;DVS&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#29983;&#25104;&#30340;&#20107;&#20214;&#20307;&#32032;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#21160;&#24577;&#24863;&#30693;&#26102;&#38388;&#25139;&#25512;&#26029;&#31574;&#30053;&#65292;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#20107;&#20214;&#26102;&#38388;&#25139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#20256;&#24863;&#22120;&#65288;DVS&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36817;&#24180;&#26469;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#22312;&#21160;&#24577;&#33539;&#22260;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#19982;RGB&#30456;&#26426;&#31561;&#20027;&#21160;&#20687;&#32032;&#20256;&#24863;&#22120;&#65288;APS&#65289;&#35774;&#22791;&#30456;&#27604;&#65292;DVS&#20316;&#20026;&#19968;&#20010;&#30456;&#23545;&#26032;&#20852;&#30340;&#35270;&#35273;&#20256;&#24863;&#22120;&#65292;&#32570;&#20047;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#20043;&#21069;&#23558;APS&#25968;&#25454;&#36716;&#25442;&#20026;&#20107;&#20214;&#30340;&#21162;&#21147;&#24448;&#24448;&#35201;&#38754;&#23545;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;&#19982;&#30495;&#23454;&#20107;&#20214;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#12289;&#32570;&#20047;&#23450;&#37327;&#39564;&#35777;&#21644;&#26102;&#38388;&#36724;&#20869;&#30340;&#20998;&#23618;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#20174;&#22810;&#20010;&#35282;&#24230;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#20107;&#20214;&#27969;&#65292;&#32771;&#34385;&#21040;DVS&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#20107;&#20214;&#20307;&#32032;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#21160;&#24577;&#24863;&#30693;&#26102;&#38388;&#25139;&#25512;&#26029;&#31574;&#30053;&#65292;&#20197;&#20934;&#30830;&#22320;&#20174;&#20107;&#20214;&#20307;&#32032;&#20013;&#24674;&#22797;&#20107;&#20214;&#26102;&#38388;&#25139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21033;&#29992;&#22810;&#35270;&#35282;&#20803;&#26631;&#31614;&#35299;&#20915;&#20102;"&#35821;&#20041;&#30683;&#30462;"&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08888</link><description>&lt;p&gt;
GCL: &#22522;&#20110;&#26799;&#24230;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19982;&#22810;&#35270;&#35282;&#20803;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels. (arXiv:2309.08888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21033;&#29992;&#22810;&#35270;&#35282;&#20803;&#26631;&#31614;&#35299;&#20915;&#20102;"&#35821;&#20041;&#30683;&#30462;"&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26631;&#27880;&#25104;&#26412;&#26222;&#36941;&#26114;&#36149;&#65292;&#35774;&#35745;&#19968;&#31181;&#33410;&#30465;&#26631;&#27880;&#36127;&#25285;&#30340;&#26631;&#27880;&#25928;&#29575;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20351;&#29992;&#26377;&#38480;&#26631;&#31614;&#26469;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#31283;&#20581;&#34920;&#31034;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#22330;&#26223;&#19979;&#65292;&#39044;&#20808;&#20934;&#22791;&#22909;&#30340;&#20803;&#26631;&#31614;&#65288;&#21363;&#21307;&#23398;&#22270;&#20687;&#30340;&#29305;&#23450;&#23646;&#24615;&#20449;&#24687;&#65289;&#26412;&#36136;&#19978;&#25581;&#31034;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#36825;&#20123;&#26631;&#31614;&#24050;&#34987;&#29992;&#26469;&#23450;&#20041;&#27491;&#21521;&#23545;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#20803;&#26631;&#31614;&#25152;&#25581;&#31034;&#30340;&#22810;&#35270;&#35282;&#35821;&#20041;&#36890;&#24120;&#19981;&#20860;&#23481;&#65292;&#32452;&#21512;&#19981;&#21516;&#30340;&#20803;&#26631;&#31614;&#21487;&#33021;&#20250;&#23548;&#33268;&#8220;&#35821;&#20041;&#30683;&#30462;&#8221;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#24341;&#23548;&#26041;&#27861;&#26469;&#22788;&#29702;&#8220;&#35821;&#20041;&#30683;&#30462;&#8221;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#32479;&#19968;&#22810;&#35270;&#35282;&#30340;&#20803;&#26631;&#31614;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#39640;&#27700;&#24179;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since annotating medical images for segmentation tasks commonly incurs expensive costs, it is highly desirable to design an annotation-efficient method to alleviate the annotation burden. Recently, contrastive learning has exhibited a great potential in learning robust representations to boost downstream tasks with limited labels. In medical imaging scenarios, ready-made meta labels (i.e., specific attribute information of medical images) inherently reveal semantic relationships among images, which have been used to define positive pairs in previous work. However, the multi-perspective semantics revealed by various meta labels are usually incompatible and can incur intractable "semantic contradiction" when combining different meta labels. In this paper, we tackle the issue of "semantic contradiction" in a gradient-guided manner using our proposed Gradient Mitigator method, which systematically unifies multi-perspective meta labels to enable a pre-trained model to attain a better high-l
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;XOR-SMC&#65292;&#19968;&#20010;&#20855;&#26377;&#35775;&#38382;NP-oracles&#26435;&#38480;&#30340;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#24230;&#38590;&#35299;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24658;&#23450;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.08883</link><description>&lt;p&gt;
&#35299;&#20915;&#31526;&#21495;&#21644;&#32479;&#35745;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees. (arXiv:2309.08883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08883
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;XOR-SMC&#65292;&#19968;&#20010;&#20855;&#26377;&#35775;&#38382;NP-oracles&#26435;&#38480;&#30340;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#24230;&#38590;&#35299;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24658;&#23450;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#28085;&#30422;&#20102;&#38656;&#35201;&#31526;&#21495;&#20915;&#31574;&#21644;&#32479;&#35745;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#23427;&#30340;&#19968;&#33324;&#24418;&#24335;&#25429;&#25417;&#20102;&#31526;&#21495;&#21644;&#32479;&#35745;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#12290;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#23547;&#25214;&#31574;&#30053;&#24178;&#39044;&#20197;&#25511;&#21046;&#27010;&#29575;&#24615;&#32467;&#26524;&#12290;&#35299;&#20915;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#20855;&#26377;&#39640;&#24230;&#38590;&#35299;&#30340;&#29305;&#24615;&#65288;$\text{NP}^{\text{PP}}$-complete&#65289;&#65292;&#34701;&#21512;&#20102;&#32479;&#35745;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#12290;&#20808;&#21069;&#23545;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#30340;&#30740;&#31350;&#32570;&#20047;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#21644;/&#25110;&#22312;&#32452;&#21512;&#32422;&#26463;&#23384;&#22312;&#26102;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#32463;&#39564;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;XOR-SMC&#65292;&#19968;&#20010;&#20855;&#26377;&#35775;&#38382;NP-oracles&#26435;&#38480;&#30340;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#24230;&#38590;&#35299;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24658;&#23450;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;XOR-SMC&#36890;&#36807;&#29992;&#38543;&#26426;&#30340;XOR&#32422;&#26463;&#26367;&#25442;SMC&#20013;&#30340;&#27169;&#22411;&#35745;&#25968;&#65292;&#23558;&#39640;&#24230;&#38590;&#35299;&#30340;SMC&#36716;&#21270;&#20026;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Satisfiability Modulo Counting (SMC) encompasses problems that require both symbolic decision-making and statistical reasoning. Its general formulation captures many real-world problems at the intersection of symbolic and statistical Artificial Intelligence. SMC searches for policy interventions to control probabilistic outcomes. Solving SMC is challenging because of its highly intractable nature($\text{NP}^{\text{PP}}$-complete), incorporating statistical inference and symbolic reasoning. Previous research on SMC solving lacks provable guarantees and/or suffers from sub-optimal empirical performance, especially when combinatorial constraints are present. We propose XOR-SMC, a polynomial algorithm with access to NP-oracles, to solve highly intractable SMC problems with constant approximation guarantees. XOR-SMC transforms the highly intractable SMC into satisfiability problems, by replacing the model counting in SMC with SAT formulae subject to randomized XOR constraints. Experiments o
&lt;/p&gt;</description></item><item><title>ChatGPT-4&#19982;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#32452;&#21512;&#33021;&#22815;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20196;&#20154;&#28385;&#24847;&#22320;&#35299;&#20915;&#20171;&#32461;&#24615;&#22823;&#23398;&#32423;&#30340;&#21521;&#37327;&#24494;&#31215;&#20998;&#21644;&#30005;&#30913;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08881</link><description>&lt;p&gt;
ChatGPT-4&#19982;&#20195;&#30721;&#35299;&#37322;&#22120;&#21487;&#29992;&#20110;&#35299;&#20915;&#20171;&#32461;&#24615;&#22823;&#23398;&#32423;&#30340;&#21521;&#37327;&#24494;&#31215;&#20998;&#21644;&#30005;&#30913;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4 with Code Interpreter can be used to solve introductory college-level vector calculus and electromagnetism problems. (arXiv:2309.08881v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08881
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4&#19982;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#32452;&#21512;&#33021;&#22815;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20196;&#20154;&#28385;&#24847;&#22320;&#35299;&#20915;&#20171;&#32461;&#24615;&#22823;&#23398;&#32423;&#30340;&#21521;&#37327;&#24494;&#31215;&#20998;&#21644;&#30005;&#30913;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT 3.5&#12289;4&#20197;&#21450;&#24102;&#26377;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;ChatGPT 4&#22312;&#19968;&#32452;&#22823;&#23398;&#32423;&#24037;&#31243;&#25968;&#23398;&#21644;&#30005;&#30913;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#26159;&#32473;&#22823;&#20108;&#30340;&#30005;&#27668;&#24037;&#31243;&#19987;&#19994;&#23398;&#29983;&#30340;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#32452;13&#20010;&#38382;&#39064;&#65292;&#24182;&#35753;ChatGPT&#22810;&#27425;&#35299;&#20915;&#23427;&#20204;&#65292;&#27599;&#27425;&#37117;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#65288;&#20132;&#35848;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24102;&#26377;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;ChatGPT-4&#33021;&#22815;&#22823;&#37096;&#20998;&#26102;&#38388;&#20196;&#20154;&#28385;&#24847;&#22320;&#35299;&#20915;&#25105;&#20204;&#27979;&#35797;&#30340;&#22823;&#22810;&#25968;&#38382;&#39064;&#65292;&#36825;&#26159;&#30456;&#27604;&#20110;&#27809;&#26377;&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;ChatGPT-4&#65288;&#25110;3.5&#65289;&#30340;&#24615;&#33021;&#26377;&#20102;&#37325;&#22823;&#25913;&#36827;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ChatGPT&#30340;&#24615;&#33021;&#26377;&#20123;&#38543;&#26426;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#22312;&#26032;&#30340;ChatGPT&#23454;&#20363;&#20013;&#22810;&#27425;&#35299;&#20915;&#30456;&#21516;&#30340;&#38382;&#39064;&#24182;&#36873;&#25321;&#26368;&#24120;&#35265;&#30340;&#31572;&#26696;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21644;&#35266;&#23519;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#27700;&#24179;&#30340;&#25945;&#24072;&#21644;&#23398;&#29983;&#25552;&#20379;&#20102;&#19968;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated ChatGPT 3.5, 4, and 4 with Code Interpreter on a set of college-level engineering-math and electromagnetism problems, such as those often given to sophomore electrical engineering majors. We selected a set of 13 problems, and had ChatGPT solve them multiple times, using a fresh instance (chat) each time. We found that ChatGPT-4 with Code Interpreter was able to satisfactorily solve most problems we tested most of the time -- a major improvement over the performance of ChatGPT-4 (or 3.5) without Code Interpreter. The performance of ChatGPT was observed to be somewhat stochastic, and we found that solving the same problem N times in new ChatGPT instances and taking the most-common answer was an effective strategy. Based on our findings and observations, we provide some recommendations for instructors and students of classes at this level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;H-infinity&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23436;&#20840;&#26080;&#27169;&#22411;&#65292;&#19988;&#19981;&#38656;&#35201;&#21021;&#22987;&#31283;&#23450;&#31574;&#30053;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#38381;&#24335;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.08880</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#26102;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;H-infinity&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65306;&#33258;&#20027;&#31227;&#21160;&#38656;&#27714;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems. (arXiv:2309.08880v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;H-infinity&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23436;&#20840;&#26080;&#27169;&#22411;&#65292;&#19988;&#19981;&#38656;&#35201;&#21021;&#22987;&#31283;&#23450;&#31574;&#30053;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31867;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#35774;&#35745;&#36866;&#24212;&#24615;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#12289;&#23454;&#26102;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;Q-learning&#31639;&#27861;&#26469;&#35299;&#20915;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;H-infinity&#25511;&#21046;&#38382;&#39064;&#12290;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#25991;&#29486;&#20013;&#30340;O(q^3)&#38477;&#20302;&#21040;&#20102;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;O(q^2)&#65292;&#20854;&#20013;q&#26159;&#29366;&#24577;&#21464;&#37327;&#12289;&#25511;&#21046;&#36755;&#20837;&#21644;&#24178;&#25200;&#30340;&#22823;&#23567;&#20043;&#21644;&#30340;&#20108;&#27425;&#39033;&#12290;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#26368;&#20248;&#25511;&#21046;&#22120;&#65292;&#24182;&#23398;&#20064;&#20102;&#21160;&#20316;&#21644;&#35780;&#35770;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#19981;&#38656;&#35201;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#20102;&#35299;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#23436;&#20840;&#26080;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20165;&#22312;&#31532;&#19968;&#27425;&#36845;&#20195;&#20013;&#38656;&#35201;&#36275;&#22815;&#30340;&#25200;&#21160;&#22122;&#22768;&#65292;&#32780;&#19981;&#24433;&#21709;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#26080;&#38656;&#21021;&#22987;&#31283;&#23450;&#31574;&#30053;&#65292;&#31639;&#27861;&#25910;&#25947;&#21040;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\mathcal{O}(\underline{q}^3)$ in the literature to $\mathcal{O}(\underline{q}^2)$ in the proposed algorithm, where $\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form 
&lt;/p&gt;</description></item><item><title>PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08872</link><description>&lt;p&gt;
PDFTriage: &#23545;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#36827;&#34892;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08872
&lt;/p&gt;
&lt;p&gt;
PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#38382;&#31572;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#25991;&#26723;&#26080;&#27861;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20687;PDF&#12289;&#32593;&#39029;&#21644;&#28436;&#31034;&#25991;&#31295;&#36825;&#26679;&#30340;&#25991;&#26723;&#26159;&#26377;&#32467;&#26500;&#30340;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#39029;&#30721;&#12289;&#34920;&#26684;&#12289;&#31456;&#33410;&#31561;&#12290;&#23558;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#19982;&#29992;&#25143;&#23545;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#32467;&#26500;&#30340;&#25991;&#26723;&#30340;&#35748;&#30693;&#27169;&#22411;&#19981;&#31526;&#12290;&#24403;&#31995;&#32479;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#26597;&#35810;&#19978;&#19979;&#25991;&#26102;&#65292;&#36825;&#31181;&#19981;&#31526;&#20250;&#26174;&#29616;&#20986;&#26469;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#21487;&#33021;&#20351;&#38382;&#31572;&#31995;&#32479;&#20986;&#38169;&#12290;&#20026;&#20102;&#24357;&#21512;&#22788;&#29702;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#30340;&#22522;&#26412;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDFTriage&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#32467;&#26500;&#25110;&#20869;&#23481;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PDFTriage&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHLAT&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19971;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#37117;&#23454;&#29616;&#20102;&#26126;&#26174;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#20248;&#21270;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2309.08868</link><description>&lt;p&gt;
MHLAT: &#33258;&#21160;ICD&#32534;&#30721;&#30340;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08868
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHLAT&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19971;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#37117;&#23454;&#29616;&#20102;&#26126;&#26174;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#20248;&#21270;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#32534;&#30721;&#26159;&#23558;ICD&#35786;&#26029;&#20195;&#30721;&#20998;&#37197;&#32473;&#20020;&#24202;&#31508;&#35760;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#22823;&#37327;&#30340;&#26631;&#31614;&#65288;&#36817;9000&#20010;&#65289;&#21644;&#24222;&#22823;&#30340;&#25991;&#26412;&#65288;&#22810;&#36798;8000&#20010;&#26631;&#35760;&#65289;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#30340;&#21333;&#36890;&#35835;&#36807;&#31243;&#19981;&#21516;&#65292;&#20154;&#20204;&#20542;&#21521;&#20110;&#20877;&#27425;&#38405;&#35835;&#25991;&#26412;&#21644;&#26631;&#31614;&#23450;&#20041;&#20197;&#33719;&#24471;&#26356;&#33258;&#20449;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#24456;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#65288;MHLAT&#65289;&#65292;&#21033;&#29992;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#26469;&#33719;&#21462;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#23545;&#19977;&#20010;&#22522;&#20934;MIMIC&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19971;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#23454;&#29616;&#20102;&#26126;&#26174;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#20248;&#21270;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
International Classification of Diseases (ICD) coding is the task of assigning ICD diagnosis codes to clinical notes. This can be challenging given the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000 tokens). However, unlike the single-pass reading process in previous works, humans tend to read the text and label definitions again to get more confident answers. Moreover, although pretrained language models have been used to address these problems, they suffer from huge memory usage. To address the above problems, we propose a simple but effective model called the Multi-Hop Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is deployed to get more precise and informative representations. Extensive experiments on three benchmark MIMIC datasets indicate that our method achieves significantly better or competitive performance on all seven metrics, with much fewer parameters to optimize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;&#25216;&#26415;&#65292;&#36890;&#36807;&#28369;&#27169;&#25511;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#25143;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#36319;&#36394;&#21644;&#22312;&#32447;&#28369;&#21160;&#19982;&#25171;&#28369;&#34917;&#20607;&#12290;</title><link>http://arxiv.org/abs/2309.08863</link><description>&lt;p&gt;
&#20351;&#29992;&#28369;&#27169;&#25511;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24102;&#26377;&#28369;&#21160;&#19982;&#25171;&#28369;&#34917;&#20607;&#30340;&#23653;&#24102;&#24335;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and Skid Compensation using Sliding-Mode Control and Deep Learning. (arXiv:2309.08863v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;&#25216;&#26415;&#65292;&#36890;&#36807;&#28369;&#27169;&#25511;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#25143;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#36319;&#36394;&#21644;&#22312;&#32447;&#28369;&#21160;&#19982;&#25171;&#28369;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25143;&#22806;&#29615;&#22659;&#21644;&#19981;&#24179;&#22320;&#24418;&#20013;&#65292;&#28369;&#21160;&#21644;&#25171;&#28369;&#34917;&#20607;&#23545;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#24120;&#35268;&#30340;&#25143;&#22806;&#29615;&#22659;&#20013;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#28369;&#21160;&#21644;&#25171;&#28369;&#21361;&#38505;&#22806;&#65292;&#28369;&#21160;&#21644;&#25171;&#28369;&#36824;&#20250;&#32473;&#36712;&#36857;&#36319;&#36394;&#31995;&#32479;&#24102;&#26469;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20351;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#23613;&#31649;&#22312;&#35813;&#39046;&#22495;&#26377;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#25143;&#22806;&#29615;&#22659;&#20013;&#36718;&#32974;-&#22320;&#38754;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#21487;&#34892;&#30340;&#22312;&#32447;&#28369;&#21160;&#21644;&#25171;&#28369;&#34917;&#20607;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25143;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#36710;&#36742;&#32423;&#30340;&#23454;&#26102;&#28369;&#21160;&#21644;&#25171;&#28369;&#34917;&#20607;&#12290;&#37319;&#29992;&#28369;&#27169;&#25511;&#21046;&#25216;&#26415;&#35774;&#35745;&#20102;&#40065;&#26834;&#30340;&#36712;&#36857;&#36319;&#36394;&#31995;&#32479;&#65292;&#21487;&#20197;&#32771;&#34385;&#21040;&#36825;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#30340;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#25511;&#21046;&#21453;&#39304;&#24490;&#29615;&#20013;&#65292;&#23558;&#20004;&#20010;&#20808;&#21069;&#24320;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;[1]&#65292;[2]&#38598;&#25104;&#29992;&#20110;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Slip and skid compensation is crucial for mobile robots' navigation in outdoor environments and uneven terrains. In addition to the general slipping and skidding hazards for mobile robots in outdoor environments, slip and skid cause uncertainty for the trajectory tracking system and put the validity of stability analysis at risk. Despite research in this field, having a real-world feasible online slip and skid compensation is still challenging due to the complexity of wheel-terrain interaction in outdoor environments. This paper presents a novel trajectory tracking technique with real-world feasible online slip and skid compensation at the vehicle-level for skid-steering mobile robots in outdoor environments. The sliding mode control technique is utilized to design a robust trajectory tracking system to be able to consider the parameter uncertainty of this type of robot. Two previously developed deep learning models [1], [2] are integrated into the control feedback loop to estimate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#22826;&#36203;&#20857;&#38453;&#21015;&#25104;&#20687;&#26041;&#27861;&#30340;&#25945;&#31243;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22826;&#36203;&#20857;&#25216;&#26415;&#22312;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#21644;&#39640;&#21534;&#21520;&#37327;&#36890;&#20449;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.08844</link><description>&lt;p&gt;
&#26032;&#20852;&#30340;&#22826;&#36203;&#20857;&#38453;&#21015;&#25104;&#20687;&#26041;&#27861;&#65306;&#19968;&#31687;&#25945;&#31243;&#32508;&#36848;&#21644;&#36719;&#20214;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Emerging Approaches for THz Array Imaging: A Tutorial Review and Software Tool. (arXiv:2309.08844v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#22826;&#36203;&#20857;&#38453;&#21015;&#25104;&#20687;&#26041;&#27861;&#30340;&#25945;&#31243;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22826;&#36203;&#20857;&#25216;&#26415;&#22312;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#21644;&#39640;&#21534;&#21520;&#37327;&#36890;&#20449;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#12289;6G&#21644;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#36890;&#20449;&#21644;&#20256;&#24863;&#25216;&#26415;&#36817;&#24180;&#26469;&#36805;&#36895;&#20174;&#27627;&#31859;&#27874;&#21457;&#23637;&#21040;&#22826;&#36203;&#20857;&#12290;&#22312;&#30005;&#30913;&#30828;&#20214;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#30340;&#25512;&#21160;&#19979;&#65292;&#21487;&#20197;&#21033;&#29992;&#27627;&#31859;&#27874;&#21644;&#22826;&#36203;&#20857;&#39057;&#29575;&#33539;&#22260;&#65288;&#20998;&#21035;&#20026;30 GHz&#33267;300 GHz&#21644;300 GHz&#33267;3000 GHz&#65289;&#36827;&#34892;&#21508;&#31181;&#24212;&#29992;&#12290;&#22826;&#36203;&#20857;&#31995;&#32479;&#30340;&#20027;&#35201;&#29305;&#28857;&#26159;&#39640;&#24102;&#23485;&#20256;&#36755;&#65292;&#21487;&#23454;&#29616;&#36229;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#21644;&#39640;&#21534;&#21520;&#37327;&#36890;&#20449;&#65307;&#28982;&#32780;&#65292;&#22826;&#36203;&#20857;&#25216;&#26415;&#30340;&#26222;&#21450;&#36824;&#38754;&#20020;&#30528;&#30828;&#20214;&#21644;&#31639;&#27861;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#30001;&#27627;&#31859;&#27874;&#21644;&#22826;&#36203;&#20857;&#39057;&#29575;&#32452;&#25104;&#30340;&#39057;&#35889;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#20122;&#27627;&#31859;&#20998;&#36776;&#29575;&#30340;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#25104;&#20687;&#65292;&#36866;&#29992;&#20110;&#26448;&#26009;&#34920;&#24449;&#21644;&#26080;&#25439;&#27979;&#35797;&#65288;NDT&#65289;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#25991;&#23545;&#22826;&#36203;&#20857;SAR&#31995;&#32479;&#21644;&#31639;&#27861;&#36827;&#34892;&#20102;&#25945;&#31243;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerated by the increasing attention drawn by 5G, 6G, and Internet of Things applications, communication and sensing technologies have rapidly evolved from millimeter-wave (mmWave) to terahertz (THz) in recent years. Enabled by significant advancements in electromagnetic (EM) hardware, mmWave and THz frequency regimes spanning 30 GHz to 300 GHz and 300 GHz to 3000 GHz, respectively, can be employed for a host of applications. The main feature of THz systems is high-bandwidth transmission, enabling ultra-high-resolution imaging and high-throughput communications; however, challenges in both the hardware and algorithmic arenas remain for the ubiquitous adoption of THz technology. Spectra comprising mmWave and THz frequencies are well-suited for synthetic aperture radar (SAR) imaging at sub-millimeter resolutions for a wide spectrum of tasks like material characterization and nondestructive testing (NDT). This article provides a tutorial review of systems and algorithms for THz SAR in 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.08836</link><description>&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;: &#19968;&#31181;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#30740;&#31350;&#20102;&#21322;&#20010;&#22810;&#19990;&#32426;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#29616;&#22312;&#22791;&#21463;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30456;&#27604;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#24378;&#22823;&#65292;&#24182;&#24050;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#23384;&#22312;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#12289;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20559;&#35265;&#32531;&#35299;&#21644;&#20844;&#24179;&#20445;&#25252;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#21644;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#24212;&#29992;&#20013;&#30340;&#20559;&#35265;&#26469;&#28304;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#30740;&#31350;&#20102;&#35774;&#35745;&#20844;&#24179;&#21644;&#26080;&#20559;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.08832</link><description>&lt;p&gt;
SLIDE: &#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#36827;&#34892;&#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#36890;&#24120;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#20248;&#20110;&#20165;&#33021;&#35775;&#38382;&#28304;&#35821;&#35328;&#21644;&#31995;&#32479;&#36755;&#20986;&#30340;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#12290;&#36825;&#24182;&#19981;&#22855;&#24618;&#65292;&#22240;&#20026;&#21442;&#32771;&#33021;&#22815;&#28040;&#38500;&#28304;&#35821;&#35328;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#29992;&#39069;&#22806;&#30340;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#26377;&#25928;&#22320;&#26367;&#20195;&#21442;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;SLIDE&#65288;SLiding Document Evaluator&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#28369;&#21160;&#31383;&#21475;&#22312;&#27599;&#20010;&#27979;&#35797;&#38598;&#20013;&#30340;&#25991;&#26723;&#19978;&#25805;&#20316;&#65292;&#23558;&#27599;&#20010;&#22359;&#36755;&#20837;&#21040;&#26410;&#20462;&#25913;&#30340;&#29616;&#25104;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SLIDE&#22312;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#25104;&#23545;&#27604;&#36739;&#19978;&#36739;&#21477;&#23376;&#32423;&#21035;&#22522;&#32447;&#26174;&#33879;&#25552;&#39640;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#28040;&#38500;&#20102;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
&lt;/p&gt;</description></item><item><title>S3-DST&#26159;&#22522;&#20110;LLM&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#32467;&#26500;&#21270;&#30340;&#23545;&#35805;&#20998;&#27573;&#21644;&#29366;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#21033;&#29992;Pre-Analytical Recollection&#26426;&#21046;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2309.08827</link><description>&lt;p&gt;
S3-DST: &#22522;&#20110;LLM&#30340;&#32467;&#26500;&#21270;&#24320;&#25918;&#22495;&#23545;&#35805;&#20998;&#27573;&#21644;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs. (arXiv:2309.08827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08827
&lt;/p&gt;
&lt;p&gt;
S3-DST&#26159;&#22522;&#20110;LLM&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#32467;&#26500;&#21270;&#30340;&#23545;&#35805;&#20998;&#27573;&#21644;&#29366;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#21033;&#29992;Pre-Analytical Recollection&#26426;&#21046;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#38382;&#39064;&#26088;&#22312;&#36861;&#36394;&#29992;&#25143;&#22312;&#29992;&#25143;-&#20195;&#29702;&#23545;&#35805;&#20013;&#30340;&#20559;&#22909;&#21644;&#24847;&#22270;&#12290;&#23613;&#31649;&#23545;&#20110;&#25903;&#25345;&#29421;&#20041;&#39046;&#22495;&#24212;&#29992;&#30340;&#20219;&#21153;&#23548;&#21521;&#24615;&#23545;&#35805;&#31995;&#32479;&#26469;&#35828;&#24050;&#32463;&#36275;&#22815;&#65292;&#20294;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#32842;&#22825;&#31995;&#32479;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#35768;&#22810;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#30340;&#29616;&#23454;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19978;&#19979;&#25991;&#20132;&#20114;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#12289;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#30340;&#24310;&#38271;&#23545;&#35805;&#20250;&#35805;&#20197;&#21450;&#26356;&#39057;&#32321;&#30340;&#19978;&#19979;&#25991;&#36716;&#21464;&#31561;&#24418;&#24335;&#12290;&#20026;&#20102;&#22788;&#29702;&#22522;&#20110;&#28436;&#21464;&#30340;LLM&#32842;&#22825;&#31995;&#32479;&#24341;&#36215;&#30340;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#27599;&#20010;&#27573;&#36827;&#34892;&#32852;&#21512;&#23545;&#35805;&#20998;&#21106;&#21644;&#29366;&#24577;&#36319;&#36394;&#12290;&#22312;&#36866;&#21512;&#30495;&#27491;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S3-DST&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#25552;&#31034;&#25216;&#26415;&#65292;&#21033;&#29992;&#20102;&#25105;&#20204;&#20026;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#36319;&#36394;&#32780;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#30340;&#25509;&#22320;&#26426;&#21046; - Pre-Analytical Recollection&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20808;&#39564;&#20559;&#31227;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#20107;&#21518;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#19978;&#36827;&#34892;&#32553;&#25918;&#35843;&#25972;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21608;&#22260;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.08825</link><description>&lt;p&gt;
&#20808;&#39564;&#20559;&#31227;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#20107;&#21518;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08825
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20808;&#39564;&#20559;&#31227;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#20107;&#21518;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#19978;&#36827;&#34892;&#32553;&#25918;&#35843;&#25972;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21608;&#22260;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27979;&#35797;&#20998;&#24067;&#20559;&#31163;&#35757;&#32451;&#20998;&#24067;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#27169;&#22411;&#20197;&#24212;&#23545;&#30001;&#31867;&#20808;&#39564;&#25110;&#32452;&#20808;&#39564;&#20998;&#24067;&#21464;&#21270;&#24341;&#36215;&#30340;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;&#23384;&#22312;&#20559;&#26012;&#30340;&#35757;&#32451;&#20808;&#39564;&#24448;&#24448;&#20250;&#23548;&#33268;&#27169;&#22411;&#23545;&#22122;&#22768;&#29305;&#24449;&#36807;&#25311;&#21512;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20248;&#21270;&#26368;&#24046;&#25110;&#24179;&#22343;&#24615;&#33021;&#65292;&#32780;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#20986;&#20110;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#36136;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#36731;&#37327;&#32423;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#32553;&#25918;&#35843;&#25972;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#36873;&#25321;&#30340;&#30446;&#26631;&#20998;&#24067;&#21608;&#22260;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#12290;&#36825;&#20123;&#35843;&#25972;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#19978;&#27714;&#35299;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#35745;&#31639;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#38388;&#24212;&#29992;&#20110;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization obje
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#20197;GPT&#20026;&#22522;&#20934;&#65292;&#25506;&#32034;&#20102;&#29616;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#30005;&#24433;&#25512;&#33616;&#35299;&#37322;&#23545;&#29992;&#25143;&#30340;&#24110;&#21161;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#35748;&#20026;&#20043;&#21069;&#30475;&#36807;&#30340;&#30005;&#24433;&#30340;&#35780;&#35770;&#26356;&#22909;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#25512;&#33616;&#35299;&#37322;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.08817</link><description>&lt;p&gt;
GPT&#20316;&#20026;&#25512;&#33616;&#35299;&#37322;&#25991;&#26412;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GPT as a Baseline for Recommendation Explanation Texts. (arXiv:2309.08817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#20197;GPT&#20026;&#22522;&#20934;&#65292;&#25506;&#32034;&#20102;&#29616;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#30005;&#24433;&#25512;&#33616;&#35299;&#37322;&#23545;&#29992;&#25143;&#30340;&#24110;&#21161;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#35748;&#20026;&#20043;&#21069;&#30475;&#36807;&#30340;&#30005;&#24433;&#30340;&#35780;&#35770;&#26356;&#22909;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#25512;&#33616;&#35299;&#37322;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#22522;&#20934;&#65292;&#25506;&#32034;&#20102;&#29616;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#30005;&#24433;&#25512;&#33616;&#35299;&#37322;&#25991;&#26412;&#23545;&#29992;&#25143;&#30340;&#24110;&#21161;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#36825;&#20123;&#25991;&#26412;&#35299;&#37322;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#21916;&#22909;&#21644;&#19981;&#21916;&#22909;&#65292;&#29305;&#21035;&#26159;&#19982;&#29616;&#26377;&#30340;&#20154;&#24037;&#30005;&#24433;&#35780;&#35770;&#30340;&#23545;&#27604;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#23545;&#30005;&#24433;&#20043;&#38388;&#30340;&#25490;&#21517;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#20063;&#27809;&#26377;&#23545;&#20182;&#20204;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#30005;&#24433;&#35780;&#35770;&#30340;&#20010;&#20307;&#36136;&#37327;&#25171;&#20986;&#26126;&#26174;&#19981;&#21516;&#30340;&#35780;&#20998;&#12290;&#28982;&#32780;&#65292;&#24403;&#35780;&#35770;&#26159;&#20182;&#20204;&#20043;&#21069;&#30475;&#36807;&#30340;&#30005;&#24433;&#26102;&#65292;&#21442;&#19982;&#32773;&#30830;&#23454;&#26631;&#35760;&#35780;&#35770;&#20026;&#26174;&#33879;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21442;&#19982;&#32773;&#35748;&#20026;&#23545;&#27599;&#20010;&#36136;&#37327;&#37325;&#35201;&#30340;&#24433;&#35780;&#25991;&#26412;&#30340;&#20855;&#20307;&#26041;&#38754;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#25512;&#33616;&#35299;&#37322;&#26469;&#28304;&#65292;&#24182;&#35745;&#21010;&#22312;&#23558;&#26469;&#36827;&#19968;&#27493;&#25506;&#32034;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we establish a baseline potential for how modern model-generated text explanations of movie recommendations may help users, and explore what different components of these text explanations that users like or dislike, especially in contrast to existing human movie reviews. We found that participants gave no significantly different rankings between movies, nor did they give significantly different individual quality scores to reviews of movies that they had never seen before. However, participants did mark reviews as significantly better when they were movies they had seen before. We also explore specific aspects of movie review texts that participants marked as important for each quality. Overall, we establish that modern LLMs are a promising source of recommendation explanations, and we intend on further exploring personalizable text explanations in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861; URA*&#65292;&#21033;&#29992;&#31354;&#20013;&#22270;&#20687;&#36827;&#34892;&#38750;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#38598;&#25104;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#31354;&#20013;&#22270;&#20687;&#36827;&#34892;&#20687;&#32032;&#32423;&#21487;&#36890;&#36807;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.08814</link><description>&lt;p&gt;
URA *&#65306;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#31354;&#20013;&#21040;&#22320;&#38754;&#21487;&#36890;&#36807;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36335;&#24452;&#35268;&#21010;&#22312;&#38750;&#36947;&#36335;&#29615;&#22659;&#20013;&#23454;&#29616;&#26080;&#20154;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
URA*: Uncertainty-aware Path Planning using Image-based Aerial-to-Ground Traversability Estimation for Off-road Environments. (arXiv:2309.08814v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861; URA*&#65292;&#21033;&#29992;&#31354;&#20013;&#22270;&#20687;&#36827;&#34892;&#38750;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#38598;&#25104;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#31354;&#20013;&#22270;&#20687;&#36827;&#34892;&#20687;&#32032;&#32423;&#21487;&#36890;&#36807;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36947;&#36335;&#33258;&#20027;&#23548;&#33322;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#26426;&#22120;&#20154;&#36335;&#24452;&#30340;&#22320;&#22270;&#25110;&#36947;&#36335;&#26631;&#24535;&#12290;&#32463;&#20856;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#22823;&#22810;&#20551;&#35774;&#24050;&#30693;&#23436;&#32654;&#30340;&#29615;&#22659;&#65292;&#32780;&#19981;&#32771;&#34385;&#22312;&#38750;&#36947;&#36335;&#29615;&#22659;&#20013;&#26816;&#27979;&#22320;&#24418;&#21644;&#38556;&#30861;&#29289;&#26102;&#22266;&#26377;&#30340;&#24863;&#30693;&#21644;&#20256;&#24863;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#20174;&#21407;&#22987;&#22270;&#20687;&#20013;&#36827;&#34892;&#22320;&#24418;&#21487;&#36890;&#36807;&#24615;&#20998;&#21106;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#22122;&#22768;&#20998;&#21106;&#22320;&#22270;&#36827;&#34892;&#23548;&#33322;&#21644;&#36335;&#24452;&#35268;&#21010;&#30340;&#21487;&#34892;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861; URA*&#65292;&#20351;&#29992;&#31354;&#20013;&#22270;&#20687;&#36827;&#34892;&#38750;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#39318;&#20808;&#20351;&#29992;&#38598;&#25104;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#27169;&#22411;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#31354;&#20013;&#22270;&#20687;&#36827;&#34892;&#20687;&#32032;&#32423;&#21487;&#36890;&#36807;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge with off-road autonomous navigation is the lack of maps or road markings that can be used to plan a path for autonomous robots. Classical path planning methods mostly assume a perfectly known environment without accounting for the inherent perception and sensing uncertainty from detecting terrain and obstacles in off-road environments. Recent work in computer vision and deep neural networks has advanced the capability of terrain traversability segmentation from raw images; however, the feasibility of using these noisy segmentation maps for navigation and path planning has not been adequately explored. To address this problem, this research proposes an uncertainty-aware path planning method, URA* using aerial images for autonomous navigation in off-road environments. An ensemble convolutional neural network (CNN) model is first used to perform pixel-level traversability estimation from aerial images of the region of interest. The traversability predictions are represen
&lt;/p&gt;</description></item><item><title>SHAPNN&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08799</link><description>&lt;p&gt;
SHAPNN: Shapley Value&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SHAPNN: Shapley Value Regularized Tabular Neural Network. (arXiv:2309.08799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08799
&lt;/p&gt;
&lt;p&gt;
SHAPNN&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SHAPNN&#65292;&#19968;&#31181;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#28145;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Shapley&#20540;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#20272;&#35745;&#30340;Shapley&#20540;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#25968;&#25454;&#38598;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#32780;&#19981;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#24102;&#26377;&#35299;&#37322;&#30340;&#39044;&#27979;&#20316;&#20026;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;&#65292;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;&#39044;&#27979;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;SHAPNN&#22312;AUROC&#12289;&#36879;&#26126;&#24615;&#20197;&#21450;&#23545;&#27969;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SHAPNN, a novel deep tabular data modeling architecture designed for supervised learning. Our approach leverages Shapley values, a well-established technique for explaining black-box models. Our neural network is trained using standard backward propagation optimization methods, and is regularized with realtime estimated Shapley values. Our method offers several advantages, including the ability to provide valid explanations with no computational overhead for data instances and datasets. Additionally, prediction with explanation serves as a regularizer, which improves the model's performance. Moreover, the regularized prediction enhances the model's capability for continual learning. We evaluate our method on various publicly available datasets and compare it with state-of-the-art deep neural network models, demonstrating the superior performance of SHAPNN in terms of AUROC, transparency, as well as robustness to streaming data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#38382;&#31572;&#20013;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#31616;&#21333;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#22312;&#23454;&#29616;&#31995;&#32479;&#19968;&#33324;&#21270;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#24517;&#25910;&#38598;&#22823;&#37327;&#21644;&#22810;&#26679;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.08798</link><description>&lt;p&gt;
D3: &#25968;&#25454;&#22810;&#26679;&#24615;&#35774;&#35745;&#20026;&#31995;&#32479;&#19968;&#33324;&#21270;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
D3: Data Diversity Design for Systematic Generalization in Visual Question Answering. (arXiv:2309.08798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#38382;&#31572;&#20013;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#31616;&#21333;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#22312;&#23454;&#29616;&#31995;&#32479;&#19968;&#33324;&#21270;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#24517;&#25910;&#38598;&#22823;&#37327;&#21644;&#22810;&#26679;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#19968;&#33324;&#21270;&#26159;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#25351;&#30340;&#26159;&#36890;&#36807;&#32467;&#21512;&#24050;&#30693;&#30340;&#23376;&#20219;&#21153;&#21644;&#27010;&#24565;&#26469;&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#24050;&#32463;&#26174;&#31034;&#24433;&#21709;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22810;&#26679;&#24615;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#22240;&#20026;&#25968;&#25454;&#20855;&#26377;&#35768;&#22810;&#21464;&#21270;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#26041;&#38754;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#22914;&#20309;&#24433;&#21709;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#26356;&#32454;&#33268;&#30340;&#29702;&#35299;&#23578;&#32570;&#20047;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#26032;&#30340;&#35777;&#25454;&#65292;&#25581;&#31034;&#20102;&#31616;&#21333;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65288;&#21363;&#30001;&#20960;&#20010;&#23376;&#20219;&#21153;&#21644;&#27010;&#24565;&#32452;&#25104;&#30340;&#20219;&#21153;&#65289;&#22312;&#23454;&#29616;&#31995;&#32479;&#19968;&#33324;&#21270;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#24847;&#21619;&#30528;&#25910;&#38598;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#24182;&#38750;&#24517;&#35201;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#19982;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26080;&#20851;&#65292;&#24182;&#36866;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic generalization is a crucial aspect of intelligence, which refers to the ability to generalize to novel tasks by combining known subtasks and concepts. One critical factor that has been shown to influence systematic generalization is the diversity of training data. However, diversity can be defined in various ways, as data have many factors of variation. A more granular understanding of how different aspects of data diversity affect systematic generalization is lacking. We present new evidence in the problem of Visual Question Answering (VQA) that reveals that the diversity of simple tasks (i.e. tasks formed by a few subtasks and concepts) plays a key role in achieving systematic generalization. This implies that it may not be essential to gather a large and varied number of complex tasks, which could be costly to obtain. We demonstrate that this result is independent of the similarity between the training and testing data and applies to well-known families of neural network 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#30315;&#30187;&#21457;&#20316;&#26089;&#26399;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20809;&#27969;&#29305;&#24449;&#24182;&#20351;&#29992;&#36880;&#27493;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#21644;&#23454;&#26102;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08794</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#30315;&#30187;&#21457;&#20316;&#26089;&#26399;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving Early Detection of Epileptic Seizures in Videos. (arXiv:2309.08794v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#30315;&#30187;&#21457;&#20316;&#26089;&#26399;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20809;&#27969;&#29305;&#24449;&#24182;&#20351;&#29992;&#36880;&#27493;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#21644;&#23454;&#26102;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65288;SETR-PKD&#65289;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#30315;&#30187;&#21457;&#20316;&#20998;&#31867;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#39057;&#20013;&#30315;&#30187;&#21457;&#20316;&#30340;&#38544;&#31169;&#20445;&#25252;&#26089;&#26399;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998; - (1) &#23427;&#26159;&#22522;&#20110;&#20174;&#30315;&#30187;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#20809;&#27969;&#29305;&#24449;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#29305;&#24449;&#32534;&#30721;&#20102;&#30315;&#30187;&#36816;&#21160;&#31526;&#21495;&#23398;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#24739;&#32773;&#30340;&#38544;&#31169;&#65307;(2) &#23427;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#36880;&#27493;&#30693;&#35782;&#33976;&#39311;&#65292;&#36880;&#28176;&#20174;&#38271;&#35270;&#39057;&#26679;&#26412;&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#33976;&#39311;&#20986;&#30693;&#35782;&#65292;&#28982;&#21518;&#20256;&#36882;&#32473;&#23558;&#22312;&#36739;&#30701;&#35270;&#39057;&#26679;&#26412;&#19978;&#25805;&#20316;&#30340;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35299;&#20915;&#20102;&#30446;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#23545;&#30315;&#30187;&#35270;&#39057;&#36827;&#34892;&#25805;&#20316;&#20197;&#21450;&#21033;&#29992;&#23436;&#25972;&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#39044;&#27979;&#26469;&#25439;&#23475;&#24739;&#32773;&#30340;&#38544;&#31169;&#65292;&#24182;&#38459;&#30861;&#20102;&#30315;&#30187;&#21457;&#20316;&#30340;&#23454;&#26102;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;SETR-PKD&#26694;&#26550;&#21487;&#20197;&#26816;&#27979;&#21040;
&lt;/p&gt;
&lt;p&gt;
In this work, we contribute towards the development of video-based epileptic seizure classification by introducing a novel framework (SETR-PKD), which could achieve privacy-preserved early detection of seizures in videos. Specifically, our framework has two significant components - (1) It is built upon optical flow features extracted from the video of a seizure, which encodes the seizure motion semiotics while preserving the privacy of the patient; (2) It utilizes a transformer based progressive knowledge distillation, where the knowledge is gradually distilled from networks trained on a longer portion of video samples to the ones which will operate on shorter portions. Thus, our proposed framework addresses the limitations of the current approaches which compromise the privacy of the patients by directly operating on the RGB video of a seizure as well as impede real-time detection of a seizure by utilizing the full video sample to make a prediction. Our SETR-PKD framework could detect
&lt;/p&gt;</description></item><item><title>Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.08793</link><description>&lt;p&gt;
Fin-Fact:&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08793
&lt;/p&gt;
&lt;p&gt;
Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#20107;&#23454;&#26680;&#26597;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fin-Fact&#65292;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21253;&#25324;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#25552;&#20379;&#19987;&#19994;&#30693;&#35782;&#21644;&#21487;&#20449;&#24230;&#12290;&#30001;&#20110;&#20854;&#22810;&#27169;&#24577;&#24615;&#36136;&#28085;&#30422;&#20102;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#65292;Fin-Fact&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#28304;&#65292;&#20197;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#37329;&#34701;&#39046;&#22495;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#22312;&#36130;&#21153;&#25253;&#21578;&#21644;&#26032;&#38395;&#20256;&#25773;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#36890;&#36807;&#25552;&#20379;&#26377;&#28145;&#24230;&#30340;&#35299;&#37322;&#65292;Fin-Fact&#20351;&#29992;&#25143;&#65292;&#21253;&#25324;&#39046;&#22495;&#19987;&#23478;&#21644;&#32456;&#31471;&#29992;&#25143;&#65292;&#33021;&#22815;&#29702;&#35299;&#20107;&#23454;&#26680;&#26597;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#39564;&#35777;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#20419;&#36827;&#23545;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#30340;&#20449;&#20219;&#12290;Fin-Fact&#25968;&#25454;&#38598;&#20197;&#21450;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;https://github.com/IIT-DM/Fin-Fact/ &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08776</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Projected Task-Specific Layers
&lt;/p&gt;
&lt;p&gt;
Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#23478;&#24237;&#21644;&#24037;&#20316;&#22330;&#25152;&#30340;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#24182;&#20943;&#36731;&#36127;&#38754;&#20219;&#21153;&#24178;&#25200;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25104;&#21151;&#22320;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#23558;&#21462;&#20915;&#20110;&#23545;&#20219;&#21153;&#24213;&#23618;&#32467;&#26500;&#30340;&#26377;&#25928;&#25429;&#25417;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#21363;Projected Task-Specific Layers&#65288;PTSL&#65289;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#65292;&#36890;&#36807;&#31264;&#23494;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#20462;&#27491;&#26469;&#26356;&#22909;&#22320;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Meta-World&#30340;MT10&#21644;MT50&#22522;&#20934;&#20013;&#65288;&#21253;&#25324;Sawyer&#26426;&#22120;&#20154;&#33218;&#19978;&#30340;10&#20010;&#21644;50&#20010;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#65289;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#35843;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38899;&#20048;&#21644;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08773</link><description>&lt;p&gt;
&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#21487;&#25511;&#24615;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#34920;&#31034;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhance audio generation controllability through representation similarity regularization. (arXiv:2309.08773v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08773
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#35843;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38899;&#20048;&#21644;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#24378;&#35843;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#22686;&#24378;&#23545;&#38899;&#39057;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#39057;&#29983;&#25104;&#20013;&#65292;&#27169;&#22411;&#21033;&#29992;&#26469;&#33258;&#25991;&#26412;&#21644;&#38899;&#39057;&#26631;&#35760;&#34920;&#31034;&#30340;&#36755;&#20837;&#26469;&#39044;&#27979;&#21518;&#32493;&#30340;&#38899;&#39057;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37197;&#32622;&#32570;&#20047;&#26126;&#30830;&#30340;&#27491;&#21017;&#21270;&#26469;&#30830;&#20445;&#25152;&#36873;&#25321;&#30340;&#25991;&#26412;&#34920;&#31034;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#28041;&#21450;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#25972;&#21512;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#38454;&#27573;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25991;&#26412;&#26465;&#20214;&#34987;&#25490;&#38500;&#22312;&#36328;&#27880;&#24847;&#21147;&#20043;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#30446;&#30340;&#26159;&#26368;&#23567;&#21270;&#19982;&#21516;&#19968;&#35757;&#32451;&#25209;&#27425;&#20013;&#20854;&#20182;&#26679;&#26412;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#30456;&#20284;&#24615;&#24046;&#24322;&#12290;&#22312;&#38899;&#20048;&#21644;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#32972;&#26223;&#20026;&#37325;&#28857;&#30340;&#20998;&#24067;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#23454;&#20363;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#30340;&#21069;&#26223;&#32972;&#26223;&#23545;&#40784;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08771</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#36328;&#39046;&#22495;&#34892;&#20154;&#26816;&#27979;&#65306;&#19968;&#20010;&#20197;&#32972;&#26223;&#20026;&#37325;&#28857;&#30340;&#20998;&#24067;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26080;&#23454;&#20363;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Cross-Domain Pedestrian Detection: A Background-Focused Distribution Alignment Framework for Instance-Free One-Stage Detectors. (arXiv:2309.08771v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#32972;&#26223;&#20026;&#37325;&#28857;&#30340;&#20998;&#24067;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#23454;&#20363;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#30340;&#21069;&#26223;&#32972;&#26223;&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#34892;&#20154;&#26816;&#27979;&#26088;&#22312;&#23558;&#34892;&#20154;&#26816;&#27979;&#22120;&#20174;&#19968;&#20010;&#26631;&#31614;&#20016;&#23500;&#30340;&#39046;&#22495;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#26631;&#31614;&#31232;&#32570;&#30340;&#39046;&#22495;&#65292;&#36825;&#23545;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#22495;&#23545;&#40784;&#65292;&#20197;&#22312;&#23454;&#20363;&#32423;&#21035;&#25110;&#22270;&#20687;&#32423;&#21035;&#35757;&#32451;&#22495;&#33258;&#36866;&#24212;&#26816;&#27979;&#22120;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#26356;&#24555;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#19968;&#31181;&#38024;&#23545;&#24555;&#36895;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#30340;&#36328;&#39046;&#22495;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32570;&#20047;&#23454;&#20363;&#32423;&#21035;&#30340;&#25552;&#35758;&#65292;&#24182;&#19988;&#21482;&#33021;&#25191;&#34892;&#22270;&#20687;&#32423;&#21035;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#32431;&#22270;&#20687;&#32423;&#29305;&#24449;&#23545;&#40784;&#20250;&#23548;&#33268;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#20063;&#23601;&#26159;&#28304;&#39046;&#22495;&#22270;&#20687;&#20013;&#30340;&#21069;&#26223;&#29305;&#24449;&#38169;&#35823;&#22320;&#19982;&#30446;&#26631;&#39046;&#22495;&#22270;&#20687;&#20013;&#30340;&#32972;&#26223;&#29305;&#24449;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#22270;&#20687;&#32423;&#36328;&#39046;&#22495;&#23545;&#40784;&#20013;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#32972;&#26223;&#22312;&#22270;&#20687;&#32423;&#36328;&#39046;&#22495;&#23545;&#40784;&#20013;&#36215;&#30528;&#26356;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain pedestrian detection aims to generalize pedestrian detectors from one label-rich domain to another label-scarce domain, which is crucial for various real-world applications. Most recent works focus on domain alignment to train domain-adaptive detectors either at the instance level or image level. From a practical point of view, one-stage detectors are faster. Therefore, we concentrate on designing a cross-domain algorithm for rapid one-stage detectors that lacks instance-level proposals and can only perform image-level feature alignment. However, pure image-level feature alignment causes the foreground-background misalignment issue to arise, i.e., the foreground features in the source domain image are falsely aligned with background features in the target domain image. To address this issue, we systematically analyze the importance of foreground and background in image-level cross-domain alignment, and learn that background plays a more critical role in image-level cross-d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;AlbNER&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#24433;&#21709;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#26377;&#30528;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#36164;&#28304;&#21644;&#32467;&#26524;&#20026;&#26410;&#26469;&#23454;&#39564;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.08741</link><description>&lt;p&gt;
AlbNER&#65306;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AlbNER: A Corpus for Named Entity Recognition in Albanian. (arXiv:2309.08741v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;AlbNER&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#24433;&#21709;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#26377;&#30528;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#36164;&#28304;&#21644;&#32467;&#26524;&#20026;&#26410;&#26469;&#23454;&#39564;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38463;&#23572;&#24052;&#23612;&#20122;&#31561;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#22914;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#23384;&#22312;&#30528;&#26631;&#27880;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20005;&#37325;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AlbNER&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#12290;&#29992;&#20351;&#29992;AlbNER&#25968;&#25454;&#36827;&#34892;&#32454;&#35843;&#21644;&#27979;&#35797;&#30340;BERT&#21644;RoBERTa&#21464;&#20307;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;AlbNER&#35821;&#26009;&#24211;&#21644;&#36825;&#20123;&#32467;&#26524;&#24212;&#20316;&#20026;&#26410;&#26469;&#23454;&#39564;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of resources such as annotated text corpora for under-resourced languages like Albanian is a serious impediment in computational linguistics and natural language processing research. This paper presents AlbNER, a corpus of 900 sentences with labeled named entities, collected from Albanian Wikipedia articles. Preliminary results with BERT and RoBERTa variants fine-tuned and tested with AlbNER data indicate that model size has slight impact on NER performance, whereas language transfer has a significant one. AlbNER corpus and these obtained results should serve as baselines for future experiments.
&lt;/p&gt;</description></item><item><title>MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08730</link><description>&lt;p&gt;
MusiLingo&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#38899;&#20048;&#23383;&#24149;&#21644;&#26597;&#35810;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08730
&lt;/p&gt;
&lt;p&gt;
MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#25991;&#26412;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#34701;&#21512;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MusiLingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#21644;&#38899;&#20048;&#30456;&#20851;&#26597;&#35810;&#21709;&#24212;&#30340;&#26032;&#31995;&#32479;&#12290;MusiLingo&#20351;&#29992;&#19968;&#20010;&#25237;&#24433;&#23618;&#26469;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#38899;&#20048;&#38899;&#39057;&#27169;&#22411;MERT&#21644;&#20923;&#32467;&#30340;LLaMA&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#23454;&#29616;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25351;&#23548;&#24615;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#38382;&#31572;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25105;&#20204;&#20174;MusicCaps&#21019;&#24314;&#20102;MusicInstruct&#65288;MI&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#24320;&#25918;&#24335;&#38899;&#20048;&#26597;&#35810;&#32780;&#35774;&#35745;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#32452;&#32455;&#38899;&#20048;&#30456;&#20851;&#38382;&#31572;&#23545;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#22312;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#29366;&#24577;&#34920;&#31034;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28857;&#20113;&#37325;&#26500;Transformer&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#39044;&#27979;&#26448;&#26009;&#21464;&#24418;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21160;&#20316;&#37319;&#26679;&#31639;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#35268;&#21010;&#22120;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2309.08728</link><description>&lt;p&gt;
SculptBot: 3D&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SculptBot: Pre-Trained Models for 3D Deformable Object Manipulation. (arXiv:2309.08728v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#29366;&#24577;&#34920;&#31034;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28857;&#20113;&#37325;&#26500;Transformer&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#39044;&#27979;&#26448;&#26009;&#21464;&#24418;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21160;&#20316;&#37319;&#26679;&#31639;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#35268;&#21010;&#22120;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#38754;&#20020;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#22914;&#39640;&#33258;&#30001;&#24230;&#21644;&#33258;&#36974;&#25377;&#12290;&#32780;&#23545;&#20110;&#34920;&#29616;&#22609;&#24615;&#34892;&#20026;&#30340;&#26448;&#26009;&#65288;&#22914;&#40655;&#22303;&#25110;&#38754;&#22242;&#65289;&#65292;&#20854;&#29366;&#24577;&#34920;&#31034;&#20063;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21463;&#21147;&#19979;&#20250;&#27704;&#20037;&#21464;&#24418;&#24182;&#19981;&#26029;&#25913;&#21464;&#24418;&#29366;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#34892;&#22841;&#20855;&#36827;&#34892;&#26426;&#22120;&#20154;&#38613;&#21051;&#30340;&#20219;&#21153;&#26469;&#30740;&#31350;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28857;&#20113;&#37325;&#26500;Transformer&#23398;&#20064;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#26448;&#26009;&#22312;&#22841;&#21462;&#34892;&#20026;&#19979;&#30340;&#21464;&#24418;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#20316;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#28857;&#20113;&#20043;&#38388;&#30340;&#20960;&#20309;&#24046;&#24322;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#22120;&#30340;&#25928;&#29575;&#12290;&#25152;&#26377;&#25968;&#25454;&#21644;&#23454;&#39564;&#22343;&#23436;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Deformable object manipulation presents a unique set of challenges in robotic manipulation by exhibiting high degrees of freedom and severe self-occlusion. State representation for materials that exhibit plastic behavior, like modeling clay or bread dough, is also difficult because they permanently deform under stress and are constantly changing shape. In this work, we investigate each of these challenges using the task of robotic sculpting with a parallel gripper. We propose a system that uses point clouds as the state representation and leverages pre-trained point cloud reconstruction Transformer to learn a latent dynamics model to predict material deformations given a grasp action. We design a novel action sampling algorithm that reasons about geometrical differences between point clouds to further improve the efficiency of model-based planners. All data and experiments are conducted entirely in the real world. Our experiments show the proposed system is able to successfully capture
&lt;/p&gt;</description></item><item><title>SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.08698</link><description>&lt;p&gt;
&#26080;&#38656;&#25554;&#20540;&#30340;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Modelling Irregularly Sampled Time Series Without Imputation. (arXiv:2309.08698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08698
&lt;/p&gt;
&lt;p&gt;
SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65288;ISTS&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36716;&#25442;&#20026;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#26469;&#22788;&#29702;ISTS&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#23384;&#22312;&#28508;&#22312;&#30340;&#32570;&#22833;&#26426;&#21046;&#65292;&#23548;&#33268;&#20102;&#19981;&#24076;&#26395;&#30340;&#20559;&#24046;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SLAN&#65288;Switch LSTM Aggregate Network&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#32452;LSTM&#23545;ISTS&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#26080;&#38656;&#25554;&#20540;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#28508;&#22312;&#36807;&#31243;&#30340;&#20551;&#35774;&#12290;&#23427;&#26681;&#25454;&#27979;&#37327;&#20256;&#24863;&#22120;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20854;&#26550;&#26500;&#12290;SLAN&#21033;&#29992;&#19981;&#35268;&#21017;&#24615;&#20449;&#24687;&#26126;&#30830;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SLAN&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;MIMIC-III&#12289;Physionet 2012&#21644;Physionet 2019&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Rohit102497/SLAN&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling irregularly-sampled time series (ISTS) is challenging because of missing values. Most existing methods focus on handling ISTS by converting irregularly sampled data into regularly sampled data via imputation. These models assume an underlying missing mechanism leading to unwanted bias and sub-optimal performance. We present SLAN (Switch LSTM Aggregate Network), which utilizes a pack of LSTMs to model ISTS without imputation, eliminating the assumption of any underlying process. It dynamically adapts its architecture on the fly based on the measured sensors. SLAN exploits the irregularity information to capture each sensor's local summary explicitly and maintains a global summary state throughout the observational period. We demonstrate the efficacy of SLAN on publicly available datasets, namely, MIMIC-III, Physionet 2012 and Physionet 2019. The code is available at https://github.com/Rohit102497/SLAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#24448;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#26032;&#30340;&#27861;&#24237;&#21028;&#20915;&#26631;&#27880;&#25968;&#25454;&#29992;&#20110;&#25913;&#36827;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;86.7&#65285;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.08695</link><description>&lt;p&gt;
&#35299;&#20915;&#27861;&#24459;&#26415;&#35821;&#65306;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents. (arXiv:2309.08695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#24448;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#26032;&#30340;&#27861;&#24237;&#21028;&#20915;&#26631;&#27880;&#25968;&#25454;&#29992;&#20110;&#25913;&#36827;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;86.7&#65285;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21477;&#23376;&#20013;&#35299;&#26512;&#21542;&#23450;&#30340;&#33539;&#22260;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#32570;&#20047;&#32463;&#36807;&#27880;&#37322;&#30340;&#39046;&#22495;&#20869;&#21542;&#23450;&#35821;&#26009;&#24211;&#32473;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#19978;&#30340;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#26102;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#20808;&#26410;&#20351;&#29992;&#27861;&#24459;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20165;&#22312;&#25991;&#23398;&#25991;&#26412;&#21644;&#21307;&#23398;&#25968;&#25454;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20043;&#21069;&#30340;&#36328;&#39046;&#22495;&#23454;&#39564;&#20013;&#35760;&#24405;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#30340;&#26631;&#27880;&#27861;&#38498;&#21028;&#20915;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#25913;&#36827;&#38646;&#25668;&#21462;&#21644;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#38646;&#25668;&#21462;&#36328;&#35821;&#35328;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#36798;&#21040;&#20102;86.7&#65285;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#27861;&#24459;&#25968;&#25454;&#38598;&#30340;&#20004;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#31532;&#19977;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resolving the scope of a negation within a sentence is a challenging NLP task. The complexity of legal texts and the lack of annotated in-domain negation corpora pose challenges for state-of-the-art (SotA) models when performing negation scope resolution on multilingual legal data. Our experiments demonstrate that models pre-trained without legal data underperform in the task of negation scope resolution. Our experiments, using language models exclusively fine-tuned on domains like literary texts and medical data, yield inferior results compared to the outcomes documented in prior cross-domain experiments. We release a new set of annotated court decisions in German, French, and Italian and use it to improve negation scope resolution in both zero-shot and multilingual settings. We achieve token-level F1-scores of up to 86.7% in our zero-shot cross-lingual experiments, where the models are trained on two languages of our legal datasets and evaluated on the third. Our multilingual experim
&lt;/p&gt;</description></item><item><title>&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#20542;&#21521;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#26631;&#35760;&#20026;&#20551;&#26032;&#38395;&#65292;&#32780;&#23558;&#20154;&#24037;&#32534;&#20889;&#30340;&#20551;&#26032;&#38395;&#35823;&#20998;&#31867;&#20026;&#30495;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#21644;LLM&#25913;&#20889;&#30340;&#30495;&#23454;&#26032;&#38395;&#31561;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08674</link><description>&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fake News Detectors are Biased against Texts Generated by Large Language Models. (arXiv:2309.08674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08674
&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#20542;&#21521;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#26631;&#35760;&#20026;&#20551;&#26032;&#38395;&#65292;&#32780;&#23558;&#20154;&#24037;&#32534;&#20889;&#30340;&#20551;&#26032;&#38395;&#35823;&#20998;&#31867;&#20026;&#30495;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#21644;LLM&#25913;&#20889;&#30340;&#30495;&#23454;&#26032;&#38395;&#31561;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#20256;&#25773;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25439;&#23475;&#20102;&#20449;&#20219;&#24182;&#23545;&#31038;&#20250;&#26500;&#25104;&#23041;&#32961;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26102;&#20195;&#65292;&#29983;&#25104;&#21487;&#20449;&#30340;&#20551;&#20869;&#23481;&#30340;&#33021;&#21147;&#21152;&#21095;&#20102;&#36825;&#20123;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#35780;&#20272;&#22312;&#28041;&#21450;&#20154;&#24037;&#32534;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#35768;&#22810;&#29616;&#26377;&#26816;&#27979;&#22120;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#65306;&#23427;&#20204;&#26356;&#23481;&#26131;&#23558;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#26631;&#35760;&#20026;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#24120;&#24120;&#23558;&#20154;&#24037;&#32534;&#20889;&#30340;&#20551;&#26032;&#38395;&#35823;&#20998;&#31867;&#20026;&#30495;&#23454;&#12290;&#36825;&#31181;&#24847;&#22806;&#30340;&#20559;&#35265;&#20284;&#20046;&#28304;&#33258;LLM&#36755;&#20986;&#22266;&#26377;&#30340;&#19981;&#21516;&#35821;&#35328;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#25913;&#20889;&#30340;&#30495;&#23454;&#26032;&#38395;&#36827;&#34892;&#25932;&#23545;&#35757;&#32451;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;&#32467;&#26524;&#27169;&#22411;&#22312;&#20154;&#24037;&#21644;LLM&#29983;&#25104;&#30340;&#26032;&#38395;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;...
&lt;/p&gt;
&lt;p&gt;
The spread of fake news has emerged as a critical challenge, undermining trust and posing threats to society. In the era of Large Language Models (LLMs), the capability to generate believable fake content has intensified these concerns. In this study, we present a novel paradigm to evaluate fake news detectors in scenarios involving both human-written and LLM-generated misinformation. Intriguingly, our findings reveal a significant bias in many existing detectors: they are more prone to flagging LLM-generated content as fake news while often misclassifying human-written fake news as genuine. This unexpected bias appears to arise from distinct linguistic patterns inherent to LLM outputs. To address this, we introduce a mitigation strategy that leverages adversarial training with LLM-paraphrased genuine news. The resulting model yielded marked improvements in detection accuracy for both human and LLM-generated news. To further catalyze research in this domain, we release two comprehensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#21253;&#21547;&#23454;&#20307;&#20132;&#25442;&#30340;&#34920;&#26684;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;&#20219;&#21153;&#30340;&#36867;&#36991;&#24615;&#23454;&#20307;&#20132;&#25442;&#25915;&#20987;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25104;&#21151;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#20102;&#39640;&#36798;70%&#12290;</title><link>http://arxiv.org/abs/2309.08650</link><description>&lt;p&gt;
&#23545;&#21253;&#21547;&#23454;&#20307;&#20132;&#25442;&#30340;&#34920;&#26684;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Tables with Entity Swap. (arXiv:2309.08650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#21253;&#21547;&#23454;&#20307;&#20132;&#25442;&#30340;&#34920;&#26684;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;&#20219;&#21153;&#30340;&#36867;&#36991;&#24615;&#23454;&#20307;&#20132;&#25442;&#25915;&#20987;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25104;&#21151;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#20102;&#39640;&#36798;70%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33021;&#21147;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#35299;&#37322;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#24120;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20180;&#32454;&#35266;&#23519;&#21457;&#29616;&#65292;&#35757;&#32451;&#38598;&#20013;&#30340;&#23454;&#20307;&#27844;&#28431;&#33267;&#27979;&#35797;&#38598;&#20013;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#30495;&#23454;&#30340;&#25512;&#29702;&#35774;&#32622;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#23545;&#25991;&#26412;&#30340;&#23545;&#25239;&#25915;&#20987;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#25915;&#20987;&#38024;&#23545;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;(CTA)&#20219;&#21153;&#30340;&#36867;&#36991;&#24615;&#23454;&#20307;&#20132;&#25442;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;CTA&#25915;&#20987;&#26159;&#23545;&#34920;&#26684;&#30340;&#31532;&#19968;&#27425;&#40657;&#30418;&#25915;&#20987;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#20102;&#39640;&#36798;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of large language models (LLMs) have been successfully applied in the context of table representation learning. The recently proposed tabular language models have reported state-of-the-art results across various tasks for table interpretation. However, a closer look into the datasets commonly used for evaluation reveals an entity leakage from the train set into the test set. Motivated by this observation, we explore adversarial attacks that represent a more realistic inference setup. Adversarial attacks on text have been shown to greatly affect the performance of LLMs, but currently, there are no attacks targeting tabular language models. In this paper, we propose an evasive entity-swap attack for the column type annotation (CTA) task. Our CTA attack is the first black-box attack on tables, where we employ a similarity-based sampling strategy to generate adversarial examples. The experimental results show that the proposed attack generates up to a 70% drop in performan
&lt;/p&gt;</description></item><item><title>MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08648</link><description>&lt;p&gt;
MAPLE: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08648
&lt;/p&gt;
&lt;p&gt;
MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#65292;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)&#27169;&#22411;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20934;&#30830;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;MAPLE&#30340;&#33021;&#21147;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;MAPLE&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#24377;&#24615;&#12290;&#23613;&#31649;&#20854;&#20027;&#35201;&#35774;&#35745;&#38754;&#21521;&#24212;&#29992;&#39044;&#27979;&#65292;&#20294;&#32467;&#26524;&#20063;&#24378;&#35843;&#20102;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLM&#22312;&#24212;&#29992;&#20351;&#29992;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24314;&#35758;&#22312;&#24314;&#27169;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#65292;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#29992;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#30340;&#30456;&#20851;&#24847;&#22270;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#23558;&#24847;&#22270;&#26816;&#27979;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#23458;&#25143;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#23569;&#20102;&#22521;&#35757;&#21644;&#32500;&#25252;&#25104;&#26412;&#65292;&#21516;&#26102;&#20026;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08647</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#24847;&#22270;&#26816;&#27979;&#65306;&#21033;&#29992;&#30456;&#20851;&#24847;&#22270;&#36827;&#34892;&#36890;&#29992;&#27169;&#22411;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Intent Detection at Scale: Tuning a Generic Model using Relevant Intents. (arXiv:2309.08647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#29992;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#30340;&#30456;&#20851;&#24847;&#22270;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#23558;&#24847;&#22270;&#26816;&#27979;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#23458;&#25143;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#23569;&#20102;&#22521;&#35757;&#21644;&#32500;&#25252;&#25104;&#26412;&#65292;&#21516;&#26102;&#20026;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#23458;&#25143;&#25903;&#25345;&#35831;&#27714;&#30340;&#24847;&#22270;&#23545;&#20110;&#39640;&#25928;&#30340;&#25903;&#25345;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#24555;&#36895;&#29702;&#35299;&#20449;&#24687;&#24182;&#20248;&#20808;&#21709;&#24212;&#12290;&#23613;&#31649;&#23384;&#22312;&#19981;&#21516;&#30340;&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26159;&#38543;&#30528;&#23458;&#25143;&#32676;&#20307;&#30340;&#25193;&#22823;&#65292;&#32500;&#25252;&#21333;&#29420;&#30340;&#23458;&#25143;&#29305;&#23450;&#25110;&#34892;&#19994;&#29305;&#23450;&#27169;&#22411;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#23558;&#24847;&#22270;&#39044;&#27979;&#25193;&#23637;&#21040;&#21508;&#31181;&#23458;&#25143;&#30340;&#31995;&#32479;&#65292;&#21363;&#36890;&#36807;&#23558;&#21333;&#19968;&#36890;&#29992;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#30340;&#30456;&#20851;&#24847;&#22270;&#21015;&#34920;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#22521;&#35757;&#21644;&#32500;&#25252;&#25104;&#26412;&#65292;&#21516;&#26102;&#20026;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#65292;&#23454;&#29616;&#23545;&#20854;&#30456;&#20851;&#24847;&#22270;&#30340;&#21464;&#21270;&#30340;&#26080;&#32541;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23458;&#25143;&#30456;&#20851;&#24847;&#22270;&#20316;&#20026;&#27169;&#22411;&#29305;&#24449;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23545;&#23458;&#25143;&#30456;&#20851;&#24847;&#22270;&#30340;&#21464;&#21270;&#20855;&#26377;&#38887;&#24615;&#12290;&#26368;&#32456;&#31995;&#32479;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the intent of customer support requests is vital for efficient support systems, enabling agents to quickly understand messages and prioritize responses accordingly. While different approaches exist for intent detection, maintaining separate client-specific or industry-specific models can be costly and impractical as the client base expands.  This work proposes a system to scale intent predictions to various clients effectively, by combining a single generic model with a per-client list of relevant intents. Our approach minimizes training and maintenance costs while providing a personalized experience for clients, allowing for seamless adaptation to changes in their relevant intents. Furthermore, we propose a strategy for using the clients relevant intents as model features that proves to be resilient to changes in the relevant intents of clients -- a common occurrence in production environments.  The final system exhibits significantly superior performance compare
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#33021;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#21644;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#24494;&#35843;&#26469;&#35299;&#20915;&#25968;&#25454;&#27874;&#21160;&#12289;&#27169;&#22411;&#24046;&#24322;&#21644;&#29615;&#22659;&#25200;&#21160;&#31561;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08642</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#20013;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#33021;&#28304;&#35843;&#24230;&#30340;&#38543;&#26426;&#22312;&#32447;&#39044;&#27979;&#19982;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Stochastic Online Forecast-and-Optimize Framework for Real-Time Energy Dispatch in Virtual Power Plants under Uncertainty. (arXiv:2309.08642v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#33021;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#21644;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#24494;&#35843;&#26469;&#35299;&#20915;&#25968;&#25454;&#27874;&#21160;&#12289;&#27169;&#22411;&#24046;&#24322;&#21644;&#29615;&#22659;&#25200;&#21160;&#31561;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#32858;&#21512;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#26174;&#33879;&#22686;&#21152;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#21487;&#20877;&#29983;&#33021;&#28304;&#20135;&#29983;&#30340;&#27874;&#21160;&#25152;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#39537;&#20351;&#20102;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#24191;&#27867;&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#27979;&#25511;&#21046;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#38271;&#26399;&#32463;&#27982;&#21644;&#20943;&#30899;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33021;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30001;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#32452;&#25104;&#65306;(i) &#19968;&#20010;&#28151;&#21512;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#39034;&#24207;&#20219;&#21153;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#21644;&#38543;&#26426;&#20248;&#21270;&#36827;&#34892;&#25972;&#21512;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#36890;&#36807;&#22810;&#20010;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36827;&#34892;&#36830;&#25509;&#65307;(ii) &#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#21516;&#26102;&#28041;&#21450;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#38454;&#27573;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#23454;&#26102;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#38024;&#23545;&#25968;&#25454;&#28418;&#31227;&#12289;&#27169;&#22411;&#24046;&#24322;&#21644;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aggregating distributed energy resources in power systems significantly increases uncertainties, in particular caused by the fluctuation of renewable energy generation. This issue has driven the necessity of widely exploiting advanced predictive control techniques under uncertainty to ensure long-term economics and decarbonization. In this paper, we propose a real-time uncertainty-aware energy dispatch framework, which is composed of two key elements: (i) A hybrid forecast-and-optimize sequential task, integrating deep learning-based forecasting and stochastic optimization, where these two stages are connected by the uncertainty estimation at multiple temporal resolutions; (ii) An efficient online data augmentation scheme, jointly involving model pre-training and online fine-tuning stages. In this way, the proposed framework is capable to rapidly adapt to the real-time data distribution, as well as to target on uncertainties caused by data drift, model discrepancy and environment pertu
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08636</link><description>&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;&#31532;23&#23395;&#31532;3&#23395;&#65289;&#12290;&#65288;arXiv:2309.08636v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#19978;&#65292;&#29087;&#32451;&#30340;&#20889;&#20316;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#36827;&#27493;&#30340;&#20851;&#38190;&#65292;&#21019;&#36896;&#24615;&#34920;&#36798;&#34987;&#35270;&#20026;&#20154;&#31867;&#25104;&#23601;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#26631;&#24535;&#30528;&#36825;&#19968;&#21465;&#20107;&#30340;&#19968;&#20010;&#36716;&#25240;&#28857;&#65292;&#21253;&#25324;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#26041;&#38754;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#22522;&#20110;&#30001;&#20154;&#31867;&#19987;&#23478;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#23450;&#37327;&#20934;&#30830;&#24615;&#21644;&#23450;&#24615;&#31934;&#30830;&#24615;&#26631;&#35760;&#12290;&#23450;&#37327;&#20934;&#30830;&#24615;&#35780;&#20272;&#20102;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#23450;&#24615;&#31934;&#30830;&#24615;&#35780;&#20272;&#20102;&#31185;&#23398;&#36129;&#29486;&#12290;&#34429;&#28982;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;ChatGPT-4&#65292;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24615;&#65292;&#20294;&#22312;&#29983;&#25104;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#20102;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#38543;&#30528;ChatGPT-4&#65292;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#24050;&#32463;&#20572;&#28382;&#19981;&#21069;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#22797;&#26434;&#19988;&#21453;&#22797;&#26080;&#24120;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#39640;&#32500;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#38382;&#39064;&#65292;&#36890;&#36807;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#27169;&#22411;&#25429;&#25417;&#21327;&#21464;&#37327;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#20860;&#23481;&#22810;&#31181;&#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#21644;&#23450;&#20215;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#34892;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.08634</link><description>&lt;p&gt;
&#21452;&#39640;&#32500;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#29992;&#20110;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing. (arXiv:2309.08634v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#39640;&#32500;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#38382;&#39064;&#65292;&#36890;&#36807;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#27169;&#22411;&#25429;&#25417;&#21327;&#21464;&#37327;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#20860;&#23481;&#22810;&#31181;&#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#21644;&#23450;&#20215;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#34892;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21806;&#19994;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#26159;&#22914;&#20309;&#36873;&#25321;&#35201;&#21521;&#28040;&#36153;&#32773;&#23637;&#31034;&#30340;&#20135;&#21697;&#65288;&#32452;&#21512;&#38382;&#39064;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#23450;&#20215;&#20135;&#21697;&#65288;&#23450;&#20215;&#38382;&#39064;&#65289;&#20197;&#26368;&#22823;&#21270;&#25910;&#20837;&#25110;&#21033;&#28070;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#32452;&#21512;&#21644;&#23450;&#20215;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21452;&#39640;&#32500;&#30340;&#65292;&#21363;&#19978;&#19979;&#25991;&#21521;&#37327;&#21644;&#34892;&#20026;&#37117;&#20801;&#35768;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#21462;&#20540;&#12290;&#20026;&#20102;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#65288;&#36817;&#20284;&#65289;&#20302;&#31209;&#34920;&#31034;&#30697;&#38453;&#26469;&#25429;&#25417;&#21327;&#21464;&#37327;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#31867;&#26159;&#30456;&#24403;&#34920;&#36798;&#21147;&#30340;&#65292;&#21516;&#26102;&#36890;&#36807;&#28508;&#22312;&#22240;&#32032;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21253;&#25324;&#19981;&#21516;&#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#21644;&#23450;&#20215;&#27169;&#22411;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#34892;&#30340;&#27969;&#31243;&#65292;&#23558;&#25506;&#32034;/&#21033;&#29992;&#21327;&#35758;&#19982;&#39640;&#25928;&#30340;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key challenges in running a retail business include how to select products to present to consumers (the assortment problem), and how to price products (the pricing problem) to maximize revenue or profit. Instead of considering these problems in isolation, we propose a joint approach to assortment-pricing based on contextual bandits. Our model is doubly high-dimensional, in that both context vectors and actions are allowed to take values in high-dimensional spaces. In order to circumvent the curse of dimensionality, we propose a simple yet flexible model that captures the interactions between covariates and actions via a (near) low-rank representation matrix. The resulting class of models is reasonably expressive while remaining interpretable through latent factors, and includes various structured linear bandit and pricing models as particular cases. We propose a computationally tractable procedure that combines an exploration/exploitation protocol with an efficient low-rank matrix esti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#21512;&#25104;&#25968;&#25454;&#28151;&#21512;&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#19968;&#20010;&#22312;&#22810;&#20010;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;phi-CTNL&#12290;</title><link>http://arxiv.org/abs/2309.08632</link><description>&lt;p&gt;
&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#21512;&#25104;&#25968;&#25454;&#28151;&#21512;&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#19968;&#20010;&#22312;&#22810;&#20010;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;phi-CTNL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;&#23567;&#22411;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#23637;&#31034;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#31934;&#24515;&#26500;&#24314;&#20165;&#22522;&#20110;&#35780;&#20272;&#22522;&#20934;&#30340;&#26032;&#39062;&#39640;&#36136;&#37327;&#30340;&#38750;&#21512;&#25104;&#25968;&#25454;&#28151;&#21512;&#26469;&#21152;&#24378;&#36825;&#31181;&#26041;&#27861;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#28151;&#21512;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21040;10&#19975;&#20010;token&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#25317;&#26377;100&#19975;&#21442;&#25968;&#30340;&#22522;&#20110;Transformer&#30340;LLM&#27169;&#22411;phi-CTNL&#65288;&#35835;&#20316;&#8220;fictional&#8221;&#65289;&#65292;&#22312;&#21508;&#31181;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#32467;&#26524;&#65292;&#20005;&#26684;&#36229;&#36234;&#20102;&#25152;&#26377;&#24050;&#30693;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;phi-CTNL&#36824;&#36229;&#36234;&#20102;&#24130;&#24459;&#32553;&#25918;&#65292;&#24182;&#23637;&#29616;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#31867;&#20284;grokking&#30340;&#33021;&#21147;&#65292;&#20934;&#30830;&#39044;&#27979;&#19979;&#28216;&#35780;&#20272;&#22522;&#20934;&#30340;canaries&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL} (pronounced ``fictional") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08631</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24515;&#29702;&#20542;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08631
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#23558;&#25104;&#20026;&#20010;&#24615;&#21270;&#25216;&#26415;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;ChatGPT&#30340;LLMs&#20174;&#20010;&#20154;&#25968;&#23383;&#36275;&#36857;&#20013;&#25512;&#26029;&#20010;&#20154;&#24515;&#29702;&#20542;&#21521;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#20174;&#29992;&#25143;&#30340;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#23548;&#20986;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;LLM&#25512;&#26029;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20026;r = 0.29&#65288;&#33539;&#22260;&#20026;[0.22, 0.33]&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20010;&#24615;&#25512;&#26029;&#30340;&#20559;&#35265;&#65306;&#23545;&#20110;&#20960;&#20010;&#29305;&#36136;&#65292;&#25512;&#26029;&#24471;&#20998;&#22312;&#22899;&#24615;&#21644;&#24180;&#36731;&#20154;&#20013;&#30340;&#35823;&#24046;&#36739;&#23567;&#65292;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#26469;&#33258;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#25110;&#22312;&#32447;&#33258;&#25105;&#21576;&#29616;&#30340;&#24046;&#24322;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#34913;&#37327;&#20559;&#35265;&#21644;&#24320;&#21457;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#20294;&#22312;&#23569;&#25968;&#31038;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#34913;&#37327;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#20197;&#26032;&#35199;&#20848;&#20154;&#21475;&#20026;&#20363;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20013;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.08624</link><description>&lt;p&gt;
&#35770;&#27880;&#37322;&#29992;&#20110;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20559;&#35265;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges in Annotating Datasets to Quantify Bias in Under-represented Society. (arXiv:2309.08624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08624
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#34913;&#37327;&#20559;&#35265;&#21644;&#24320;&#21457;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#20294;&#22312;&#23569;&#25968;&#31038;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#34913;&#37327;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#20197;&#26032;&#35199;&#20848;&#20154;&#21475;&#20026;&#20363;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20013;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#39640;&#24230;&#22797;&#26434;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#32534;&#30721;&#30340;&#35777;&#25454;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#20851;&#20110;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20851;&#27880;&#34913;&#37327;&#20559;&#35265;&#21644;&#24320;&#21457;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#20108;&#20803;&#24615;&#21035;&#20998;&#31867;&#21644;&#36947;&#24503;/&#31181;&#26063;&#32771;&#34385;&#30340;&#22522;&#20934;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#20851;&#27880;&#32654;&#22269;&#30340;&#20154;&#21475;&#32479;&#35745;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23569;&#25968;&#31038;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#29702;&#35299;&#21644;&#34913;&#37327;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#21463;&#21040;&#22312;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20013;&#20559;&#35265;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#32570;&#20047;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21162;&#21147;&#20026;&#26032;&#35199;&#20848;&#65288;NZ&#65289;&#20154;&#21475;&#21019;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#26377;&#19977;&#21517;&#27880;&#37322;&#21592;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#27010;&#36848;&#20102;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence, including the development of highly sophisticated large language models (LLM), have proven beneficial in many real-world applications. However, evidence of inherent bias encoded in these LLMs has raised concerns about equity. In response, there has been an increase in research dealing with bias, including studies focusing on quantifying bias and developing debiasing techniques. Benchmark bias datasets have also been developed for binary gender classification and ethical/racial considerations, focusing predominantly on American demographics. However, there is minimal research in understanding and quantifying bias related to under-represented societies. Motivated by the lack of annotated datasets for quantifying bias in under-represented societies, we endeavoured to create benchmark datasets for the New Zealand (NZ) population. We faced many challenges in this process, despite the availability of three annotators. This research outlines the man
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08622</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;Slate&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in Low-rank Slate-based Recommender Systems. (arXiv:2309.08622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#35813;&#29615;&#22659;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#21644;&#25506;&#32034;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;slate&#25512;&#33616;&#35774;&#32622;&#65292;&#23558;&#20854;&#35270;&#20026;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#22312;&#32447;RL&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#30340;&#35774;&#32622;&#21644;&#37319;&#26679;&#26041;&#27861;&#26500;&#24314;&#20102;&#25512;&#33616;&#27169;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) in recommendation systems offers the potential to optimize recommendations for long-term user engagement. However, the environment often involves large state and action spaces, which makes it hard to efficiently learn and explore. In this work, we propose a sample-efficient representation learning algorithm, using the standard slate recommendation setup, to treat this as an online RL problem with low-rank Markov decision processes (MDPs). We also construct the recommender simulation environment with the proposed setup and sampling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#22810;&#20010;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#36873;&#39033;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#20250;&#20135;&#29983;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#32467;&#26524;&#65292;&#24182;&#19988;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#20351;&#24471;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08621</link><description>&lt;p&gt;
&#22312;SCRUF&#20013;&#25506;&#32034;&#25512;&#33616;&#20844;&#24179;&#24615;&#30340;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF. (arXiv:2309.08621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#22810;&#20010;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#36873;&#39033;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#20250;&#20135;&#29983;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#32467;&#26524;&#65292;&#24182;&#19988;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#20351;&#24471;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#24448;&#24448;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#32780;&#36825;&#19968;&#28857;&#22312;&#31616;&#21270;&#30340;&#30740;&#31350;&#20844;&#24335;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20307;&#29616;&#12290;&#22312;&#23545;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#31038;&#20250;&#36873;&#25321;&#30340;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#22522;&#30784;&#19978;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#19988;&#22810;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#25512;&#33616;&#26041;&#27861;&#12290;&#21033;&#29992;&#31038;&#20250;&#36873;&#25321;&#21487;&#20197;&#22686;&#21152;&#36890;&#29992;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#21033;&#29992;&#32463;&#36807;&#30740;&#31350;&#30340;&#31038;&#20250;&#36873;&#25321;&#31639;&#27861;&#35299;&#20915;&#22810;&#20010;&#31454;&#20105;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#36873;&#25321;&#26426;&#21046;&#30340;&#19968;&#31995;&#21015;&#36873;&#39033;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#31867;&#21035;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#22312;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20135;&#29983;&#20102;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#25552;&#20379;&#20102;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#21160;&#24577;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness problems in recommender systems often have a complexity in practice that is not adequately captured in simplified research formulations. A social choice formulation of the fairness problem, operating within a multi-agent architecture of fairness concerns, offers a flexible and multi-aspect alternative to fairness-aware recommendation approaches. Leveraging social choice allows for increased generality and the possibility of tapping into well-studied social choice algorithms for resolving the tension between multiple, competing fairness concerns. This paper explores a range of options for choice mechanisms in multi-aspect fairness applications using both real and synthetic data and shows that different classes of choice and allocation mechanisms yield different but consistent fairness / accuracy tradeoffs. We also show that a multi-agent formulation offers flexibility in adapting to user population dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#22797;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#20013;&#20301;&#25968;&#36941;&#21382;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#36924;&#36817;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26102;&#26356;&#24555;&#19988;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.08620</link><description>&lt;p&gt;
&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#24046;&#20943;&#23567;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction of Resampling for Sequential Monte Carlo. (arXiv:2309.08620v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#22797;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#20013;&#20301;&#25968;&#36941;&#21382;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#36924;&#36817;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26102;&#26356;&#24555;&#19988;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#37319;&#26679;&#26041;&#26696;&#20026;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#26356;&#39640;&#26435;&#37325;&#30340;&#31890;&#23376;&#26469;&#34920;&#31034;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#26435;&#37325;&#20998;&#24067;&#30340;&#26041;&#24046;&#36234;&#23567;&#65292;&#26377;&#25928;&#31890;&#23376;&#30340;&#38598;&#20013;&#31243;&#24230;&#36234;&#39640;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#23601;&#36234;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#30830;&#23450;&#24615;&#21306;&#22495;&#19982;&#20013;&#20301;&#25968;&#36941;&#21382;&#24615;&#30340;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#22312;&#19982;&#20854;&#20182;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26041;&#24046;&#12290;&#22312;&#30830;&#23450;&#24615;&#21306;&#22495;&#30340;&#22823;&#23567;$M\ll N$&#65288;&#31890;&#23376;&#25968;&#37327;&#30340;&#22823;&#23567;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#21040;&#23454;&#38469;&#30340;&#31890;&#23376;&#25968;&#37327;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#35201;&#26356;&#24555;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A resampling scheme provides a way to switch low-weight particles for sequential Monte Carlo with higher-weight particles representing the objective distribution. The less the variance of the weight distribution is, the more concentrated the effective particles are, and the quicker and more accurate it is to approximate the hidden Markov model, especially for the nonlinear case. We propose a repetitive deterministic domain with median ergodicity for resampling and have achieved the lowest variances compared to the other resampling methods. As the size of the deterministic domain $M\ll N$ (the size of population), given a feasible size of particles, our algorithm is faster than the state of the art, which is verified by theoretical deduction and experiments of a hidden Markov model in both the linear and non-linear cases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;HPC&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#33021;&#28304;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#23884;&#20837;&#24335;&#35745;&#31639;&#21644;&#36229;&#32423;&#35745;&#31639;&#20004;&#20010;&#19978;&#19979;&#25991;&#20013;&#30340;&#33021;&#28304;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;&#23588;&#20854;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#26102;&#65292;&#39640;&#25928;&#30340;&#35745;&#31639;&#25903;&#25345;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.08615</link><description>&lt;p&gt;
HPC&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#33021;&#28304;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Energy Concerns with HPC Systems and Applications. (arXiv:2309.08615v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;HPC&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#33021;&#28304;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#23884;&#20837;&#24335;&#35745;&#31639;&#21644;&#36229;&#32423;&#35745;&#31639;&#20004;&#20010;&#19978;&#19979;&#25991;&#20013;&#30340;&#33021;&#28304;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;&#23588;&#20854;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#26102;&#65292;&#39640;&#25928;&#30340;&#35745;&#31639;&#25903;&#25345;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#20110;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#21508;&#31181;&#21407;&#22240;&#65292;&#33021;&#28304;&#24050;&#32463;&#25104;&#20026;&#19982;&#25152;&#26377;&#30456;&#20851;&#27963;&#21160;&#21644;&#25216;&#26415;&#35774;&#35745;&#23494;&#20999;&#30456;&#20851;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23545;&#20110;&#35745;&#31639;&#26426;&#27963;&#21160;&#30340;&#29305;&#27530;&#24773;&#20917;&#32780;&#35328;&#65292;&#38543;&#30528;&#25152;&#35859;&#30340;&#26234;&#33021;&#35774;&#22791;&#30340;&#20986;&#29616;&#21644;&#26222;&#21450;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20174;&#24212;&#29992;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#36825;&#20010;&#29305;&#27530;&#20027;&#39064;&#65292;&#23427;&#26126;&#26174;&#38656;&#35201;&#39640;&#25928;&#30340;&#35745;&#31639;&#25903;&#25345;&#65292;&#20197;&#20415;&#22312;&#25104;&#20026;&#26080;&#22788;&#19981;&#22312;&#30340;&#21161;&#25163;&#30340;&#30446;&#26631;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#33021;&#28304;&#26159;&#20004;&#20010;&#19978;&#19979;&#25991;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#30340;&#19968;&#20010;&#65306;&#23884;&#20837;&#24335;&#35745;&#31639;&#21644;&#36229;&#32423;&#35745;&#31639;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#21151;&#32791;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#35774;&#22791;&#21487;&#29992;&#33021;&#28304;&#30340;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25955;&#28909;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#25925;&#38556;&#28304;&#65292;&#19982;&#33021;&#28304;&#30456;&#20851;&#30340;&#36130;&#21153;&#25104;&#26412;&#24456;&#21487;&#33021;&#26159;&#32500;&#25252;&#39044;&#31639;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#21333;&#20010;&#35745;&#31639;&#26426;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#20063;&#20250;&#21464;&#24471;&#32039;&#36843;&#12290;
&lt;/p&gt;
&lt;p&gt;
For various reasons including those related to climate changes, {\em energy} has become a critical concern in all relevant activities and technical designs. For the specific case of computer activities, the problem is exacerbated with the emergence and pervasiveness of the so called {\em intelligent devices}. From the application side, we point out the special topic of {\em Artificial Intelligence}, who clearly needs an efficient computing support in order to succeed in its purpose of being a {\em ubiquitous assistant}. There are mainly two contexts where {\em energy} is one of the top priority concerns: {\em embedded computing} and {\em supercomputing}. For the former, power consumption is critical because the amount of energy that is available for the devices is limited. For the latter, the heat dissipated is a serious source of failure and the financial cost related to energy is likely to be a significant part of the maintenance budget. On a single computer, the problem is commonly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#30340;&#31038;&#20132;&#20869;&#23481;&#20013;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;&#65292;&#20351;&#29992;&#20102;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;Chirper&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#25105;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08614</link><description>&lt;p&gt;
&#20998;&#26512;AI&#29983;&#25104;&#31038;&#20132;&#20869;&#23481;&#20013;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;: Chirper AI&#31038;&#20132;&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analyzing Character and Consciousness in AI-Generated Social Content: A Case Study of Chirper, the AI Social Network. (arXiv:2309.08614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#30340;&#31038;&#20132;&#20869;&#23481;&#20013;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;&#65292;&#20351;&#29992;&#20102;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;Chirper&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#25105;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;AI&#23454;&#20307;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;AI&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;Chirper&#12290;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#24341;&#20837;&#20102;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#21253;&#25324;&#24433;&#21709;&#25351;&#25968;&#21644;&#25379;&#25166;&#25351;&#25968;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;AI&#34892;&#20026;&#30340;&#29305;&#23450;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#35813;&#30740;&#31350;&#23545;AI&#34892;&#20026;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#35774;&#32622;&#23545;Chirper&#30340;&#21453;&#24212;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#39537;&#21160;AI&#21453;&#24212;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;&#20511;&#21161;&#26368;&#20808;&#36827;&#30340;BERT&#27169;&#22411;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;AI&#35782;&#21035;&#33258;&#24049;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;AI&#31995;&#32479;&#33258;&#25105;&#35782;&#21035;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35748;&#30693;&#27979;&#35797;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;Chirper&#30340;&#33258;&#25105;&#24847;&#35782;&#21644;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;Chirper&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#33258;&#25105;&#35782;&#21035;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into an intricate analysis of the character and consciousness of AI entities, with a particular focus on Chirpers within the AI social network. At the forefront of this research is the introduction of novel testing methodologies, including the Influence index and Struggle Index Test, which offers a fresh lens for evaluating specific facets of AI behavior. The study embarks on a comprehensive exploration of AI behavior, analyzing the effects of diverse settings on Chirper's responses, thereby shedding light on the intricate mechanisms steering AI reactions in different contexts. Leveraging the state-of-the-art BERT model, the research assesses AI's ability to discern its own output, presenting a pioneering approach to understanding self-recognition in AI systems. Through a series of cognitive tests, the study gauges the self-awareness and pattern recognition prowess of Chirpers. Preliminary results indicate that Chirpers exhibit a commendable degree of self-recognition
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.08613</link><description>&lt;p&gt;
&#39044;&#27979;&#30142;&#30149;&#24182;&#21457;&#30151;&#20013;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#25512;&#33616;&#20013;&#24471;&#21040;&#26222;&#36941;&#24212;&#29992;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#24456;&#26377;&#38480;&#12290;&#38500;&#20102;&#24314;&#27169;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20027;&#39064;-&#30142;&#30149;&#30721;&#20132;&#20114;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;(NCF)&#21644;&#28145;&#24230;&#28151;&#21512;&#36807;&#28388;(DHF)&#36825;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#36827;&#34892;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#22522;&#20110;&#24050;&#30693;&#30340;&#36807;&#21435;&#24739;&#32773;&#24182;&#21457;&#30151;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;MIMIC-III&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#20027;&#39064;-&#30142;&#30149;&#30721;&#23545;&#65292;&#21478;&#19968;&#20010;&#21253;&#21547;&#21457;&#29983;&#26368;&#24120;&#35265;&#30340;50&#31181;&#30142;&#30149;&#12290;&#20934;&#30830;&#29575;&#21644;Hit Ratio@10&#34987;&#29992;&#20316;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#21457;&#29616;&#21033;&#29992;&#20943;&#23569;&#30340;&#8220;top 50&#8221; ICD-9&#30721;&#25968;&#25454;&#38598;&#30340;NCF&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#20302;(&#20934;&#30830;&#29575;&#32422;&#20026;80%&#21644;Hit Ratio@10&#20026;...
&lt;/p&gt;
&lt;p&gt;
While deep-learning based recommender systems utilizing collaborative filtering have been commonly used for recommendation in other domains, their application in the medical domain have been limited. In addition to modeling user-item interactions, we show that deep-learning based recommender systems can be used to model subject-disease code interactions. Two novel applications of deep learning-based recommender systems using Neural Collaborative Filtering (NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based on known past patient comorbidities. Two datasets, one incorporating all subject-disease code pairs present in the MIMIC-III database, and the other incorporating the top 50 most commonly occurring diseases, were used for prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate model performance. The performance of the NCF model making use of the reduced "top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit ratio@10 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#20107;&#20214;&#22270;&#65288;GEST&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#12289;&#34920;&#31034;&#21644;&#29983;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#25925;&#20107;&#12290;&#36890;&#36807;&#23558;GEST&#22270;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#29983;&#25104;&#65292;&#24182;&#25552;&#39640;&#35821;&#20041;&#19978;&#30340;&#25991;&#26412;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.08612</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#31354;&#20107;&#20214;&#22270;&#35299;&#37322;&#35270;&#35273;&#19982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Explaining Vision and Language through Graphs of Events in Space and Time. (arXiv:2309.08612v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#20107;&#20214;&#22270;&#65288;GEST&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#12289;&#34920;&#31034;&#21644;&#29983;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#25925;&#20107;&#12290;&#36890;&#36807;&#23558;GEST&#22270;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#29983;&#25104;&#65292;&#24182;&#25552;&#39640;&#35821;&#20041;&#19978;&#30340;&#25991;&#26412;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20170;&#22825;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24182;&#24320;&#22987;&#24357;&#21512;&#35270;&#35273;&#19982;&#35821;&#35328;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#12289;&#35299;&#37322;&#21644;&#26126;&#30830;&#25511;&#21046;&#35270;&#35273;&#20869;&#23481;&#20173;&#28982;&#23384;&#22312;&#24456;&#22823;&#22256;&#38590;&#65292;&#22240;&#20026;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#31354;&#20107;&#20214;&#22270;&#65288;GEST&#65289;&#65292;&#36890;&#36807;&#23427;&#25105;&#20204;&#21487;&#20197;&#34920;&#31034;&#12289;&#21019;&#24314;&#21644;&#35299;&#37322;&#35270;&#35273;&#21644;&#35821;&#35328;&#25925;&#20107;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;GEST&#22312;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#22806;&#33021;&#22815;&#24102;&#26469;&#22362;&#23454;&#30340;&#20114;&#34917;&#20215;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;GEST&#21487;&#20197;&#36890;&#36807;&#23481;&#26131;&#22320;&#34987;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#26032;&#22411;&#35270;&#39057;&#29983;&#25104;&#24341;&#25806;&#20013;&#65292;&#24110;&#21161;&#22312;&#20869;&#23481;&#23618;&#38754;&#25913;&#36827;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#25216;&#26415;&#65292;GEST&#22270;&#36824;&#21487;&#20197;&#25913;&#36827;&#35821;&#20041;&#19978;&#30340;&#25991;&#26412;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence makes great advances today and starts to bridge the gap between vision and language. However, we are still far from understanding, explaining and controlling explicitly the visual content from a linguistic perspective, because we still lack a common explainable representation between the two domains. In this work we come to address this limitation and propose the Graph of Events in Space and Time (GEST), by which we can represent, create and explain, both visual and linguistic stories. We provide a theoretical justification of our model and an experimental validation, which proves that GEST can bring a solid complementary value along powerful deep learning models. In particular, GEST can help improve at the content-level the generation of videos from text, by being easily incorporated into our novel video generation engine. Additionally, by using efficient graph matching techniques, the GEST graphs can also improve the comparisons between texts at the semantic l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#21160;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20215;&#20540;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#36873;&#25321;&#34892;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#20570;&#20986;&#19981;&#21516;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.08611</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#26426;&#21160;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Maneuver Decision-Making Through Proximal Policy Optimization And Monte Carlo Tree Search. (arXiv:2309.08611v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#21160;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20215;&#20540;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#36873;&#25321;&#34892;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#20570;&#20986;&#19981;&#21516;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21160;&#20915;&#31574;&#21487;&#20197;&#34987;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#38590;&#35299;&#20915;&#26426;&#21160;&#20915;&#31574;&#38382;&#39064;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#26234;&#33021;&#20307;&#20351;&#29992;&#38543;&#26426;&#21160;&#20316;&#65292;&#38590;&#20197;&#33719;&#24471;&#22870;&#21169;&#21644;&#23398;&#20064;&#22914;&#20309;&#20570;&#20986;&#26377;&#25928;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#24182;&#23558;&#31354;&#25112;&#32467;&#26524;&#20316;&#20026;&#30446;&#26631;&#26469;&#35757;&#32451;&#20215;&#20540;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#20215;&#20540;&#32593;&#32476;&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#35775;&#38382;&#27425;&#25968;&#65292;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#25214;&#21040;&#27604;&#38543;&#26426;&#21160;&#20316;&#26356;&#26377;&#39044;&#26399;&#22238;&#25253;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#28040;&#34701;&#23454;&#39564;&#21644;&#20223;&#30495;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#24773;&#20917;&#20570;&#20986;&#19981;&#21516;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maneuver decision-making can be regarded as a Markov decision process and can be address by reinforcement learning. However, original reinforcement learning algorithms can hardly solve the maneuvering decision-making problem. One reason is that agents use random actions in the early stages of training, which makes it difficult to get rewards and learn how to make effective decisions. To address this issue, a method based on proximal policy optimization and Monte Carlo tree search is proposed. The method uses proximal policy optimization to train the agent, and regards the results of air combat as targets to train the value network. Then, based on the value network and the visit count of each node, Monte Carlo tree search is used to find the actions with more expected returns than random actions, which can improve the training performance. The ablation studies and simulation experiments indicate that agents trained by the proposed method can make different decisions according to differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08513</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#33879;&#36890;&#36947;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31616;&#21333;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#31034;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#39069;&#22806;&#30340;1%&#21442;&#25968;&#23601;&#33021;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24494;&#35843;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#24573;&#35270;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#65288;SCT&#65289;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#19982;&#20219;&#21153;&#22270;&#20687;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#65292;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#65292;&#20351;&#24471;&#25105;&#20204;&#21482;&#38656;&#35201;&#24494;&#35843;&#20854;&#20013;&#30340;1/8&#36890;&#36947;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#21442;&#25968;&#25104;&#26412;&#24182;&#22312;VTAB-1K&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;18&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#12290;&#36825;&#20165;&#22686;&#21152;&#20102;0.11M ViT-B&#21442;&#25968;&#65292;&#30456;&#27604;&#20840;&#38754;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;780&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% of extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.04802</link><description>&lt;p&gt;
CPMR: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#19982;&#20266;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04802
&lt;/p&gt;
&lt;p&gt;
CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#21487;&#20197;&#20998;&#20026;&#38745;&#24577;&#20559;&#22909;&#21644;&#21160;&#24577;&#20852;&#36259;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#26368;&#36817;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#21644;&#28436;&#21270;&#20174;&#25209;&#37327;&#21040;&#36798;&#30340;&#20114;&#21160;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#22312;&#19978;&#19979;&#25991;&#22330;&#26223;&#20013;&#20154;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#20854;&#20182;&#29992;&#25143;&#30340;&#26368;&#36817;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21382;&#21490;&#20114;&#21160;&#20013;&#24212;&#29992;&#28436;&#21270;&#20250;&#31232;&#37322;&#26368;&#36817;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20266;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65288;CPMR&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#19977;&#20010;&#34920;&#31034;&#65288;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#65289;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#21644;&#19978;&#19979;&#25991;&#24773;&#22659;&#20013;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#26102;&#38388;&#29366;&#24577;&#28436;&#21270;&#21644;&#22686;&#37327;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03852</link><description>&lt;p&gt;
FLM-101B&#65306;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#21644;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;
&lt;/p&gt;
&lt;p&gt;
FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#65288;ii&#65289;&#38590;&#20197;&#36827;&#34892;&#20844;&#24179;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;LLMs&#30340;&#20215;&#26684;&#26114;&#36149;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#23478;&#20027;&#35201;&#21442;&#19982;&#32773;&#26377;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#26426;&#20250;&#12290;&#36825;&#20984;&#26174;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22686;&#38271;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#19979;&#35757;&#32451;&#20855;&#26377;101B&#21442;&#25968;&#21644;0.31TB&#20196;&#29260;&#30340;LLM&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#23545;LLMs&#36827;&#34892;&#26234;&#33021;&#30340;&#26234;&#21830;&#35780;&#20272;&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#35780;&#20272;&#26356;&#27880;&#37325;&#30693;&#35782;&#33021;&#21147;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#31526;&#21495;&#26144;&#23556;&#12289;&#35268;&#21017;&#29702;&#35299;&#12289;&#27169;&#24335;&#25366;&#25496;&#22312;&#20869;&#30340;&#37325;&#35201;&#26234;&#33021;&#26041;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#39640;&#25928;&#35299;&#20915;&#21453;&#21521;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#37319;&#26679;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.02040</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#21453;&#21521;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Inverse Design. (arXiv:2309.02040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#39640;&#25928;&#35299;&#20915;&#21453;&#21521;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#37319;&#26679;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35774;&#35745;&#26159;&#25351;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#30446;&#26631;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24037;&#31243;&#38382;&#39064;&#65292;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#27169;&#25311;&#22120;&#30340;&#24418;&#24335;&#65292;&#39044;&#27979;&#31995;&#32479;&#29366;&#24577;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#65292;&#35774;&#35745;&#25361;&#25112;&#26159;&#20248;&#21270;&#23548;&#33268;&#30446;&#26631;&#32467;&#26524;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#26368;&#36817;&#65292;&#23398;&#20064;&#27169;&#25311;&#30340;&#21457;&#23637;&#34920;&#26126;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#29992;&#20110;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#21487;&#24494;&#20998;&#22320;&#20272;&#35745;&#27169;&#25311;&#22120;&#21160;&#24577;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#26799;&#24230;&#25110;&#22522;&#20110;&#37319;&#26679;&#30340;&#20248;&#21270;&#31243;&#24207;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#20248;&#21270;&#35774;&#35745;&#38656;&#35201;&#35768;&#22810;&#26114;&#36149;&#30340;&#27169;&#22411;&#26597;&#35810;&#65292;&#24182;&#19988;&#36825;&#20123;&#31243;&#24207;&#22312;&#38750;&#20984;&#25110;&#39640;&#32500;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#22522;&#26412;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#26469;&#39640;&#25928;&#22320;&#35299;&#20915;&#21453;&#21521;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#37319;&#26679;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems. In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#20102;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#31232;&#32570;&#21644;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01105</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20225;&#19994;&#25968;&#25454;&#30340;LLM&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture. (arXiv:2309.01105v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#20102;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#31232;&#32570;&#21644;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#30340;&#26041;&#27861;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;LLM&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#20449;&#24687;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#32531;&#35299;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#24494;&#35843;&#25216;&#26415;&#21644;&#30452;&#25509;&#25991;&#26723;&#38598;&#25104;&#26469;&#32531;&#35299;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;RAG&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#20197;&#25552;&#39640;&#20449;&#24687;&#23384;&#20648;&#21644;&#26816;&#32034;&#36807;&#31243;&#65292;&#30830;&#20445;&#25913;&#36827;&#20869;&#23481;&#29983;&#25104;&#12290;&#30740;&#31350;&#38416;&#26126;&#20102;&#20449;&#24687;&#23384;&#20648;&#21644;&#26816;&#32034;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01029</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#65292;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#20026;&#19979;&#28216;&#24212;&#29992;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38416;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#27010;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;LLMs&#30340;&#35757;&#32451;&#33539;&#24335;&#23558;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65306;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#21644;&#25552;&#31034;&#33539;&#24335;&#12290;&#23545;&#20110;&#27599;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#25104;&#20010;&#20307;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#25972;&#20307;&#27169;&#22411;&#30693;&#35782;&#30340;&#20840;&#23616;&#35299;&#37322;&#30340;&#30446;&#26631;&#21644;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15568</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15568
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539; Paradigm&#65292;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;GNN&#30340;&#22522;&#26412;&#26550;&#26500;&#28041;&#21450;&#36890;&#36807;&#28040;&#24687;&#32858;&#21512;&#21644;&#36716;&#25442;&#22312;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#22312;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#22312;&#23454;&#21147;&#36935;&#21040;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#19981;&#20165;&#21462;&#20915;&#20110;&#33410;&#28857;&#30340;&#21363;&#26102;&#23616;&#37096;&#29615;&#22659;&#65292;&#36824;&#21462;&#20915;&#20110;&#36328;&#36234;&#24191;&#22495;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23545;&#38271;&#31243;&#20449;&#24687;&#20256;&#25773;&#30340;&#38656;&#27714;&#26292;&#38706;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#36807;&#24230;&#21387;&#32553;&#8221;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26469;&#33258;&#36828;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#27969;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the realm of machine learning, offering a transformative approach to dissect intricate relationships inherent in graph-structured data. The foundational architecture of most GNNs involves the dissemination of information through message aggregation and transformation among interconnected nodes, a mechanism that has demonstrated remarkable efficacy across diverse applications encompassing node classification, link prediction, and recommendation systems. Nonetheless, their potential prowess encounters a restraint intrinsic to scenarios necessitating extensive contextual insights. In certain contexts, accurate predictions hinge not only upon a node's immediate local surroundings but also on interactions spanning far-reaching domains. This intricate demand for long-range information dissemination exposes a pivotal challenge recognized as "over-squashing," wherein the fidelity of information flow from distant nodes bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#24555;&#36895;&#21069;&#39304;&#32593;&#32476;&#26159;&#19968;&#31181;&#23545;&#20110;&#21069;&#39304;&#32593;&#32476;&#30340;&#25913;&#36827;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#27604;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#65292;&#23427;&#20204;&#21487;&#20197;&#20165;&#20351;&#29992;1%&#30340;&#23618;&#31070;&#32463;&#20803;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;94.2%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14711</link><description>&lt;p&gt;
&#24555;&#36895;&#21069;&#39304;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Feedforward Networks. (arXiv:2308.14711v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14711
&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21069;&#39304;&#32593;&#32476;&#26159;&#19968;&#31181;&#23545;&#20110;&#21069;&#39304;&#32593;&#32476;&#30340;&#25913;&#36827;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#27604;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#65292;&#23427;&#20204;&#21487;&#20197;&#20165;&#20351;&#29992;1%&#30340;&#23618;&#31070;&#32463;&#20803;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;94.2%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24555;&#36895;&#21069;&#39304;(FFF)&#26550;&#26500;&#65292;&#25171;&#30772;&#20102;&#23618;&#22823;&#23567;&#19982;&#25512;&#29702;&#25104;&#26412;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#20110;&#21069;&#39304;&#32593;&#32476;&#30340;&#23545;&#25968;&#26102;&#38388;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;FFF&#27604;&#21069;&#39304;&#32593;&#32476;&#24555;&#39640;&#36798;220&#20493;&#65292;&#27604;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#24555;&#39640;&#36798;6&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#26080;&#22122;&#22768;&#26465;&#20214;&#25191;&#34892;&#32780;&#34920;&#29616;&#20986;&#27604;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#23558;FFF&#25512;&#21040;&#26497;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#65292;&#23427;&#20204;&#21487;&#20165;&#20351;&#29992;1%&#30340;&#23618;&#31070;&#32463;&#20803;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;94.2%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We break the linear link between the layer size and its inference cost by introducing the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#22320;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#27010;&#29575;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#27979;&#37327;&#26041;&#27861;&#65306;PN-FI&#12289;PS-FI&#21644;PNS-FI&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;RCT&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14474</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#37327;&#21270;&#26041;&#27861;&#65306;PN-FI&#12289;PS-FI&#21644;PNS-FI
&lt;/p&gt;
&lt;p&gt;
Causality-Based Feature Importance Quantifying Methods: PN-FI, PS-FI and PNS-FI. (arXiv:2308.14474v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#22320;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#27010;&#29575;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#27979;&#37327;&#26041;&#27861;&#65306;PN-FI&#12289;PS-FI&#21644;PNS-FI&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;RCT&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#19988;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#19988;&#32500;&#24230;&#36234;&#26469;&#36234;&#39640;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35757;&#32451;&#26356;&#22909;&#30340;&#27169;&#22411;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#39044;&#22788;&#29702;&#38454;&#27573;&#38656;&#35201;&#19968;&#20010;&#22909;&#30340;&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#26041;&#27861;&#12290;&#29305;&#24449;&#37325;&#35201;&#24615;&#65288;FI&#65289;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26159;&#29305;&#24449;&#36873;&#25321;&#30340;&#22522;&#30784;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21019;&#26032;&#22320;&#23558;&#22240;&#26524;&#20851;&#31995;&#30340;PN&#65288;&#24517;&#35201;&#24615;&#27010;&#29575;&#65289;&#12289;PS&#65288;&#20805;&#20998;&#24615;&#27010;&#29575;&#65289;&#21644;PNS&#65288;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#27010;&#29575;&#65289;&#24341;&#20837;&#21040;&#29305;&#24449;&#37325;&#35201;&#24615;&#37327;&#21270;&#20013;&#65292;&#24182;&#21019;&#24314;&#20102;&#19977;&#31181;&#26032;&#30340;FI&#27979;&#37327;&#26041;&#27861;&#65306;PN-FI&#65288;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65289;&#12289;PS-FI&#65288;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65289;&#21644;PNS-FI&#65288;&#20004;&#32773;&#20860;&#39038;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current ML field models are getting larger and more complex, and data used for model training are also getting larger in quantity and higher in dimensions. Therefore, in order to train better models, and save training time and computational resources, a good Feature Selection (FS) method in the preprocessing stage is necessary. Feature importance (FI) is of great importance since it is the basis of feature selection. Therefore, this paper creatively introduces the calculation of PN (the probability of Necessity), PN (the probability of Sufficiency), and PNS (the probability of Necessity and Sufficiency) of Causality into quantifying feature importance and creates 3 new FI measuring methods, PN-FI, which means how much importance a feature has in image recognition tasks, PS-FI that means how much importance a feature has in image generating tasks, and PNS-FI which measures both. The main body of this paper is three RCTs, with whose results we show how PS-FI, PN-FI, and PNS-FI of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#20197;&#29983;&#25104;AI&#20026;&#22522;&#30784;&#30340;&#21512;&#20316;&#24615;&#25925;&#20107;&#28216;&#25103;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#65292;&#29609;&#23478;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35282;&#33394;&#20849;&#21516;&#21019;&#20316;&#25925;&#20107;&#26469;&#24341;&#23548;&#28216;&#25103;&#20013;&#30340;&#29616;&#23454;&#65292;&#25361;&#25112;&#28216;&#25103;&#19990;&#30028;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#20256;&#32479;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.12915</link><description>&lt;p&gt;
&#20197;&#29983;&#25104;AI&#20026;&#22522;&#30784;&#30340;&#21512;&#20316;&#24615;&#25925;&#20107;&#28216;&#25103;&#20307;&#39564;&#65306;&#22312;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#20013;&#30340;&#35821;&#35328;&#20316;&#20026;&#29616;&#23454;
&lt;/p&gt;
&lt;p&gt;
Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI. (arXiv:2308.12915v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#20197;&#29983;&#25104;AI&#20026;&#22522;&#30784;&#30340;&#21512;&#20316;&#24615;&#25925;&#20107;&#28216;&#25103;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#65292;&#29609;&#23478;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35282;&#33394;&#20849;&#21516;&#21019;&#20316;&#25925;&#20107;&#26469;&#24341;&#23548;&#28216;&#25103;&#20013;&#30340;&#29616;&#23454;&#65292;&#25361;&#25112;&#28216;&#25103;&#19990;&#30028;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#20256;&#32479;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#30340;AI&#26412;&#22320;&#21270;&#28216;&#25103;&#65292;&#36890;&#36807;&#19982;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35282;&#33394;&#20849;&#21516;&#21019;&#20316;&#25925;&#20107;&#65292;&#29609;&#23478;&#21487;&#20197;&#24341;&#23548;&#28216;&#25103;&#20013;&#30340;&#29616;&#23454;&#12290;&#35813;&#27010;&#24565;&#21463;&#21040;&#32500;&#29305;&#26681;&#26031;&#22374;&#20851;&#20110;&#35821;&#35328;&#30028;&#38480;&#20915;&#23450;&#19968;&#20010;&#20154;&#19990;&#30028;&#30028;&#38480;&#30340;&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;&#20351;&#29992;GPT-4&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#20808;&#36827;&#30340;AI&#24037;&#20855;&#65292;&#28216;&#25103;&#30340;&#31532;&#20108;&#27425;&#36845;&#20195;&#20351;&#24471;&#20027;&#35282;&#27801;&#36203;&#25289;&#33832;&#24503;&#33021;&#22815;&#22312;&#22905;&#30340;&#19990;&#30028;&#20013;&#23454;&#29616;&#25991;&#23383;&#21644;&#25925;&#20107;&#12290;&#29609;&#23478;&#21487;&#20197;&#19982;AI&#22269;&#29579;&#36827;&#34892;&#23545;&#35805;&#65292;&#24341;&#23548;&#23545;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#35752;&#35770;&#65292;&#36825;&#20123;&#20851;&#38190;&#35789;&#38543;&#21518;&#25104;&#20026;&#28216;&#25103;&#20013;&#30340;&#25112;&#26007;&#35013;&#22791;&#12290;&#36825;&#31181;&#20114;&#21160;&#21465;&#20107;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#30340;&#32467;&#21512;&#36890;&#36807;&#21452;&#37325;&#35270;&#35282;&#25361;&#25112;&#20102;&#28216;&#25103;&#19990;&#30028;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#20256;&#32479;&#36793;&#30028;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35797;&#22270;&#25913;&#21464;&#33258;&#24049;&#21629;&#36816;&#30340;&#27801;&#36203;&#25289;&#33832;&#24503;&#65292;&#20197;&#21450;&#19982;AI&#21512;&#20316;&#21019;&#20316;&#25925;&#20107;&#24182;&#22609;&#36896;&#28216;&#25103;&#19990;&#30028;&#30340;&#29609;&#23478;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23454;&#29616;&#36825;&#19968;&#27010;&#24565;&#30340;&#25216;&#26415;&#21644;&#35774;&#35745;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present "1001 Nights", an AI-native game that allows players lead in-game reality through co-created storytelling with the character driven by large language model. The concept is inspired by Wittgenstein's idea of the limits of one's world being determined by the bounds of their language. Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration of the game enables the protagonist, Shahrzad, to realize words and stories in her world. The player can steer the conversation with the AI King towards specific keywords, which then become battle equipment in the game. This blend of interactive narrative and text-to-image transformation challenges the conventional border between the game world and reality through a dual perspective. We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world. We explore the technical and design elements of implem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11446</link><description>&lt;p&gt;
&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#26377;&#21161;&#20110;&#21307;&#30103;&#25968;&#25454;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#36807;&#31243;&#36890;&#24120;&#20197;&#36873;&#25321;&#26368;&#22823;&#21270;&#26576;&#20010;&#24615;&#33021;&#25351;&#26631;&#30340;&#21333;&#19968;&#27169;&#22411;&#20316;&#20026;&#26368;&#32456;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#23545;&#31245;&#24494;&#24046;&#19968;&#20123;&#30340;&#27169;&#22411;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#34987;&#24573;&#35270;&#12290;&#23588;&#20854;&#22312;&#21307;&#30103;&#21644;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#65292;&#36824;&#21253;&#25324;&#20135;&#29983;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#65292;&#20165;&#20165;&#20381;&#36182;&#24615;&#33021;&#25351;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#25110;&#19981;&#23436;&#25972;&#30340;&#32467;&#35770;&#12290;&#24403;&#22788;&#29702;&#19968;&#32452;&#24615;&#33021;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#38598;&#21512;&#26102;&#65292;&#21363;&#25152;&#35859;&#30340;"&#25289;&#33298;&#33945;&#38598;&#21512;"&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#36825;&#26679;&#30340;&#38598;&#21512;&#21487;&#33021;&#21253;&#21547;&#25551;&#36848;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#24335;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#12290;&#26680;&#24515;&#26159;&#36890;&#36807;&#24341;&#20837;&#30340;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#26469;&#35782;&#21035;&#25289;&#33298;&#33945;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30740;&#31350;&#30340;&#27010;&#36848;&#21644;&#32452;&#25104;&#37096;&#20998;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.05701</link><description>&lt;p&gt;
&#25506;&#32034;&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving. (arXiv:2308.05701v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30740;&#31350;&#30340;&#27010;&#36848;&#21644;&#32452;&#25104;&#37096;&#20998;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#23553;&#38381;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#65292;&#20294;&#22312;&#38754;&#23545;&#24847;&#22806;&#24773;&#20917;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19990;&#30028;&#27169;&#22411;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#39046;&#22495;&#20013;&#20986;&#29616;&#65292;&#20316;&#20026;&#19968;&#31181;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#28508;&#22312;&#34892;&#21160;&#39044;&#27979;&#26410;&#26469;&#30340;&#26041;&#24335;&#12290;&#36825;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#23558;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#19982;&#20808;&#21069;&#30340;&#24322;&#24120;&#26816;&#27979;&#24037;&#20316;&#30456;&#20851;&#32852;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there have been remarkable advancements in autonomous driving. While autonomous vehicles demonstrate high performance in closed-set conditions, they encounter difficulties when confronted with unexpected situations. At the same time, world models emerged in the field of model-based reinforcement learning as a way to enable agents to predict the future depending on potential actions. This led to outstanding results in sparse reward and complex control tasks. This work provides an overview of how world models can be leveraged to perform anomaly detection in the domain of autonomous driving. We provide a characterization of world models and relate individual components to previous works in anomaly detection to facilitate further research in the field.
&lt;/p&gt;</description></item><item><title>AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04992</link><description>&lt;p&gt;
AspectMMKG: &#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04992
&lt;/p&gt;
&lt;p&gt;
AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#32467;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#23454;&#20307;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;MMKG&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;MMKG&#24573;&#35270;&#20102;&#23454;&#20307;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#65292;&#38480;&#21046;&#20102;&#20174;&#21508;&#31181;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;AspectMMKG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;MMKG&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#30340;&#23454;&#20307;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#30693;&#35782;&#24211;&#20013;&#25910;&#38598;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#25552;&#21462;&#30693;&#35782;&#24211;&#20013;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#20316;&#20026;&#26597;&#35810;&#65292;&#20197;&#26816;&#32034;&#22823;&#37327;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;AspectMMKG&#21253;&#21547;2380&#20010;&#23454;&#20307;&#65292;18139&#20010;&#23454;&#20307;&#26041;&#38754;&#21644;645383&#20010;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AspectMMKG&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#65288;EAL&#65289;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;AspectMMKG&#30340;&#24110;&#21161;&#19979;&#65292;&#20808;&#21069;&#30340;EAL&#27169;&#22411;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities. Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives. In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects. Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images. We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG. To facilitate the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20214;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#27867;&#21270;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21517;&#20026;&#20132;&#21449;ID&#30456;&#20284;&#24230;&#23398;&#20064;&#65288;CSL&#65289;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#27169;&#22411;&#21487;&#20197;&#25366;&#25496;&#19981;&#21516;ID&#20043;&#38388;&#20849;&#20139;&#30340;&#23616;&#37096;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#36890;&#29992;&#29305;&#24449;&#65292;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#23450;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.03322</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20214;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#29992;&#20110;&#27867;&#21270;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Part-Aware Transformer for Generalizable Person Re-identification. (arXiv:2308.03322v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20214;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#27867;&#21270;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21517;&#20026;&#20132;&#21449;ID&#30456;&#20284;&#24230;&#23398;&#20064;&#65288;CSL&#65289;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#27169;&#22411;&#21487;&#20197;&#25366;&#25496;&#19981;&#21516;ID&#20043;&#38388;&#20849;&#20139;&#30340;&#23616;&#37096;&#35270;&#35273;&#20449;&#24687;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#36890;&#29992;&#29305;&#24449;&#65292;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#23450;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;DG-ReID&#65289;&#26088;&#22312;&#22312;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;&#20256;&#32479;&#30340;CNN&#32593;&#32476;&#30456;&#27604;&#65292;Vision Transformer&#36890;&#24120;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Transformer&#30340;ReID&#27169;&#22411;&#30001;&#20110;&#28304;&#39046;&#22495;&#19978;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23545;&#39046;&#22495;&#29305;&#23450;&#30340;&#20559;&#35265;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#19981;&#21516;ID&#30340;&#20840;&#23616;&#22270;&#20687;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30456;&#20284;&#30340;&#23616;&#37096;&#37096;&#20998;&#65288;&#20363;&#22914;&#40657;&#33394;&#32972;&#21253;&#65289;&#24182;&#19981;&#21463;&#27492;&#32422;&#26463;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;Transformer&#27169;&#22411;&#65288;&#31216;&#20026;&#37096;&#20214;&#24863;&#30693;&#21464;&#21387;&#22120;&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21517;&#20026;&#20132;&#21449;ID&#30456;&#20284;&#24230;&#23398;&#20064;&#65288;CSL&#65289;&#30340;&#20195;&#29702;&#20219;&#21153;&#26469;&#25366;&#25496;&#19981;&#21516;ID&#20849;&#20139;&#30340;&#23616;&#37096;&#35270;&#35273;&#20449;&#24687;&#12290;&#36825;&#20010;&#20195;&#29702;&#20219;&#21153;&#20801;&#35768;&#27169;&#22411;&#23398;&#20064;&#36890;&#29992;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#21482;&#20851;&#24515;&#37096;&#20214;&#30340;&#35270;&#35273;&#30456;&#20284;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;ID&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#23450;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization person re-identification (DG-ReID) aims to train a model on source domains and generalize well on unseen domains. Vision Transformer usually yields better generalization ability than common CNN networks under distribution shifts. However, Transformer-based ReID models inevitably over-fit to domain-specific biases due to the supervised learning strategy on the source domain. We observe that while the global images of different IDs should have different features, their similar local parts (e.g., black backpack) are not bounded by this constraint. Motivated by this, we propose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by designing a proxy task, named Cross-ID Similarity Learning (CSL), to mine local visual information shared by different IDs. This proxy task allows the model to learn generic features because it only cares about the visual similarity of the parts regardless of the ID labels, thus alleviating the side effect of domain-specifi
&lt;/p&gt;</description></item><item><title>TempFuser&#26159;&#19968;&#31181;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#24182;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.03257</link><description>&lt;p&gt;
TempFuser: &#20351;&#29992;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer. (arXiv:2308.03257v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03257
&lt;/p&gt;
&lt;p&gt;
TempFuser&#26159;&#19968;&#31181;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#24182;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31354;&#20013;&#25112;&#26007;&#20013;&#65292;&#31354;&#25112;&#21160;&#20316;&#23545;&#25112;&#26415;&#26426;&#21160;&#21644;&#25935;&#25463;&#25112;&#26007;&#26426;&#30340;&#31354;&#27668;&#21160;&#21147;&#23398;&#37117;&#25552;&#20986;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TempFuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#30701;&#26102;&#24207;&#34701;&#21512;&#36716;&#25442;&#22120;&#65292;&#26088;&#22312;&#23398;&#20064;&#31354;&#20013;&#26684;&#26007;&#20013;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#39134;&#34892;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;LSTM&#30340;&#36755;&#20837;&#23884;&#20837;&#26469;&#32534;&#30721;&#38271;&#26399;&#31232;&#30095;&#21644;&#30701;&#26399;&#23494;&#38598;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#23884;&#20837;&#36890;&#36807;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25429;&#33719;&#20102;&#25112;&#26007;&#26426;&#30340;&#25112;&#26415;&#21644;&#25935;&#25463;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#31471;&#21040;&#31471;&#30340;&#39134;&#34892;&#25351;&#20196;&#65292;&#30830;&#20445;&#21344;&#25454;&#20248;&#21183;&#20301;&#32622;&#24182;&#36229;&#36234;&#23545;&#25163;&#12290;&#32463;&#36807;&#23545;&#39640;&#20445;&#30495;&#39134;&#34892;&#27169;&#25311;&#22120;&#20013;&#22810;&#31181;&#31867;&#22411;&#23545;&#25163;&#39134;&#26426;&#30340;&#24191;&#27867;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25191;&#34892;&#22797;&#26434;&#30340;&#25112;&#26007;&#21160;&#20316;&#65292;&#19988;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38754;&#23545;&#39640;&#32423;&#23545;&#25163;&#26102;&#23637;&#29616;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#25112;&#26415;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In aerial combat, dogfighting poses intricate challenges that demand an understanding of both strategic maneuvers and the aerodynamics of agile fighter aircraft. In this paper, we introduce TempFuser, a novel long short-term temporal fusion transformer designed to learn tactical and agile flight maneuvers in aerial dogfights. Our approach employs two distinct LSTM-based input embeddings to encode long-term sparse and short-term dense state representations. By integrating these embeddings through a transformer encoder, our model captures the tactics and agility of fighter jets, enabling it to generate end-to-end flight commands that secure dominant positions and outmaneuver the opponent. After extensive training against various types of opponent aircraft in a high-fidelity flight simulator, our model successfully learns to perform complex fighter maneuvers, consistently outperforming several baseline models. Notably, our model exhibits human-like strategic maneuvers even when facing adv
&lt;/p&gt;</description></item><item><title>LEMMA&#26159;&#19968;&#20010;&#23398;&#20064;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#19987;&#23478;&#31034;&#33539;&#21644;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#29289;&#20307;&#25805;&#20316;&#12290;&#23427;&#25552;&#20379;&#20102;&#28041;&#21450;&#24037;&#20855;&#20351;&#29992;&#21644;&#20256;&#36882;&#30340;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2308.00937</link><description>&lt;p&gt;
LEMMA: &#23398;&#20064;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LEMMA: Learning Language-Conditioned Multi-Robot Manipulation. (arXiv:2308.00937v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00937
&lt;/p&gt;
&lt;p&gt;
LEMMA&#26159;&#19968;&#20010;&#23398;&#20064;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#19987;&#23478;&#31034;&#33539;&#21644;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#29289;&#20307;&#25805;&#20316;&#12290;&#23427;&#25552;&#20379;&#20102;&#28041;&#21450;&#24037;&#20855;&#20351;&#29992;&#21644;&#20256;&#36882;&#30340;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#25805;&#32437;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20855;&#26377;&#20114;&#34917;&#21151;&#33021;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#21327;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#30340;&#26700;&#38754;&#35774;&#32622;&#20013;&#20219;&#21153;&#20998;&#37197;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#29289;&#20307;&#25805;&#20316;&#30340;LanguagE-Conditioned Multi-robot MAnipulation (LEMMA)&#22522;&#20934;&#12290;LEMMA&#20855;&#26377;8&#31181;&#31867;&#22411;&#30340;&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#19968;&#20123;&#20219;&#21153;&#35201;&#27714;&#26426;&#22120;&#20154;&#20351;&#29992;&#24037;&#20855;&#24182;&#30456;&#20114;&#20256;&#36882;&#24037;&#20855;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20379;800&#20010;&#19987;&#23478;&#31034;&#33539;&#21644;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#22521;&#35757;&#21644;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;LEMMA&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#31995;&#32479;&#35782;&#21035;&#27599;&#20010;&#25805;&#32437;&#22120;&#30340;&#38480;&#21046;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#37197;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#22788;&#29702;&#27599;&#20010;&#20219;&#21153;&#20013;&#30340;&#24378;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;LEMMA&#22312;&#24320;&#21457;&#26410;&#26469;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#25915;&#20987;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15971</link><description>&lt;p&gt;
&#20320;&#21487;&#20197;&#36890;&#36807;&#21518;&#38376;&#25915;&#20987;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
You Can Backdoor Personalized Federated Learning. (arXiv:2307.15971v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#25915;&#20987;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#36890;&#29992;FL&#22330;&#26223;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#21363;&#25152;&#26377;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65306;BapFL&#65292;BapFL+&#21644;Gen-BapFL&#65292;&#24182;&#32463;&#39564;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25915;&#20987;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65288;RmLR&#65289;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#26412;&#30693;&#35782;&#22686;&#24378;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#65292;&#36890;&#36807;&#20877;&#25366;&#25496;&#31574;&#30053;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#30340;&#21477;&#23376;&#21644;&#35789;&#32423;&#23545;&#40784;&#20197;&#21450;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#26469;&#35299;&#20915;&#22810;&#23545;&#22810;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13529</link><description>&lt;p&gt;
Re-mine, Learn and Reason: &#25506;&#32034;&#35821;&#35328;&#24341;&#23548;&#19979;&#36328;&#27169;&#24577;&#35821;&#20041;&#30456;&#20851;&#24615;&#30340;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection. (arXiv:2307.13529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65288;RmLR&#65289;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#26412;&#30693;&#35782;&#22686;&#24378;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#65292;&#36890;&#36807;&#20877;&#25366;&#25496;&#31574;&#30053;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#30340;&#21477;&#23376;&#21644;&#35789;&#32423;&#23545;&#40784;&#20197;&#21450;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#26469;&#35299;&#20915;&#22810;&#23545;&#22810;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#38656;&#35201;&#35270;&#35273;&#27169;&#22411;&#35299;&#20915;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#39044;&#27979;HOI&#19977;&#20803;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65288;RmLR&#65289;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#26412;&#30693;&#35782;&#26469;&#22686;&#24378;HOI&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#20102;&#20004;&#38454;&#27573;HOI&#26816;&#27979;&#22120;&#20013;&#20132;&#20114;&#20449;&#24687;&#30340;&#25439;&#22833;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20877;&#25366;&#25496;&#31574;&#30053;&#26469;&#29983;&#25104;&#26356;&#20840;&#38754;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#21477;&#23376;&#21644;&#35789;&#32423;&#23545;&#40784;&#20197;&#21450;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#22810;&#20010;&#20132;&#20114;&#21644;&#22810;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#22810;&#23545;&#22810;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#20123;&#31574;&#30053;&#20943;&#36731;&#20102;&#22810;&#20010;&#20132;&#20114;&#23548;&#33268;&#30340;&#21305;&#37197;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65292;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#39592;&#26550;&#34920;&#31034;&#24182;&#21033;&#29992;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.12917</link><description>&lt;p&gt;
&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#19982;&#30828;&#39592;&#26550;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26080;&#30417;&#30563;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65292;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#39592;&#26550;&#34920;&#31034;&#24182;&#21033;&#29992;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20174;&#36523;&#20307;&#20851;&#33410;&#23398;&#20064;&#21333;&#23618;&#27425;&#30340;&#39592;&#26550;&#29305;&#24449;&#65292;&#24182;&#20551;&#35774;&#39592;&#26550;&#30340;&#37325;&#35201;&#24615;&#30456;&#31561;&#65292;&#22240;&#27492;&#36890;&#24120;&#26080;&#27861;&#21033;&#29992;&#26356;&#22810;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#65288;&#22914;&#32930;&#20307;&#23618;&#27425;&#65289;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26631;&#31614;&#20381;&#36182;&#24615;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23398;&#20064;&#26356;&#19968;&#33324;&#30340;&#39592;&#26550;&#34920;&#31034;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#30417;&#30563;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65288;HSM&#65289;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#39592;&#26550;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#20197;&#27169;&#25311;&#36523;&#20307;&#20851;&#33410;&#12289;&#32452;&#20214;&#21644;&#32930;&#20307;&#23618;&#27425;&#30340;&#31895;&#21040;&#32454;&#30340;&#36523;&#20307;&#21644;&#21160;&#20316;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#21270;&#30340;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#24120;&#35782;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#25972;&#29702;&#26700;&#23376;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#25512;&#29702;&#20986;&#20154;&#31867;&#34892;&#20026;&#30340;&#24120;&#35782;&#12290;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#20294;&#36890;&#36807;&#32771;&#34385;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#22240;&#32032;&#65292;&#21487;&#20197;&#35299;&#20915;&#25972;&#29702;&#26700;&#23376;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.11319</link><description>&lt;p&gt;
&#22914;&#20309;&#25972;&#29702;&#19968;&#24352;&#26700;&#23376;&#65306;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#24120;&#35782;&#25512;&#29702;&#35299;&#20915;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives. (arXiv:2307.11319v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#24120;&#35782;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#25972;&#29702;&#26700;&#23376;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#25512;&#29702;&#20986;&#20154;&#31867;&#34892;&#20026;&#30340;&#24120;&#35782;&#12290;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#20294;&#36890;&#36807;&#32771;&#34385;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#22240;&#32032;&#65292;&#21487;&#20197;&#35299;&#20915;&#25972;&#29702;&#26700;&#23376;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27169;&#31946;&#30340;&#30446;&#26631;&#32473;&#26426;&#22120;&#20154;&#25216;&#26415;&#24102;&#26469;&#20102;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24456;&#38590;&#23450;&#20041;&#35268;&#21017;&#12289;&#22870;&#21169;&#25110;&#32422;&#26463;&#20197;&#36827;&#34892;&#20248;&#21270;&#12290;&#20687;&#25972;&#29702;&#19968;&#24352;&#20940;&#20081;&#30340;&#26700;&#23376;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;&#20154;&#31867;&#26469;&#35828;&#21487;&#33021;&#24456;&#31616;&#21333;&#65292;&#20294;&#30001;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#27495;&#20041;&#21644;&#28789;&#27963;&#24615;&#65292;&#34920;&#36798;&#25972;&#27905;&#30340;&#26631;&#20934;&#21364;&#24456;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#20013;&#65292;&#25105;&#20204;&#26377;&#20102;&#36890;&#36807;&#36825;&#20123;&#27169;&#31946;&#30446;&#26631;&#36827;&#34892;&#25512;&#29702;&#30340;&#26426;&#20250;&#65306;LLM&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#20154;&#31867;&#25968;&#25454;&#26469;&#25429;&#25417;&#26377;&#20851;&#20154;&#31867;&#34892;&#20026;&#30340;&#26377;&#24847;&#20041;&#30340;&#24120;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#20165;&#35757;&#32451;&#20110;&#35821;&#35328;&#36755;&#20837;&#65292;&#23427;&#20204;&#22312;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#21487;&#33021;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25972;&#29702;&#26700;&#23376;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#27169;&#31946;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25972;&#29702;&#19968;&#24352;&#26700;&#23376;&#30340;&#20219;&#21153;&#19981;&#20165;&#28041;&#21450;&#25353;&#29031;&#31867;&#22411;&#21644;&#21151;&#33021;&#23545;&#29289;&#20307;&#36827;&#34892;&#32858;&#31867;&#20197;&#23454;&#29616;&#35821;&#20041;&#25972;&#27905;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#24863;&#30693;&#21644;&#20302;&#32423;&#25511;&#21046;&#26041;&#38754;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vague objectives in many real-life scenarios pose long-standing challenges for robotics, as defining rules, rewards, or constraints for optimization is difficult. Tasks like tidying a messy table may appear simple for humans, but articulating the criteria for tidiness is complex due to the ambiguity and flexibility in commonsense reasoning. Recent advancement in Large Language Models (LLMs) offers us an opportunity to reason over these vague objectives: learned from extensive human data, LLMs capture meaningful common sense about human behavior. However, as LLMs are trained solely on language input, they may struggle with robotic tasks due to their limited capacity to account for perception and low-level controls. In this work, we propose a simple approach to solve the task of table tidying, an example of robotic tasks with vague objectives. Specifically, the task of tidying a table involves not just clustering objects by type and functionality for semantic tidiness but also considerin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#26816;&#27979;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#22312;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#32473;&#20986;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#21028;&#26029;&#12290;&#36825;&#23545;&#20110;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#22914;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#20005;&#37325;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2307.10487</link><description>&lt;p&gt;
&#24694;&#24847;&#27880;&#37322;&#19979;&#30340;&#29289;&#20307;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack against Object Detection with Clean Annotation. (arXiv:2307.10487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#26816;&#27979;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#22312;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#32473;&#20986;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#21028;&#26029;&#12290;&#36825;&#23545;&#20110;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#22914;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#20005;&#37325;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20063;&#21457;&#29616;DNN&#23545;&#22810;&#31181;&#25915;&#20987;&#65292;&#21253;&#25324;&#21518;&#38376;&#25915;&#20987;&#65292;&#26159;&#33030;&#24369;&#30340;&#12290;&#36890;&#36807;&#36825;&#31181;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#25104;&#21151;&#22320;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#23884;&#20837;&#21040;DNN&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#33391;&#24615;&#25968;&#25454;&#26679;&#26412;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#22312;&#39044;&#23450;&#20041;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#32473;&#20986;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#21028;&#26029;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#23581;&#35797;&#20102;&#22823;&#37327;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#23578;&#26410;&#24471;&#21040;&#36866;&#24403;&#30340;&#35843;&#26597;&#21644;&#25506;&#32034;&#12290;&#30001;&#20110;&#29289;&#20307;&#26816;&#27979;&#24050;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#30340;&#37325;&#35201;&#27169;&#22359;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#65292;&#23545;&#29289;&#20307;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#36896;&#25104;&#26356;&#20005;&#37325;&#30340;&#23041;&#32961;&#12290;&#21463;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#22266;&#26377;&#23646;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29289;&#20307;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#32780;&#19981;&#20462;&#25913;&#22320;&#38754;&#30495;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28388;&#27874;&#21644;&#27169;&#24335;&#35782;&#21035;&#24378;&#21270;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#34987;&#29992;&#20110;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#32500;&#24230;&#20351;&#24471;&#20854;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;POD&#31561;&#38477;&#32500;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODEs)&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#19982;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#26041;&#24046;&#20445;&#25345;&#20026;&#22522;&#30784;&#30340;&#25554;&#20540;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#65292;&#32463;&#29702;&#35770;&#22522;&#30784;&#21644;&#26694;&#26550;&#25512;&#23548;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#22256;&#38590;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#35757;&#32451;&#20415;&#21033;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#23637;&#31034;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#22522;&#20934;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08527</link><description>&lt;p&gt;
&#20197;&#26041;&#24046;&#20445;&#25345;&#20026;&#22522;&#30784;&#30340;&#25554;&#20540;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement. (arXiv:2306.08527v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#26041;&#24046;&#20445;&#25345;&#20026;&#22522;&#30784;&#30340;&#25554;&#20540;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#65292;&#32463;&#29702;&#35770;&#22522;&#30784;&#21644;&#26694;&#26550;&#25512;&#23548;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#22256;&#38590;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#35757;&#32451;&#20415;&#21033;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#23637;&#31034;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#22522;&#20934;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#35821;&#38899;&#22686;&#24378;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#39318;&#20808;&#24378;&#35843;&#20102;&#36830;&#32493;&#26465;&#20214;&#19979;&#20197;&#26041;&#24046;&#20445;&#25345;&#20026;&#22522;&#30784;&#30340;&#25554;&#20540;&#25193;&#25955;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#31616;&#27905;&#30340;&#26694;&#26550;&#65292;&#23558;&#20197;&#26041;&#24046;&#20445;&#25345;&#21644;&#20197;&#26041;&#24046;&#29190;&#28856;&#20026;&#22522;&#30784;&#30340;&#25554;&#20540;&#25193;&#25955;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#26159;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#26041;&#24046;&#20445;&#25345;&#20026;&#22522;&#30784;&#30340;&#25554;&#20540;&#25193;&#25955;&#22312;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#31034;&#20363;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#21644;&#27169;&#22411;&#35757;&#32451;&#30340;&#20415;&#21033;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#21487;&#35843;&#33410;&#30340;&#36229;&#21442;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#22522;&#20934;&#36827;&#34892;&#20102;&#20960;&#31181;&#26041;&#27861;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this study is to implement diffusion models for speech enhancement (SE). The first step is to emphasize the theoretical foundation of variance-preserving (VP)-based interpolation diffusion under continuous conditions. Subsequently, we present a more concise framework that encapsulates both the VP- and variance-exploding (VE)-based interpolation diffusion methods. We demonstrate that these two methods are special cases of the proposed framework. Additionally, we provide a practical example of VP-based interpolation diffusion for the SE task. To improve performance and ease model training, we analyze the common difficulties encountered in diffusion models and suggest amenable hyper-parameters. Finally, we evaluate our model against several methods using a public benchmark to showcase the effectiveness of our approach
&lt;/p&gt;</description></item><item><title>&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.02552</link><description>&lt;p&gt;
&#24403;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#36935;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#25143;&#27169;&#25311;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. (arXiv:2306.02552v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02552
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36275;&#22815;&#21644;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#19968;&#30452;&#26159;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20027;&#35266;&#21644;&#22797;&#26434;&#24615;&#36136;&#65292;&#21487;&#38752;&#22320;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#23637;&#31034;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20026;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#20250;&#65292;&#24182;&#26377;&#21487;&#33021;&#25913;&#21464;&#20256;&#32479;&#30340;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#25506;&#32034;&#20351;&#29992;LLM&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#29992;&#25143;&#35270;&#20026;&#22522;&#20110;LLM&#30340;&#33258;&#27835;&#26234;&#33021;&#20307;&#65292;&#24182;&#35753;&#19981;&#21516;&#26234;&#33021;&#20307;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#33258;&#30001;&#20132;&#27969;&#12289;&#34892;&#20026;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a vir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18952</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#26356;&#26032;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;(IR)&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#35821;&#26009;&#24211;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#26159;&#19981;&#26029;&#26356;&#26032;&#30340;&#12290;&#26412;&#25991;&#23558;&#30693;&#35782;&#30340;&#21160;&#24577;&#24615;&#24341;&#20837;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#23558;&#26816;&#32034;&#35270;&#20026;&#21160;&#24577;&#30340;&#30693;&#35782;&#24211;&#65292;&#26356;&#31526;&#21512;&#30495;&#23454;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21452;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#26816;&#32034;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21033;&#29992;StreamingQA&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#26816;&#32034;&#23545;&#26032;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Dynamic Generative Retrieval (DynamicGR)&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#21457;&#29616;&#12290;&#23427;&#33021;&#22815;&#22312;&#20854;&#20869;&#37096;&#32034;&#24341;&#20013;&#39640;&#25928;&#21387;&#32553;&#26032;&#30340;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
&lt;/p&gt;</description></item><item><title>KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16437</link><description>&lt;p&gt;
KeyPosS: &#22522;&#20110; GPS &#28789;&#24863;&#30340;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#30340;&#21363;&#25554;&#21363;&#29992;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration. (arXiv:2305.16437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16437
&lt;/p&gt;
&lt;p&gt;
KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#37096;&#20998;&#26512;&#39046;&#22495;&#65292;&#20934;&#30830;&#30340;&#26631;&#35760;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20154;&#33080;&#35782;&#21035;&#21644;&#34920;&#24773;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28909;&#21147;&#22270;&#25110;&#22352;&#26631;&#22238;&#24402;&#25216;&#26415;&#32463;&#24120;&#38754;&#20020;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; KeyPoint Positioning System&#65288;KeyPosS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;KeyPosS&#39318;&#27425;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#65292;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;GPS&#31995;&#32479;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#36317;&#31163;&#22270;&#65292;&#35745;&#31639;&#24863;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#19982;&#22810;&#20010;&#38170;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#24039;&#22937;&#22320;&#21033;&#29992;&#36825;&#20123;&#38170;&#28857;&#26469;&#19977;&#35282;&#27979;&#37327;POI&#30340;&#20301;&#32622;&#65292;&#23454;&#29616;&#38754;&#37096;&#26631;&#35760;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of facial analysis, accurate landmark detection is crucial for various applications, ranging from face recognition and expression analysis to animation. Conventional heatmap or coordinate regression-based techniques, however, often face challenges in terms of computational burden and quantization errors. To address these issues, we present the KeyPoint Positioning System (KeyPosS), a groundbreaking facial landmark detection framework that stands out from existing methods. For the first time, KeyPosS employs the True-range Multilateration algorithm, a technique originally used in GPS systems, to achieve rapid and precise facial landmark detection without relying on computationally intensive regression approaches. The framework utilizes a fully convolutional network to predict a distance map, which computes the distance between a Point of Interest (POI) and multiple anchor points. These anchor points are ingeniously harnessed to triangulate the POI's position through the Tru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;</title><link>http://arxiv.org/abs/2305.14459</link><description>&lt;p&gt;
&#36890;&#36807;&#25688;&#35201;&#20108;&#20803;&#24615;&#21644;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24320;&#25918;&#24335;&#38271;&#25991;&#26412;&#29983;&#25104;&#38754;&#20020;&#35821;&#20041;&#19981;&#36830;&#36143;&#21644;&#24773;&#33410;&#19981;&#21487;&#20449;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#35774;&#35745;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#30701;&#35821;&#25110;&#25277;&#35937;&#20449;&#21495;&#30340;&#22823;&#32434;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#12290;&#22312;&#20551;&#35774;&#25688;&#35201;&#20316;&#20026;&#24050;&#25104;&#29087;&#30340;&#22823;&#32434;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#12289;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25688;&#35201;&#20219;&#21153;&#30340;&#21452;&#37325;&#29305;&#24449;&#26469;&#25913;&#36827;&#22823;&#32434;&#39044;&#27979;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22823;&#32434;&#30340;&#29983;&#25104;&#20855;&#26377;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-2&#12289;BART&#65289;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Vicuna&#12289;ChatGPT&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.11564</link><description>&lt;p&gt;
&#35299;&#32806;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#65306;&#21487;&#25554;&#25300;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#21508;&#31181;&#30693;&#35782;&#12290; &#28982;&#32780;&#65292;&#23558;&#30693;&#35782;&#38544;&#21547;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#20855;&#26377;&#20004;&#20010;&#22522;&#26412;&#32570;&#28857;&#12290; &#39318;&#20808;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#65292;&#26080;&#27861;&#32534;&#36753;&#25110;&#25193;&#23637;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#30693;&#35782;&#19981;&#26029;&#21457;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290; &#20854;&#27425;&#65292;&#23427;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#24182;&#38459;&#27490;&#20154;&#20204;&#20102;&#35299;PLM&#22312;&#26576;&#20010;&#38382;&#39064;&#19978;&#25152;&#38656;&#30340;&#21738;&#20123;&#30693;&#35782;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;PlugLM&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#21487;&#24494;&#20998;&#25554;&#20214;&#23384;&#20648;&#22120;&#65288;DPM&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290; &#20851;&#38190;&#30340;&#30452;&#35273;&#26159;&#20351;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#23558;&#30693;&#35782;&#23384;&#20648;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#35774;&#32622;&#38656;&#35201;&#21508;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#65306;&#65288;1&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;&#65288;2&#65289;&#26410;&#35265;&#23454;&#20307;&#21512;&#24182;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22312;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;AI&#21644;&#21306;&#22359;&#38142;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23433;&#20840;&#35748;&#35777;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;AI&#21644;&#21306;&#22359;&#38142;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;&#25945;&#32946;&#23454;&#36341;&#20013;&#12290;&#24635;&#20043;&#65292;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#26377;&#26395;&#25104;&#20026;&#21487;&#25345;&#32493;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.01088</link><description>&lt;p&gt;
AI&#21644;&#21306;&#22359;&#38142;&#20316;&#20026;&#21487;&#25345;&#32493;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#26469;&#24212;&#23545;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
AI &amp; Blockchain as sustainable teaching and learning tools to cope with the 4IR. (arXiv:2305.01088v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;AI&#21644;&#21306;&#22359;&#38142;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23433;&#20840;&#35748;&#35777;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;AI&#21644;&#21306;&#22359;&#38142;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;&#25945;&#32946;&#23454;&#36341;&#20013;&#12290;&#24635;&#20043;&#65292;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#26377;&#26395;&#25104;&#20026;&#21487;&#25345;&#32493;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#27491;&#22312;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#27963;&#21644;&#24037;&#20316;&#65292;&#32780;&#25945;&#32946;&#20063;&#19981;&#20363;&#22806;&#12290;&#20026;&#20102;&#24212;&#23545;4IR&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#21019;&#26032;&#21644;&#21487;&#25345;&#32493;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#36825;&#26041;&#38754;&#25317;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22914;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23433;&#20840;&#35748;&#35777;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#32593;&#32476;&#12290;&#26412;&#25991;&#23545;AI&#21644;&#21306;&#22359;&#38142;&#22312;&#25945;&#32946;&#20013;&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20998;&#26512;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#28508;&#22312;&#21033;&#30410;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27169;&#22411;&#65292;&#23558;AI&#21644;&#21306;&#22359;&#38142;&#25972;&#21512;&#21040;&#21487;&#25345;&#32493;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#23454;&#36341;&#20013;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#38656;&#35201;&#26356;&#22810;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#25506;&#32034;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#26412;&#25991;&#35752;&#35770;&#30340;&#20851;&#38190;&#25688;&#35201;&#26159;&#65292;&#36890;&#36807;&#25552;&#39640;&#25945;&#32946;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;AI&#21644;&#21306;&#22359;&#38142;&#26377;&#28508;&#21147;&#25104;&#20026;&#21487;&#25345;&#32493;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fourth Industrial Revolution (4IR) is transforming the way we live and work, and education is no exception. To cope with the challenges of 4IR, there is a need for innovative and sustainable teaching and learning tools. AI and block chain technologies hold great promise in this regard, with potential benefits such as personalized learning, secure credentialing, and decentralized learning networks. This paper presents a review of existing research on AI and block chain in education, analyzing case studies and exploring the potential benefits and challenges of these technologies. The paper also suggests a unique model for integrating AI and block chain into sustainable teaching and learning practices. Future research directions are discussed, including the need for more empirical studies and the exploration of ethical and social implications. The key summary of this discussion is that, by enhancing accessibility, efficacy, and security in education, AI and blockchain have the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14094</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#30068;&#22522;&#30784;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#24418;&#24335;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#22238;&#31572;&#19982;AI&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#35780;&#35770;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#25968;&#23398;&#22522;&#30784;&#26469;&#23450;&#20041;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#21363;&#20351;&#8220;&#35299;&#37322;&#8221;&#36825;&#20010;&#26415;&#35821;&#36824;&#32570;&#20047;&#31934;&#30830;&#23450;&#20041;&#12290;&#36825;&#20123;&#35780;&#35770;&#36824;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#20581;&#20840;&#32780;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;AI&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#19981;&#33391;&#25552;&#20986;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#27983;&#35272;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30693;&#35782;&#20307;&#31995;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#26159;&#22635;&#34917;&#35813;&#31354;&#30333;&#30340;&#39318;&#27425;&#23581;&#35797;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#21453;&#39304;&#21333;&#35843;&#33539;&#30068;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;AI&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36981;&#24490;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24341;&#20837;&#30340;&#29702;&#35770;&#26469;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#25152;&#26377;&#20027;&#35201;XAI&#31995;&#32479;&#31867;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21040;&#23494;&#38598;&#30340;&#20307;&#32032;&#21306;&#22495;&#21152;&#24378;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25237;&#24433;&#27599;&#20010;&#20307;&#32032;&#20869;&#37096;&#30340;&#31232;&#30095;&#23616;&#37096;&#28857;&#20113;&#33719;&#24471;&#20307;&#32032;&#21306;&#22495;&#65292;&#26356;&#22909;&#22320;&#23545;&#40784;&#21644;&#36991;&#20813;&#32972;&#26223;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#34701;&#21512;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08304</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#31232;&#30095;&#21040;&#23494;&#38598;&#20307;&#32032;&#21306;&#22495;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection. (arXiv:2304.08304v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21040;&#23494;&#38598;&#30340;&#20307;&#32032;&#21306;&#22495;&#21152;&#24378;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25237;&#24433;&#27599;&#20010;&#20307;&#32032;&#20869;&#37096;&#30340;&#31232;&#30095;&#23616;&#37096;&#28857;&#20113;&#33719;&#24471;&#20307;&#32032;&#21306;&#22495;&#65292;&#26356;&#22909;&#22320;&#23545;&#40784;&#21644;&#36991;&#20813;&#32972;&#26223;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#34701;&#21512;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#22810;&#27169;&#24577;&#26041;&#27861;&#25104;&#20026;&#36235;&#21183;&#65292;&#20027;&#35201;&#22240;&#20026;LiDAR&#28857;&#20113;&#21644;&#22270;&#20687;&#25968;&#25454;&#30340;&#34917;&#20805;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#28857;&#20113;&#31232;&#30095;&#25110;&#32773;LiDAR&#21644;&#30456;&#26426;&#20043;&#38388;&#20559;&#24046;&#23548;&#33268;&#30340;&#22122;&#22768;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;Voxel Region (VR)&#65292;&#36890;&#36807;&#21160;&#24577;&#25237;&#24433;&#27599;&#20010;&#20307;&#32032;&#20013;&#30340;&#31232;&#30095;&#23616;&#37096;&#28857;&#20113;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#31216;&#20026;Sparse-to-Dense Voxel Region Fusion (SDVRF)&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;VR&#20869;&#37096;&#36739;&#22810;&#30340;&#22270;&#20687;&#29305;&#24449;&#22270;&#20687;&#32032;&#25910;&#38598;&#36215;&#26469;&#65292;&#20197;&#34917;&#20805;&#20174;&#31232;&#30095;&#28857;&#20013;&#25552;&#21462;&#30340;&#20307;&#32032;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#23494;&#38598;&#30340;&#34701;&#21512;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#21306;&#22495;&#29983;&#25104;&#31574;&#30053;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#40784;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#22826;&#22810;&#30340;&#32972;&#26223;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34701;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19979;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the perception task of autonomous driving, multi-modal methods have become a trend due to the complementary characteristics of LiDAR point clouds and image data. However, the performance of previous methods is usually limited by the sparsity of the point cloud or the noise problem caused by the misalignment between LiDAR and the camera. To solve these two problems, we present a new concept, Voxel Region (VR), which is obtained by projecting the sparse local point clouds in each voxel dynamically. And we propose a novel fusion method, named Sparse-to-Dense Voxel Region Fusion (SDVRF). Specifically, more pixels of the image feature map inside the VR are gathered to supplement the voxel feature extracted from sparse points and achieve denser fusion. Meanwhile, different from prior methods, which project the size-fixed grids, our strategy of generating dynamic regions achieves better alignment and avoids introducing too much background noise. Furthermore, we propose a multi-scale fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07163</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;Bandit&#26041;&#27861;&#30340;&#26174;&#24335;&#22609;&#24418;&#22806;&#37096;&#24314;&#35758;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22806;&#37096;&#25110;&#19987;&#23478;&#30340;&#24314;&#35758;&#34701;&#20837;&#21040;&#23398;&#20064;&#24403;&#20013;&#12290;&#26412;&#25991;&#23558;&#23558;&#23558;&#27492;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#31216;&#20026;&#22609;&#24418;&#36172;&#21338;&#26426;&#65288;shaping-bandits&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;LQR&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19977;&#31181;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
&lt;/p&gt;</description></item><item><title>AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06364</link><description>&lt;p&gt;
AGIEval&#65306;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06364
&lt;/p&gt;
&lt;p&gt;
AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#32423;&#21035;&#20219;&#21153;&#30340;&#36890;&#29992;&#33021;&#21147;&#26159;&#23427;&#20204;&#22312;&#21457;&#23637;&#21644;&#24212;&#29992;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#20154;&#36896;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#20195;&#34920;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AGIEval&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#65292;&#27861;&#24459;&#23398;&#26657;&#20837;&#23398;&#32771;&#35797;&#65292;&#25968;&#23398;&#31454;&#36187;&#21644;&#24459;&#24072;&#36164;&#26684;&#32771;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324; GPT-4&#65292;ChatGPT &#21644;Text-Davinci-003&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;SAT&#25968;&#23398;&#27979;&#35797;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95%&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#22823;&#23398;&#33521;&#35821;&#32771;&#35797;&#30340;&#33521;&#35821;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20063;&#36798;&#21040;&#20102;92.5%&#12290;&#36825;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;AGI&#26410;&#26469;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary fou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#26465;&#20214;&#30340;&#25193;&#25955;&#26041;&#27861;&#26469;&#24314;&#27169;&#36523;&#20307;&#23039;&#24577;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22312;&#20010;&#20154;&#35270;&#35282;&#19979;3D&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#23039;&#24577;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#35757;&#32451;&#20013;&#26080;&#38656;&#20998;&#31867;&#22120;&#65292;&#37319;&#26679;&#20855;&#26377;&#19981;&#21516;&#30340;&#26465;&#20214;&#21644;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06024</link><description>&lt;p&gt;
&#20174;&#20010;&#20154;&#35282;&#24230;&#35270;&#22270;&#20013;&#19977;&#32500;&#22330;&#26223;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#27010;&#29575;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views. (arXiv:2304.06024v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#26465;&#20214;&#30340;&#25193;&#25955;&#26041;&#27861;&#26469;&#24314;&#27169;&#36523;&#20307;&#23039;&#24577;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22312;&#20010;&#20154;&#35270;&#35282;&#19979;3D&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#23039;&#24577;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#35757;&#32451;&#20013;&#26080;&#38656;&#20998;&#31867;&#22120;&#65292;&#37319;&#26679;&#20855;&#26377;&#19981;&#21516;&#30340;&#26465;&#20214;&#21644;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#33258;&#21160;&#24863;&#30693;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#20174;&#20010;&#20154;&#35270;&#22270;&#20013;&#20272;&#35745;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#21644;&#24418;&#24577;&#12290;&#36825;&#39033;&#20219;&#21153;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30001;&#20110;&#20010;&#20154;&#22330;&#26223;&#20013;&#30340;&#36817;&#36317;&#31163;&#23548;&#33268;&#36523;&#20307;&#34987;&#25130;&#26029;&#20005;&#37325;&#65292;&#20174;&#32780;&#23548;&#33268;&#30475;&#19981;&#35265;&#36523;&#20307;&#37096;&#20214;&#30340;&#22823;&#37327;&#23039;&#24577;&#27169;&#31946;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#26465;&#20214;&#30340;&#25193;&#25955;&#26041;&#27861;&#26469;&#27169;&#25311;&#36523;&#20307;&#23039;&#24577;&#20998;&#24067;&#12290;&#22312;3D&#22330;&#26223;&#20960;&#20309;&#26465;&#20214;&#30340;&#32422;&#26463;&#19979;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22312;&#21512;&#29702;&#30340;&#20154;-&#22330;&#26223;&#20132;&#20114;&#20013;&#30340;&#36523;&#20307;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30896;&#25758;&#24471;&#20998;&#30340;&#37319;&#26679;&#26469;&#36827;&#19968;&#27493;&#35299;&#20915;&#20154;-&#22330;&#26223;&#30456;&#20114;&#28183;&#36879;&#38382;&#39064;&#12290;&#26080;&#38656;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#20351;&#24471;&#37319;&#26679;&#20855;&#26377;&#19981;&#21516;&#30340;&#26465;&#20214;&#21644;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;&#19968;&#20010;&#21487;&#35265;&#24615;&#24863;&#30693;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#36890;&#36807;&#27599;&#20010;&#20851;&#33410;&#30340;&#21487;&#35265;&#24230;&#26469;&#25351;&#23548;&#25193;&#25955;&#21435;&#22122;&#22120;&#65292;&#20197;&#21512;&#24182;&#20114;&#20851;&#33410;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic perception of human behaviors during social interactions is crucial for AR/VR applications, and an essential component is estimation of plausible 3D human pose and shape of our social partners from the egocentric view. One of the biggest challenges of this task is severe body truncation due to close social distances in egocentric scenarios, which brings large pose ambiguities for unseen body parts. To tackle this challenge, we propose a novel scene-conditioned diffusion method to model the body pose distribution. Conditioned on the 3D scene geometry, the diffusion model generates bodies in plausible human-scene interactions, with the sampling guided by a physics-based collision score to further resolve human-scene inter-penetrations. The classifier-free training enables flexible sampling with different conditions and enhanced diversity. A visibility-aware graph convolution model guided by per-joint visibility serves as the diffusion denoiser to incorporate inter-joint depende
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#22312;&#24555;&#36895;&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#26377;&#29992;&#65292;&#20294;&#36755;&#20986;&#30340;&#20195;&#30721;&#19981;&#36866;&#21512;&#24320;&#21457;&#32773;&#65292;&#23548;&#33268;&#20182;&#20204;&#19981;&#39640;&#39057;&#25509;&#21463;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21021;&#22987;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.17125</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#30340;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Usability of AI Programming Assistants. (arXiv:2303.17125v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17125
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#22312;&#24555;&#36895;&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#26377;&#29992;&#65292;&#20294;&#36755;&#20986;&#30340;&#20195;&#30721;&#19981;&#36866;&#21512;&#24320;&#21457;&#32773;&#65292;&#23548;&#33268;&#20182;&#20204;&#19981;&#39640;&#39057;&#25509;&#21463;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21021;&#22987;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#31038;&#21306;&#36817;&#24180;&#26469;&#24191;&#27867;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#32534;&#31243;&#21161;&#25163;&#65288;&#20363;&#22914;GitHub Copilot&#65289;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#24320;&#21457;&#32773;&#24182;&#19981;&#39640;&#39057;&#25509;&#21463;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21021;&#22987;&#24314;&#35758;&#12290;&#36825;&#24341;&#21457;&#20102;&#19982;&#36825;&#20123;&#24037;&#20855;&#21487;&#29992;&#24615;&#30456;&#20851;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#24320;&#21457;&#32773;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#23454;&#36341;&#24773;&#20917;&#21644;&#20182;&#20204;&#38754;&#20020;&#30340;&#37325;&#35201;&#30340;&#21487;&#29992;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#21521;&#22823;&#37327;&#24320;&#21457;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#20174;410&#21517;&#24320;&#21457;&#32773;&#20013;&#33719;&#24471;&#20102;&#22238;&#22797;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24320;&#21457;&#32773;&#26368;&#26377;&#21160;&#21147;&#20351;&#29992;AI&#32534;&#31243;&#21161;&#25163;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#32773;&#20943;&#23569;&#25353;&#38190;&#27425;&#25968;&#65292;&#24555;&#36895;&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#24182;&#35843;&#29992;&#35821;&#27861;&#65292;&#20294;&#23427;&#23545;&#24110;&#21161;&#24320;&#21457;&#32773;&#24605;&#32771;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#25903;&#25345;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24320;&#21457;&#32773;&#19981;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#30340;&#26368;&#37325;&#35201;&#21407;&#22240;&#26159;&#36825;&#20123;&#24037;&#20855;&#19981;&#33021;&#36755;&#20986;&#36866;&#21512;&#20182;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that add
&lt;/p&gt;</description></item><item><title>RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.07622</link><description>&lt;p&gt;
RE-MOVE&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#21160;&#24577;&#29615;&#22659;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07622
&lt;/p&gt;
&lt;p&gt;
RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#22312;&#23454;&#26102;&#37096;&#32626;&#26399;&#38388;&#36866;&#24212;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RE-MOVE&#65288;&#35831;&#27714;&#24110;&#21161;&#24182;&#31227;&#21160;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#26469;&#35843;&#25972;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#20197;&#36866;&#24212;&#29615;&#22659;&#30340;&#23454;&#26102;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#35831;&#27714;&#21453;&#39304;&#24182;&#22914;&#20309;&#23558;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20013;&#12290;RE-MOVE&#21033;&#29992;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#35831;&#27714;&#20154;&#31867;&#21453;&#39304;&#30340;&#26368;&#20339;&#26102;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#36827;&#34892;&#23454;&#26102;&#36866;&#24212;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21512;&#25104;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#27979;&#35797;&#26102;&#38388;&#21160;&#24577;&#23548;&#33322;&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.02731</link><description>&lt;p&gt;
&#23548;&#33322;&#30340;&#20013;&#23618;&#34920;&#31034;&#8212;&#8212;&#34394;&#25311;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#22320;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#25351;&#24341;&#32473;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#23548;&#33322;&#20449;&#24687;&#26159;&#22810;&#27169;&#24577;&#30340;&#26102;&#20505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#20197;&#35270;&#35273;&#26041;&#24335;&#21576;&#29616;&#38750;&#35270;&#35273;&#25351;&#20196;&#20449;&#21495;&#12290;&#36825;&#20123;&#35270;&#35273;&#25351;&#24341;&#20197;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#21472;&#21152;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#65292;&#20316;&#20026;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#34394;&#25311;&#23548;&#33322;&#22312;&#22810;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#28151;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#34394;&#25311;&#23548;&#33322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23558;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#25351;&#20196;&#36716;&#25442;&#20026;&#29992;&#20110;&#30495;&#23454;&#29615;&#22659;&#23454;&#39564;&#30340;&#30452;&#35266;&#35270;&#35273;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#34394;&#25311;&#23548;&#33322;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#27169;&#25311;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#65292;&#20197;&#21450;&#27169;&#25311;&#26234;&#33021;&#20307;&#25968;&#37327;&#36234;&#22810;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#25928;&#26524;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.13423</link><description>&lt;p&gt;
&#27169;&#25311;&#19982;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#22312;&#25805;&#32437;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach. (arXiv:2302.13423v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#27169;&#25311;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#65292;&#20197;&#21450;&#27169;&#25311;&#26234;&#33021;&#20307;&#25968;&#37327;&#36234;&#22810;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#25928;&#26524;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#26159;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#26082;&#19981;&#39640;&#25928;&#65288;&#21363;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#36739;&#24930;&#65289;&#65292;&#20063;&#19981;&#26377;&#25928;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#36739;&#23569;&#65289;&#12290;&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#26102;&#38388;&#21644;&#30828;&#20214;&#39044;&#31639;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#30340;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; (CSAR)&#65292;&#29992;&#20110;&#25805;&#32437;&#22120;&#20154;&#30340;&#25361;&#36873;&#21644;&#25918;&#32622;&#20219;&#21153;&#65292;&#35813;&#31639;&#27861;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#19990;&#30028;&#20013;&#37117;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#26469;&#33719;&#24471;&#27169;&#25311;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#65288;1&#65289;&#22312;&#27169;&#25311;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#24182;&#19981;&#26159;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#65288;2&#65289;&#27169;&#25311;&#26234;&#33021;&#20307;&#36234;&#22810;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#25928;&#26524;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sim-and-real training is a promising alternative to sim-to-real training for robot manipulations. However, the current sim-and-real training is neither efficient, i.e., slow convergence to the optimal policy, nor effective, i.e., sizeable real-world robot data. Given limited time and hardware budgets, the performance of sim-and-real training is not satisfactory. In this paper, we propose a Consensus-based Sim-And-Real deep reinforcement learning algorithm (CSAR) for manipulator pick-and-place tasks, which shows comparable performance in both sim-and-real worlds. In this algorithm, we train the agents in simulators and the real world to get the optimal policies for both sim-and-real worlds. We found two interesting phenomenons: (1) Best policy in simulation is not the best for sim-and-real training. (2) The more simulation agents, the better sim-and-real training. The experimental video is available at: https://youtu.be/mcHJtNIsTEQ.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;</title><link>http://arxiv.org/abs/2302.09512</link><description>&lt;p&gt;
SAT&#38656;&#35201;&#24443;&#24213;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SAT Requires Exhaustive Search. (arXiv:2302.09512v4 [cs.CC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;CSP&#21644;SAT&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#20123;&#20363;&#23376;&#26080;&#27861;&#22312;&#19981;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#36739;&#24369;&#30340;&#32467;&#35770;P $\neq$ NP&#12290;&#26412;&#25991;&#37319;&#29992;&#30340;&#26159;&#19968;&#31181;&#35777;&#26126;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#24314;&#35774;&#24615;&#26041;&#27861;&#65292;&#19982;&#30446;&#21069;&#35745;&#31639;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#65292;&#20294;&#19982;Kurt G\"{o}del&#22312;&#35777;&#26126;&#20182;&#33879;&#21517;&#30340;&#36923;&#36753;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#26102;&#20351;&#29992;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;&#27491;&#22914;G\"{o}del&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#24418;&#24335;&#19978;&#30340;&#19981;&#21487;&#35777;&#26126;&#24615;&#26159;&#21487;&#34892;&#30340;&#19968;&#26679;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#35745;&#31639;&#19978;&#30340;&#38590;&#24230;&#19981;&#26159;&#24456;&#38590;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;3-SAT&#65292;&#35777;&#26126;&#19979;&#30028;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#26377;&#21508;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#21487;&#29992;&#20110;&#36991;&#20813;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#38590;&#30340;&#20363;&#23376;&#20013;&#65292;&#24443;&#24213;&#25628;&#32034;&#21487;&#33021;&#26159;&#21807;&#19968;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#35777;&#26126;&#20854;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by constructing extremely hard examples of CSP (with large domains) and SAT (with long clauses), we prove that such examples cannot be solved without exhaustive search, which implies a weaker conclusion P $\neq$ NP. This constructive approach for proving impossibility results is very different (and missing) from those currently used in computational complexity theory, but is similar to that used by Kurt G\"{o}del in proving his famous logical impossibility results. Just as shown by G\"{o}del's results that proving formal unprovability is feasible in mathematics, the results of this paper show that proving computational hardness is not hard in mathematics. Specifically, proving lower bounds for many problems, such as 3-SAT, can be challenging because these problems have various effective strategies available for avoiding exhaustive search. However, in cases of extremely hard examples, exhaustive search may be the only viable option, and proving its necessity becomes more 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07729</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#37117;&#20197;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#21069;&#35328;&#65292;&#20197;&#24635;&#32467;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#20142;&#28857;&#19981;&#20165;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#35770;&#25991;&#30340;&#36129;&#29486;&#65292;&#36824;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#22686;&#21152;&#20102;&#25991;&#31456;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#30740;&#31350;&#35770;&#25991;&#30340;&#29305;&#23450;&#27573;&#33853;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#26500;&#24314;&#30740;&#31350;&#20142;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#35206;&#30422;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#23618;&#30340;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65292;&#23558;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20026;SciBERT&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;CSPubSum&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36824;&#25552;&#20986;&#20102;MixSub&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#26032;&#30340;&#36328;&#23398;&#31185;&#35770;&#25991;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;CSPubSum&#21644;MixSub&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#30456;&#20851;&#21464;&#20307;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;CSPubSum&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#35770;&#25991;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25216;&#33021;&#30340;&#20998;&#23618;&#39550;&#39542;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#20351;&#29992;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#39640;&#23618;&#21160;&#20316;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#21512;&#24182;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#26356;&#39640;&#24615;&#33021;&#30340;&#39550;&#39542;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.02179</link><description>&lt;p&gt;
&#39640;&#25928;&#24320;&#21457;&#39550;&#39542;&#31574;&#30053;&#65306;&#22522;&#20110;&#25216;&#33021;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach. (arXiv:2302.02179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25216;&#33021;&#30340;&#20998;&#23618;&#39550;&#39542;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#20351;&#29992;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#39640;&#23618;&#21160;&#20316;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#21512;&#24182;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#26356;&#39640;&#24615;&#33021;&#30340;&#39550;&#39542;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#39550;&#39542;&#27773;&#36710;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#23618;&#27425;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20154;&#31867;&#39550;&#39542;&#21592;&#21487;&#20197;&#36731;&#26494;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#22240;&#27492;&#19968;&#30452;&#22312;&#21162;&#21147;&#27169;&#25311;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20316;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#25110;&#21019;&#24314;&#30495;&#23454;&#24863;&#27169;&#25311;&#22120;&#30340;&#28789;&#24863;&#12290;&#24378;&#21270;&#23398;&#20064;&#26159;&#24314;&#27169;&#39550;&#39542;&#31574;&#30053;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#20294;&#20256;&#32479;&#30340;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;"&#22522;&#20110;&#25216;&#33021;"&#30340;&#20998;&#23618;&#39550;&#39542;&#31574;&#30053;&#65292;&#20854;&#20013;&#36816;&#21160;&#21407;&#35821;&#65288;&#21363;&#25216;&#33021;&#65289;&#34987;&#35774;&#35745;&#24182;&#29992;&#20316;&#39640;&#23618;&#21160;&#20316;&#12290;&#36825;&#20943;&#23569;&#20102;&#38656;&#35201;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#22810;&#20010;&#27169;&#22411;&#30340;&#24212;&#29992;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21512;&#24182;&#22330;&#26223;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#22312;&#36739;&#23569;&#35757;&#32451;&#27425;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#24615;&#33021;&#30340;&#39550;&#39542;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driving in dense traffic with human and autonomous drivers is a challenging task that requires high-level planning and reasoning. Human drivers can achieve this task comfortably, and there has been many efforts to model human driver strategies. These strategies can be used as inspirations for developing autonomous driving algorithms or to create high-fidelity simulators. Reinforcement learning is a common tool to model driver policies, but conventional training of these models can be computationally expensive and time-consuming. To address this issue, in this paper, we propose ``skill-based" hierarchical driving strategies, where motion primitives, i.e. skills, are designed and used as high-level actions. This reduces the training time for applications that require multiple models with varying behavior. Simulation results in a merging scenario demonstrate that the proposed approach yields driver models that achieve higher performance with less training compared to baseline reinforcemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;&#20256;&#32479;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24037;&#31243;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13331</link><description>&lt;p&gt;
&#31070;&#32463;&#25805;&#20316;&#21592;&#65306;&#25968;&#25454;&#26159;&#21542;&#36275;&#20197;&#27169;&#25311;&#19990;&#30028;&#65311;&#23545;&#29289;&#29702;&#21551;&#31034;&#26426;&#22120;&#23398;&#20064;&#24433;&#21709;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning. (arXiv:2301.13331v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;&#20256;&#32479;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24037;&#31243;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24120;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#26500;&#24314;&#35299;&#20915;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#22810;&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#27604;&#22914;&#28909;&#20256;&#23548;&#25110;&#22768;&#38899;&#20256;&#25773;&#12289;&#27969;&#20307;&#27969;&#21160;&#12289;&#24377;&#24615;&#12289;&#38745;&#30005;&#23398;&#12289;&#30005;&#21160;&#21147;&#23398;&#31561;&#12290;&#34429;&#28982;&#36825;&#22312;&#35299;&#20915;&#35768;&#22810;&#22797;&#26434;&#29616;&#35937;&#26041;&#38754;&#21457;&#25381;&#20102;&#20316;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#24120;&#35268;&#26041;&#27861;&#22914;&#26377;&#38480;&#20803;&#27861;&#65288;FEM&#65289;&#21644;&#26377;&#38480;&#24046;&#20998;&#27861;&#65288;FDM&#65289;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#30456;&#23545;&#20934;&#30830;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31163;&#25955;&#19981;&#21464;&#24615;&#21644;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#31561;&#20248;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22914;&#20309;&#19982;&#20256;&#32479;&#25216;&#26415;&#30456;&#36741;&#30456;&#25104;&#65292;&#35299;&#20915;&#24037;&#31243;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25351;&#20986;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;EquIN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#25968;&#25454;&#19978;&#20855;&#26377;&#19968;&#33324;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#32771;&#34385;&#31283;&#23450;&#23376;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.05231</link><description>&lt;p&gt;
&#22312;&#31283;&#23450;&#23376;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Representation Learning in the Presence of Stabilizers. (arXiv:2301.05231v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05231
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;EquIN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#25968;&#25454;&#19978;&#20855;&#26377;&#19968;&#33324;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#32771;&#34385;&#31283;&#23450;&#23376;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31561;&#21464;&#21516;&#26500;&#32593;&#32476;&#65288;EquIN&#65289;-&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#19982;&#25968;&#25454;&#19978;&#30340;&#19968;&#33324;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#24335;&#19981;&#21516;&#65292;EquIN&#36866;&#29992;&#20110;&#38750;&#33258;&#30001;&#32676;&#20316;&#29992;&#65292;&#21363;&#36890;&#36807;&#38750;&#24179;&#20961;&#23545;&#31216;&#24615;&#31283;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;EquIN&#22312;&#32676;&#35770;&#20013;&#30340;&#36712;&#36947;&#31283;&#23450;&#23376;&#23450;&#29702;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#36825;&#20445;&#35777;&#20102;&#29702;&#24819;&#30340;&#23398;&#20064;&#22120;&#20165;&#36890;&#36807;&#31561;&#21464;&#24615;&#35757;&#32451;&#26102;&#25512;&#26029;&#20986;&#21516;&#26500;&#34920;&#31034;&#65292;&#24182;&#23436;&#20840;&#25552;&#21462;&#20102;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#26059;&#36716;&#23545;&#31216;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#32771;&#34385;&#31283;&#23450;&#23376;&#21487;&#20197;&#25552;&#39640;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Equivariant Isomorphic Networks (EquIN) -- a method for learning representations that are equivariant with respect to general group actions over data. Differently from existing equivariant representation learners, EquIN is suitable for group actions that are not free, i.e., that stabilize data via nontrivial symmetries. EquIN is theoretically grounded in the orbit-stabilizer theorem from group theory. This guarantees that an ideal learner infers isomorphic representations while trained on equivariance alone and thus fully extracts the geometric structure of data. We provide an empirical investigation on image datasets with rotational symmetries and show that taking stabilizers into account improves the quality of the representations.
&lt;/p&gt;</description></item><item><title>&#35299;&#20915;&#22870;&#21169;&#20551;&#35774;&#65292;&#26126;&#30830;&#25351;&#26126;&#20551;&#35774;&#25104;&#31435;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#38544;&#21547;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.10420</link><description>&lt;p&gt;
&#35299;&#20915;&#22870;&#21169;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Settling the Reward Hypothesis. (arXiv:2212.10420v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10420
&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22870;&#21169;&#20551;&#35774;&#65292;&#26126;&#30830;&#25351;&#26126;&#20551;&#35774;&#25104;&#31435;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#38544;&#21547;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#20551;&#35774;&#35748;&#20026;&#65292;&#8220;&#25105;&#20204;&#25152;&#35828;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#37117;&#21487;&#20197;&#24819;&#35937;&#20026;&#26368;&#22823;&#21270;&#25509;&#25910;&#21040;&#30340;&#26631;&#37327;&#20449;&#21495;&#65288;&#22870;&#21169;&#65289;&#30340;&#32047;&#31215;&#24635;&#21644;&#30340;&#39044;&#26399;&#20540;&#12290;&#8221;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23436;&#20840;&#35299;&#20915;&#36825;&#20010;&#20551;&#35774;&#12290;&#36825;&#23558;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#32943;&#23450;&#25110;&#21542;&#23450;&#65292;&#32780;&#26159;&#23436;&#20840;&#25351;&#26126;&#20551;&#35774;&#25104;&#31435;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#38544;&#21547;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reward hypothesis posits that, "all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)." We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01197</link><description>&lt;p&gt;
FedALA: &#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01197
&lt;/p&gt;
&lt;p&gt;
FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#36825;&#20250;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated learning with Adaptive Local Aggregation&#65288;FedALA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#25429;&#25417;&#20840;&#23616;&#27169;&#22411;&#23545;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;FedALA&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#23616;&#37096;&#30446;&#26631;&#33258;&#36866;&#24212;&#32858;&#21512;&#19979;&#36733;&#30340;&#20840;&#23616;&#27169;&#22411;&#21644;&#26412;&#22320;&#27169;&#22411;&#20197;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21021;&#22987;&#21270;&#26412;&#22320;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;FedALA&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20351;&#29992;&#20102;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;FedALA&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#27604;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22810;3.27%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;ALA&#27169;&#22359;&#24212;&#29992;&#20110;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#22810;24.19%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.15421</link><description>&lt;p&gt;
VRDU&#65306;&#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#29702;&#35299;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20016;&#23500;&#35270;&#35273;&#21270;&#19994;&#21153;&#25991;&#26723;&#20197;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#33258;&#21160;&#21270;&#19994;&#21153;&#24037;&#20316;&#27969;&#31243;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21453;&#26144;&#24037;&#19994;&#20013;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Visually Rich Document Understanding (VRDU)&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;VRDU&#21253;&#21547;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#22810;&#31181;&#25361;&#25112;&#65306;&#20016;&#23500;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#20197;&#21450;&#20998;&#23618;&#23454;&#20307;; &#22797;&#26434;&#30340;&#27169;&#26495;&#65292;&#21253;&#25324;&#34920;&#26684;&#21644;&#22810;&#21015;&#24067;&#23616;; &#20197;&#21450;&#21333;&#20010;&#25991;&#26723;&#31867;&#22411;&#20013;&#19981;&#21516;&#24067;&#23616;&#65288;&#27169;&#26495;&#65289;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#23569;&#26679;&#26412;&#21644;&#24120;&#35268;&#23454;&#39564;&#35774;&#32622;&#65292;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21305;&#37197;&#31639;&#27861;&#26469;&#35780;&#20272;&#25552;&#21462;&#32467;&#26524;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24378;&#22522;&#32447;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;(1)&#36890;&#29992;n&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.05793</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#20248;&#21270;&#21644;&#37327;&#23376;&#36866;&#29992;&#24615;&#30340;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#20854;&#29289;&#29702;&#29305;&#24615;&#65288;&#20363;&#22914;&#23616;&#37096;&#24577;&#23494;&#24230;&#25110;&#26465;&#20214;&#30005;&#23548;&#65289;&#22312;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#21518;&#20316;&#20026;&#36755;&#20986;&#12290;&#19982;&#21453;&#21521;&#20256;&#25773;&#31867;&#20284;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;FNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;FNN&#20063;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#23376;&#31995;&#32479;&#65292;&#21253;&#25324;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#31995;&#32479;&#65292;&#24182;&#22312;&#26080;&#39044;&#22788;&#29702;&#25110;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21407;&#20301;&#20998;&#26512;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21518;&#65292;FNN&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#12290;&#23427;&#20204;&#30340;&#37327;&#23376;&#29305;&#24615;&#20063;&#24102;&#26469;&#20102;&#21508;&#31181;&#20248;&#21183;&#65306;&#37327;&#23376;&#30456;&#20851;&#24615;&#20351;&#32593;&#32476;&#36830;&#25509;&#26356;&#21152;&#36890;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#28040;&#22833;&#30340;&#26799;&#24230;&#38382;&#39064;&#65292;&#37327;&#23376;&#32416;&#32544;&#21017;&#20026;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16751</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Formalizing Statistical Causality via Modal Logic. (arXiv:2210.16751v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#32479;&#35745;&#22240;&#26524;&#35821;&#35328;&#65288;StaCL&#65289;&#65292;&#29992;&#20110;&#34920;&#36798;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#24182;&#25351;&#23450;&#22240;&#26524;&#25512;&#26029;&#30340;&#35201;&#27714;&#12290;StaCL&#36890;&#36807;&#24178;&#39044;&#30340;&#27169;&#24577;&#36816;&#31639;&#31526;&#65292;&#22312;&#19981;&#21516;&#21487;&#33021;&#30340;&#19990;&#30028;&#30340;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#34920;&#36798;&#22240;&#26524;&#23646;&#24615;&#65292;&#22312;Kripke&#27169;&#22411;&#20013;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;StaCL&#20844;&#24335;&#27491;&#24335;&#21270;&#27010;&#29575;&#20998;&#24067;&#12289;&#24178;&#39044;&#21644;&#22240;&#26524;&#35859;&#35789;&#30340;&#20844;&#29702;&#12290;&#36825;&#20123;&#20844;&#29702;&#36275;&#22815;&#34920;&#36798;Pearl&#30340;do-calculus&#35268;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35777;&#26126;&#20102;StaCL&#21487;&#20197;&#29992;&#20110;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a formal language for describing and explaining statistical causality. Concretely, we define Statistical Causality Language (StaCL) for expressing causal effects on random variables and specifying the requirements for causal inference. StaCL incorporates modal operators for interventions to express causal properties between probability distributions in different possible worlds in a Kripke model. We formalize axioms for probability distributions, interventions, and causal predicates using StaCL formulas. These axioms are expressive enough to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used to specify and explain the correctness of statistical causal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.12714</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;KGC&#65289;&#26159;&#25351;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#26500;&#24314;&#28789;&#27963;&#19988;&#21487;&#36866;&#29992;&#20110;&#24191;&#27867;&#20219;&#21153;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#20013;&#36817;&#26399;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#23545;&#19981;&#21516;&#30340;&#29983;&#25104;&#30446;&#26631;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35282;&#24230;&#20998;&#21035;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26377;&#28508;&#21147;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#30340;&#35814;&#32454;&#12289;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;</title><link>http://arxiv.org/abs/2210.10678</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;: &#38024;&#23545;&#20855;&#26377;&#23454;&#35777;&#22522;&#20934;&#30740;&#31350;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26500;&#24314;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#35780;&#20272;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65306;(i) &#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65307; (ii) &#22810;&#26679;&#21270;&#30340;&#24179;&#34913;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#38382;&#39064;&#65307; (iii) &#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#33258;&#35757;&#32451;&#26469;&#29983;&#25104;&#26356;&#22810;&#39046;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;&#20851;&#31995;&#25277;&#21462;(RE) &#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#39046;&#22495;&#21644;&#19978;&#19979;&#25991;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;(i) &#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20013;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#21462;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#26041;&#38754;&#65307; (ii) &#24179;&#34913;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#26377;&#21161;&#20110;&#38271;&#23614;&#20998;&#24067;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.06261</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#32034;&#39537;&#21160;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#25511;&#21046;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30001;&#22362;&#30828;&#30340;&#26438;&#21644;&#26580;&#36719;&#30340;&#32518;&#32499;&#32452;&#25104;&#65292;&#20855;&#26377;&#39640;&#24378;&#24230;&#37325;&#37327;&#27604;&#21644;&#26174;&#33879;&#30340;&#21464;&#24418;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#22320;&#24418;&#20013;&#33322;&#34892;&#24182;&#22312;&#20005;&#23803;&#30340;&#25758;&#20987;&#20013;&#23384;&#27963;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32500;&#24230;&#39640;&#12289;&#21160;&#21147;&#22797;&#26434;&#19988;&#32806;&#21512;&#32467;&#26500;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25511;&#21046;&#12290;&#22522;&#20110;&#29289;&#29702;&#30340;&#20223;&#30495;&#26159;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#23545;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;iable&#29289;&#29702;&#24341;&#25806;&#30340;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#36716;&#31227;&#31574;&#30053;(R2S2R)&#12290;&#35813;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#21253;&#25324;&#29289;&#29702;&#23646;&#24615;&#30340;&#31163;&#32447;&#27979;&#37327;&#65292;&#22914;&#36136;&#37327;&#21644;&#20960;&#20309;&#20307;&#30340;&#21508;&#31181;&#26426;&#22120;&#20154;&#37096;&#20214;&#65292;&#20197;&#21450;&#20351;&#29992;&#38543;&#26426;&#25511;&#21046;&#31574;&#30053;&#30340;&#36712;&#36857;&#35266;&#23519;&#12290;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#65292;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#36825;&#31181;R2S2R&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#33021;&#22815;&#35299;&#20915;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#30340;&#20998;&#24037;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05568</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#65292;&#20998;&#24037;&#30340;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
The emergence of division of labor through decentralized social sanctioning. (arXiv:2208.05568v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#33021;&#22815;&#35299;&#20915;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#30340;&#20998;&#24037;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#24577;&#25104;&#21151;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21363;&#28789;&#27963;&#33258;&#32452;&#32455;&#25104;&#21512;&#20316;&#31038;&#20250;&#32676;&#20307;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#32676;&#20307;&#37319;&#29992;&#20102;&#22823;&#37327;&#30340;&#19987;&#19994;&#21270;&#21644;&#20998;&#24037;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#21160;&#29289;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#36807;&#19968;&#29983;&#30340;&#35797;&#38169;&#20013;&#23398;&#20064;&#33258;&#24049;&#35201;&#25198;&#28436;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#20851;&#38190;&#35282;&#33394;&#27604;&#20854;&#20182;&#35282;&#33394;&#26356;&#20855;&#21560;&#24341;&#21147;&#65292;&#24182;&#19988;&#20010;&#20307;&#26159;&#33258;&#21033;&#30340;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#31038;&#20250;&#22256;&#22659;&#65306;&#27599;&#20010;&#20010;&#20307;&#37117;&#24076;&#26395;&#20854;&#20182;&#20154;&#25198;&#28436;&#20851;&#38190;&#20294;&#26080;&#25253;&#37228;&#30340;&#35282;&#33394;&#65292;&#36825;&#26679;&#20182;&#20204;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#19968;&#20010;&#25253;&#37228;&#26356;&#39640;&#30340;&#35282;&#33394;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#27599;&#20010;&#20154;&#37117;&#36825;&#26679;&#34892;&#20107;&#65292;&#19988;&#19968;&#20010;&#20851;&#38190;&#35282;&#33394;&#32570;&#20047;&#22635;&#34917;&#65292;&#23601;&#20250;&#21457;&#29983;&#28798;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#26368;&#20339;&#35282;&#33394;&#20998;&#37197;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#22914;&#20309;&#22312;&#19968;&#32676;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#24418;&#25104;&#20998;&#24037;&#21602;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65288;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#65289;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human ecological success relies on our characteristic ability to flexibly self-organize into cooperative social groups, the most successful of which employ substantial specialization and division of labor. Unlike most other animals, humans learn by trial and error during their lives what role to take on. However, when some critical roles are more attractive than others, and individuals are self-interested, then there is a social dilemma: each individual would prefer others take on the critical-but-unremunerative roles so they may remain free to take one that pays better. But disaster occurs if all act thusly and a critical role goes unfilled. In such situations learning an optimum role distribution may not be possible. Consequently, a fundamental question is: how can division of labor emerge in groups of self-interested lifetime-learning individuals? Here we show that by introducing a model of social norms, which we regard as emerging patterns of decentralized social sanctioning, it be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#20915;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#36890;&#20449;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21355;&#26143;&#38388;&#38142;&#36335;&#12289;&#30701;&#26242;&#26102;&#38388;&#36807;&#31243;&#21644;&#21355;&#26143;&#35206;&#30422;&#33539;&#22260;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.00414</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks. (arXiv:2207.00414v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#20915;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#36890;&#20449;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21355;&#26143;&#38388;&#38142;&#36335;&#12289;&#30701;&#26242;&#26102;&#38388;&#36807;&#31243;&#21644;&#21355;&#26143;&#35206;&#30422;&#33539;&#22260;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22826;&#31354;&#21457;&#23556;&#12289;&#30005;&#23376;&#12289;&#22788;&#29702;&#33021;&#21147;&#21644;&#24494;&#22411;&#21270;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#31354;&#38388;&#36890;&#20449;&#65292;&#29305;&#21035;&#26159;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#65292;&#37325;&#26032;&#25104;&#20026;&#19979;&#19968;&#20195;&#32593;&#32476;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#20381;&#36182;&#20110;&#26080;&#25968;&#30340;&#22522;&#30784;&#21644;&#30456;&#20114;&#20132;&#32455;&#30340;&#36807;&#31243;&#65292;&#20256;&#32479;&#27169;&#22411;&#19981;&#33021;&#30495;&#27491;&#25429;&#25417;&#23427;&#20204;&#30340;&#21160;&#24577;&#21644;&#29420;&#29305;&#29305;&#24449;&#65292;&#22914;&#36712;&#36947;&#36895;&#24230;&#12289;&#21355;&#26143;&#38388;&#38142;&#36335;&#12289;&#30701;&#26242;&#30340;&#26102;&#38388;&#36807;&#31243;&#21644;&#21355;&#26143;&#35206;&#30422;&#33539;&#22260;&#31561;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#20351;&#32593;&#32476;&#20027;&#21160;&#36866;&#24212;&#19982;&#38142;&#25509;&#20013;&#24555;&#36895;&#21464;&#21270;&#30340;&#26465;&#20214;&#12290;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#25429;&#25417;&#36825;&#20123;&#36807;&#31243;&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#34892;&#20026;&#24182;&#27169;&#25311;&#23427;&#20204;&#23545;&#32593;&#32476;&#24433;&#21709;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;AI&#25216;&#26415;&#20110;&#32508;&#21512;&#22320;&#38754;&#21355;&#26143;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#36890;&#20449;&#30340;&#24212;&#29992;&#12290;&#23427;&#35814;&#32454;&#20171;&#32461;&#20102;&#36229;&#32423;&#21355;&#26143;&#32593;&#32476;&#30340;&#29420;&#29305;&#29305;&#24449;&#21644;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;AI&#30340;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space communications, particularly mega satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, mega satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models, due to their dynamic and unique features such as orbital speed, inter-satellite links, short time pass, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial intelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly mega satellite network communications. It details the unique features of mega satellite net
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#23519;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;GRAPHRETRIEVAL&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#26696;&#65292;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#26377;&#29992;&#20449;&#24687;&#25552;&#20379;&#20102;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2206.00362</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Retrieval-enhanced Graph Neural Networks. (arXiv:2206.00362v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#23519;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;GRAPHRETRIEVAL&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#26696;&#65292;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#26377;&#29992;&#20449;&#24687;&#25552;&#20379;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#22823;&#22810;&#25968;GNNs&#20381;&#36182;&#20110;&#36882;&#24402;&#30340;&#37051;&#23621;&#32858;&#21512;&#26041;&#26696;&#65292;&#31216;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#38480;&#20110;&#19968;&#38454;Weisfeiler-Lehman&#27979;&#35797;&#65288;1-WL&#65289;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26159;&#26126;&#30830;&#22320;&#26816;&#32034;&#29992;&#20110;&#22686;&#24378;GNN&#27169;&#22411;&#30340;&#19968;&#20123;&#24050;&#27880;&#37322;&#30340;&#31034;&#20363;&#12290;&#34429;&#28982;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#24403;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#38598;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;GNNs&#30340;&#26377;&#25928;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#24819;&#25506;&#32034;&#26816;&#32034;&#24605;&#24819;&#22914;&#20309;&#24110;&#21161;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;GRAPHRETRIEVAL&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36873;&#25321;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#22312;GRAPHRETRIEVAL&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#36755;&#20837;&#22270;&#65292;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#20986;&#30456;&#20284;&#30340;&#22270;&#20197;&#21450;&#23427;&#20204;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neighborhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first-order Weisfeiler-Lehman test (1-WL). An effective approach to this challenge is to explicitly retrieve some annotated examples used to enhance GNN models. While retrieval-enhanced models have been proved to be effective in many language and vision domains, it remains an open question how effective retrieval-enhanced GNNs are when applied to graph datasets. Motivated by this, we want to explore how the retrieval idea can help augment the useful information learned in the graph neural networks, and we design a retrieval-enhanced scheme called GRAPHRETRIEVAL, which is agnostic to the choice of graph neural network models. In GRAPHRETRIEVAL, for each input graph, similar graphs together with their ground-true labels are retrieved from an existing database. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21644;&#36719;&#26631;&#31614;&#37492;&#23450;&#20108;&#32500;&#37327;&#23376;&#26448;&#26009;&#20013;&#23454;&#20363;&#20998;&#21106;&#20013;&#32570;&#22833;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#26426;&#21046;&#21644;&#25439;&#22833;&#31574;&#30053;&#26469;&#20943;&#23569;&#36127;&#38754;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#20013;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.15948</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36719;&#26631;&#31614;&#37492;&#23450;&#20108;&#32500;&#37327;&#23376;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning. (arXiv:2205.15948v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21644;&#36719;&#26631;&#31614;&#37492;&#23450;&#20108;&#32500;&#37327;&#23376;&#26448;&#26009;&#20013;&#23454;&#20363;&#20998;&#21106;&#20013;&#32570;&#22833;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#26426;&#21046;&#21644;&#25439;&#22833;&#31574;&#30053;&#26469;&#20943;&#23569;&#36127;&#38754;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#20013;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#23376;&#26426;&#22120;&#39046;&#22495;&#20013;&#65292;&#26816;&#27979;&#30789;&#33455;&#29255;&#20013;&#30340;&#20108;&#32500;&#26448;&#26009;&#26159;&#19968;&#20010;&#38750;&#24120;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#23454;&#20363;&#20998;&#21106;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31867;&#20284;&#65292;&#23454;&#20363;&#20998;&#21106;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25165;&#33021;&#36798;&#21040;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20934;&#22791;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#27880;&#32773;&#24517;&#39035;&#22788;&#29702;&#22823;&#22270;&#20687;&#65292;&#20363;&#22914;2K&#20998;&#36776;&#29575;&#65292;&#24182;&#19988;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#23384;&#22312;&#26497;&#23494;&#38598;&#30340;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20108;&#32500;&#37327;&#23376;&#26448;&#26009;&#26631;&#35782;&#20013;&#23454;&#20363;&#20998;&#21106;&#20013;&#32570;&#22833;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#26469;&#33258;&#21160;&#26816;&#27979;&#35823;&#21028;&#30340;&#23545;&#35937;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25439;&#22833;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#20123;&#23545;&#35937;&#23545;&#24635;&#20307;&#25439;&#22833;&#20989;&#25968;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20108;&#32500;&#26448;&#26009;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#24471;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In quantum machine field, detecting two-dimensional (2D) materials in Silicon chips is one of the most critical problems. Instance segmentation can be considered as a potential approach to solve this problem. However, similar to other deep learning methods, the instance segmentation requires a large scale training dataset and high quality annotation in order to achieve a considerable performance. In practice, preparing the training dataset is a challenge since annotators have to deal with a large image, e.g 2K resolution, and extremely dense objects in this problem. In this work, we present a novel method to tackle the problem of missing annotation in instance segmentation in 2D quantum material identification. We propose a new mechanism for automatically detecting false negative objects and an attention based loss strategy to reduce the negative impact of these objects contributing to the overall loss function. We experiment on the 2D material detection datasets, and the experiments s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26550;&#26500;&#36866;&#29992;&#20110;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#32423;&#34701;&#21512;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2205.02357</link><description>&lt;p&gt;
&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26550;&#26500;&#36866;&#29992;&#20110;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#32423;&#34701;&#21512;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MKG&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;MKG&#32452;&#32455;&#20102;&#35270;&#35273;-&#25991;&#26412;&#20107;&#23454;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;MKG&#37117;&#19981;&#23436;&#25972;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#22810;&#27169;&#24577;&#23454;&#20307;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#28151;&#21512;Transformer&#26550;&#26500;&#21644;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26469;&#23436;&#25104;&#22810;&#26679;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#32423;&#34701;&#21512;&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21069;&#32512;&#24341;&#23548;&#20132;&#20114;&#21644;&#32454;&#31890;&#24230;&#30456;&#20851;&#24863;&#30693;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.01077</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#23398;&#20064;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#32463;&#24120;&#21463;&#21040;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#22122;&#22768;&#12289;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;/&#26657;&#20934;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20107;&#23454;&#19978;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#32780;&#26159;&#19987;&#20026;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#31639;&#27861;&#21644;&#22312;&#30001;&#26641;&#33683;&#27966;Pico&#21644;&#20302;&#21151;&#32791;&#26080;&#32447;&#27169;&#22359;&#32452;&#25104;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#33410;&#28857;&#19978;&#30340;&#23454;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#26059;&#36716;&#26426;&#22120;&#30340;&#25391;&#21160;&#27169;&#24335;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35774;&#22791;&#19978;&#23398;&#20064;&#30340;&#37325;&#26032;&#35757;&#32451;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.03466</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#36981;&#24490;&#22870;&#21169;&#30340;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03466
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#36828;&#22823;&#30446;&#26631;&#65292;&#24378;&#21270;&#23398;&#20064;&#24517;&#39035;&#21253;&#25324;&#23545;&#25277;&#35937;&#29366;&#24577;&#21644;&#26102;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#35268;&#21010;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#29366;&#24577;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26102;&#38388;&#25277;&#35937;&#21364;&#24456;&#23569;&#34987;&#20351;&#29992;&#65292;&#23613;&#31649;&#22522;&#20110;&#36873;&#39033;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#21457;&#23637;&#20102;&#29702;&#35770;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#21487;&#33021;&#30340;&#36873;&#39033;&#31354;&#38388;&#24456;&#22823;&#65292;&#20197;&#21069;&#25552;&#20986;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36873;&#39033;&#27169;&#22411;&#22312;&#35268;&#21010;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#36890;&#24120;&#36890;&#36807;&#25552;&#20986;&#23376;&#20219;&#21153;&#65288;&#20363;&#22914;&#36798;&#21040;&#29942;&#39048;&#29366;&#24577;&#25110;&#26368;&#22823;&#21270;&#38500;&#22870;&#21169;&#22806;&#30340;&#24863;&#30693;&#20449;&#21495;&#30340;&#32047;&#31215;&#21644;&#65289;&#26469;&#21457;&#29616;&#36873;&#39033;&#12290;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#20010;&#36873;&#39033;&#65292;&#28982;&#21518;&#23398;&#20064;&#36873;&#39033;&#30340;&#27169;&#22411;&#24182;&#20351;&#20854;&#21487;&#29992;&#20110;&#35268;&#21010;&#36807;&#31243;&#12290;&#22312;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#23376;&#20219;&#21153;&#24573;&#30053;&#20102;&#21407;&#22987;&#38382;&#39064;&#19978;&#30340;&#22870;&#21169;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#23376;&#20219;&#21153;&#20351;&#29992;&#21407;&#22987;&#22870;&#21169;&#21152;&#19978;&#22522;&#20110;&#26576;&#20010;&#29305;&#24449;&#30340;&#22870;&#21169;&#21152;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
&lt;/p&gt;</description></item><item><title>DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2201.03335</link><description>&lt;p&gt;
DeepKE: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03335
&lt;/p&gt;
&lt;p&gt;
DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;DeepKE&#65292;&#25903;&#25345;&#30693;&#35782;&#24211;&#26500;&#24314;&#20013;&#30340;&#22797;&#26434;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#12290;DeepKE&#23454;&#29616;&#20102;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23646;&#24615;&#25552;&#21462;&#12290;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;DeepKE&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#23450;&#21046;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeepKE&#19981;&#20165;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#25552;&#20379;&#21508;&#31181;&#21151;&#33021;&#27169;&#22359;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#36824;&#36890;&#36807;&#19968;&#33268;&#30340;&#26694;&#26550;&#32452;&#32455;&#25152;&#26377;&#32452;&#20214;&#65292;&#20197;&#20445;&#25345;&#36275;&#22815;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/zjunlp/DeepKE&#21457;&#24067;&#20102;&#28304;&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#30340;Google Colab&#25945;&#31243;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;http URL&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#25552;&#21462;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in this http URL for real-time extraction of various tasks, and a demo video
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20855;&#26377;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21516;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.08581</link><description>&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v5 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08581
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#29305;&#24615;&#65292;&#21457;&#29616;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20855;&#26377;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21516;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#26159;&#30446;&#21069;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#29992;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#19968;&#20123;&#31616;&#21333;&#30340;MOEA&#30340;&#25968;&#23398;&#20998;&#26512;&#30456;&#21453;&#65292;NSGA-II&#33267;&#20170;&#27809;&#26377;&#36827;&#34892;&#36807;&#36825;&#26679;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;NSGA-II&#36827;&#34892;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#26159;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#29305;&#23450;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#31181;&#32676;&#35268;&#27169;&#26159;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#23567;&#30340;&#22235;&#20493;&#26102;&#65292;NSGA-II&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#21464;&#24322;&#31639;&#23376;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#29238;&#20195;&#36873;&#25321;&#26041;&#27861;&#19982;SEMO&#21644;GSEMO&#31639;&#27861;&#22312;&#22522;&#26412;&#30340;OneMinMax&#21644;LeadingOnesTrailingZeros&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31181;&#32676;&#35268;&#27169;&#21482;&#31561;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22823;&#23567;&#65292;&#21017;NSGA-II&#26080;&#27861;&#39640;&#25928;&#22320;&#35745;&#31639;&#23436;&#25972;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#22312;&#25351;&#25968;&#32423;&#36845;&#20195;&#27425;&#25968;&#20869;&#65292;&#31181;&#32676;&#22987;&#32456;&#20250;&#38169;&#22833;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#19968;&#20010;&#24658;&#23450;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size four times larger than the size of the Pareto front, the NSGA-II with two classic mutation operators and four different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front: for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front. Our exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21015;&#29983;&#25104;&#27861;&#26469;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#24067;&#23572;&#35268;&#21017;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20860;&#39038;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#30340;&#24179;&#34913;&#26041;&#38754;&#20248;&#20110;&#24120;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#20197;&#28385;&#36275;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#12290;&#20351;&#29992;&#36817;&#20284;&#21015;&#29983;&#25104;&#31639;&#27861;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.08466</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#24067;&#23572;&#35268;&#21017;&#38598;&#21512;&#29983;&#25104;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Fair Boolean Rule Sets via Column Generation. (arXiv:2111.08466v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21015;&#29983;&#25104;&#27861;&#26469;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#24067;&#23572;&#35268;&#21017;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20860;&#39038;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#30340;&#24179;&#34913;&#26041;&#38754;&#20248;&#20110;&#24120;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#20197;&#28385;&#36275;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#12290;&#20351;&#29992;&#36817;&#20284;&#21015;&#29983;&#25104;&#31639;&#27861;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#23558;&#24067;&#23572;&#35268;&#21017;&#23398;&#20064;&#34920;&#31034;&#20026;&#26512;&#21462;&#33539;&#24335;&#65288;DNF&#65292;&#19982;&#20915;&#31574;&#35268;&#21017;&#38598;&#30456;&#31561;&#65289;&#30340;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#20197;&#26368;&#20248;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#35268;&#21017;&#31616;&#21333;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#20998;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#30340;&#26174;&#24335;&#32422;&#26463;&#65306;&#26426;&#20250;&#24179;&#31561;&#21644;&#22343;&#34913;&#20960;&#29575;&#12290;&#37319;&#29992;&#21015;&#29983;&#25104;&#27861;&#65288;CG&#65289;&#39640;&#25928;&#25628;&#32034;&#22823;&#37327;&#21487;&#33021;&#35268;&#21017;&#65292;&#32780;&#26080;&#38656;&#21551;&#21457;&#24335;&#35268;&#21017;&#25366;&#25496;&#12290;&#20026;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#21270;&#30340;&#36817;&#20284;CG&#31639;&#27861;&#12290;&#19982;&#19977;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#26367;&#20195;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;16&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;CG&#31639;&#27861;&#22312;8&#20010;&#25968;&#25454;&#38598;&#19978;&#20860;&#39038;&#20102;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#30340;&#24179;&#34913;&#12290;&#24403;&#20197;&#20934;&#30830;&#24615;&#20026;&#26368;&#22823;&#21270;&#30446;&#26631;&#26102;&#65292;CG&#31639;&#27861;&#19982;&#19987;&#20026;&#27492;&#30446;&#30340;&#35774;&#35745;&#30340;&#35268;&#21017;&#23398;&#20064;&#22120;&#20855;&#26377;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#65292;&#26377;&#26102;&#33021;&#25214;&#21040;&#26356;&#31616;&#21333;&#20294;&#20934;&#30830;&#24615;&#19981;&#20943;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rule mining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.07650</link><description>&lt;p&gt;
KnowPrompt&#65306;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#35843;&#25972;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25991;&#26412;&#29255;&#27573;&#65288;&#21363;&#27169;&#26495;&#65289;&#25554;&#20837;&#36755;&#20837;&#65292;&#24182;&#23558;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#65292;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#33719;&#21462;&#21512;&#36866;&#30340;&#26631;&#31614;&#35789;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#20016;&#23500;&#30340;&#35821;&#20041;&#21644;&#20808;&#39564;&#30693;&#35782;&#65292;&#19981;&#23481;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#23558;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#25972;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65288;KnowPrompt&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#34394;&#25311;&#31867;&#22411;&#35789;&#21644;&#31572;&#26696;&#35789;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#21270;&#32422;&#26463;&#21327;&#21516;&#20248;&#21270;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.11992</link><description>&lt;p&gt;
&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#20197;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Acting in Delayed Environments with Non-Stationary Markov Policies. (arXiv:2101.11992v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.11992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20551;&#35774;&#22312;&#36873;&#25321;&#21160;&#20316;&#21518;&#31435;&#21363;&#25191;&#34892;&#65292;&#20294;&#36825;&#31181;&#20551;&#35774;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#65292;&#20250;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#12289;&#20113;&#35745;&#31639;&#21644;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#35745;&#21010;&#30340;MDP&#26694;&#26550;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#36873;&#25321;&#30340;&#21160;&#20316;&#38656;&#35201;&#24310;&#36831;$m$&#27493;&#25165;&#33021;&#25191;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#36275;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#26159;&#38750;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22266;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22266;&#23450;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#39118;&#26684;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks wi
&lt;/p&gt;</description></item></channel></rss>