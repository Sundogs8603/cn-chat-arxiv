<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#22522;&#20110;&#25968;&#23398;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#30340;&#25968;&#25454;&#38656;&#27714;&#24046;&#24322;&#65292;&#24182;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10608</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#22411;&#32988;&#36807;&#19978;&#19975;&#20010;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
A model is worth tens of thousands of examples. (arXiv:2303.10608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10608
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#22522;&#20110;&#25968;&#23398;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#30340;&#25968;&#25454;&#38656;&#27714;&#24046;&#24322;&#65292;&#24182;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#25968;&#23398;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#24050;&#34987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#21462;&#20195;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#30001;&#20110;&#29702;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#26080;&#27861;&#35780;&#20272;&#65292;&#36825;&#20123;&#26679;&#26412;&#37327;&#36890;&#24120;&#20351;&#29992;&#31895;&#30053;&#30340;&#32463;&#39564;&#27861;&#21017;&#36827;&#34892;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35268;&#21017;&#20165;&#24314;&#35758;&#32593;&#32476;&#24212;&#35813;&#20309;&#26102;&#36215;&#20316;&#29992;&#65292;&#20294;&#24182;&#19981;&#28041;&#21450;&#20256;&#32479;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65306;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#25165;&#33021;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#20445;&#25345;&#21516;&#27493;&#25110;&#32773;&#65292;&#22914;&#26524;&#21487;&#33021;&#30340;&#35805;&#65292;&#36229;&#36234;&#23427;&#20204;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#26469;&#20174;&#32463;&#39564;&#19978;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#26681;&#25454;&#31934;&#30830;&#23450;&#20041;&#30340;&#25968;&#23398;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#24050;&#30693;&#26368;&#20248;&#25110;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#25968;&#25454;&#26080;&#20851;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional signal processing methods relying on mathematical data generation models have been cast aside in favour of deep neural networks, which require vast amounts of data. Since the theoretical sample complexity is nearly impossible to evaluate, these amounts of examples are usually estimated with crude rules of thumb. However, these rules only suggest when the networks should work, but do not relate to the traditional methods. In particular, an interesting question is: how much data is required for neural networks to be on par or outperform, if possible, the traditional model-based methods? In this work, we empirically investigate this question in two simple examples, where the data is generated according to precisely defined mathematical models, and where well-understood optimal or state-of-the-art mathematical data-agnostic solutions are known. A first problem is deconvolving one-dimensional Gaussian signals and a second one is estimating a circle's radius and location in rando
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24040;&#22411;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#22788;&#29702;&#31227;&#21160;&#29992;&#25143;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#36718;&#24310;&#36831;&#38477;&#20302;&#30340;&#30446;&#26631;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.10580</link><description>&lt;p&gt;
&#24040;&#22411;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks. (arXiv:2303.10580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24040;&#22411;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#22788;&#29702;&#31227;&#21160;&#29992;&#25143;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#36718;&#24310;&#36831;&#38477;&#20302;&#30340;&#30446;&#26631;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539; paradigm&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#32593;&#32476;&#20013;&#21508;&#31181;&#31227;&#21160;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#24102;&#26469;&#30340;&#24322;&#26500;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;UE&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#20197;&#21450;&#24102;&#26469;&#30340;&#22797;&#26434;&#34892;&#25919;&#24037;&#20316;&#65292;&#23558;PFL&#31639;&#27861;&#20174;&#20854;&#20256;&#32479;&#30340;&#21452;&#23618;&#26694;&#26550;&#20999;&#25442;&#21040;&#22810;&#23618;&#26694;&#26550;&#26159;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;HPFL&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#24040;&#22411;MEC&#32593;&#32476;&#20013;&#37096;&#32626;PFL&#30340;&#31639;&#27861;&#12290;&#22312;HPFL&#20013;&#65292;UE&#34987;&#21010;&#20998;&#20026;&#22810;&#20010;&#38598;&#32676;&#65292;&#27599;&#20010;&#38598;&#32676;&#20013;&#30340;UE&#21516;&#27493;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#36716;&#21457;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#65288;ES&#65289;&#36827;&#34892;&#36793;&#32536;&#27169;&#22411;&#32858;&#21512;&#65292;&#32780;ES&#21322;&#24322;&#27493;&#22320;&#23558;&#20854;&#36793;&#32536;&#27169;&#22411;&#36716;&#21457;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#19978;&#36848;&#35757;&#32451;&#26041;&#24335;&#22312;&#35757;&#32451;&#25439;&#22833;&#21644;&#36718;&#24310;&#36831;&#20043;&#38388;&#36798;&#21040;&#20102;&#19968;&#20010;&#26435;&#34913;&#12290;HPFL&#20197;&#20998;&#23618;&#26041;&#24335;&#32467;&#21512;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#36718;&#24310;&#36831;&#38477;&#20302;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#22312;&#22521;&#35757;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;PFL&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning (PFL) is a new Federated Learning (FL) paradigm, particularly tackling the heterogeneity issues brought by various mobile user equipments (UEs) in mobile edge computing (MEC) networks. However, due to the ever-increasing number of UEs and the complicated administrative work it brings, it is desirable to switch the PFL algorithm from its conventional two-layer framework to a multiple-layer one. In this paper, we propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs in each cluster forward their local updates to the edge server (ES) synchronously for edge model aggregation, while the ESs forward their edge models to the cloud server semi-asynchronously for global model aggregation. The above training manner leads to a tradeoff between the training loss in each round and the round latency. HPFL combines the objectives of training loss minimization and round 
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10538</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning for Solving the Travelling Salesman Problem. (arXiv:2303.10538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10538
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;UTSP&#65292;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;GNN&#36755;&#20986;&#19968;&#20010;&#28909;&#21147;&#22270;&#34920;&#31034;&#27599;&#20010;&#36793;&#25104;&#20026;&#26368;&#20248;&#36335;&#24452;&#30340;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#26681;&#25454;&#28909;&#21147;&#22270;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#37096;&#20998;&#25512;&#21160;&#27169;&#22411;&#25214;&#21040;&#26368;&#30701;&#30340;&#36335;&#24452;&#65292;&#21478;&#19968;&#37096;&#20998;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#30830;&#20445;&#36335;&#24452;&#24418;&#25104;&#21704;&#23494;&#39039;&#24490;&#29615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UTSP&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;TSP&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21442;&#25968;&#25928;&#29575;&#21644;&#25968;&#25454;&#25928;&#29575;&#22343;&#36739;&#39640;&#65306;&#19982;&#24378;&#21270;&#23398;&#20064;&#25110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20165;&#21344;&#29992;&#32422;10&#65285;&#30340;&#21442;&#25968;&#21644;&#32422;0.2&#65285;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose UTSP, an unsupervised learning (UL) framework for solving the Travelling Salesman Problem (TSP). We train a Graph Neural Network (GNN) using a surrogate loss. The GNN outputs a heat map representing the probability for each edge to be part of the optimal path. We then apply local search to generate our final prediction based on the heat map. Our loss function consists of two parts: one pushes the model to find the shortest path and the other serves as a surrogate for the constraint that the route should form a Hamiltonian Cycle. Experimental results show that UTSP outperforms the existing data-driven TSP heuristics. Our approach is parameter efficient as well as data efficient: the model takes $\sim$ 10\% of the number of parameters and $\sim$ 0.2\% of training samples compared with reinforcement learning or supervised learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;TempT&#65292;&#36890;&#36807;&#30830;&#20445;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#39057;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#12290;&#20854;&#20165;&#21033;&#29992;&#35270;&#35273;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#22312;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#20026;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2303.10536</link><description>&lt;p&gt;
TempT&#65306;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TempT: Temporal consistency for Test-time adaptation. (arXiv:2303.10536v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;TempT&#65292;&#36890;&#36807;&#30830;&#20445;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#39057;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#12290;&#20854;&#20165;&#21033;&#29992;&#35270;&#35273;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#22312;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#20026;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;TempT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#20445;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#23545;&#35270;&#39057;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#12290;TempT&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#35270;&#39057;&#20013;&#30340;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#12290;&#25105;&#20204;&#23558;TempT&#22312;AffWild2&#25968;&#25454;&#38598;&#19978;&#20316;&#20026;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#27604;&#36187;&#65288;ABAW&#65289;&#31532;&#20116;&#23626;&#30740;&#35752;&#20250;&#21644;&#31454;&#36187;&#20013;&#30340;&#34920;&#24773;&#20998;&#31867;&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#30340;&#35270;&#35273;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20102;&#27969;&#34892;&#30340;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#32780;&#19981;&#26159;&#36739;&#22823;&#30340;&#24207;&#21015;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TempT&#19982;&#24448;&#24180;&#25253;&#21578;&#30340;&#34920;&#29616;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20854;&#26377;&#25928;&#24615;&#20026;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, we introduce TempT, a novel method for test time adaptation on videos by ensuring temporal coherence of predictions across sequential frames. TempT is a powerful tool with broad applications in computer vision tasks, including facial expression recognition (FER) in videos. We evaluate TempT's performance on the AffWild2 dataset as part of the Expression Classification Challenge at the 5th Workshop and Competition on Affective Behavior Analysis in the wild (ABAW). Our approach focuses solely on the unimodal visual aspect of the data and utilizes a popular 2D CNN backbone, in contrast to larger sequential or attention based models. Our experimental results demonstrate that TempT has competitive performance in comparison to previous years reported performances, and its efficacy provides a compelling proof of concept for its use in various real world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65288;LURE&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#36951;&#24536;&#38454;&#27573;&#21644;&#37325;&#23398;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#36951;&#24536;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#20449;&#24687;&#21644;&#23545;&#27867;&#21270;&#24615;&#30340;&#23398;&#20064;&#26469;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.10455</link><description>&lt;p&gt;
&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks. (arXiv:2303.10455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10455
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65288;LURE&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#36951;&#24536;&#38454;&#27573;&#21644;&#37325;&#23398;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#36951;&#24536;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#20449;&#24687;&#21644;&#23545;&#27867;&#21270;&#24615;&#30340;&#23398;&#20064;&#26469;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#24120;&#26159;&#22312;&#25552;&#21069;&#25552;&#20379;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#32463;&#24120;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#20197;&#22359;&#30340;&#24418;&#24335;&#20986;&#29616;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#35757;&#32451;DNN&#30340;&#26368;&#20339;&#31574;&#30053;&#30340;&#37325;&#35201;&#32771;&#34385;&#65292;&#20363;&#22914;&#26159;&#21542;&#22312;&#27599;&#20010;&#26032;&#25968;&#25454;&#22359;&#21040;&#36798;&#26102;&#20351;&#29992;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#28201;&#21551;&#21160;&#65289;&#25110;&#32773;&#27599;&#24403;&#26377;&#26032;&#30340;&#25968;&#25454;&#22359;&#21487;&#29992;&#26102;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#12290;&#34429;&#28982;&#21518;&#32773;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#28040;&#32791;&#26356;&#22810;&#30340;&#36164;&#28304;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#20102;&#28201;&#21551;&#21160;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65288;LURE&#65289;&#12290;LURE&#22312;&#36951;&#24536;&#38454;&#27573;&#21644;&#37325;&#23398;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#36951;&#24536;&#38454;&#27573;&#36890;&#36807;&#25968;&#25454;&#20381;&#36182;&#26041;&#24335;&#30340;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#26377;&#36873;&#25321;&#22320;&#36951;&#24536;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#20449;&#24687;&#65292;&#32780;&#37325;&#23398;&#38454;&#27573;&#21017;&#24378;&#35843;&#23545;&#27867;&#21270;&#24615;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generaliza
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#24182;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21516;&#26102;&#37319;&#29992;&#26799;&#24230;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2303.10434</link><description>&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#20013;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Resilient Federated Learning at Edge. (arXiv:2303.10434v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#24182;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21516;&#26102;&#37319;&#29992;&#26799;&#24230;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25308;&#21344;&#24237;&#23481;&#38169;&#21644;&#36890;&#20449;&#25928;&#29575;&#22312;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20855;&#26377;&#37325;&#23614;&#24615;&#36136;&#30340;&#19981;&#35268;&#21017;&#25968;&#25454;&#26102;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21516;&#26102;&#20445;&#25345;&#25308;&#21344;&#24237;&#23481;&#38169;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#26368;&#20248;&#32479;&#35745;&#35823;&#24046;&#29575;&#26469;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#22788;&#29702;&#37325;&#23614;&#25968;&#25454;&#12289;&#21516;&#26102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#25910;&#25947;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#31639;&#27861;&#65292;&#37319;&#29992;&#26799;&#24230;&#21387;&#32553;&#25216;&#26415;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33410;&#30465;&#36890;&#20449;&#25104;&#26412;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20445;&#25345;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#32479;&#35745;&#36895;&#29575;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both Byzantine resilience and communication efficiency have attracted tremendous attention recently for their significance in edge federated learning. However, most existing algorithms may fail when dealing with real-world irregular data that behaves in a heavy-tailed manner. To address this issue, we study the stochastic convex and non-convex optimization problem for federated learning at edge and show how to handle heavy-tailed data while retaining the Byzantine resilience, communication efficiency and the optimal statistical error rates simultaneously. Specifically, we first present a Byzantine-resilient distributed gradient descent algorithm that can handle the heavy-tailed data and meanwhile converge under the standard assumptions. To reduce the communication overhead, we further propose another algorithm that incorporates gradient compression techniques to save communication costs during the learning process. Theoretical analysis shows that our algorithms achieve order-optimal st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10430</link><description>&lt;p&gt;
NoisyHate&#65306;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#19979;&#23545;&#20869;&#23481;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20855;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#22312;&#32447;&#25991;&#26412;&#26159;&#19968;&#31181;&#23041;&#32961;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#32593;&#32476;&#39578;&#25200;&#12290;&#23613;&#31649;&#35768;&#22810;&#24179;&#21488;&#37319;&#21462;&#20102;&#25514;&#26045;&#65292;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26469;&#20943;&#23569;&#20854;&#24433;&#21709;&#65292;&#20294;&#37027;&#20123;&#26377;&#23475;&#20869;&#23481;&#21457;&#24067;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#26377;&#23475;&#35789;&#27719;&#30340;&#25340;&#20889;&#26469;&#36867;&#36991;&#31995;&#32479;&#12290;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#21333;&#35789;&#20063;&#31216;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#25200;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#23450;&#30340;&#25216;&#26415;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#35782;&#21035;&#36825;&#20123;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25200;&#21160;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25200;&#21160;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25307;&#21215;&#20102;&#19968;&#32452;&#24037;&#20154;&#26469;&#35780;&#20272;&#27492;&#27979;&#35797;&#38598;&#30340;&#36136;&#37327;&#24182;&#21024;&#38500;&#20302;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#26597;&#25105;&#20204;&#30340;&#25200;&#21160;&#26159;&#21542;&#21487;&#20197;&#24402;&#19968;&#21270;&#20026;&#20854;&#24178;&#20928;&#29256;&#26412;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#26032;&#29289;&#20307;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#21253;&#25324;&#20351;&#29992;&#20998;&#23618;&#19977;&#20803;&#20998;&#31867;&#21306;&#22495;&#25552;&#35758;&#32593;&#32476;&#26469;&#23450;&#20301;&#28508;&#22312;&#30340;&#26410;&#26631;&#27880;&#26032;&#29289;&#20307;&#65292;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#26032;&#30340;&#30446;&#26631;&#24615;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10422</link><description>&lt;p&gt;
&#25552;&#39640;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identification of Novel Classes for Improving Few-Shot Object Detection. (arXiv:2303.10422v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#26032;&#29289;&#20307;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#21253;&#25324;&#20351;&#29992;&#20998;&#23618;&#19977;&#20803;&#20998;&#31867;&#21306;&#22495;&#25552;&#35758;&#32593;&#32476;&#26469;&#23450;&#20301;&#28508;&#22312;&#30340;&#26410;&#26631;&#27880;&#26032;&#29289;&#20307;&#65292;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#26032;&#30340;&#30446;&#26631;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#39033;&#32321;&#37325;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32597;&#35265;&#30340;&#29289;&#20307;&#12290;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;(FSOD)&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#27599;&#20010;&#31867;&#21035;&#30340;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#23454;&#29616;&#24378;&#22823;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#23545;&#20110;FSOD&#26469;&#35828;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#25361;&#25112;&#26159;&#26410;&#26631;&#27880;&#30340;&#26032;&#31867;&#21035;&#30340;&#23454;&#20363;&#20986;&#29616;&#22312;&#32972;&#26223;&#20013;&#65292;&#36825;&#20123;&#29289;&#20307;&#34920;&#29616;&#31867;&#20284;&#20110;&#26631;&#31614;&#22122;&#38899;&#65292;&#23548;&#33268;FSOD&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#26469;&#26816;&#27979;&#24182;&#21033;&#29992;&#36825;&#20123;&#26410;&#26631;&#27880;&#30340;&#26032;&#29289;&#20307;&#20316;&#20026;&#35757;&#32451;&#26102;&#30340;&#27491;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;FSOD&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#19977;&#20803;&#20998;&#31867;&#21306;&#22495;&#25552;&#35758;&#32593;&#32476;(HTRPN)&#26469;&#23450;&#20301;&#28508;&#22312;&#30340;&#26410;&#26631;&#27880;&#26032;&#29289;&#20307;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#26032;&#30340;&#30446;&#26631;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#21306;&#22495;&#25552;&#35758;&#32593;&#32476;(RPN)&#30340;&#23618;&#27425;&#25277;&#26679;&#31574;&#30053;&#65292;&#20063;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional training of deep neural networks requires a large number of the annotated image which is a laborious and time-consuming task, particularly for rare objects. Few-shot object detection (FSOD) methods offer a remedy by realizing robust object detection using only a few training samples per class. An unexplored challenge for FSOD is that instances from unlabeled novel classes that do not belong to the fixed set of training classes appear in the background. These objects behave similarly to label noise, leading to FSOD performance degradation. We develop a semi-supervised algorithm to detect and then utilize these unlabeled novel objects as positive samples during training to improve FSOD performance. Specifically, we propose a hierarchical ternary classification region proposal network (HTRPN) to localize the potential unlabeled novel objects and assign them new objectness labels. Our improved hierarchical sampling strategy for the region proposal network (RPN) also boosts the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#29992;&#20110;&#24773;&#24863;&#20272;&#35745;&#65292;&#22312;CVPR 2023 ABAW&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;0.361&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10421</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#24773;&#24863;&#20272;&#35745;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Mutilmodal Feature Extraction and Attention-based Fusion for Emotion Estimation in Videos. (arXiv:2303.10421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#29992;&#20110;&#24773;&#24863;&#20272;&#35745;&#65292;&#22312;CVPR 2023 ABAW&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;0.361&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#30340;&#19981;&#26029;&#25913;&#36827;&#20351;&#24471;&#35745;&#31639;&#24773;&#24863;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;CVPR 2023 ABAW&#31454;&#36187;&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;&#12290;&#24773;&#24863;&#20998;&#26512;&#24212;&#35813;&#20174;&#22810;&#20010;&#32500;&#24230;&#20837;&#25163;&#65292;&#22635;&#34917;&#21333;&#19968;&#19981;&#23436;&#21892;&#30340;&#24773;&#24863;&#36890;&#36947;&#65292;&#26368;&#32456;&#36890;&#36807;&#25311;&#21512;&#22810;&#20010;&#32467;&#26524;&#30830;&#23450;&#24773;&#24863;&#20542;&#21521;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#27604;&#36187;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#38271;&#24230;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;&#23039;&#21183;&#21644;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#29992;&#20110;&#24773;&#24863;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.361&#30340;&#20934;&#30830;&#24230;&#12290;&#20195;&#30721;&#21487;&#22312;[https://github.com/xkwangcn/ABAW-5th-RT-IAI]&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous improvement of human-computer interaction technology makes it possible to compute emotions. In this paper, we introduce our submission to the CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW). Sentiment analysis in human-computer interaction should, as far as possible Start with multiple dimensions, fill in the single imperfect emotion channel, and finally determine the emotion tendency by fitting multiple results. Therefore, We exploited multimodal features extracted from video of different lengths from the competition dataset, including audio, pose and images. Well-informed emotion representations drive us to propose a Attention-based multimodal framework for emotion estimation. Our system achieves the performance of 0.361 on the validation dataset. The code is available at [https://github.com/xkwangcn/ABAW-5th-RT-IAI].
&lt;/p&gt;</description></item><item><title>ExplainFix&#37319;&#29992;&#22266;&#23450;&#28388;&#27874;&#22120;&#21644;&#31934;&#31616;&#30340;&#32593;&#32476;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10408</link><description>&lt;p&gt;
ExplainFix: &#21487;&#35299;&#37322;&#30340;&#22266;&#23450;&#31354;&#38388;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ExplainFix: Explainable Spatially Fixed Deep Networks. (arXiv:2303.10408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10408
&lt;/p&gt;
&lt;p&gt;
ExplainFix&#37319;&#29992;&#22266;&#23450;&#28388;&#27874;&#22120;&#21644;&#31934;&#31616;&#30340;&#32593;&#32476;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#35774;&#35745;&#21407;&#21017;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#31354;&#38388;&#28388;&#27874;&#22120;&#26435;&#37325;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#22266;&#23450;&#65292;&#32780;&#19981;&#24517;&#23398;&#20064;&#65307;&#21482;&#38656;&#24456;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#21363;&#21487;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35270;&#21270;&#27169;&#22411;&#35299;&#37322;&#12289;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#25552;&#21319;&#20197;&#21450;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24037;&#20855;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;ExplainFix&#27169;&#22411;&#27604;&#23436;&#20840;&#23398;&#20064;&#30340;&#27169;&#22411;&#23569;&#20102;&#22810;&#36798;100&#20493;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#26680;&#65292;&#32780;&#21305;&#37197;&#25110;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the "fixed filters" principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the "nimbleness" principle that only few network parameters suffice. We contribute (a) visual model-based explanations, (b) speed and accuracy gains, and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to 100x fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17\% faster with channel pruning), matching or improved predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#28145;&#24230;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#19977;&#32500;&#20219;&#21153;&#65292;&#20854;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#22120;&#26469;&#31934;&#30830;&#25429;&#33719;&#23616;&#37096;&#31934;&#32454;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#22810;&#39057;&#29575;&#34701;&#21512;&#27169;&#22359;&#26469;&#25233;&#21046;&#39640;&#39057;&#24418;&#29366;&#29305;&#24449;&#27874;&#21160;&#65292;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10406</link><description>&lt;p&gt;
3DQD: &#22522;&#20110;&#37096;&#20998;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#30340;&#24191;&#20041;&#28145;&#24230;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process. (arXiv:2303.10406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#28145;&#24230;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#19977;&#32500;&#20219;&#21153;&#65292;&#20854;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#22120;&#26469;&#31934;&#30830;&#25429;&#33719;&#23616;&#37096;&#31934;&#32454;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#22810;&#39057;&#29575;&#34701;&#21512;&#27169;&#22359;&#26469;&#25233;&#21046;&#39640;&#39057;&#24418;&#29366;&#29305;&#24449;&#27874;&#21160;&#65292;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20808;&#39564;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;&#22810;&#31181;&#19977;&#32500;&#20219;&#21153;&#65292;&#21253;&#25324;&#26080;&#26465;&#20214;&#24418;&#29366;&#29983;&#25104;&#12289;&#28857;&#20113;&#23436;&#25104;&#21644;&#36328;&#27169;&#24577;&#24418;&#29366;&#29983;&#25104;&#31561;&#12290;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#31934;&#30830;&#25429;&#33719;&#23616;&#37096;&#31934;&#32454;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#26469;&#22522;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#32039;&#20945;&#30340;&#30721;&#26412;&#24182;&#32034;&#24341;&#26412;&#22320;&#20960;&#20309;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#22120;&#26469;&#24314;&#27169;&#19981;&#21516;&#26631;&#35760;&#20043;&#38388;&#22266;&#26377;&#30340;&#32467;&#26500;&#20381;&#36182;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39057;&#29575;&#34701;&#21512;&#27169;&#22359;&#65288;MFM&#65289;&#65292;&#20197;&#22810;&#39057;&#29575;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#25351;&#23548;&#65292;&#25233;&#21046;&#39640;&#39057;&#24418;&#29366;&#29305;&#24449;&#30340;&#27874;&#21160;&#12290;&#19978;&#36848;&#35774;&#35745;&#20849;&#21516;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#29305;&#24449;&#21644;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#33021;&#21147;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a generalized 3D shape generation prior model, tailored for multiple 3D tasks including unconditional shape generation, point cloud completion, and cross-modality shape generation, etc. On one hand, to precisely capture local fine detailed shape information, a vector quantized variational autoencoder (VQ-VAE) is utilized to index local geometry from a compactly learned codebook based on a broad set of task training data. On the other hand, a discrete diffusion generator is introduced to model the inherent structural dependencies among different tokens. In the meantime, a multi-frequency fusion module (MFM) is developed to suppress high-frequency shape feature fluctuations, guided by multi-frequency contextual information. The above designs jointly equip our proposed 3D shape prior model with high-fidelity, diverse features as well as the capability of cross-modality alignment, and extensive experiments have demonstrated superior performances on various 3D shape generation ta
&lt;/p&gt;</description></item><item><title>FedRight &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#21462;&#27169;&#22411;&#29305;&#24449;&#65292;&#21363;&#27169;&#22411;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#29256;&#26435;&#30340;&#20445;&#25252;&#12290;&#23427;&#22312;&#26377;&#25928;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.10399</link><description>&lt;p&gt;
FedRight: &#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedRight: An Effective Model Copyright Protection for Federated Learning. (arXiv:2303.10399v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10399
&lt;/p&gt;
&lt;p&gt;
FedRight &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#21462;&#27169;&#22411;&#29305;&#24449;&#65292;&#21363;&#27169;&#22411;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#29256;&#26435;&#30340;&#20445;&#25252;&#12290;&#23427;&#22312;&#26377;&#25928;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#21516;&#26102;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#21487;&#35266;&#30340;&#21033;&#28070;&#65292;&#23427;&#24050;&#34987;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#23454;&#36341;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#35841;&#25317;&#26377;&#27169;&#22411;&#65292;&#22914;&#20309;&#20445;&#25252;&#29256;&#26435;&#24050;&#25104;&#20026;&#19968;&#20010;&#30495;&#27491;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20013;&#24515;&#21270;&#22330;&#26223;&#19979;&#30340;&#36130;&#20135;&#26435;&#20445;&#25252;&#26041;&#27861;&#65288;&#22914;&#27700;&#21360;&#23884;&#20837;&#21644;&#27169;&#22411;&#25351;&#32441;&#65289;&#30452;&#35266;&#19978;&#33021;&#22815;&#35299;&#20915; FL &#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110; FL &#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#65292;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#12289;&#21442;&#25968;&#32858;&#21512;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#27425;&#27491;&#24335;&#23450;&#20041;&#20102; FL &#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102; FedRight &#26469;&#20445;&#25252;&#22522;&#20110;&#27169;&#22411;&#25351;&#32441;&#30340;&#27169;&#22411;&#29256;&#26435;&#65292;&#21363;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#25552;&#21462;&#27169;&#22411;&#29305;&#24449;&#12290;FedRight &#22312;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65306;&#65288;i&#65289;&#26377;&#25928;&#24615;&#65306;&#23427;&#36890;&#36807;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#21462;&#27169;&#22411;&#29305;&#24449;&#65292;&#21487;&#20197;&#30001;&#31532;&#19977;&#26041;&#36827;&#34892;&#39564;&#35777;&#65307;&#65288;ii&#65289;&#23433;&#20840;&#24615;&#65306;&#23427;&#20026;&#26412;&#22320;&#27169;&#22411;&#25552;&#20379;&#20445;&#25252;&#65292;&#24182;&#38450;&#27490;&#28508;&#22312;&#25915;&#20987;&#32773;&#31363;&#21462;&#27169;&#22411;&#65307;&#65288;iii&#65289;&#40065;&#26834;&#24615;&#65306;&#23427;&#21487;&#20197;&#25269;&#24481;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#24182;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#20445;&#25252;&#24615;&#33021;&#65307;&#65288;iv&#65289;&#25928;&#29575;&#65306;&#23427;&#24341;&#20837;&#36731;&#37327;&#21270;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), an effective distributed machine learning framework, implements model training and meanwhile protects local data privacy. It has been applied to a broad variety of practice areas due to its great performance and appreciable profits. Who owns the model, and how to protect the copyright has become a real problem. Intuitively, the existing property rights protection methods in centralized scenarios (e.g., watermark embedding and model fingerprints) are possible solutions for FL. But they are still challenged by the distributed nature of FL in aspects of the no data sharing, parameter aggregation, and federated training settings. For the first time, we formalize the problem of copyright protection for FL, and propose FedRight to protect model copyright based on model fingerprints, i.e., extracting model features by generating adversarial examples as model fingerprints. FedRight outperforms previous works in four key aspects: (i) Validity: it extracts model features
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;WFST&#26694;&#26550;&#30340;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#8220;Compose-Transducer&#8221;&#21644;&#8220;Grid-Transducer&#8221;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;W-Transducer Loss&#26469;&#23637;&#31034;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;RNN-T&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10384</link><description>&lt;p&gt;
&#22522;&#20110;WFST&#26694;&#26550;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Powerful and Extensible WFST Framework for RNN-Transducer Losses. (arXiv:2303.10384v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;WFST&#26694;&#26550;&#30340;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#8220;Compose-Transducer&#8221;&#21644;&#8220;Grid-Transducer&#8221;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;W-Transducer Loss&#26469;&#23637;&#31034;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;RNN-T&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#31227;&#22120;&#65288;WFST&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#31616;&#21270;&#23545;RNN-Transducer&#65288;RNN-T&#65289; Losses&#30340;&#20462;&#25913;&#24320;&#21457;&#12290;&#29616;&#26377;&#30340;RNN-T&#23454;&#29616;&#20351;&#29992;&#19982;CUDA&#30456;&#20851;&#30340;&#20195;&#30721;&#65292;&#38590;&#20197;&#25193;&#23637;&#21644;&#35843;&#35797;&#12290;WFST&#26131;&#20110;&#26500;&#24314;&#21644;&#25193;&#23637;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#35843;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#22522;&#20110;WFST&#30340;RNN-T&#23454;&#29616;&#65306;&#65288;1&#65289;&#8220;Compose-Transducer&#8221;&#65292;&#23427;&#22522;&#20110;&#22768;&#23398;&#21644;&#25991;&#26412;&#26550;&#26500;&#30340;WFST&#22270;&#32452;&#21512;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#21644;&#26131;&#20110;&#20462;&#25913;&#65307;&#65288;2&#65289;&#8220;Grid-Transducer&#8221;&#65292;&#30452;&#25509;&#26500;&#24314;&#26230;&#26684;&#29992;&#20110;&#36827;&#19968;&#27493;&#35745;&#31639;&#65292;&#26368;&#32039;&#20945;&#21644;&#35745;&#31639;&#25928;&#29575;&#26368;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;W-Transducer Loss&#65292;&#21363;Connectionist Temporal Classification with Wild Cards&#30340;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#32570;&#23569;&#36716;&#24405;&#24320;&#22836;&#37096;&#20998;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;RNN-T&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) "Compose-Transducer", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) "Grid-Transducer", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and 
&lt;/p&gt;</description></item><item><title>CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10365</link><description>&lt;p&gt;
CroSel: &#29992;&#20110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#20449;&#20266;&#26631;&#31614;&#30340;&#36328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10365
&lt;/p&gt;
&lt;p&gt;
CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;(PLL)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#26377;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;ground-truth&#26631;&#31614;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#22522;&#20110;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;PLL&#20013;&#30340;&#26631;&#31614;&#27495;&#20041;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#35201;&#35782;&#21035;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21644;&#23436;&#25972;&#22320;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CroSel&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#30340;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#26469;&#35782;&#21035;&#22823;&#22810;&#25968;&#35757;&#32451;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#21449;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#30456;&#20114;&#36873;&#25321;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;co-mix&#65292;&#20197;&#36991;&#20813;&#22240;&#34394;&#20551;&#36873;&#25321;&#32780;&#24341;&#36215;&#30340;&#26679;&#26412;&#28010;&#36153;&#21644;&#24494;&#23567;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CroSel&#33021;&#22815;&#25361;&#36873;&#20986;&#22823;&#22810;&#25968;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.10343</link><description>&lt;p&gt;
LossMix&#65306;&#31616;&#21270;&#21644;&#24191;&#27867;&#24212;&#29992; Mixup &#20110;&#30446;&#26631;&#26816;&#27979;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LossMix: Simplify and Generalize Mixup for Object Detection and Beyond. (arXiv:2303.10343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#22686;&#24378;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20294;&#30001;&#20110;&#31354;&#38388;&#38169;&#20301;&#12289;&#21069;&#26223;/&#32972;&#26223;&#21306;&#20998;&#20197;&#21450;&#22810;&#20010;&#23454;&#20363;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#26131;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#30417;&#30563;&#25554;&#20540;&#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102; LossMix&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;insight&#26159;&#65292;&#36890;&#36807;&#25554;&#20540;&#25439;&#22833;&#35823;&#24046;&#26469;&#35843;&#25972;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#35268;&#33539;&#28151;&#21512;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;ground truth&#26631;&#31614;&#12290;&#22312;PASCAL VOC&#21644;MS COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LossMix&#22987;&#32456;&#20248;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#39046;&#22495;m...
&lt;/p&gt;
&lt;p&gt;
The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation, which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Building on this framework, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix consistently outperforms currently popular mixing strategies. Furthermore, we design a two-stage domain m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#24320;&#28304;&#25918;&#23556;&#23398;&#20449;&#24687;&#31995;&#32479;&#20013;&#24341;&#20837;&#19968;&#20010;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#65292;&#36890;&#36807;Few-shot&#23398;&#20064;&#21644;Swarm&#23398;&#20064;&#26041;&#27861;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#24182;&#23398;&#20064;&#20174;&#25918;&#23556;&#23398;&#23478;&#30340;&#26356;&#27491;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.10338</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#23884;&#20837;&#24320;&#28304;&#25918;&#23556;&#23398;&#20449;&#24687;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
A general-purpose AI assistant embedded in an open-source radiology information system. (arXiv:2303.10338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#24320;&#28304;&#25918;&#23556;&#23398;&#20449;&#24687;&#31995;&#32479;&#20013;&#24341;&#20837;&#19968;&#20010;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#65292;&#36890;&#36807;Few-shot&#23398;&#20064;&#21644;Swarm&#23398;&#20064;&#26041;&#27861;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#24182;&#23398;&#20064;&#20174;&#25918;&#23556;&#23398;&#23478;&#30340;&#26356;&#27491;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#30340;AI&#27169;&#22411;&#24050;&#32463;&#22312;&#25509;&#36817;&#25110;&#36229;&#36234;&#20154;&#31867;&#30340;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21355;&#29983;&#20449;&#24687;&#26631;&#20934;&#12289;&#19978;&#19979;&#25991;&#21644;&#24037;&#20316;&#27969;&#24046;&#24322;&#20197;&#21450;&#25968;&#25454;&#26631;&#27880;&#24046;&#24322;&#65292;AI&#27169;&#22411;&#19982;&#20154;&#31867;&#25918;&#23556;&#23398;&#23478;&#30340;&#21512;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;DICOM&#26631;&#20934;SR&#27880;&#37322;&#30340;AI&#27169;&#22411;&#26381;&#21153;&#38598;&#25104;&#21040;&#24320;&#28304;&#30340;LibreHealth&#25918;&#23556;&#23398;&#20449;&#24687;&#31995;&#32479;&#65288;RIS&#65289;&#20013;&#30340;OHIF&#26597;&#30475;&#22120;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#30340;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#33021;&#21147;&#65292;&#21253;&#25324;Few-shot&#23398;&#20064;&#21644;Swarm&#23398;&#20064;&#26041;&#27861;&#26469;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;AI&#27169;&#22411;&#12290;&#22522;&#20110;&#26426;&#22120;&#25945;&#23398;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#22312;RIS&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#20154;&#31867;&#25918;&#23556;&#23398;&#23478;&#21487;&#20197;&#21551;&#29992;/&#31105;&#29992;AI&#27880;&#37322;&#20197;&#21450;&#8220;&#20462;&#22797;&#8221;/&#37325;&#26032;&#26631;&#27880;AI&#27880;&#37322;&#12290;&#36825;&#20123;&#27880;&#37322;&#28982;&#21518;&#34987;&#29992;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#25918;&#23556;&#23398;&#23478;&#21644;AI&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20854;&#20013;AI&#20805;&#24403;&#21161;&#25163;&#65292;&#22312;&#26816;&#27979;&#21040;&#24322;&#24120;&#26102;&#21487;&#20197;&#25552;&#31034;&#25918;&#23556;&#23398;&#23478;&#65292;&#21516;&#26102;&#20174;&#25918;&#23556;&#23398;&#23478;&#30340;&#26356;&#27491;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology AI models have made significant progress in near-human performance or surpassing it. However, AI model's partnership with human radiologist remains an unexplored challenge due to the lack of health information standards, contextual and workflow differences, and data labeling variations. To overcome these challenges, we integrated an AI model service that uses DICOM standard SR annotations into the OHIF viewer in the open-source LibreHealth Radiology Information Systems (RIS). In this paper, we describe the novel Human-AI partnership capabilities of the platform, including few-shot learning and swarm learning approaches to retrain the AI models continuously. Building on the concept of machine teaching, we developed an active learning strategy within the RIS, so that the human radiologist can enable/disable AI annotations as well as "fix"/relabel the AI annotations. These annotations are then used to retrain the models. This helps establish a partnership between the radiologist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25277;&#35937;&#19988;&#36890;&#29992;&#30340;&#36793;&#32536;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#20013;&#65292;&#31216;&#20026;Edge-aware Plug-and-play Scheme (EPS)&#65292;&#20854;&#26680;&#24515;&#26159;&#36793;&#32536;&#23485;&#24230;/&#21402;&#24230;&#20445;&#25345;&#30340;&#25351;&#23548;&#24615;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2303.10307</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#35821;&#20041;&#20998;&#21106;&#30340;&#36793;&#32536;&#24863;&#30693;&#21363;&#25554;&#21363;&#29992;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Edge-aware Plug-and-play Scheme for Semantic Segmentation. (arXiv:2303.10307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25277;&#35937;&#19988;&#36890;&#29992;&#30340;&#36793;&#32536;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#20013;&#65292;&#31216;&#20026;Edge-aware Plug-and-play Scheme (EPS)&#65292;&#20854;&#26680;&#24515;&#26159;&#36793;&#32536;&#23485;&#24230;/&#21402;&#24230;&#20445;&#25345;&#30340;&#25351;&#23548;&#24615;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#31181;&#32463;&#20856;&#19988;&#22522;&#30784;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#26088;&#22312;&#23558;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#21040;&#20854;&#23545;&#24212;&#30340;&#31867;&#21035;&#12290;&#36817;&#26399;&#30340;&#19968;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#36793;&#32536;&#20449;&#24687;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#26080;&#27861;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#25110;&#20219;&#21153;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25277;&#35937;&#19988;&#36890;&#29992;&#30340;&#36793;&#32536;&#30417;&#30563;&#26041;&#27861;&#65292;&#31216;&#20026;Edge-aware Plug-and-play Scheme (EPS)&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#20013;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36793;&#32536;&#23485;&#24230;/&#21402;&#24230;&#20445;&#25345;&#30340;&#25351;&#23548;&#24615;&#35821;&#20041;&#20998;&#21106;&#12290;EPS&#39318;&#20808;&#20174;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#20855;&#26377;&#39044;&#23450;&#20041;&#36793;&#32536;&#21402;&#24230;&#30340;&#36793;&#32536;&#30495;&#20540;&#65288;Edge GT&#65289;&#65292;&#28982;&#21518;&#38024;&#23545;&#20219;&#20309;&#32593;&#32476;&#32467;&#26500;&#65292;&#30452;&#25509;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#35299;&#30721;&#22120;&#22836;&#20197;Edge GT&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#30830;&#20445;&#36793;&#32536;&#21402;&#24230;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;&#25439;&#22833;&#65292;&#31216;&#20026;Polar Hausdorff&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a classic and fundamental computer vision problem dedicated to assigning each pixel with its corresponding class. Some recent methods introduce edge-based information for improving the segmentation performance. However these methods are specific and limited to certain network architectures, and they can not be transferred to other models or tasks. Therefore, we propose an abstract and universal edge supervision method called Edge-aware Plug-and-play Scheme (EPS), which can be easily and quickly applied to any semantic segmentation models. The core is edge-width/thickness preserving guided for semantic segmentation. The EPS first extracts the Edge Ground Truth (Edge GT) with a predefined edge thickness from the training data; and then for any network architecture, it directly copies the decoder head for the auxiliary task with the Edge GT supervision. To ensure the edge thickness preserving consistantly, we design a new boundarybased loss, called Polar Hausdorff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31119;&#21033;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#31639;&#32422;&#26463;&#19979;&#22810;&#32452;&#20214;POMDP&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#26469;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.10302</link><description>&lt;p&gt;
&#35299;&#20915;&#39044;&#31639;&#32422;&#26463;&#19979;&#22810;&#32452;&#20214;POMDP&#30340;&#31119;&#21033;&#26368;&#22823;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs. (arXiv:2303.10302v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31119;&#21033;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#31639;&#32422;&#26463;&#19979;&#22810;&#32452;&#20214;POMDP&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#26469;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#23454;&#38469;&#30340;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#29420;&#31435;&#21160;&#24577;&#22522;&#30784;&#35774;&#26045;&#32452;&#20214;&#30340;&#32500;&#25252;&#21644;&#26816;&#26597;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23547;&#25214;&#22810;&#32452;&#20214;&#39044;&#31639;&#32422;&#26463;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#24102;&#39044;&#31639;&#30340;POMDP&#27169;&#22411;&#65288;b-POMDP&#65289;&#65292;&#22312;&#36981;&#23432;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#25105;&#20204;&#25214;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#26399;&#30340;&#24773;&#20917;&#19979;&#65292;b-POMDP&#30340;&#20540;&#20989;&#25968;&#25110;&#26368;&#22823;&#25910;&#30410;&#26159;&#39044;&#31639;&#30340;&#20985;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#21508;&#20010;&#32452;&#20214;POMDP&#20043;&#38388;&#30340;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#26469;&#35745;&#31639;&#22810;&#32452;&#20214;&#39044;&#31639;&#32422;&#26463;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#34987;&#34920;&#31034;&#20026;&#19968;&#31181;&#31119;&#21033;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#20540;&#20989;&#25968;&#30340;&#20985;&#24615;&#36136;&#26469;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Markov Decision Processes (POMDPs) provide an efficient way to model real-world sequential decision making processes. Motivated by the problem of maintenance and inspection of a group of infrastructure components with independent dynamics, this paper presents an algorithm to find the optimal policy for a multi-component budget-constrained POMDP. We first introduce a budgeted-POMDP model (b-POMDP) which enables us to find the optimal policy for a POMDP while adhering to budget constraints. Next, we prove that the value function or maximal collected reward for a b-POMDP is a concave function of the budget for the finite horizon case. Our second contribution is an algorithm to calculate the optimal policy for a multi-component budget-constrained POMDP by finding the optimal budget split among the individual component POMDPs. The optimal budget split is posed as a welfare maximization problem and the solution is computed by exploiting the concave nature of the value fu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#23545;&#25239;&#24615;patch&#26412;&#22320;&#21270;&#22120;DUET&#31639;&#27861;&#65292;&#21487;&#20197;&#23450;&#20301;&#22270;&#20687;&#19978;&#30340;&#23545;&#25239;&#24615;patch&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#25104;&#20687;&#31561;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.10291</link><description>&lt;p&gt;
&#26816;&#27979;&#36229;&#20986;&#38408;&#20540;&#19981;&#30830;&#23450;&#24615;&#65288;DUET&#65289;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;PATCH&#26412;&#22320;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Detection of Uncertainty in Exceedance of Threshold (DUET): An Adversarial Patch Localizer. (arXiv:2303.10291v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10291
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#23545;&#25239;&#24615;patch&#26412;&#22320;&#21270;&#22120;DUET&#31639;&#27861;&#65292;&#21487;&#20197;&#23450;&#20301;&#22270;&#20687;&#19978;&#30340;&#23545;&#25239;&#24615;patch&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#25104;&#20687;&#31561;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#25915;&#20987;&#65288;&#22914;&#23545;&#25239;&#24615;patch&#65289;&#65292;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#23545;&#25239;&#24615;patch&#26412;&#22320;&#21270;&#22120;&#26469;&#20026;&#23545;&#25239;&#24615;patch&#26816;&#27979;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#65292;&#35813;&#26412;&#22320;&#21270;&#22120;&#21487;&#20197;&#22312;&#22270;&#20687;&#19978;&#23450;&#20301;&#23545;&#25239;&#24615;patch&#65292;&#20801;&#35768;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;patch&#36991;&#20813;&#25110;patch&#37325;&#24314;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#8220;&#36229;&#20986;&#38408;&#20540;&#19981;&#30830;&#23450;&#24615;&#26816;&#27979;&#8221;&#65288;DUET&#65289;&#31639;&#27861;&#26469;&#37327;&#21270;&#25105;&#20204;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20010;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30830;&#23450;&#23545;&#25239;&#24615;patch&#26412;&#22320;&#21270;&#30340;&#20449;&#24515;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#25104;&#20687;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#26412;&#22320;&#21270;&#23545;&#25239;&#24615;patch&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;DUET&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#25105;&#20204;&#25152;&#36873;&#25321;&#30340;&#27169;&#22411;&#20808;&#39564;&#21644;&#19981;&#21516;&#35821;&#35328;&#20013;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#37319;&#29992;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of defenses against physical world attacks such as adversarial patches is gaining traction within the research community. We contribute to the field of adversarial patch detection by introducing an uncertainty-based adversarial patch localizer which localizes adversarial patch on an image, permitting post-processing patch-avoidance or patch-reconstruction. We quantify our prediction uncertainties with the development of \textit{\textbf{D}etection of \textbf{U}ncertainties in the \textbf{E}xceedance of \textbf{T}hreshold} (DUET) algorithm. This algorithm provides a framework to ascertain confidence in the adversarial patch localization, which is essential for safety-sensitive applications such as self-driving cars and medical imaging. We conducted experiments on localizing adversarial patches and found our proposed DUET model outperforms baseline models. We then conduct further analyses on our choice of model priors and the adoption of Bayesian Neural Networks in different l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#36793;&#32536;&#23545;&#25239;&#26816;&#27979;&#65288;MEAD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#22312;&#29289;&#29702;&#19990;&#30028;&#29289;&#20307;&#19978;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#65292;&#20174;&#32780;&#22312;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#24212;&#29992;&#30340;&#34394;&#25311;&#19990;&#30028;&#20013;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10288</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#23545;&#25239;&#26816;&#27979;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#21040;&#34394;&#25311;&#29616;&#23454;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mobile Edge Adversarial Detection for Digital Twinning to the Metaverse with Deep Reinforcement Learning. (arXiv:2303.10288v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#36793;&#32536;&#23545;&#25239;&#26816;&#27979;&#65288;MEAD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#22312;&#29289;&#29702;&#19990;&#30028;&#29289;&#20307;&#19978;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#65292;&#20174;&#32780;&#22312;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#24212;&#29992;&#30340;&#34394;&#25311;&#19990;&#30028;&#20013;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#30340;&#24212;&#29992;&#24517;&#39035;&#23558;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#24555;&#36895;&#26144;&#23556;&#21040;&#34394;&#25311;&#29616;&#23454;&#65288;&#20063;&#31216;&#20026;Metaverse&#65289;&#65292;&#20197;&#36827;&#34892;&#22686;&#24378;&#29616;&#23454;&#36741;&#21161;&#39550;&#39542;&#31561;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#23383;&#21452;&#29983;&#25216;&#26415;&#24212;&#29992;&#30340;&#22686;&#38271;&#65292;&#21487;&#33021;&#20250;&#21463;&#21040;&#25915;&#20987;&#32773;&#30340;&#24178;&#25200;&#65292;&#36825;&#20123;&#25915;&#20987;&#32773;&#21487;&#33021;&#20250;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#29289;&#20307;&#19978;&#65288;&#20363;&#22914;&#27773;&#36710;&#12289;&#36335;&#26631;&#25110;&#36947;&#36335;&#19978;&#65289;&#25918;&#32622;&#23545;&#25239;&#24615;&#30340;&#36148;&#29255;&#65292;&#20174;&#32780;&#25197;&#26354;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#25968;&#23383;&#21452;&#29983;&#20307;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#36793;&#32536;&#23545;&#25239;&#26816;&#27979;&#65288;MEAD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32593;&#32476;&#36793;&#32536;&#26816;&#27979;&#23545;&#25239;&#24615;&#36148;&#29255;&#65292;&#38477;&#20302;&#20102;&#20013;&#22830;Metaverse Map Service Provider&#65288;MMSP&#65289;&#30340;&#36164;&#28304;&#36127;&#25285;&#12290;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;MEAD&#26816;&#27979;&#23545;&#25239;&#24615;&#36148;&#29255;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time Digital Twinning of physical world scenes onto the Metaverse is necessary for a myriad of applications such as augmented-reality (AR) assisted driving. In AR assisted driving, physical environment scenes are first captured by Internet of Vehicles (IoVs) and are uploaded to the Metaverse. A central Metaverse Map Service Provider (MMSP) will aggregate information from all IoVs to develop a central Metaverse Map. Information from the Metaverse Map can then be downloaded into individual IoVs on demand and be delivered as AR scenes to the driver. However, the growing interest in developing AR assisted driving applications which relies on digital twinning invites adversaries. These adversaries may place physical adversarial patches on physical world objects such as cars, signboards, or on roads, seeking to contort the virtual world digital twin. Hence, there is a need to detect these physical world adversarial patches. Nevertheless, as real-time, accurate detection of adversarial p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#24503;&#33452;&#23612;&#30340;&#19977;&#20540;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22797;&#21512;&#26465;&#20214;&#12289;&#26465;&#20214;&#38543;&#26426;&#37327;&#12289;p&#19968;&#33268;&#24615;&#21644;p&#34164;&#21547;&#12290;&#24182;&#35777;&#26126;&#20102;&#26465;&#20214;&#20107;&#20214;&#30340;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#36845;&#20195;&#26465;&#20214;&#21644;&#22312;&#27492;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#19979;&#30340;"$A$&#25110;$B$"&#30340;&#25512;&#29702;&#25512;&#23548;&#21040;&#26465;&#20214;&#30340;&#26080;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10268</link><description>&lt;p&gt;
&#20851;&#20110;&#19977;&#20540;&#36923;&#36753;&#12289;&#22797;&#21512;&#26465;&#20214;&#21644;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Trivalent Logics, Compound Conditionals, and Probabilistic Deduction Theorems. (arXiv:2303.10268v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#24503;&#33452;&#23612;&#30340;&#19977;&#20540;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22797;&#21512;&#26465;&#20214;&#12289;&#26465;&#20214;&#38543;&#26426;&#37327;&#12289;p&#19968;&#33268;&#24615;&#21644;p&#34164;&#21547;&#12290;&#24182;&#35777;&#26126;&#20102;&#26465;&#20214;&#20107;&#20214;&#30340;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#36845;&#20195;&#26465;&#20214;&#21644;&#22312;&#27492;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#19979;&#30340;"$A$&#25110;$B$"&#30340;&#25512;&#29702;&#25512;&#23548;&#21040;&#26465;&#20214;&#30340;&#26080;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#26465;&#20214;&#20107;&#20214;&#12289;&#22797;&#21512;&#26465;&#20214;&#12289;&#26465;&#20214;&#38543;&#26426;&#37327;&#12289;p&#19968;&#33268;&#24615;&#21644;p&#34164;&#21547;&#30340;&#19968;&#20123;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22238;&#39038;&#24503;&#33452;&#23612;&#23545;&#26465;&#20214;&#20998;&#26512;&#30340;&#19977;&#20540;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#26465;&#20214;&#21644;&#26465;&#20214;&#36172;&#27880;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#20294;&#26159;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#24503;&#33452;&#23612;&#26089;&#26399;&#30340;&#19977;&#20540;&#36923;&#36753;&#20998;&#26512;&#65292;&#22522;&#20110;&#20182;&#30340;&#21518;&#26399;&#24605;&#24819;&#65292;&#26088;&#22312;&#23558;&#20182;&#30340;&#25552;&#35758;&#25552;&#39640;&#21040;&#26356;&#39640;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#20004;&#31687;&#26368;&#36817;&#30340;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#26465;&#20214;&#30340;&#19977;&#20540;&#36923;&#36753;&#21644;&#23427;&#20204;&#23545;&#36923;&#36753;&#26377;&#25928;&#24615;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#25105;&#20204;&#23545;&#22797;&#21512;&#26465;&#20214;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26465;&#20214;&#20107;&#20214;&#30340;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#20960;&#20010;&#20363;&#23376;&#65292;&#30740;&#31350;&#20102;&#19968;&#20123;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#12290;&#25105;&#20204;&#20851;&#27880;&#36845;&#20195;&#26465;&#20214;&#21644;&#22312;&#25105;&#20204;&#27010;&#29575;&#25512;&#29702;&#23450;&#29702;&#30340;&#20809;&#32447;&#19979;&#65292;&#8220;$A$&#25110;$B$&#8221;&#30340;&#26512;&#21462;&#25512;&#23548;&#21040;&#26465;&#20214;&#30340;&#26080;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;"$A$&#25110;$B$"&#30340;&#25512;&#29702;&#25512;&#23548;&#21040;&#26465;&#20214;&#8220;&#22914;&#26524;$C$&#65292;&#21017;$A$&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we recall some results for conditional events, compound conditionals, conditional random quantities, p-consistency, and p-entailment. Then, we show the equivalence between bets on conditionals and conditional bets, by reviewing de Finetti's trivalent analysis of conditionals. But our approach goes beyond de Finetti's early trivalent logical analysis and is based on his later ideas, aiming to take his proposals to a higher level. We examine two recent articles that explore trivalent logics for conditionals and their definitions of logical validity and compare them with our approach to compound conditionals. We prove a Probabilistic Deduction Theorem for conditional events. After that, we study some probabilistic deduction theorems, by presenting several examples. We focus on iterated conditionals and the invalidity of the Import-Export principle in the light of our Probabilistic Deduction Theorem. We use the inference from a disjunction, "$A$ or $B$", to the conditional,"i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27169;&#24577;&#36830;&#25509;&#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;&#31181;&#32676;&#23398;&#20064;&#30340;&#23398;&#20064;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.10225</link><description>&lt;p&gt;
&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;:&#40065;&#26834;&#27169;&#24577;&#36830;&#25509;&#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks. (arXiv:2303.10225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27169;&#24577;&#36830;&#25509;&#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;&#31181;&#32676;&#23398;&#20064;&#30340;&#23398;&#20064;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26159;&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#29702;&#38454;&#27573;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#33021;&#21147;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#24378;&#21270;&#40065;&#26834;&#24615;&#35757;&#32451;&#25216;&#26415;&#33021;&#22815;&#25552;&#39640;&#23545;&#19968;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22810;&#26679;&#21270;&#30340;$\ell_p$&#25915;&#20987;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;$\ell_p$&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27169;&#24577;&#36830;&#25509; (RMC) &#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#22522;&#20110;&#31181;&#32676;&#23398;&#20064;&#30340;&#23398;&#20064;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;RMC&#65292;&#33021;&#22815;&#25628;&#32034;&#20004;&#20010;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#21253;&#21547;&#39640;&#40065;&#26834;&#24615;&#28857;&#30340;&#36335;&#24452;&#20197;&#25269;&#24481;&#22810;&#26679;&#21270;&#30340;$\ell_p$&#25915;&#20987;&#12290;&#22522;&#20110;RMC&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;&#22522;&#20110;RMC&#30340;&#20248;&#21270;&#65292;&#20854;&#20013;RMC&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#22810;&#26679;&#21270;$\ell_p$&#40065;&#26834;&#24615;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;&#22522;&#26412;&#21333;&#20803;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#19982;&#20165;&#36873;&#25321;&#23376;&#38598;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30456;&#32467;&#21512;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#32452;&#36739;&#23567;&#30340;&#20195;&#34920;&#24615;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21487;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness is a key concept in measuring the ability of neural networks to defend against adversarial attacks during the inference phase. Recent studies have shown that despite the success of improving adversarial robustness against a single type of attack using robust training techniques, models are still vulnerable to diversified $\ell_p$ attacks. To achieve diversified $\ell_p$ robustness, we propose a novel robust mode connectivity (RMC)-oriented adversarial defense that contains two population-based learning phases. The first phase, RMC, is able to search the model parameter space between two pre-trained models and find a path containing points with high robustness against diversified $\ell_p$ attacks. In light of the effectiveness of RMC, we develop a second phase, RMC-based optimization, with RMC serving as the basic unit for further enhancement of neural network diversified $\ell_p$ robustness. To increase computational efficiency, we incorporate learning with a sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#23545;&#38544;&#24335;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#35775;&#38382;&#38590;&#20197;&#33719;&#24471;&#30340;&#26174;&#24335;&#26631;&#31614;&#65292;&#24182;&#21457;&#29616;&#20102;&#31616;&#21333;&#32780;&#24120;&#29992;&#30340;softmax&#21551;&#21457;&#24335;&#22312;&#24179;&#34913;&#31639;&#27861;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10218</link><description>&lt;p&gt;
&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Federated Contextual Bandit Algorithms. (arXiv:2303.10218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#23545;&#38544;&#24335;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#35775;&#38382;&#38590;&#20197;&#33719;&#24471;&#30340;&#26174;&#24335;&#26631;&#31614;&#65292;&#24182;&#21457;&#29616;&#20102;&#31616;&#21333;&#32780;&#24120;&#29992;&#30340;softmax&#21551;&#21457;&#24335;&#22312;&#24179;&#34913;&#31639;&#27861;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#22312;&#23398;&#20064;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#25935;&#24863;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;&#65292;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#38544;&#24335;&#20449;&#21495;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#24456;&#38590;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#26174;&#24335;&#26631;&#31614;&#12290;&#25105;&#20204;&#37319;&#29992;&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#32852;&#37030;&#35774;&#32622;&#20174;&#20013;&#24515;&#21270;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#20986;&#30528;&#21517;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#27169;&#25311;&#30340;&#19968;&#31995;&#21015;&#22330;&#26223;&#20013;&#20180;&#32454;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#35774;&#32622;&#65292;&#20363;&#22914;&#21021;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38543;&#21518;&#29992;&#25143;&#20132;&#20114;&#20043;&#38388;&#30340;&#21508;&#31181;&#19981;&#23545;&#40784;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#38750;&#31283;&#23450;&#24615;&#21644;/&#25110;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#36896;&#25104;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31616;&#21333;&#21644;&#24120;&#29992;&#30340;softmax&#21551;&#21457;&#24335;&#22312;&#24179;&#34913;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the adoption of federated learning increases for learning from sensitive data local to user devices, it is natural to ask if the learning can be done using implicit signals generated as users interact with the applications of interest, rather than requiring access to explicit labels which can be difficult to acquire in many tasks. We approach such problems with the framework of federated contextual bandits, and develop variants of prominent contextual bandit algorithms from the centralized seting for the federated setting. We carefully evaluate these algorithms in a range of scenarios simulated using publicly available datasets. Our simulations model typical setups encountered in the real-world, such as various misalignments between an initial pre-trained model and the subsequent user interactions due to non-stationarity in the data and/or heterogeneity across clients. Our experiments reveal the surprising effectiveness of the simple and commonly used softmax heuristic in balancing 
&lt;/p&gt;</description></item><item><title>BotShape&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#34892;&#20026;&#27169;&#24335;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#37325;&#35201;&#30340;&#34892;&#20026;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10214</link><description>&lt;p&gt;
BotShape&#65306;&#19968;&#31181;&#22522;&#20110;&#34892;&#20026;&#27169;&#24335;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BotShape: A Novel Social Bots Detection Approach via Behavioral Patterns. (arXiv:2303.10214v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10214
&lt;/p&gt;
&lt;p&gt;
BotShape&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#34892;&#20026;&#27169;&#24335;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#37325;&#35201;&#30340;&#34892;&#20026;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#20934;&#30830;&#26816;&#27979;&#26426;&#22120;&#20154;&#36134;&#25143;&#24182;&#20943;&#36731;&#20854;&#23545;&#30495;&#23454;&#29992;&#25143;&#30340;&#26377;&#23475;&#24433;&#21709;&#65288;&#22914;&#35823;&#23548;&#12289;&#35875;&#35328;&#21644;&#22403;&#22334;&#20449;&#24687;&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21407;&#22987;&#20107;&#20214;&#26085;&#24535;&#26500;&#24314;&#20102;&#34892;&#20026;&#24207;&#21015;&#65292;&#24182;&#25552;&#21462;&#20102;&#20851;&#38190;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26426;&#22120;&#20154;&#36134;&#25143;&#19982;&#30495;&#23454;&#29992;&#25143;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#26426;&#22120;&#20154;&#36134;&#25143;&#20043;&#38388;&#30456;&#20284;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#31995;&#32479;BotShape&#65292;&#33258;&#21160;&#25429;&#25417;&#34892;&#20026;&#24207;&#21015;&#21644;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#20998;&#31867;&#22120;&#26816;&#27979;&#26426;&#22120;&#20154;&#36134;&#25143;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#20998;&#31867;&#22120;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;98.52&#65285;&#65292;&#24179;&#22343;f1&#20998;&#25968;&#20026;96.65&#65285;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;BotShape&#26159;&#19968;&#31181;&#26032;&#30340;&#36134;&#21495;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#37325;&#35201;&#30340;&#34892;&#20026;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
An essential topic in online social network security is how to accurately detect bot accounts and relieve their harmful impacts (e.g., misinformation, rumor, and spam) on genuine users. Based on a real-world data set, we construct behavioral sequences from raw event logs. After extracting critical characteristics from behavioral time series, we observe differences between bots and genuine users and similar patterns among bot accounts. We present a novel social bot detection system BotShape, to automatically catch behavioral sequences and characteristics as features for classifiers to detect bots. We evaluate the detection performance of our system in ground-truth instances, showing an average accuracy of 98.52% and an average f1-score of 96.65% on various types of classifiers. After comparing it with other research, we conclude that BotShape is a novel approach to profiling an account, which could improve performance for most methods by providing significant behavioral features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#26469;&#25552;&#39640;DL&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MRI&#30340;&#33041;&#37096;sCT&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10202</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22522;&#20110;&#33041;&#37096;MRI&#21040;CT&#21512;&#25104;&#30340;&#23545;&#27604;&#24230;&#27867;&#21270;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring contrast generalisation in deep learning-based brain MRI-to-CT synthesis. (arXiv:2303.10202v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#26469;&#25552;&#39640;DL&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MRI&#30340;&#33041;&#37096;sCT&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#24050;&#32463;&#25552;&#20986;&#24182;&#22312;&#20020;&#24202;&#19978;&#36234;&#26469;&#36234;&#21463;&#21040;&#35748;&#21487;&#30340;&#21512;&#25104;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;sCT&#65289;&#20197;&#20415;&#22522;&#20110;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30340;&#25918;&#30103;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26368;&#36817;&#23637;&#31034;&#20102;&#20174;&#22266;&#23450;MRI&#37319;&#38598;&#20013;&#29983;&#25104;&#20934;&#30830;sCT&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;MRI&#21327;&#35758;&#21487;&#33021;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#25110;&#22312;&#19981;&#21516;&#20013;&#24515;&#20043;&#38388;&#26377;&#25152;&#19981;&#21516;&#65292;&#23548;&#33268;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#32780;&#29983;&#25104;&#20302;&#36136;&#37327;&#30340;sCT&#12290;&#30446;&#30340;&#65306;&#30740;&#31350;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#20197;&#22686;&#21152;&#29992;&#20110;&#29983;&#25104;&#33041;&#37096;sCT &#30340;DL&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#25910;&#38598;95&#21517;&#25509;&#21463;&#25918;&#30103;&#30340;&#24739;&#32773;&#30340;CT&#21644;&#30456;&#24212;&#30340;T1&#21152;&#26435;MRI /&#19981;&#21152;&#23545;&#27604;&#21058;&#65292;T2&#21152;&#26435;&#21644;FLAIR MRI&#65292;&#20854;&#20013;&#32771;&#34385;FLAIR&#20316;&#20026;&#30740;&#31350;&#27867;&#21270;&#30340;&#26410;&#35265;&#24207;&#21015;&#12290;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20934;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#25110;&#19981;&#21253;&#25324;FLAIR&#24207;&#21015;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#27809;&#26377;DR&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#35780;&#20272;&#20102;&#22270;&#20687;&#30456;&#20284;&#24615;&#21644;&#22522;&#20110;sCT&#30340;&#21058;&#37327;&#35745;&#21010;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#36873;&#25321;&#26368;&#20339;&#34920;&#29616;&#30340;DR&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Synthetic computed tomography (sCT) has been proposed and increasingly clinically adopted to enable magnetic resonance imaging (MRI)-based radiotherapy. Deep learning (DL) has recently demonstrated the ability to generate accurate sCT from fixed MRI acquisitions. However, MRI protocols may change over time or differ between centres resulting in low-quality sCT due to poor model generalisation. Purpose: investigating domain randomisation (DR) to increase the generalisation of a DL model for brain sCT generation. Methods: CT and corresponding T1-weighted MRI with/without contrast, T2-weighted, and FLAIR MRI from 95 patients undergoing RT were collected, considering FLAIR the unseen sequence where to investigate generalisation. A ``Baseline'' generative adversarial network was trained with/without the FLAIR sequence to test how a model performs without DR. Image similarity and accuracy of sCT-based dose plans were assessed against CT to select the best-performing DR approach a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;SFE&#65292;&#30001;&#25628;&#32034;&#20195;&#29702;&#21644;&#20004;&#20010;&#31639;&#23376;&#25191;&#34892;&#25506;&#32034;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38477;&#32500;&#21518;&#24615;&#33021;&#19981;&#33021;&#25552;&#39640;</title><link>http://arxiv.org/abs/2303.10182</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#31616;&#21333;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;SFE
&lt;/p&gt;
&lt;p&gt;
SFE: A Simple, Fast and Efficient Feature Selection Algorithm for High-Dimensional Data. (arXiv:2303.10182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;SFE&#65292;&#30001;&#25628;&#32034;&#20195;&#29702;&#21644;&#20004;&#20010;&#31639;&#23376;&#25191;&#34892;&#25506;&#32034;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38477;&#32500;&#21518;&#24615;&#33021;&#19981;&#33021;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#8212;&#8212;SFE&#65288;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#65289;&#12290;SFE&#31639;&#27861;&#20351;&#29992;&#25628;&#32034;&#20195;&#29702;&#21644;&#20004;&#20010;&#31639;&#23376;&#65288;&#38750;&#36873;&#25321;&#21644;&#36873;&#25321;&#65289;&#25191;&#34892;&#25628;&#32034;&#36807;&#31243;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#38750;&#36873;&#25321;&#31639;&#23376;&#22312;&#25972;&#20010;&#38382;&#39064;&#25628;&#32034;&#31354;&#38388;&#20013;&#25628;&#32034;&#19982;&#20998;&#31867;&#32467;&#26524;&#19981;&#30456;&#20851;&#12289;&#20887;&#20313;&#12289;&#24494;&#19981;&#36275;&#36947;&#21644;&#26377;&#22122;&#22768;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#30340;&#29366;&#24577;&#20174;&#36873;&#25321;&#27169;&#24335;&#26356;&#25913;&#20026;&#38750;&#36873;&#25321;&#27169;&#24335;&#12290;&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;&#36873;&#25321;&#31639;&#23376;&#22312;&#38382;&#39064;&#25628;&#32034;&#31354;&#38388;&#20013;&#25628;&#32034;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#36739;&#22823;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#30340;&#29366;&#24577;&#20174;&#38750;&#36873;&#25321;&#27169;&#24335;&#26356;&#25913;&#20026;&#36873;&#25321;&#27169;&#24335;&#12290;&#35813;SFE&#31639;&#27861;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20294;&#26159;&#65292;&#22312;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#20043;&#21518;&#65292;&#23427;&#30340;&#24615;&#33021;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#23545;&#38477;&#32500;&#21518;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new feature selection algorithm, called SFE (Simple, Fast, and Efficient), is proposed for high-dimensional datasets. The SFE algorithm performs its search process using a search agent and two operators: non-selection and selection. It comprises two phases: exploration and exploitation. In the exploration phase, the non-selection operator performs a global search in the entire problem search space for the irrelevant, redundant, trivial, and noisy features, and changes the status of the features from selected mode to non-selected mode. In the exploitation phase, the selection operator searches the problem search space for the features with a high impact on the classification results, and changes the status of the features from non-selected mode to selected mode. The proposed SFE is successful in feature selection from high-dimensional datasets. However, after reducing the dimensionality of a dataset, its performance cannot be increased significantly. In these situations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10181</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#26041;&#38754;&#30340;&#26368;&#26032;&#31361;&#30772;&#65292;&#20351;&#20854;&#24471;&#21040;&#24555;&#36895;&#21457;&#23637;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#65292;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#26041;&#38754;&#30340;&#36164;&#28304;&#28040;&#32791;&#26159;&#24040;&#22823;&#30340;&#12290;&#36825;&#20123;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#21487;&#33021;&#20250;&#38459;&#30861;&#36825;&#20123;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#21162;&#21147;&#24341;&#20837;&#36164;&#28304;&#25928;&#29575;&#30340;&#27010;&#24565;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#37327;&#21270;&#26469;&#20943;&#36731;&#20869;&#23384;&#28040;&#32791;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#21033;&#29992;&#65292;&#20294;&#21487;&#33021;&#20250;&#20197;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29305;&#21035;&#26159;&#22312;&#35786;&#25152;&#31561;&#20851;&#38190;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.10180</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23433;&#20840;&#30340;&#19993;&#27850;&#37210;&#20840;&#40635;&#21058;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#40635;&#37257;&#26377;&#26395;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#40635;&#37257;&#31649;&#29702;&#65292;&#20351;&#40635;&#37257;&#24072;&#20813;&#20110;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#24739;&#32773;&#25163;&#26415;&#25252;&#29702;&#30340;&#26368;&#20851;&#38190;&#26041;&#38754;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#20110;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20294;&#31163;&#20020;&#24202;&#24212;&#29992;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#35299;&#20915;&#23398;&#20064;&#40635;&#37257;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#20197;&#32531;&#35299;&#33073;&#32447;&#24773;&#20917;&#19979;Q&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#22312;&#26234;&#33021;&#20307;&#30340;&#22521;&#35757;&#20013;&#28155;&#21152;&#19968;&#39033;&#31574;&#30053;&#32422;&#26463;&#39033;&#65292;&#20197;&#20445;&#25345;&#26234;&#33021;&#20307;&#21644;&#40635;&#37257;&#24072;&#30340;&#31574;&#30053;&#20998;&#24067;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20840;&#40635;&#24773;&#26223;&#19979;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PCQL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.10130</link><description>&lt;p&gt;
GPT&#26159;GPT&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#24433;&#21709;&#30340;&#26089;&#26399;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#26032;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#32844;&#19994;&#19982;GPT&#33021;&#21147;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GPT-4&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#33267;&#23569;&#26377;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#21463;&#21040;GPT&#24341;&#20837;&#30340;&#24433;&#21709;&#65292;&#32780;&#32422;19%&#30340;&#24037;&#20154;&#21487;&#33021;&#20250;&#30475;&#21040;&#33267;&#23569;50%&#30340;&#20219;&#21153;&#21463;&#21040;&#24433;&#21709;&#12290;&#24433;&#21709;&#33539;&#22260;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#65292;&#39640;&#25910;&#20837;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#39118;&#38505;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24433;&#21709;&#24182;&#19981;&#23616;&#38480;&#20110;&#26368;&#36817;&#29983;&#20135;&#29575;&#22686;&#38271;&#36739;&#39640;&#30340;&#34892;&#19994;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#20855;&#26377;&#36890;&#29992;&#25216;&#26415;&#65288;GPT&#65289;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
&lt;/p&gt;</description></item><item><title>TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.09807</link><description>&lt;p&gt;
TKN&#65306;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#20851;&#38190;&#28857;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction. (arXiv:2303.09807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09807
&lt;/p&gt;
&lt;p&gt;
TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#24191;&#27867;&#29992;&#36884;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36807;&#20110;&#24378;&#35843;&#20934;&#30830;&#24615;&#65292;&#24573;&#35270;&#20102;&#30001;&#20110;&#36807;&#20110;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#32780;&#23548;&#33268;&#30340;&#36739;&#24930;&#30340;&#39044;&#27979;&#36895;&#24230;&#20197;&#21450;&#36807;&#22810;&#30340;&#20887;&#20313;&#20449;&#24687;&#23398;&#20064;&#21644;GPU&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TKN&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#12290;TKN&#26159;&#25105;&#20204;&#30446;&#21069;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#20445;&#25345;&#20854;&#20182;&#24615;&#33021;&#12290;&#22312;KTH&#21644;Human Action 3D&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;TKN&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video prediction is a complex time-series forecasting task with great potential in many use cases. However, conventional methods overemphasize accuracy while ignoring the slow prediction speed caused by complicated model structures that learn too much redundant information with excessive GPU memory consumption. Furthermore, conventional methods mostly predict frames sequentially (frame-by-frame) and thus are hard to accelerate. Consequently, valuable use cases such as real-time danger prediction and warning cannot achieve fast enough inference speed to be applicable in reality. Therefore, we propose a transformer-based keypoint prediction neural network (TKN), an unsupervised learning method that boost the prediction process via constrained information extraction and parallel prediction scheme. TKN is the first real-time video prediction solution to our best knowledge, while significantly reducing computation costs and maintaining other performance. Extensive experiments on KTH and Hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;POI-MetaBlock&#30340;&#26032;&#27169;&#22359;&#65292;&#32467;&#21512;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#21644;&#20132;&#36890;&#29305;&#24449;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#36827;&#34892;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09789</link><description>&lt;p&gt;
&#22522;&#20110;&#22478;&#24066;&#21306;&#22495;&#21151;&#33021;&#24341;&#23548;&#30340;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Urban Regional Function Guided Traffic Flow Prediction. (arXiv:2303.09789v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;POI-MetaBlock&#30340;&#26032;&#27169;&#22359;&#65292;&#32467;&#21512;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#21644;&#20132;&#36890;&#29305;&#24449;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#36827;&#34892;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#39044;&#27979;&#26159;&#26102;&#31354;&#20998;&#26512;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#38500;&#20102;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#22478;&#24066;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#20063;&#22312;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#21306;&#22495;&#21151;&#33021;&#23646;&#24615;&#30340;&#25506;&#32034;&#20027;&#35201;&#38598;&#20013;&#22312;&#28155;&#21152;&#39069;&#22806;&#30340;&#25299;&#25169;&#32467;&#26500;&#19978;&#65292;&#24573;&#30053;&#20102;&#21151;&#33021;&#23646;&#24615;&#23545;&#21306;&#22495;&#20132;&#36890;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;POI-MetaBlock&#30340;&#26032;&#27169;&#22359;&#65292;&#21033;&#29992;&#27599;&#20010;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#65288;&#30001;&#20852;&#36259;&#28857;&#20998;&#24067;&#34920;&#31034;&#65289;&#20316;&#20026;&#20803;&#25968;&#25454;&#26469;&#36827;&#19968;&#27493;&#25366;&#25496;&#19981;&#21516;&#21151;&#33021;&#21306;&#22495;&#30340;&#19981;&#21516;&#20132;&#36890;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;POI-MetaBlock&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#32467;&#21512;POI&#21644;&#26102;&#38388;&#20449;&#24687;&#29983;&#25104;&#27599;&#20010;&#21306;&#22495;&#30340;&#21160;&#24577;&#27880;&#24847;&#21442;&#25968;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#21508;&#31181;&#21306;&#22495;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of traffic flow is a challenging yet crucial problem in spatial-temporal analysis, which has recently gained increasing interest. In addition to spatial-temporal correlations, the functionality of urban areas also plays a crucial role in traffic flow prediction. However, the exploration of regional functional attributes mainly focuses on adding additional topological structures, ignoring the influence of functional attributes on regional traffic patterns. Different from the existing works, we propose a novel module named POI-MetaBlock, which utilizes the functionality of each region (represented by Point of Interest distribution) as metadata to further mine different traffic characteristics in areas with different functions. Specifically, the proposed POI-MetaBlock employs a self-attention architecture and incorporates POI and time information to generate dynamic attention parameters for each region, which enables the model to fit different traffic patterns of various ar
&lt;/p&gt;</description></item><item><title>SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09540</link><description>&lt;p&gt;
SemDeDup:&#36890;&#36807;&#35821;&#20041;&#21435;&#37325;&#23454;&#29616;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;arXiv:2303.09540v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09540
&lt;/p&gt;
&lt;p&gt;
SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#28023;&#37327;&#25968;&#25454;&#30340;&#22686;&#21152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#22411;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#38500;&#26597;&#25214;&#31934;&#30830;&#37325;&#22797;&#39033;&#22806;&#65292;&#22823;&#37096;&#20998;&#26410;&#32463;&#31934;&#24515;&#31579;&#36873;&#65292;&#21487;&#33021;&#23384;&#22312;&#24456;&#22810;&#20887;&#20313;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SemDeDup&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#65306;&#21363;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#25968;&#25454;&#23545;&#12290;&#21435;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#21487;&#20197;&#20445;&#25345;&#24615;&#33021;&#24182;&#21152;&#36895;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#20197;&#22806;&#24471;&#21040;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#37096;&#20998;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;C4&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;SemDeDup&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#36136;&#37327;&#23884;&#20837;&#31616;&#21333;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#26356;&#24555;&#22320;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.09038</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Prompt Learning&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65306;&#32467;&#26524;&#12289;&#38480;&#21046;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20854;&#31867;&#20284;&#20154;&#31867;&#34920;&#36798;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20415;&#24739;&#32773;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#24471;&#21040;&#26356;&#22909;&#30340;&#21307;&#30103;&#25945;&#32946;&#12290;&#30740;&#31350;&#37319;&#38598;&#20102;62&#20221;&#20302;&#21058;&#37327;&#33016;&#37096;CT&#32954;&#30284;&#31579;&#26597;&#25195;&#25551;&#21644;76&#20221;&#33041;MRI&#36716;&#31227;&#24615;&#31579;&#26597;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#35780;&#20215;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;0.07&#22788;&#65292;&#20449;&#24687;&#38169;&#35823;0.11&#22788;&#12290;&#23601;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#32780;&#35328;&#65292;&#23427;&#20204;&#26159;&#19968;&#33324;&#24615;&#30340;&#30456;&#20851;&#24314;&#35758;&#65292;&#20363;&#22914;&#20445;&#25345;&#19982;&#21307;&#29983;&#30340;&#38543;&#35775;&#21644;&#23494;&#20999;&#30417;&#27979;&#20219;&#20309;&#30151;&#29366;&#65292;&#23545;&#20110;&#20849;138&#20010;&#30149;&#20363;&#20013;&#30340;&#32422;37&#65285;&#65292;ChatGPT&#25552;&#20379;&#20102;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#26377;&#20851;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Answer Set Programming&#23454;&#29616;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#20154;&#31867;&#23545;&#35805;&#30340;AutoConcierge&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26377;&#20851;&#38468;&#36817;&#39184;&#21381;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08941</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20132;&#20114;&#24335;&#29305;&#23450;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#65292;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs. (arXiv:2303.08941v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Answer Set Programming&#23454;&#29616;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#20154;&#31867;&#23545;&#35805;&#30340;AutoConcierge&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26377;&#20851;&#38468;&#36817;&#39184;&#21381;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23454;&#29616;&#19982;&#20154;&#31867;&#30340;&#20154;&#31867;&#23545;&#35805;&#30456;&#20284;&#30340;&#36890;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#32463;&#20856;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20381;&#36182;&#20110;&#27169;&#24335;&#21305;&#37197;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#29702;&#35299;&#21477;&#23376;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#21709;&#24212;&#12290;&#35201;&#29983;&#25104;&#20445;&#35777;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#24517;&#39035;&#8220;&#29702;&#35299;&#8221;&#21477;&#23376;&#30340;&#35821;&#20041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#8220;&#29702;&#35299;&#8221;&#65292;&#38656;&#35201;&#22522;&#20110;&#36923;&#36753;&#30340;&#65288;&#24120;&#35782;&#65289;&#25512;&#29702;&#26041;&#27861;&#65292;&#20363;&#22914;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;AutoConcierge&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;LLMs&#21644;ASP&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#35805;&#20195;&#29702;&#65292;&#21487;&#20197;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#21463;&#38480;&#39046;&#22495;&#20869;&#30340;&#20154;&#31867;&#23545;&#35805;&#12290;AutoConcierge&#19987;&#27880;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#39046;&#22495;-&#26681;&#25454;&#29992;&#25143;&#30340;&#21916;&#22909;&#24314;&#35758;&#20182;&#20204;&#38468;&#36817;&#30340;&#39184;&#21381;&#12290;AutoConcierge&#23558;&#20132;&#20114;&#24335;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#35805;&#35821;&#65292;&#30830;&#23450;&#20854;&#20013;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-like communication with machines remains a classic, challenging topic in the field of Knowledge Representation and Reasoning and Natural Language Processing. These Large Language Models (LLMs) rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses. To generate an assuredly correct response, one has to "understand" the semantics of a sentence. To achieve this "understanding", logic-based (commonsense) reasoning methods such as Answer Set Programming (ASP) are arguably needed. In this paper, we describe the AutoConcierge system that leverages LLMs and ASP to develop a conversational agent that can truly "understand" human dialogs in restricted domains. AutoConcierge is focused on a specific domain-advising users about restaurants in their local area based on their preferences. AutoConcierge will interactively understand a user's utterances, identify the missing information in them
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Bi-VAEGAN &#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#12289;L_2 &#33539;&#25968;&#29305;&#24449;&#24402;&#19968;&#21270;&#21644;&#26356;&#22797;&#26434;&#30340;&#20808;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#36328;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08698</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-directional Distribution Alignment for Transductive Zero-Shot Learning. (arXiv:2303.08698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Bi-VAEGAN &#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#12289;L_2 &#33539;&#25968;&#29305;&#24449;&#24402;&#19968;&#21270;&#21644;&#26356;&#22797;&#26434;&#30340;&#20808;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#36328;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#21363;&#26410;&#35265;&#31867;&#21035;&#30340;&#30495;&#23454;&#21644;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#23613;&#31649;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#20351;&#29992;&#26410;&#35265;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#65292;&#20294;&#20998;&#24067;&#20559;&#31227;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; TZSL &#27169;&#22411;&#65288;&#21629;&#21517;&#20026; Bi-VAEGAN&#65289;&#65292;&#36890;&#36807;&#21152;&#24378;&#35270;&#35273;&#31354;&#38388;&#21644;&#36741;&#21161;&#31354;&#38388;&#20043;&#38388;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#36825;&#31181;&#20559;&#31227;&#12290;&#27169;&#22411;&#35774;&#35745;&#30340;&#20851;&#38190;&#25552;&#35758;&#21253;&#25324;&#65288;1&#65289;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#65292;&#65288;2&#65289;&#22522;&#20110; L_2 &#33539;&#25968;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29305;&#24449;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#65288;3&#65289;&#26356;&#22797;&#26434;&#30340;&#26410;&#35265;&#31867;&#21035;&#20808;&#39564;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#20351;&#29992;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#35780;&#20272;&#26102;&#65292;Bi-VAEGAN &#22312;&#26631;&#20934;&#21644;&#24191;&#20041; TZSL &#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/Zhicaiwww/Bi-VAEGAN &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that zero-shot learning (ZSL) can suffer severely from the problem of domain shift, where the true and learned data distributions for the unseen classes do not match. Although transductive ZSL (TZSL) attempts to improve this by allowing the use of unlabelled examples from the unseen classes, there is still a high level of distribution shift. We propose a novel TZSL model (named as Bi-VAEGAN), which largely improves the shift by a strengthened distribution alignment between the visual and auxiliary spaces. The key proposal of the model design includes (1) a bi-directional distribution alignment, (2) a simple but effective L_2-norm based feature normalization approach, and (3) a more sophisticated unseen class prior estimation approach. In benchmark evaluation using four datasets, Bi-VAEGAN achieves the new state of the arts under both the standard and generalized TZSL settings. Code could be found at https://github.com/Zhicaiwww/Bi-VAEGAN
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#22270;&#20687;&#24341;&#23548;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#21160;&#30830;&#23450;&#33107;&#39592;&#22797;&#20301;&#26041;&#21521;&#65292;&#36827;&#32780;&#25552;&#39640;&#39592;&#25240;&#25163;&#26415;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25913;&#21892;&#24739;&#32773;&#30340;&#25163;&#26415;&#25928;&#26524;&#65292;&#38477;&#20302;&#24739;&#32773;&#21457;&#29983;&#39592;&#24615;&#20851;&#33410;&#28814;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2303.08105</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#36381;&#39592;&#39592;&#25240;&#20462;&#22797;&#30340;&#22270;&#20687;&#24341;&#23548;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Guidance for Robot-Assisted Ankle Fracture Repair. (arXiv:2303.08105v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#22270;&#20687;&#24341;&#23548;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#21160;&#30830;&#23450;&#33107;&#39592;&#22797;&#20301;&#26041;&#21521;&#65292;&#36827;&#32780;&#25552;&#39640;&#39592;&#25240;&#25163;&#26415;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25913;&#21892;&#24739;&#32773;&#30340;&#25163;&#26415;&#25928;&#26524;&#65292;&#38477;&#20302;&#24739;&#32773;&#21457;&#29983;&#39592;&#24615;&#20851;&#33410;&#28814;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#28041;&#21450;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#33107;&#39592;&#22797;&#20301;&#30340;&#22270;&#20687;&#24341;&#23548;&#26694;&#26550;&#65292;&#24182;&#26088;&#22312;&#20135;&#29983;&#21644;&#35777;&#26126;&#33258;&#21160;&#30830;&#23450;&#33107;&#39592;&#22797;&#20301;&#26041;&#21521;&#30340;&#36719;&#20214;&#30340;&#27491;&#30830;&#21151;&#33021;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20943;&#21387;&#31243;&#24207;&#65292;&#20174;&#32780;&#20943;&#23569;&#25163;&#26415;&#26102;&#38388;&#21644;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#29702;&#24819;&#30340;&#26368;&#32456;&#33107;&#39592;&#20301;&#32622;&#35823;&#24046;&#20943;&#23567;&#12289;&#38887;&#24102;&#32467;&#26500;&#24674;&#22797;&#24471;&#21040;&#25913;&#21892;&#20197;&#21450;&#21019;&#20260;&#21518;&#39592;&#24615;&#20851;&#33410;&#28814;&#21457;&#29983;&#29575;&#38477;&#20302;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project concerns developing and validating an image guidance framework for application to a robotic-assisted fibular reduction in ankle fracture surgery. The aim is to produce and demonstrate proper functioning of software for automatic determination of directions for fibular repositioning with the ultimate goal of application to a robotic reduction procedure that can reduce the time and complexity of the procedure as well as provide the benefits of reduced error in ideal final fibular position, improved syndesmosis restoration and reduced incidence of post-traumatic osteoarthritis. The focus of this product will be developing and testing the image guidance software, from the input of preoperative images through the steps of automated segmentation and registration until the output of a final transformation that can be used as instructions to a robot on how to reposition the fibula, but will not involve developing or implementing the hardware of the robot itself.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2303.07205</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#23383;&#30340;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#39640;&#24230;&#22797;&#26434;&#19988;&#20960;&#20046;&#38590;&#20197;&#21306;&#20998;&#20986;&#26159;&#21542;&#20026;&#20154;&#31867;&#21019;&#20316;&#30340; LLM &#29983;&#25104;&#25991;&#23383;&#12290;&#20294;&#26159;&#65292;&#36825;&#20063;&#24341;&#21457;&#20102;&#23545;&#27492;&#31867;&#25991;&#23383;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#22312;&#25945;&#32946;&#31995;&#32479;&#20013;&#36896;&#25104;&#28151;&#20081;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20851;&#20110;&#20854;&#25104;&#23601;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#29616;&#26377;&#30340; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#23545;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25511;&#21046;&#21644;&#30417;&#31649;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292;&#21253;&#25324;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#20197;&#25512;&#21160; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06152</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#25110;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65311;&#8212;&#8212;&#23545;&#35937;&#21644;&#24037;&#20855;&#21151;&#33021;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#29992;&#20110;&#35774;&#35745;&#29702;&#35299;&#12289;&#25913;&#36827;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation. (arXiv:2303.06152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates how a particular object and its participation in the processes it is designed to support can be represented in a general function representational language and framework, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#21644;&#24037;&#20855;&#30340;&#21151;&#33021;&#26041;&#38754;&#30340;&#29702;&#35299;&#23545;&#20110;&#25903;&#25345;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#19982;&#21508;&#31181;&#23545;&#35937;&#12289;&#32467;&#26500;&#21644;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#20197;&#24110;&#21161;&#23454;&#29616;&#20854;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#21151;&#33021;&#30340;&#35814;&#32454;&#29702;&#35299;&#20063;&#21487;&#20197;&#23548;&#33268;&#35774;&#35745;&#25913;&#36827;&#21644;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25805;&#20316;&#65292;&#19968;&#26041;&#38754;&#65292;&#22686;&#24378;&#20154;&#31867;&#29983;&#27963;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#38149;&#65289;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#28856;&#36807;&#31243;&#65289;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#35814;&#32454;&#35828;&#26126;&#28041;&#21450;&#30340;&#36807;&#31243;&#21644;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#8212;&#8212;&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65292;&#25110;&#32773;&#20026;&#20160;&#20040;&#26576;&#20010;&#37096;&#20214;&#22312;&#29006;&#38149;&#20013;&#30340;&#20301;&#32622;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the functional aspects of objects and tools is of paramount importance in supporting an intelligent system in navigating around in the environment and interacting with various objects, structures, and systems, to help fulfil its goals. A detailed understanding of functionalities can also lead to design improvements and novel designs that would enhance the operations of AI and robotic systems on the one hand, and human lives on the other. This paper demonstrates how a particular object - in this case, a frying pan - and its participation in the processes it is designed to support - in this case, the frying process - can be represented in a general function representational language and framework, that can be used to flesh out the processes and functionalities involved, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions - why is something a good frying pan, say, or why a certain part on t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#35745;&#31639;&#36830;&#32493;&#30340;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#65292;&#36825;&#23545;&#20110;&#35774;&#35745;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25171;&#24320;&#20102;&#26032;&#30340;&#31361;&#30772;&#21475;&#12290;</title><link>http://arxiv.org/abs/2303.05518</link><description>&lt;p&gt;
&#35745;&#31639;&#36830;&#32493;&#30340;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;
&lt;/p&gt;
&lt;p&gt;
Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#35745;&#31639;&#36830;&#32493;&#30340;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#65292;&#36825;&#23545;&#20110;&#35774;&#35745;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25171;&#24320;&#20102;&#26032;&#30340;&#31361;&#30772;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#22823;&#21270;&#25240;&#25187;&#21644;&#26377;&#38480;&#26102;&#38388;&#27493;&#38271;&#32047;&#31215;&#22870;&#21169;&#30340;&#32463;&#20856;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#65306;&#23384;&#22312;&#31639;&#27861;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#35745;&#31639;&#65292;&#39640;&#27010;&#29575;&#22320;&#23398;&#20064;&#21040;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#36229;&#36234;&#32463;&#20856;&#32047;&#31215;&#22870;&#21169;&#30340;&#26032;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#25351;&#23450;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#30446;&#26631;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#22312;&#20004;&#31181;&#20998;&#26512;&#35774;&#32622;&#20013;&#20805;&#20998;&#26465;&#20214;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#65292;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#30340;&#21487;PAC&#23398;&#20064;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#20165;&#32771;&#34385;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#19968;&#20010;&#30446;&#26631;&#20316;&#20026;&#19968;&#20010;oracle&#26159;&#19968;&#33268;&#36830;&#32493;&#30340;&#65292;&#37027;&#20040;&#23427;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#32771;&#34385;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#19968;&#20010;&#30446;&#26631;&#26159;&#35745;&#31639;&#36830;&#32493;&#30340;&#65292;&#24182;&#28385;&#36275;&#8220;&#32479;&#19968;&#36830;&#32493;&#26465;&#20214;&#8221;&#65292;&#37027;&#20040;&#23427;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#36229;&#36234;&#32047;&#31215;&#22870;&#21169;&#30340;&#30446;&#26631;&#30340;&#21487;&#23398;&#20064;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BPBA&#35774;&#32622;&#21644;BTOL&#35757;&#32451;&#31574;&#30053;&#20004;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#29992;&#25143;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#21644;&#21033;&#29992;&#22522;&#30784;/&#28304;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.03709</link><description>&lt;p&gt;
Bootstrap The Original Latent: &#20174;&#40657;&#30418;&#27169;&#22411;&#23398;&#20064;&#31169;&#26377;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bootstrap The Original Latent: Learning a Private Model from a Black-box Model. (arXiv:2303.03709v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BPBA&#35774;&#32622;&#21644;BTOL&#35757;&#32451;&#31574;&#30053;&#20004;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#29992;&#25143;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#21644;&#21033;&#29992;&#22522;&#30784;/&#28304;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Back-Propagated Black-Box Adaptation&#65288;BPBA&#65289;&#30340;&#26032;&#35774;&#32622;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#25968;&#25454;/&#27169;&#22411;&#38544;&#31169;&#21644;&#29992;&#25143;&#38656;&#27714;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25351;&#23548;&#12290;&#35813;&#35774;&#32622;&#21487;&#20197;&#31616;&#21270;&#22522;&#30784;/&#28304;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#38450;&#27490;&#22522;&#30784;/&#28304;&#27169;&#22411;&#30340;&#27844;&#28431;&#21644;&#35823;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bootstrap The Original Latent&#65288;BTOL&#65289;&#30340;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;/&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#30001;&#39046;&#22495;&#36866;&#37197;&#22120;&#21644;&#20919;&#20923;-&#35299;&#20923;&#31574;&#30053;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, considering the balance of data/model privacy of model owners and user needs, we propose a new setting called Back-Propagated Black-Box Adaptation (BPBA) for users to better train their private models via the guidance of the back-propagated results of a Black-box foundation/source model. Our setting can ease the usage of foundation/source models as well as prevent the leakage and misuse of foundation/source models. Moreover, we also propose a new training strategy called Bootstrap The Original Latent (BTOL) to fully utilize the foundation/source models. Our strategy consists of a domain adapter and a freeze-and-thaw strategy. We apply our BTOL under BPBA and Black-box UDA settings on three different datasets. Experiments show that our strategy is efficient and robust in various settings without manual augmentations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.02393</link><description>&lt;p&gt;
Seq-HyGAN: &#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Seq-HyGAN: Sequence Classification via Hypergraph Attention Network. (arXiv:2303.02393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#20998;&#31867;&#22312;&#19981;&#21516;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#22522;&#22240;&#32452;&#20998;&#31867;&#21644;&#22312;&#21830;&#19994;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#20047;&#26174;&#24335;&#30340;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#25429;&#33719;&#30456;&#37051;&#32467;&#26500;&#36830;&#25509;&#24182;&#24573;&#30053;&#24207;&#21015;&#20043;&#38388;&#30340;&#20840;&#23616;&#12289;&#39640;&#38454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Seq-HyGAN&#12290;&#20026;&#20102;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#36229;&#36793;&#65292;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#30340;&#23376;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#21452;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sequence classification has a wide range of real-world applications in different domains, such as genome classification in health and anomaly detection in business. However, the lack of explicit features in sequence data makes it difficult for machine learning models. While Neural Network (NN) models address this with learning features automatically, they are limited to capturing adjacent structural connections and ignore global, higher-order information between the sequences. To address these challenges in the sequence classification problems, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN. To capture the complex structural similarity between sequence data, we first create a hypergraph where the sequences are depicted as hyperedges and subsequences extracted from sequences are depicted as nodes. Additionally, we introduce an attention-based Hypergraph Neural Network model that utilizes a two-level attention mechanism. This model generates a sequence representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#65292;&#24182;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.01325</link><description>&lt;p&gt;
&#21069;&#24448;&#36127;&#36131;&#20219;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
A Pathway Towards Responsible AI Generated Content. (arXiv:2303.01325v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#65292;&#24182;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#20869;&#23481;&#28085;&#30422;&#20102;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;AIGC&#24050;&#32463;&#25104;&#20026;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#24182;&#26368;&#36817;&#22312;&#20854;&#36127;&#36131;&#20219;&#20351;&#29992;&#26041;&#38754;&#21463;&#21040;&#20102;&#35768;&#22810;&#25209;&#35780;&#12290;&#22312;&#26412;&#35270;&#37326;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#21487;&#33021;&#38459;&#30861;AIGC&#22312;&#23454;&#36341;&#20013;&#20581;&#24247;&#21457;&#23637;&#21644;&#37096;&#32626;&#30340;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#26469;&#33258;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#35760;&#24405;&#24050;&#30693;&#21644;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;AIGC&#30340;&#20219;&#20309;&#21487;&#33021;&#30340;&#28389;&#29992;&#24773;&#20917;&#65292;&#26088;&#22312;&#24341;&#36215;&#23545;&#28508;&#22312;&#39118;&#38505;&#21644;&#28389;&#29992;&#30340;&#27880;&#24847;&#65292;&#24110;&#21161;&#31038;&#20250;&#28040;&#38500;&#38556;&#30861;&#65292;&#24182;&#20419;&#36827;&#26356;&#21152;&#36947;&#24503;&#21644;&#23433;&#20840;&#30340;AIGC&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24212;&#23545;&#36825;&#20123;&#39118;&#38505;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#21516;&#26102;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;AIGC&#33021;&#22815;&#36127;&#36131;&#20219;&#22320;&#20026;&#31038;&#20250;&#24102;&#26469;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Generated Content (AIGC) has received tremendous attention within the past few years, with content ranging from image, text, to audio, video, etc. Meanwhile, AIGC has become a double-edged sword and recently received much criticism regarding its responsible usage. In this vision paper, we focus on three main concerns that may hinder the healthy development and deployment of AIGC in practice, including risks from privacy, bias, toxicity, misinformation, and intellectual property (IP). By documenting known and potential risks, as well as any possible misuse scenarios of AIGC, the aim is to draw attention to potential risks and misuse, help society to eliminate obstacles, and promote the more ethical and secure deployment of AIGC. Additionally, we provide insights into the promising directions for tackling these risks while constructing generative models, enabling AIGC to be used responsibly to benefit society.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#20262;&#29702;&#29702;&#24615;&#30340;&#26367;&#20195;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#26041;&#27861;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#20262;&#29702;&#36947;&#24503;&#19982;&#29702;&#24615;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26126;&#26174;&#30340;&#38271;&#26399;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23450;&#29702;&#35777;&#26126;&#22120;&#22312;&#27801;&#30418;&#20013;&#23454;&#29616;&#30340;&#22266;&#26377;&#23433;&#20840;&#30340;&#23454;&#29616;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.00752</link><description>&lt;p&gt;
&#26080;&#38656;&#20215;&#20540;&#19968;&#33268;&#24615;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Safety without alignment. (arXiv:2303.00752v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#20262;&#29702;&#29702;&#24615;&#30340;&#26367;&#20195;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#26041;&#27861;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#20262;&#29702;&#36947;&#24503;&#19982;&#29702;&#24615;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26126;&#26174;&#30340;&#38271;&#26399;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23450;&#29702;&#35777;&#26126;&#22120;&#22312;&#27801;&#30418;&#20013;&#23454;&#29616;&#30340;&#22266;&#26377;&#23433;&#20840;&#30340;&#23454;&#29616;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22522;&#20110;&#20262;&#29702;&#29702;&#24615;&#65288;Gewirth:1978&#65289;&#30340;&#23433;&#20840;&#24615;&#26367;&#20195;&#26041;&#27861;&#30340;&#21457;&#23637;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23450;&#29702;&#35777;&#26126;&#22120;&#22312;&#27801;&#30418;&#20013;&#23454;&#29616;&#30340;&#22266;&#26377;&#23433;&#20840;&#30340;&#23454;&#29616;&#36335;&#24452;&#12290;&#38543;&#30528;AGI&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#21487;&#33021;&#20250;&#36880;&#28176;&#28040;&#22833;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#24615;&#21482;&#33021;&#22686;&#21152;&#65288;&#21542;&#21017;&#26356;&#29702;&#24615;&#30340;&#20010;&#20307;&#23558;&#20855;&#26377;&#26174;&#30528;&#30340;&#36827;&#21270;&#20248;&#21183;&#65289;&#22240;&#27492;&#65292;&#23558;&#20262;&#29702;&#36947;&#24503;&#19982;&#29702;&#24615;&#32852;&#31995;&#36215;&#26469;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#38271;&#26399;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, the dominant paradigm in AI safety is alignment with human values. Here we describe progress on developing an alternative approach to safety, based on ethical rationalism (Gewirth:1978), and propose an inherently safe implementation path via hybrid theorem provers in a sandbox. As AGIs evolve, their alignment may fade, but their rationality can only increase (otherwise more rational ones will have a significant evolutionary advantage) so an approach that ties their ethics to their rationality has clear long-term advantages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#24182;&#25506;&#31350;&#19981;&#21516;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21487;&#20197;&#36827;&#34892;&#22240;&#23376;&#20998;&#35299;&#65292;&#32780;&#24102;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21017;&#38656;&#35201;&#29305;&#21035;&#32771;&#34385;&#19981;&#21516;&#30340;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2302.14146</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#19982;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#20013;&#30340;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Markov Conditions and Factorization in Logical Credal Networks. (arXiv:2302.14146v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#24182;&#25506;&#31350;&#19981;&#21516;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21487;&#20197;&#36827;&#34892;&#22240;&#23376;&#20998;&#35299;&#65292;&#32780;&#24102;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#21017;&#38656;&#35201;&#29305;&#21035;&#32771;&#34385;&#19981;&#21516;&#30340;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#35843;&#26597;&#19981;&#21516;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36923;&#36753;&#20449;&#20219;&#32593;&#32476;&#30340;&#32467;&#26500;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#27809;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32467;&#26500;&#23558;&#23548;&#33268;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#22240;&#23376;&#20998;&#35299;&#32467;&#26524;&#12290;&#23545;&#20110;&#24102;&#26377;&#26377;&#21521;&#24490;&#29615;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#39532;&#23572;&#31185;&#22827;&#26465;&#20214;&#12289;&#22240;&#23376;&#20998;&#35299;&#32467;&#26524;&#21644;&#35268;&#33539;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the recently proposed language of Logical Credal Networks, in particular investigating the consequences of various Markov conditions. We introduce the notion of structure for a Logical Credal Network and show that a structure without directed cycles leads to a well-known factorization result. For networks with directed cycles, we analyze the differences between Markov conditions, factorization results, and specification requirements.
&lt;/p&gt;</description></item><item><title>AugGPT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#26631;&#35760;&#30340;&#29983;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#26679;&#26412;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13007</link><description>&lt;p&gt;
AugGPT&#65306;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AugGPT: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13007
&lt;/p&gt;
&lt;p&gt;
AugGPT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#26631;&#35760;&#30340;&#29983;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#26679;&#26412;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26159;&#21463;&#38480;&#21046;&#30340;&#26679;&#26412;&#37327;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20811;&#26381;&#25361;&#25112;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#19988;&#36136;&#37327;&#26356;&#20302;&#65292;&#36825;&#19968;&#25361;&#25112;&#29305;&#21035;&#31361;&#20986;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#25968;&#25454;&#19981;&#21464;&#24615;&#24182;&#22686;&#21152;&#26679;&#26412;&#37327;&#65292;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#30340;&#19968;&#31181;&#33258;&#28982;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26159;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#30340;&#27491;&#30830;&#26631;&#35760;&#65288;&#32570;&#20047;&#24544;&#23454;&#24230;&#65289;&#65292;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65288;&#32570;&#20047;&#32039;&#20945;&#24615;&#65289;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#21457;ChatGPT&#26041;&#38754;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#65288;&#21517;&#20026;AugGPT&#65289;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.10681</link><description>&lt;p&gt;
FrankenSplit:&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AI&#21152;&#36895;&#22120;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#22823;&#27169;&#22411;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#35831;&#27714;&#19979;&#25918;&#65292;&#32780;&#39640;&#32500;&#25968;&#25454;&#23558;&#20105;&#22842;&#26377;&#38480;&#30340;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#24182;&#22312;&#21453;&#26144;&#36793;&#32536;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#19981;&#23545;&#31216;&#36164;&#28304;&#20998;&#37197;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SurvLIMEpy&#30340;Python&#21253;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#35745;&#31639;&#36866;&#29992;&#20110;&#24314;&#27169;&#29983;&#23384;&#20998;&#26512;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#29983;&#23384;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10571</link><description>&lt;p&gt;
SurvLIMEpy: &#19968;&#20010;&#23454;&#29616;&#20102;SurvLIME&#31639;&#27861;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
SurvLIMEpy: A Python package implementing SurvLIME. (arXiv:2302.10571v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SurvLIMEpy&#30340;Python&#21253;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#35745;&#31639;&#36866;&#29992;&#20110;&#24314;&#27169;&#29983;&#23384;&#20998;&#26512;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#29983;&#23384;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;SurvLIMEpy&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#23454;&#29616;&#20102;SurvLIME&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#36866;&#29992;&#20110;&#24314;&#27169;&#29983;&#23384;&#20998;&#26512;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21033;&#29992;&#20102;&#24182;&#34892;&#21270;&#33539;&#20363;&#65292;&#22240;&#20026;&#25152;&#26377;&#35745;&#31639;&#37117;&#20197;&#30697;&#38453;&#26041;&#24335;&#25191;&#34892;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;SurvLIMEpy&#36824;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;&#35813;&#21253;&#25903;&#25345;&#21508;&#31181;&#29983;&#23384;&#27169;&#22411;&#65292;&#20174;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21040;DeepHit&#25110;DeepSurv&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#23454;&#39564;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31639;&#27861;&#25429;&#33719;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#22312;&#20351;&#29992;&#19977;&#20010;&#24320;&#28304;&#29983;&#23384;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#32452;&#29983;&#23384;&#31639;&#27861;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SurvLIMEpy&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present SurvLIMEpy, an open-source Python package that implements the SurvLIME algorithm. This method allows to compute local feature importance for machine learning algorithms designed for modelling Survival Analysis data. Our implementation takes advantage of the parallelisation paradigm as all computations are performed in a matrix-wise fashion which speeds up execution time. Additionally, SurvLIMEpy assists the user with visualization tools to better understand the result of the algorithm. The package supports a wide variety of survival models, from the Cox Proportional Hazards Model to deep learning models such as DeepHit or DeepSurv. Two types of experiments are presented in this paper. First, by means of simulated data, we study the ability of the algorithm to capture the importance of the features. Second, we use three open source survival datasets together with a set of survival algorithms in order to demonstrate how SurvLIMEpy behaves when applied to differen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;RePrompt&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25991;&#26412;&#25552;&#31034;&#26469;&#31934;&#30830;&#34920;&#36798;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#24863;&#34920;&#29616;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.09466</link><description>&lt;p&gt;
RePrompt&#65306;&#33258;&#21160;&#25552;&#31034;&#32534;&#36753;&#20197;&#31934;&#32454;&#34920;&#36798;AI&#29983;&#25104;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions. (arXiv:2302.09466v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;RePrompt&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25991;&#26412;&#25552;&#31034;&#26469;&#31934;&#30830;&#34920;&#36798;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#24863;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#20135;&#29983;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#36825;&#21487;&#20197;&#20351;&#35270;&#35273;&#33402;&#26415;&#21019;&#20316;&#21644;&#33258;&#25105;&#34920;&#36798;&#26041;&#38754;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#29983;&#25104;&#30340;&#22270;&#20687;&#22914;&#20309;&#31934;&#30830;&#22320;&#34920;&#36798;&#36755;&#20837;&#25991;&#26412;&#30340;&#24773;&#22659;&#21644;&#24773;&#24863;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#24773;&#24863;&#34920;&#29616;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#26041;&#27861;RePrompt&#65292;&#20197;&#31934;&#30830;&#34920;&#36798;&#29983;&#25104;&#30340;&#22270;&#20687;&#20026;&#30446;&#26631;&#26469;&#23436;&#21892;&#25991;&#26412;&#25552;&#31034;&#12290;&#21463;&#21040;&#20247;&#21253;&#32534;&#36753;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#30452;&#35266;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#20363;&#22914;&#21517;&#35789;&#30340;&#25968;&#37327;&#21644;&#20855;&#20307;&#24615;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#26469;&#20998;&#26512;&#36825;&#20123;&#29305;&#24449;&#23545;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#20934;&#21017;&#26469;&#35843;&#25972;&#25991;&#26412;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#29983;&#25104;&#30340;&#22270;&#20687;&#20197;&#31934;&#30830;&#34920;&#36798;&#24773;&#24863;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;RePrompt&#26174;&#33879;&#25552;&#39640;&#20102;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#24773;&#24863;&#34920;&#29616;&#21147;&#65292;&#23588;&#20854;&#26159;&#36127;&#38754;&#24773;&#24863;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have shown impressive ability to produce images with text prompts, which could benefit creativity in visual art creation and self-expression. However, it is unclear how precisely the generated images express contexts and emotions from the input texts. We explored the emotional expressiveness of AI-generated images and developed RePrompt, an automatic method to refine text prompts toward precise expression of the generated images. Inspired by crowdsourced editing strategies, we curated intuitive text features, such as the number and concreteness of nouns, and trained a proxy model to analyze the feature effects on the AI-generated image. With model explanations of the proxy model, we curated a rubric to adjust text prompts to optimize image generation for precise emotion expression. We conducted simulation and user studies, which showed that RePrompt significantly improves the emotional expressiveness of AI-generated images, especially for negative emotions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;T2I&#27169;&#22411;&#38544;&#21547;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#26356;&#21152;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;T2I&#36866;&#37197;&#22120;&#26469;&#23545;&#40784;&#20869;&#37096;&#30693;&#35782;&#19982;&#22806;&#37096;&#25511;&#21046;&#20449;&#21495;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#32467;&#26524;&#30340;&#39068;&#33394;&#21644;&#32467;&#26500;&#26041;&#38754;&#20016;&#23500;&#30340;&#25511;&#21046;&#21644;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08453</link><description>&lt;p&gt;
T2I-Adapter&#65306;&#23398;&#20064;&#36866;&#37197;&#22120;&#20197;&#25366;&#25496;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26356;&#22810;&#21487;&#25511;&#21046;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. (arXiv:2302.08453v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;T2I&#27169;&#22411;&#38544;&#21547;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#26356;&#21152;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;T2I&#36866;&#37197;&#22120;&#26469;&#23545;&#40784;&#20869;&#37096;&#30693;&#35782;&#19982;&#22806;&#37096;&#25511;&#21046;&#20449;&#21495;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#32467;&#26524;&#30340;&#39068;&#33394;&#21644;&#32467;&#26500;&#26041;&#38754;&#20016;&#23500;&#30340;&#25511;&#21046;&#21644;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#24778;&#20154;&#29983;&#25104;&#33021;&#21147;&#24050;&#32463;&#23637;&#31034;&#20102;&#23398;&#20064;&#22797;&#26434;&#32467;&#26500;&#21644;&#26377;&#24847;&#20041;&#35821;&#20041;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20381;&#38752;&#25991;&#26412;&#25552;&#31034;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#27169;&#22411;&#25152;&#23398;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#28789;&#27963;&#31934;&#30830;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#39068;&#33394;&#21644;&#32467;&#26500;&#65289;&#26102;&#12290;&#26412;&#25991;&#26088;&#22312;&#8220;&#25366;&#25496;&#8221;T2I&#27169;&#22411;&#38544;&#21547;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#26126;&#30830;&#22320;&#20351;&#29992;&#23427;&#20204;&#26469;&#26356;&#21152;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;T2I&#36866;&#37197;&#22120;&#65292;&#23558;T2I&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#19982;&#22806;&#37096;&#25511;&#21046;&#20449;&#21495;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#22823;&#22411;T2I&#27169;&#22411;&#19981;&#21464;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#26465;&#20214;&#35757;&#32451;&#21508;&#31181;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#32467;&#26524;&#30340;&#39068;&#33394;&#21644;&#32467;&#26500;&#26041;&#38754;&#20016;&#23500;&#30340;&#25511;&#21046;&#21644;&#32534;&#36753;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;T2I&#36866;&#37197;&#22120;&#20855;&#26377;&#32452;&#21512;&#24615;&#21644;&#29983;&#25104;&#25928;&#29575;&#31561;&#23454;&#38469;&#20215;&#20540;&#30340;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987;&#30340;&#26032;&#27010;&#24565;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026; PC-Attack &#30340;&#23454;&#29992;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#21463;&#23475;&#32773;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#21644;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#30340;&#20449;&#24687;&#21363;&#21487;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#22312;&#20844;&#20849;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22240;&#27492;&#25915;&#20987;&#25104;&#21151;&#29575;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.07145</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#35775;&#38382;&#30340;&#23454;&#29992;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Practical Cross-System Shilling Attacks with Limited Access to Data. (arXiv:2302.07145v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987;&#30340;&#26032;&#27010;&#24565;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026; PC-Attack &#30340;&#23454;&#29992;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#21463;&#23475;&#32773;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#21644;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#30340;&#20449;&#24687;&#21363;&#21487;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#22312;&#20844;&#20849;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22240;&#27492;&#25915;&#20987;&#25104;&#21151;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#38144;&#25915;&#20987;&#20013;&#65292;&#25932;&#23545;&#26041;&#20250;&#21521;&#25512;&#33616;&#31995;&#32479;&#27880;&#20837;&#23569;&#37327;&#30340;&#34394;&#20551;&#29992;&#25143;&#36164;&#26009;&#65292;&#20197;&#20415;&#25512;&#24191;&#25110;&#38477;&#20302;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#12290;&#23613;&#31649;&#24050;&#32463;&#25226;&#24456;&#22810;&#30340;&#21162;&#21147;&#25918;&#22312;&#24320;&#21457;&#27169;&#25311;&#25915;&#20987;&#26041;&#27861;&#19978;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#36828;&#31163;&#23454;&#29992;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#23454;&#29992;&#25512;&#38144;&#25915;&#20987;&#26041;&#27861;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#31995;&#32479;&#25915;&#20987;&#30340;&#26032;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#36328;&#31995;&#32479;&#25915;&#20987;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987; (PC-Attack) &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#21644;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#30340;&#20449;&#24687;&#38656;&#27714;&#37327;&#24456;&#23567;&#12290;PC-Attack &#33258;&#25105;&#30417;&#30563;&#22320;&#20174;&#20844;&#20849;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#20013;&#33719;&#21462;&#20102;&#22270;&#24418;&#25299;&#25169;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#23427;&#36827;&#34892;&#24494;&#35843;&#65292;&#21033;&#29992;&#26131;&#20110;&#35775;&#38382;&#30340;&#37096;&#20998;&#30446;&#26631;&#25968;&#25454;&#26500;&#24314;&#34394;&#20551;&#36164;&#26009;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;PC-Attack &#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340; PC-Attack &#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
In shilling attacks, an adversarial party injects a few fake user profiles into a Recommender System (RS) so that the target item can be promoted or demoted. Although much effort has been devoted to developing shilling attack methods, we find that existing approaches are still far from practical. In this paper, we analyze the properties a practical shilling attack method should have and propose a new concept of Cross-system Attack. With the idea of Cross-system Attack, we design a Practical Cross-system Shilling Attack (PC-Attack) framework that requires little information about the victim RS model and the target RS data for conducting attacks. PC-Attack is trained to capture graph topology knowledge from public RS data in a self-supervised manner. Then, it is fine-tuned on a small portion of target data that is easy to access to construct fake profiles. Extensive experiments have demonstrated the superiority of PC-Attack over state-of-the-art baselines. Our implementation of PC-Attack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;UX&#26041;&#27861;&#31995;&#32479;&#65292;&#26088;&#22312;&#28385;&#36275;&#24773;&#25253;&#26102;&#20195;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.06681</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#35774;&#35745; (IX): &#24773;&#25253;&#26102;&#20195;&#19979;&#30340;&#8220;&#29992;&#25143;&#20307;&#39564;3.0&#8221;&#33539;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
User-Centered Design (IX): A "User Experience 3.0" Paradigm Framework in the Intelligence Era. (arXiv:2302.06681v5 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;UX&#26041;&#27861;&#31995;&#32479;&#65292;&#26088;&#22312;&#28385;&#36275;&#24773;&#25253;&#26102;&#20195;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#8220;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#8221;&#29702;&#24565;&#30340;&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#39046;&#22495;&#27491;&#22312;&#21521;&#24773;&#25253;&#26102;&#20195;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UX&#33539;&#24335;&#20027;&#35201;&#38024;&#23545;&#38750;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#32570;&#20047;&#38024;&#23545;&#26234;&#33021;&#31995;&#32479;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#12290;&#22312;UX&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;UX&#33539;&#24335;&#21576;&#29616;&#20986;&#36328;&#25216;&#26415;&#26102;&#20195;&#30340;&#28436;&#36827;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#24773;&#25253;&#26102;&#20195;&#23545;UX&#33539;&#24335;&#25552;&#20986;&#20102;&#26032;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;UX&#26041;&#27861;&#31995;&#32479;&#12290; &#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21253;&#25324;&#20116;&#31867;UX&#26041;&#27861;&#65306;&#29983;&#24577;&#20307;&#39564;&#65292;&#21019;&#26032;&#20307;&#39564;&#65292;AI&#20307;&#39564;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#20307;&#39564;&#65292;&#20197;&#21450;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#20307;&#39564;&#26041;&#27861;&#65292;&#27599;&#20010;&#26041;&#27861;&#37117;&#25552;&#20379;&#30456;&#24212;&#30340;&#22810;&#37325;UX&#33539;&#24335;&#21462;&#21521;&#12290;&#25552;&#20986;&#8220;UX 3.0&#8221;&#33539;&#24335;&#30340;&#24314;&#35758;&#26377;&#21161;&#20110;&#25913;&#36827;&#29616;&#26377;&#30340;UX&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of user experience (UX) based on the design philosophy of "user-centered design" is moving towards the intelligence era. Still, the existing UX paradigm mainly aims at non-intelligent systems and lacks a systematic approach to UX for intelligent systems. Throughout the development of UX, the UX paradigm shows the evolution characteristics of the cross-technology era. At present, the intelligence era has put forward new demands on the UX paradigm. For this reason, this paper proposes a "UX 3.0" paradigm framework and the corresponding UX methodology system in the intelligence era. The "UX 3.0" paradigm framework includes five categories of UX methods: ecological experience, innovation-enabled experience, AI-enabled experience, human-AI interaction-based experience, and human-AI collaboration-based experience methods, each providing corresponding multiple UX paradigmatic orientations. The proposal of the "UX 3.0" paradigm helps improve the existing UX methods and provides metho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#65292;&#25552;&#39640;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2302.06337</link><description>&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#23398;&#20064;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#65292;&#25552;&#39640;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#31526;&#21495;&#36923;&#36753;&#30693;&#35782;&#38598;&#25104;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;Logic-LNCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31867;&#20284;EM&#30340;&#36845;&#20195;&#36923;&#36753;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#24863;&#20852;&#36259;&#30340;&#36923;&#36753;&#35268;&#21017;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#30340;EM&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19968;&#31181;&#8220;&#20266;E&#27493;&#39588;&#8221;&#65292;&#20174;&#36923;&#36753;&#35268;&#21017;&#20013;&#33976;&#39311;&#20986;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#28982;&#21518;&#22312;&#8220;&#20266;M&#27493;&#39588;&#8221;&#20013;&#20351;&#29992;&#35813;&#30446;&#26631;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;&#24182;&#20026;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of symbolic logic knowledge into deep neural networks for learning from noisy crowd labels. We introduce Logic-guided Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic knowledge distillation framework that learns from both noisy labeled data and logic rules of interest. Unlike traditional EM methods, our framework contains a ``pseudo-E-step'' that distills from the logic rules a new type of learning target, which is then used in the ``pseudo-M-step'' for training the classifier. Extensive evaluations on two real-world datasets for text sentiment classification and named entity recognition demonstrate that the proposed framework improves the state-of-the-art and provides a new solution to learning from noisy crowd labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#26469;&#20943;&#23569;&#24178;&#25200;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02078</link><description>&lt;p&gt;
&#22522;&#20110;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#20851;&#31995;&#25277;&#21462;FGSI&#65306;&#19968;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FGSI: Distant Supervision for Relation Extraction method based on Fine-Grained Semantic Information. (arXiv:2302.02078v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#26469;&#20943;&#23569;&#24178;&#25200;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25552;&#21462;&#21477;&#23376;&#20013;&#26631;&#35760;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23427;&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#22312;&#21477;&#23376;&#20869;&#37096;&#30340;&#20851;&#38190;&#35821;&#20041;&#20449;&#24687;&#23545;&#23454;&#20307;&#20851;&#31995;&#30340;&#25552;&#21462;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25353;&#29031;&#23454;&#20307;&#22312;&#21477;&#23376;&#20869;&#37096;&#30340;&#20301;&#32622;&#23558;&#21477;&#23376;&#20998;&#20026;&#19977;&#27573;&#65292;&#24182;&#36890;&#36807;&#21477;&#20869;&#27880;&#24847;&#26426;&#21046;&#25214;&#21040;&#21477;&#23376;&#20869;&#37096;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#26080;&#20851;&#22122;&#22768;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;&#25152;&#25552;&#20986;&#30340;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#30340;&#27491;&#38754;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main purpose of relation extraction is to extract the semantic relationships between tagged pairs of entities in a sentence, which plays an important role in the semantic understanding of sentences and the construction of knowledge graphs. In this paper, we propose that the key semantic information within a sentence plays a key role in the relationship extraction of entities. We propose the hypothesis that the key semantic information inside the sentence plays a key role in entity relationship extraction. And based on this hypothesis, we split the sentence into three segments according to the location of the entity from the inside of the sentence, and find the fine-grained semantic features inside the sentence through the intra-sentence attention mechanism to reduce the interference of irrelevant noise information. The proposed relational extraction model can make full use of the available positive semantic information. The experimental results show that the proposed relation extra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11270</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#25104;&#23545;&#25110;$K$&#20803;&#27604;&#36739;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#21644;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#19979;&#22343;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22522;&#20110;&#23398;&#24471;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#26102;&#65292;MLE&#20250;&#22833;&#36133;&#65292;&#32780;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#25552;&#20379;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;PL&#27169;&#22411;&#19979;&#65292;&#30495;&#23454;MLE&#21644;&#23558;$k$&#20803;&#27604;&#36739;&#25286;&#20998;&#20026;&#25104;&#23545;&#27604;&#36739;&#30340;&#22791;&#36873;MLE&#37117;&#25910;&#25947;&#12290;&#32780;&#30495;&#23454;MLE&#26159;&#28176;&#36817;&#26356;&#20026;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#29616;&#26377;RLHF&#31639;&#27861;&#65288;&#22914;InstructGPT&#65289;&#30340;&#23454;&#39564;&#25104;&#21151;&#65292;&#24182;&#20026;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;&#26368;&#22823;&#29109;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35823;&#24046;&#24341;&#23548;&#30340;&#27169;&#22411;&#29992;&#20110;&#25913;&#36827;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#65292;&#23427;&#37319;&#29992;&#20102;&#38646;-shot&#35823;&#24046;&#26816;&#27979;&#12289;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#39640;&#24230;&#24182;&#34892;&#30340;&#35299;&#30721;&#31561;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.06323</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#35823;&#24046;&#24341;&#23548;&#30340;&#27169;&#22411;&#29992;&#20110;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
An Error-Guided Correction Model for Chinese Spelling Error Correction. (arXiv:2301.06323v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35823;&#24046;&#24341;&#23548;&#30340;&#27169;&#22411;&#29992;&#20110;&#25913;&#36827;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#65292;&#23427;&#37319;&#29992;&#20102;&#38646;-shot&#35823;&#24046;&#26816;&#27979;&#12289;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#39640;&#24230;&#24182;&#34892;&#30340;&#35299;&#30721;&#31561;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20173;&#26377;&#36827;&#19968;&#27493;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35823;&#24046;&#24341;&#23548;&#30340;&#32416;&#38169;&#27169;&#22411;&#65288;EGCM&#65289;&#26469;&#25913;&#36827;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#12290;&#36890;&#36807;&#20511;&#37492;BERT&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;-shot&#35823;&#24046;&#26816;&#27979;&#26041;&#27861;&#26469;&#36827;&#34892;&#21021;&#27493;&#26816;&#27979;&#65292;&#36825;&#26679;&#21487;&#20197;&#25351;&#23548;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32534;&#30721;&#26102;&#26356;&#22810;&#22320;&#20851;&#27880;&#21487;&#33021;&#38169;&#35823;&#30340;&#26631;&#35760;&#65292;&#24182;&#22312;&#29983;&#25104;&#26102;&#36991;&#20813;&#20462;&#25913;&#27491;&#30830;&#30340;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#38598;&#25104;&#35823;&#24046;&#28151;&#28102;&#38598;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#21306;&#20998;&#26131;&#28151;&#28102;&#30340;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#39640;&#24230;&#24182;&#34892;&#30340;&#35299;&#30721;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although existing neural network approaches have achieved great success on Chinese spelling correction, there is still room to improve. The model is required to avoid over-correction and to distinguish a correct token from its phonological and visually similar ones. In this paper, we propose an error-guided correction model (EGCM) to improve Chinese spelling correction. By borrowing the powerful ability of BERT, we propose a novel zero-shot error detection method to do a preliminary detection, which guides our model to attend more on the probably wrong tokens in encoding and to avoid modifying the correct tokens in generating. Furthermore, we introduce a new loss function to integrate the error confusion set, which enables our model to distinguish easily misused tokens. Moreover, our model supports highly parallel decoding to meet real application requirements. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.03364</link><description>&lt;p&gt;
&#36208;&#21521;AI-enabled&#36830;&#25509;&#20135;&#19994;: AGV&#36890;&#20449;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24037;&#19994;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;: &#24037;&#19994;&#36710;&#36742;&#38388;&#36890;&#20449;(iV2V)&#21644;&#24037;&#19994;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#21152;&#20256;&#24863;&#22120;(iV2I+)&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20004;&#20010;&#25429;&#33719;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;iV2V&#28085;&#30422;&#20102;&#33258;&#21160;&#24341;&#23548;&#36710;(AGVs)&#20043;&#38388;&#30340;&#20391;&#21521;&#38142;&#36335;&#36890;&#20449;&#22330;&#26223;&#65292;&#32780;iV2I+&#21017;&#26159;&#22312;&#24037;&#19994;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#33258;&#20027;&#28165;&#27905;&#26426;&#22120;&#20154;&#36830;&#25509;&#21040;&#31169;&#26377;&#34562;&#31389;&#32593;&#32476;&#12290;&#19981;&#21516;&#36890;&#20449;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#36830;&#21516;&#20849;&#21516;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#21033;&#29992;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#24050;&#26631;&#35760;&#21644;&#39044;&#36807;&#28388;&#65292;&#20197;&#20415;&#24555;&#36895;&#21551;&#21160;&#21644;&#24212;&#29992;&#12290;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#27979;&#35797;&#24179;&#21488;&#21644;&#27979;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;PGT&#29702;&#35770;&#27010;&#24565;&#21644;&#36817;&#20284;PGT&#30340;&#23450;&#37327;&#25216;&#26415;&#65292;&#20026;&#35780;&#20272;&#21644;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.00243</link><description>&lt;p&gt;
&#25509;&#36817;&#23792;&#20540;&#22320;&#38754;&#30495;&#30456;
&lt;/p&gt;
&lt;p&gt;
Approaching Peak Ground Truth. (arXiv:2301.00243v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00243
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;PGT&#29702;&#35770;&#27010;&#24565;&#21644;&#36817;&#20284;PGT&#30340;&#23450;&#37327;&#25216;&#26415;&#65292;&#20026;&#35780;&#20272;&#21644;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#36890;&#36807;&#35745;&#31639;&#19982;&#21442;&#32771;&#26631;&#27880;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;&#23427;&#20204;&#30340;&#30456;&#20284;&#24230;&#36827;&#34892;&#35757;&#32451;&#12290;&#23588;&#20854;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#26631;&#27880;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#20302;&#30340;&#35780;&#26631;&#32773;&#38388;&#21644;&#35780;&#26631;&#32773;&#20869;&#21487;&#38752;&#24615;&#12290;&#30001;&#20110;&#26631;&#27880;&#20165;&#21453;&#26144;&#19990;&#30028;&#30340;&#19968;&#31181;&#35299;&#37322;&#65292;&#21363;&#20351;&#27169;&#22411;&#36798;&#21040;&#20102;&#39640;&#30456;&#20284;&#24230;&#20998;&#25968;&#65292;&#36825;&#20063;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#39044;&#27979;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#29702;&#35770;&#19978;&#30340;PGT&#27010;&#24565;&#12290;PGT&#26631;&#35760;&#20102;&#19982;&#21442;&#32771;&#27880;&#37322;&#30340;&#30456;&#20284;&#24230;&#22686;&#21152;&#19981;&#20877;&#36716;&#21270;&#20026;&#26356;&#22909;&#30340;RWMP&#30340;&#28857;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35745;&#31639;&#35780;&#26631;&#32773;&#38388;&#21644;&#35780;&#26631;&#32773;&#20869;&#21487;&#38752;&#24615;&#26469;&#36817;&#20284;PGT&#30340;&#23450;&#37327;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#22238;&#39038;&#20102;&#22235;&#31867;PGT&#24863;&#30693;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are typically evaluated by computing similarity with reference annotations and trained by maximizing similarity with such. Especially in the biomedical domain, annotations are subjective and suffer from low inter- and intra-rater reliability. Since annotations only reflect one interpretation of the real world, this can lead to sub-optimal predictions even though the model achieves high similarity scores. Here, the theoretical concept of PGT is introduced. PGT marks the point beyond which an increase in similarity with the \emph{reference annotation} stops translating to better RWMP. Additionally, a quantitative technique to approximate PGT by computing inter- and intra-rater reliability is proposed. Finally, four categories of PGT-aware strategies to evaluate and improve model performance are reviewed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2212.12061</link><description>&lt;p&gt;
MN-DS&#65306;&#26032;&#38395;&#25991;&#31456;&#23618;&#27425;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#20174;2019&#24180;1&#26376;1&#26085;&#21040;2019&#24180;12&#26376;31&#26085;&#30340;&#23618;&#27425;&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#26681;&#25454;17&#20010;&#19968;&#32423;&#31867;&#21035;&#21644;109&#20010;&#20108;&#32423;&#31867;&#21035;&#30340;&#23618;&#27425;&#20998;&#31867;&#25163;&#21160;&#26631;&#35760;&#20102;&#36825;&#20123;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#25353;&#20027;&#39064;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20174;&#20107;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#26681;&#25454;&#21457;&#24067;&#30340;&#26032;&#38395;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12053</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#20998;&#26512;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Calibrating Semantic Segmentation Models: Analyses and An Algorithm. (arXiv:2212.12053v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#22270;&#20687;&#20998;&#31867;&#32622;&#20449;&#24230;&#30340;&#27169;&#22411;&#35823;&#26657;&#20934;&#65292;&#20294;&#33267;&#20170;&#20026;&#27490;&#65292;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23481;&#37327;&#12289;&#35009;&#21098;&#22823;&#23567;&#12289;&#22810;&#23610;&#24230;&#27979;&#35797;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#23545;&#26657;&#20934;&#26377;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;&#39044;&#27979;&#27491;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#38169;&#35823;&#39044;&#27979;&#65292;&#23545;&#30001;&#20110;&#36807;&#24230;&#32622;&#20449;&#32780;&#23548;&#33268;&#30340;&#35823;&#26657;&#20934;&#26356;&#20026;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#32479;&#19968;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#29616;&#26377;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36873;&#25321;&#24615;&#32553;&#25918;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a vari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.09598</link><description>&lt;p&gt;
&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#29992;&#20110;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query-as-context Pre-training for Dense Passage Retrieval. (arXiv:2212.09598v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20986;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#25552;&#39640;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#31616;&#21333;&#22320;&#35748;&#20026;&#26469;&#33258;&#21516;&#19968;&#25991;&#26723;&#30340;&#20004;&#20010;&#36890;&#36947;&#26159;&#30456;&#20851;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#29992;&#20110;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#20551;&#23450;&#20174;&#36890;&#36947;&#20013;&#25552;&#21462;&#30340;&#26597;&#35810;&#26356;&#21487;&#33021;&#19982;&#35813;&#36890;&#36947;&#30456;&#20851;&#65292;&#24182;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#12290;&#36825;&#20123;&#36890;&#36947;-&#26597;&#35810;&#23545;&#28982;&#21518;&#29992;&#20110;&#23545;&#27604;&#24615;&#25110;&#29983;&#25104;&#24615;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#36890;&#36947;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#21644;&#36328;&#39046;&#22495;&#38646;-shot&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#22686;&#30410;&#65292;&#21516;&#26102;&#21152;&#36895;&#20102;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20250;&#22312;https://github.com/deepset-ai/haystack&#19978;&#25552;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the possibility of weakly correlated pairs. Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue. Query-as-context pre-training assumes that the query derived from a passage is more likely to be relevant to that passage and forms a passage-query pair. These passage-query pairs are then used in contrastive or generative context-supervised pre-training. The pre-trained models are evaluated on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks. Experimental results show that query-as-context pre-training brings considerable gains and meanwhile speeds up training, demonstrating its effectiveness and efficiency. Our code will be available at https://g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25439;&#22833;&#20849;&#20139;&#29305;&#24449;&#30340;&#24773;&#20917;&#23545;&#36710;&#36742;&#21327;&#21516;&#24863;&#30693;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#20462;&#22797;&#32593;&#32476;&#32531;&#35299;&#20102;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#24182;&#22686;&#24378;&#20102;&#24863;&#30693;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08273</link><description>&lt;p&gt;
&#25439;&#22833;&#36890;&#20449;&#19979;&#36710;&#36742;&#20043;&#38388;&#21327;&#21516;&#24863;&#30693;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication. (arXiv:2212.08273v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25439;&#22833;&#20849;&#20139;&#29305;&#24449;&#30340;&#24773;&#20917;&#23545;&#36710;&#36742;&#21327;&#21516;&#24863;&#30693;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#20462;&#22797;&#32593;&#32476;&#32531;&#35299;&#20102;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#24182;&#22686;&#24378;&#20102;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#36710;&#36742;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#65288;&#20363;&#22914;3D&#29289;&#20307;&#26816;&#27979;&#65289;&#20013;&#12290;&#30001;&#20110;&#26377;&#30410;&#30340;&#36710;&#36742;&#38388;&#36890;&#20449;&#65288;V2V&#65289;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#29305;&#24449;&#21487;&#20197;&#20849;&#20139;&#32473;&#26412;&#36710;&#65292;&#20197;&#25552;&#39640;&#26412;&#36710;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#36825;&#34987;&#31216;&#20026;V2V&#30740;&#31350;&#20013;&#30340;&#21327;&#21516;&#24863;&#30693;&#65292;&#20854;&#31639;&#27861;&#26368;&#36817;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#21327;&#21516;&#24863;&#30693;&#31639;&#27861;&#37117;&#20551;&#23450;&#29702;&#24819;&#30340;V2V&#36890;&#20449;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22797;&#26434;&#30340;&#29616;&#23454;&#39550;&#39542;&#22330;&#26223;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25439;&#22833;&#20849;&#20139;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;V2V&#21327;&#21516;&#24863;&#30693;&#20013;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#65288;&#20363;&#22914;&#26816;&#27979;&#24615;&#33021;&#19979;&#38477;&#65289;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#20462;&#22797;&#32593;&#32476;&#65288;LCRN&#65289;&#32531;&#35299;&#20102;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#24182;&#22686;&#24378;&#20102;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely used in the perception (e.g., 3D object detection) of intelligent vehicle driving. Due to the beneficial Vehicle-to-Vehicle (V2V) communication, the deep learning based features from other agents can be shared to the ego vehicle so as to improve the perception of the ego vehicle. It is named as Cooperative Perception in the V2V research, whose algorithms have been dramatically advanced recently. However, all the existing cooperative perception algorithms assume the ideal V2V communication without considering the possible lossy shared features because of the Lossy Communication (LC) which is common in the complex real-world driving scenarios. In this paper, we first study the side effect (e.g., detection performance drop) by the lossy communication in the V2V Cooperative Perception, and then we propose a novel intermediate LC-aware feature fusion method to relieve the side effect of lossy communication by a LC-aware Repair Network (LCRN) and enhance the int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#23454;&#39564;&#26694;&#26550;&#65292;PSI-AVA&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;TAPIR&#30340;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04582</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#25163;&#26415;&#22330;&#26223;&#30340;&#25972;&#20307;&#29702;&#35299;&#65306;&#38024;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Towards Holistic Surgical Scene Understanding. (arXiv:2212.04582v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#23454;&#39564;&#26694;&#26550;&#65292;PSI-AVA&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;TAPIR&#30340;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29992;&#20110;&#30740;&#31350;&#25163;&#26415;&#24178;&#39044;&#30340;&#22522;&#20934;&#27979;&#35797;&#37117;&#38598;&#20013;&#22312;&#29305;&#23450;&#25361;&#25112;&#19978;&#65292;&#32780;&#24573;&#30053;&#20102;&#19981;&#21516;&#20219;&#21153;&#38388;&#20869;&#22312;&#30340;&#20114;&#34917;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#25163;&#26415;&#22330;&#26223;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Phase&#12289;Step&#12289;Instrument&#21644;Atomic Visual Action&#65288;PSI-AVA&#65289;&#25968;&#25454;&#38598;&#12290;PSI-AVA&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;&#20013;&#65292;&#23545;&#38271;&#26399;&#65288;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#65289;&#21644;&#30701;&#26399;&#25512;&#29702;&#65288;&#22120;&#26800;&#26816;&#27979;&#21644;&#26032;&#22411;&#21407;&#23376;&#21160;&#20316;&#35782;&#21035;&#65289;&#36827;&#34892;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAPIR&#65292;&#21363;Transformers for Action&#65292;Phase&#65292;Instrument&#21644;Steps Recognition&#65292;&#20316;&#20026;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#30340;&#24378;&#22522;&#20934;&#12290;TAPIR&#21033;&#29992;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#32423;&#27880;&#37322;&#65292;&#36890;&#36807;&#22120;&#26800;&#26816;&#27979;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20854;&#20998;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;PSI-AVA&#21644;&#20854;&#20182;&#20844;&#24320;&#25968;&#25454;&#24211;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25972;&#20307;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#26694;&#26550;&#26159;&#20805;&#20998;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset's multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04088</link><description>&lt;p&gt;
LLM-Planner: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#20307;&#20195;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35268;&#21010;&#22120;&#65292;&#35753;&#23454;&#20307;&#20195;&#29702;&#21487;&#20197;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#22312;&#35270;&#35273;&#24863;&#30693;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#25968;&#25454;&#25104;&#26412;&#21644;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#12290;&#22312;ALFRED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65306;&#23613;&#31649;&#20351;&#29992;&#30340;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#21040;0.5&#65285;&#65292;LLM-Planner&#30340;&#34920;&#29616;&#19982;&#20351;&#29992;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#30340;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#12290;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#20219;&#20309;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15046</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#20010;&#19990;&#32426;&#37324;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#38632;&#27700;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#23450;&#37327;&#38477;&#27700;&#39044;&#27979; (QPF) &#27169;&#22411;&#12289;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518; (ConvLSTM) &#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340; MetNet-2 &#31561;&#22810;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476; (PCT-CycleGAN) &#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476; (CycleGAN) &#24378;&#22823;&#30340;&#22270;&#20687;&#36716;&#25442;&#24615;&#33021;&#21551;&#21457;&#12290;PCT-CycleGAN &#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#21521;&#21069;&#21644;&#21521;&#21518;&#26102;&#38388;&#21160;&#24577;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26102;&#24207;&#24615;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#24222;&#22823;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20197;&#36924;&#36817;&#34920;&#31034;&#27599;&#20010;&#26041;&#21521;&#19978;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#21019;&#24314;&#37197;&#23545;&#20114;&#34917;&#24490;&#29615;&#20043;&#38388;&#30340;&#24378;&#20581;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PCT-CycleGAN &#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#20809;&#22330;&#21453;&#28436;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#24674;&#22797; SDF &#21442;&#25968;&#21270;&#30340; 3D &#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;&#65292;&#32780;&#26080;&#38656;&#20934;&#30830;&#30340;&#30495;&#23454;&#23039;&#21183;&#65292;&#19988;&#36895;&#24230;&#24555;&#12289;&#35745;&#31639;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.11674</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#22238;&#24402;&#20809;&#22330;&#21453;&#28436;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#37325;&#26500;&#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;
&lt;/p&gt;
&lt;p&gt;
Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion. (arXiv:2211.11674v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#20809;&#22330;&#21453;&#28436;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#24674;&#22797; SDF &#21442;&#25968;&#21270;&#30340; 3D &#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;&#65292;&#32780;&#26080;&#38656;&#20934;&#30830;&#30340;&#30495;&#23454;&#23039;&#21183;&#65292;&#19988;&#36895;&#24230;&#24555;&#12289;&#35745;&#31639;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#22312;&#20174;&#21333;&#35270;&#35282;&#36827;&#34892;&#19977;&#32500;&#37325;&#24314;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#20219;&#24847;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#20013;&#30830;&#20999;&#30340;&#22320;&#38754;&#30495;&#23454;&#23039;&#21183;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#24573;&#30053;&#20102;&#23039;&#21183;&#20272;&#35745;&#65292;&#32780;&#23039;&#21183;&#20272;&#35745;&#26159;&#26576;&#20123;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#22686;&#24378;&#29616;&#23454; (AR) &#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#26377;&#21407;&#21017;&#30340;&#31471;&#21040;&#31471;&#37325;&#24314;&#26694;&#26550;&#65292;&#20854;&#20013;&#20934;&#30830;&#30340;&#30495;&#23454;&#23039;&#21183;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#20010;&#29289;&#20307;&#30340;&#21333;&#20010;&#22270;&#20687;&#20013;&#24674;&#22797;&#20102; SDF &#21442;&#25968;&#21270;&#30340; 3D &#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;&#65292;&#32780;&#27809;&#26377;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#20010;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#26080;&#26465;&#20214;&#30340; 3D-aware &#29983;&#25104;&#22120;&#65292;&#23545;&#20854;&#24212;&#29992;&#20102;&#28151;&#21512;&#21453;&#28436;&#26041;&#26696;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#20135;&#29983;&#20102;&#35299;&#30340;&#31532;&#19968;&#20010;&#29468;&#27979;&#65292;&#28982;&#21518;&#36890;&#36807;&#20248;&#21270;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#22270;&#20687;&#36827;&#34892;&#21435;&#28210;&#26579;&#65292;&#22240;&#27492;&#36895;&#24230;&#24555;&#19988;&#35745;&#31639;&#26377;&#25928;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; DTU &#21644; Tanks &amp; Temples &#31561;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) coupled with GANs represent a promising direction in the area of 3D reconstruction from a single view, owing to their ability to efficiently model arbitrary topologies. Recent work in this area, however, has mostly focused on synthetic datasets where exact ground-truth poses are known, and has overlooked pose estimation, which is important for certain downstream applications such as augmented reality (AR) and robotics. We introduce a principled end-to-end reconstruction framework for natural images, where accurate ground-truth poses are not available. Our approach recovers an SDF-parameterized 3D shape, pose, and appearance from a single image of an object, without exploiting multiple views during training. More specifically, we leverage an unconditional 3D-aware generator, to which we apply a hybrid inversion scheme where a model produces a first guess of the solution which is then refined via optimization. Our framework can de-render an image in as few a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2211.06841</link><description>&lt;p&gt;
Point-MA2E:&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning. (arXiv:2211.06841v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#20013;&#65292;&#25513;&#33180;&#24314;&#27169;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20174;&#20854;&#25513;&#33180;&#23545;&#24212;&#37096;&#20998;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#12290;&#32771;&#34385;&#21040;&#25513;&#33180;&#21482;&#20250;&#25439;&#22351;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#28857;&#65292;&#26412;&#25991;&#25512;&#24191;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#35268;&#21017;&#30772;&#22351;&#25152;&#26377;&#36755;&#20837;&#28857;&#65292;&#20197;&#34917;&#20805;&#27969;&#34892;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;Point-MA2E&#65289;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#28857;&#20113;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#21644;&#25513;&#33180;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20174;&#20854;&#25439;&#22351;&#29256;&#26412;&#20013;&#37325;&#24314;&#21407;&#22987;&#28857;&#20113;&#12290;&#25506;&#32034;&#20102;&#21508;&#31181;&#28857;&#20113;&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#38750;Transformer&#32534;&#30721;&#22120;&#65292;&#25353;&#29031;&#24120;&#35265;&#20570;&#27861;&#30452;&#25509;&#37325;&#24314;&#26410;&#25439;&#22351;&#30340;&#28857;&#20113;&#12290;&#23545;&#20110;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23616;&#37096;&#34917;&#19969;&#21644;&#31895;&#30053;&#30340;&#20840;&#23616;&#24418;&#29366;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked modeling has demonstrated its effectiveness in self-supervised point cloud learning by reconstructing the complete point cloud from its masked counterpart. Considering that masking only corrupts partial points of the input, in this paper, we promote the affine transformation, which corrupts all input points with certain rules, to complement the popular masking strategy, leading to the Masked and Affine transformed AutoEncoder for point cloud learning (Point-MA2E). Generally, we corrupt the point cloud with affine transformation and masking as input and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Various point cloud encoders are explored in this study. For non-Transformer encoders, we follow the common practice to reconstruct the uncorrupted point cloud directly. For Transformer-based encoders, we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;</title><link>http://arxiv.org/abs/2211.05732</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Online Contract Design. (arXiv:2211.05732v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#38544;&#34255;-&#34892;&#21160;&#22996;&#25176;&#38382;&#39064;&#12290;&#22312;&#27599;&#36718;&#20013;&#65292;&#22996;&#25176;&#20154;&#21457;&#24067;&#19968;&#20221;&#21512;&#21516;&#65292;&#26681;&#25454;&#27599;&#20010;&#32467;&#26524;&#35268;&#23450;&#20195;&#29702;&#20154;&#30340;&#25903;&#20184;&#12290;&#20195;&#29702;&#20154;&#28982;&#21518;&#20570;&#20986;&#19968;&#20010;&#26368;&#22823;&#21270;&#22905;&#33258;&#24049;&#25928;&#29992;&#30340;&#25112;&#30053;&#34892;&#21160;&#36873;&#25321;&#65292;&#20294;&#30452;&#25509;&#35266;&#23519;&#19981;&#21040;&#34892;&#21160;&#12290;&#22996;&#25176;&#20154;&#35266;&#23519;&#32467;&#26524;&#24182;&#20174;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#36873;&#25321;&#20013;&#33719;&#24471;&#25928;&#29992;&#12290;&#26681;&#25454;&#36807;&#21435;&#30340;&#35266;&#23519;&#65292;&#22996;&#25176;&#20154;&#21160;&#24577;&#22320;&#35843;&#25972;&#21512;&#21516;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21512;&#21516;&#31354;&#38388;&#20026;$[0,1]^m$&#26102;&#65292;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#20026;$\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$&#65292;&#19979;&#30028;&#20026;$\Omega(T^{1-1/(m+2)})$&#65292;&#20854;&#20013;$\widetilde O$&#25490;&#38500;&#23545;&#25968;&#22240;&#23376;&#12290; &#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is $[0,1]^m$, the Stackelberg regret is upper bounded by $\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$, and lower bounded by $\Omega(T^{1-1/(m+2)})$, where $\widetilde O$ omits logarithmic factors. This result shows that exponential-in-$m$ samples are sufficient and necessary to learn a near-optimal contract, resolving an open probl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;GAN&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#22768;&#23398;&#29615;&#22659;&#20013;&#20272;&#35745;&#23460;&#20869;&#22768;&#36947;&#20914;&#28608;&#21709;&#24212;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04473</link><description>&lt;p&gt;
&#38754;&#21521;&#35821;&#38899;&#35782;&#21035;&#30340;&#25913;&#36827;&#23460;&#20869;&#22768;&#23398;&#29615;&#22659;&#19979;&#22768;&#36947;&#20914;&#28608;&#21709;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Room Impulse Response Estimation for Speech Recognition. (arXiv:2211.04473v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;GAN&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#22768;&#23398;&#29615;&#22659;&#20013;&#20272;&#35745;&#23460;&#20869;&#22768;&#36947;&#20914;&#28608;&#21709;&#24212;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19979;&#28216;&#24212;&#29992;&#24773;&#22659;&#65288;&#36828;&#22330;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65289;&#20013;&#30450;&#30446;&#20272;&#35745;&#23460;&#20869;&#22768;&#23398;&#29615;&#22659;&#19979;&#22768;&#36947;&#20914;&#28608;&#21709;&#24212;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25913;&#21892;&#22768;&#36947;&#20914;&#28608;&#21709;&#24212;&#20272;&#35745;&#21644;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20197;&#35780;&#20272;&#31070;&#32463;&#22768;&#36947;&#20914;&#28608;&#21709;&#24212;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;&#27531;&#21709;&#35821;&#38899;&#20013;&#32534;&#30721;RIR&#29305;&#24449;&#24182;&#26500;&#24314;RIR&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#33021;&#37327;&#34928;&#20943;&#32531;&#35299;&#25439;&#22833;&#26469;&#20248;&#21270;&#25429;&#33719;&#36755;&#20837;&#27531;&#21709;&#35821;&#38899;&#30340;&#33021;&#37327;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22768;&#23398;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#65288;&#33021;&#37327;&#34928;&#20943;&#32531;&#35299;&#25351;&#26631;&#19978;&#25552;&#39640;&#20102;17&#65285;&#65292;&#26089;&#26399;&#21453;&#23556;&#33021;&#37327;&#24615;&#33021;&#25351;&#26631;&#19978;&#25552;&#39640;&#20102;22&#65285;&#65289;&#65292;&#24182;&#19988;&#22312;&#35821;&#38899;&#35782;&#21035;&#35780;&#20272;&#20219;&#21153;&#20013;&#26377;6.9&#65285;&#30340;&#35789;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for blind room impulse response (RIR) estimation systems in the context of a downstream application scenario, far-field automatic speech recognition (ASR). We first draw the connection between improved RIR estimation and improved ASR performance, as a means of evaluating neural RIR estimators. We then propose a generative adversarial network (GAN) based architecture that encodes RIR features from reverberant speech and constructs an RIR from the encoded features, and uses a novel energy decay relief loss to optimize for capturing energy-based properties of the input reverberant speech. We show that our model outperforms the state-of-the-art baselines on acoustic benchmarks (by 17\% on the energy decay relief and 22\% on an early-reflection energy metric), as well as in an ASR evaluation task (by 6.9\% in word error rate).
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03295</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#38454;&#38376;&#25511;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-order Gated Aggregation Network. (arXiv:2211.03295v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#21462;&#24471;&#26368;&#36817;&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;ViT&#39118;&#26684;&#26550;&#26500;&#30340;&#25506;&#32034;&#24341;&#21457;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#20852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#36825;&#31181;&#20132;&#20114;&#21453;&#26144;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#19981;&#21516;&#23610;&#24230;&#19978;&#19979;&#25991;&#30340;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#22312;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26469;&#23450;&#21046;&#20004;&#20010;&#29305;&#24449;&#28151;&#21512;&#22120;&#65292;&#20197;&#20419;&#36827;&#36328;&#31354;&#38388;&#21644;&#36890;&#36947;&#31354;&#38388;&#30340;&#20013;&#38454;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;ImageNet&#21644;&#21253;&#25324;COCO&#30446;&#26631;&#26816;&#27979;&#12289;ADE20K&#35821;&#20041;&#20998;&#21106;&#12289;2D&amp;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20197;&#21450;&#35270;&#39057;&#39044;&#27979;&#31561;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the recent success of Vision Transformers (ViTs), explorations toward ViT-style architectures have triggered the resurgence of ConvNets. In this work, we explore the representation ability of modern ConvNets from a novel view of multi-order game-theoretic interaction, which reflects inter-variable interaction effects w.r.t.~contexts of different scales based on game theory. Within the modern ConvNet framework, we tailor the two feature mixers with conceptually simple yet effective depthwise convolutions to facilitate middle-order information across spatial and channel spaces respectively. In this light, a new family of pure ConvNet architecture, dubbed MogaNet, is proposed, which shows excellent scalability and attains competitive results among state-of-the-art models with more efficient use of parameters on ImageNet and multifarious typical vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&amp;3D human pose estimation, and video prediction. Typica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00173</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#31435;&#26377;&#25928;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#20154;&#20204;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#31526;&#21512;&#39044;&#27979;&#65292;&#20294;&#36825;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#23545;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#30340;&#33539;&#22260;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#21487;&#20197;&#35777;&#26126;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#19982;&#26222;&#36890;&#31526;&#21512;&#39044;&#27979;&#32467;&#21512;&#20351;&#29992;&#65292;&#32780;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#33258;&#36866;&#24212;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#38500;&#20102;&#29616;&#26377;&#39044;&#27979;&#25512;&#26029;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#65288;&#22914;ImageNet&#20998;&#31867;&#21644;Cityscapes&#22270;&#20687;&#20998;&#21106;&#65289;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on large-scale tasks such as ImageNet classification and Cityscapes image segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#23384;&#22312;&#25200;&#21160;&#24773;&#20917;&#19979;&#23433;&#20840;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25152;&#25511;&#21046;&#23545;&#35937;&#21644;&#25200;&#21160;&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#29575;&#28385;&#36275;&#26174;&#24335;&#29366;&#24577;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2209.15452</link><description>&lt;p&gt;
&#23384;&#22312;&#25200;&#21160;&#24773;&#20917;&#19979;&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#23384;&#22312;&#25200;&#21160;&#24773;&#20917;&#19979;&#23433;&#20840;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25152;&#25511;&#21046;&#23545;&#35937;&#21644;&#25200;&#21160;&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#29575;&#28385;&#36275;&#26174;&#24335;&#29366;&#24577;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#35768;&#22810;&#39046;&#22495;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25506;&#32034;&#29305;&#24615;&#65292;&#24403;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26102;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28041;&#21450;&#23384;&#22312;&#25200;&#21160;&#26102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#23450;&#20041;&#20026;&#20197;&#29366;&#24577;&#26126;&#30830;&#23450;&#20041;&#30340;&#38480;&#21046;&#26465;&#20214;&#30340;&#28385;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25152;&#25511;&#21046;&#23545;&#35937;&#21644;&#25200;&#21160;&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#12290;&#21363;&#20351;&#25152;&#25511;&#21046;&#23545;&#35937;&#26292;&#38706;&#20110;&#36981;&#24490;&#27491;&#24577;&#20998;&#24067;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#20445;&#35777;&#20197;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#29575;&#28385;&#36275;&#26174;&#24335;&#29366;&#24577;&#32422;&#26463;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#26469;&#26500;&#24314;&#19981;&#21253;&#21547;&#20256;&#32479;&#25506;&#32034;&#26041;&#27861;&#20013;&#30340;&#25506;&#32034;&#22240;&#32032;&#30340;&#20445;&#23432;&#36755;&#20837;&#12290;&#22312;&#25670;&#21160;&#20219;&#21153;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#26174;&#24335;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent rapid developments in reinforcement learning algorithms have been giving us novel possibilities in many fields. However, due to their exploring property, we have to take the risk into consideration when we apply those algorithms to safety-critical problems especially in real environments. In this study, we deal with a safe exploration problem in reinforcement learning under the existence of disturbance. We define the safety during learning as satisfaction of the constraint conditions explicitly defined in terms of the state and propose a safe exploration method that uses partial prior knowledge of a controlled object and disturbance. The proposed method assures the satisfaction of the explicit state constraints with a pre-specified probability even if the controlled object is exposed to a stochastic disturbance following a normal distribution. As theoretical results, we introduce sufficient conditions to construct conservative inputs not containing an exploring aspect used in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.15214</link><description>&lt;p&gt;
&#21313;&#20159;&#32423;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v6 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#26159;&#24403;&#21069;&#35768;&#22810;&#20225;&#19994;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#35768;&#22810;&#20135;&#21697;&#25552;&#20379;&#20107;&#23454;&#30693;&#35782;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#30528;&#35768;&#22810;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#20294;&#26500;&#24314;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#38656;&#35201;&#35299;&#20915;&#32467;&#26500;&#19981;&#36275;&#21644;&#22810;&#27169;&#24577;&#30340;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#31995;&#32479;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26680;&#24515;&#26412;&#20307;&#65292;&#28085;&#30422;&#21508;&#31181;&#25277;&#35937;&#20135;&#21697;&#21644;&#28040;&#36153;&#38656;&#27714;&#65292;&#24182;&#22312;&#37096;&#32626;&#30340;&#24212;&#29992;&#20013;&#25552;&#20379;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#12290;OpenBG &#26159;&#19968;&#20010;&#31354;&#21069;&#35268;&#27169;&#30340;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65306;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#12289;&#35206;&#30422;&#36229;&#36807; 1 &#30334;&#19975;&#20010;&#26680;&#24515;&#31867;/&#27010;&#24565;&#21644; 2,681 &#31181;&#20851;&#31995;&#30340; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25152;&#26377;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#30693;&#35782;&#22270;&#35889;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Knowledge Graphs (KGs) are important to many enterprises today, providing factual knowledge and structured data that steer many products and make them more intelligent. Despite their promising benefits, building business KG necessitates solving prohibitive issues of deficient structure and multiple modalities. In this paper, we advance the understanding of the practical challenges related to building KG in non-trivial real-world systems. We introduce the process of building an open business knowledge graph (OpenBG) derived from a well-known enterprise, Alibaba Group. Specifically, we define a core ontology to cover various abstract products and consumption demands, with fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is an open business KG of unprecedented scale: 2.6 billion triples with more than 88 million entities covering over 1 million core classes/concepts and 2,681 types of relations. We release all the open resources (OpenBG benchmarks) deri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#21407;&#21017;&#21644;&#31163;&#32447;&#22270;&#24418;&#35268;&#21010;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;G-UCB&#65292;&#33021;&#22815;&#24179;&#34913;&#38271;&#26399;&#25506;&#32034;&#21033;&#29992;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31181;&#21517;&#20026;&#22270;&#36172;&#21338;&#26426;&#30340;MAB&#25193;&#23637;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#22823;&#21270;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2209.09419</link><description>&lt;p&gt;
&#19968;&#31181;&#22270;&#19978;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-armed Bandit Learning on a Graph. (arXiv:2209.09419v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#21407;&#21017;&#21644;&#31163;&#32447;&#22270;&#24418;&#35268;&#21010;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;G-UCB&#65292;&#33021;&#22815;&#24179;&#34913;&#38271;&#26399;&#25506;&#32034;&#21033;&#29992;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31181;&#21517;&#20026;&#22270;&#36172;&#21338;&#26426;&#30340;MAB&#25193;&#23637;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#22823;&#21270;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#26041;&#38754;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#36873;&#25321;&#19968;&#20010;&#33218;&#23545;&#24212;&#20110;&#38480;&#21046;&#19979;&#19968;&#20010;&#21487;&#29992;&#33218;&#65288;&#21160;&#20316;&#65289;&#30340;&#36873;&#25321;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#36172;&#21338;&#26426;&#30340;MAB&#25193;&#23637;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#20174;&#19981;&#21516;&#33410;&#28857;&#20013;&#25910;&#38598;&#22870;&#21169;&#20197;&#33719;&#24471;&#26368;&#22823;&#21270;&#30340;&#25910;&#30410;&#12290;&#22270;&#23450;&#20041;&#20102;&#26234;&#33021;&#20307;&#22312;&#27599;&#19968;&#27493;&#20013;&#36873;&#25321;&#19979;&#19968;&#20010;&#21487;&#29992;&#33410;&#28857;&#30340;&#33258;&#30001;&#24230;&#12290;&#25105;&#20204;&#20551;&#35774;&#22270;&#30340;&#32467;&#26500;&#26159;&#23436;&#20840;&#21487;&#29992;&#30340;&#65292;&#20294;&#22870;&#21169;&#20998;&#24067;&#26159;&#26410;&#30693;&#30340;&#12290;&#22522;&#20110;&#31163;&#32447;&#22270;&#24418;&#35268;&#21010;&#31639;&#27861;&#21644;&#20048;&#35266;&#21407;&#21017;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;G-UCB&#65292;&#24179;&#34913;&#38271;&#26399;&#25506;&#32034;&#21033;&#29992;&#20351;&#29992;&#20048;&#35266;&#21407;&#21017;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$O(\sqrt{|S|T\log(T)}+D|S|\log T)$&#30340;&#23398;&#20064;&#36951;&#25022;&#12290;&#20854;&#20013;$|S|$&#26159;
&lt;/p&gt;
&lt;p&gt;
The multi-armed bandit(MAB) problem is a simple yet powerful framework that has been extensively studied in the context of decision-making under uncertainty. In many real-world applications, such as robotic applications, selecting an arm corresponds to a physical action that constrains the choices of the next available arms (actions). Motivated by this, we study an extension of MAB called the graph bandit, where an agent travels over a graph to maximize the reward collected from different nodes. The graph defines the agent's freedom in selecting the next available nodes at each step. We assume the graph structure is fully available, but the reward distributions are unknown. Built upon an offline graph-based planning algorithm and the principle of optimism, we design a learning algorithm, G-UCB, that balances long-term exploration-exploitation using the principle of optimism. We show that our proposed algorithm achieves $O(\sqrt{|S|T\log(T)}+D|S|\log T)$ learning regret, where $|S|$ is 
&lt;/p&gt;</description></item><item><title>AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.08645</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Exploration for Inverse Reinforcement Learning. (arXiv:2207.08645v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08645
&lt;/p&gt;
&lt;p&gt;
AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#33539;&#24335;&#12290;&#35768;&#22810;IRL&#31639;&#27861;&#38656;&#35201;&#24050;&#30693;&#30340;&#36716;&#31227;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#38656;&#35201;&#24050;&#30693;&#30340;&#19987;&#23478;&#31574;&#30053;&#65292;&#25110;&#32773;&#33267;&#23569;&#38656;&#35201;&#35775;&#38382;&#29983;&#25104;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#22826;&#24378;&#20102;&#65292;&#22240;&#20026;&#21482;&#33021;&#36890;&#36807;&#39034;&#24207;&#20132;&#20114;&#26469;&#35775;&#38382;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;IRL&#31639;&#27861;&#65306;&#20027;&#21160;&#25506;&#32034;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AceIRL&#65289;&#65292;&#23427;&#20027;&#21160;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#19987;&#23478;&#31574;&#30053;&#65292;&#24555;&#36895;&#23398;&#20064;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#35782;&#21035;&#20986;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#12290;AceIRL&#20351;&#29992;&#20808;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#25429;&#25417;&#21487;&#34892;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;AceIRL&#26159;&#31532;&#19968;&#31181;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#19988;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#21160;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert's reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;</title><link>http://arxiv.org/abs/2207.06983</link><description>&lt;p&gt;
&#22810;&#36712;&#38899;&#20048; Transformer
&lt;/p&gt;
&lt;p&gt;
Multitrack Music Transformer. (arXiv:2207.06983v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992; Transformer &#27169;&#22411;&#29983;&#25104;&#22810;&#36712;&#38899;&#20048;&#30340;&#26041;&#27861;&#22312;&#20048;&#22120;&#25968;&#37327;&#12289;&#38899;&#20048;&#29255;&#27573;&#38271;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#24050;&#26377;&#34920;&#31034;&#26041;&#24335;&#38656;&#35201;&#38271;&#24230;&#36739;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#20174;&#32780;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#21516;&#26102;&#20351;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#26356;&#30701;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; Multitrack Music Transformer&#65288;MMT&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#22312;&#20027;&#35266;&#21548;&#27979;&#35797;&#20013;&#25490;&#22312;&#20004;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#19978;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#21363;&#20852;&#21019;&#20316;&#25110;&#25509;&#36817;&#23454;&#26102;&#30340;&#21019;&#24847;&#24212;&#29992;&#20013;&#26356;&#20026;&#23454;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2206.02670</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#24341;&#23548;&#21644;&#35268;&#21010;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26080;&#20154;&#26426;&#22312;&#20844;&#20849;&#39046;&#22495;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#25915;&#20987;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#21644;&#35268;&#21010;&#65292;&#21033;&#29992;&#20154;&#24037;&#21183;&#22330;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#38556;&#30861;&#29289;&#36991;&#20813;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#22788;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#29305;&#24449;&#19982;&#22823;&#33041;&#31070;&#32463;&#20803;&#23545;&#20110;&#35821;&#38899;&#21050;&#28608;&#30340;&#21453;&#24212;&#33021;&#22815;&#24418;&#25104;&#31867;&#20284;&#30340;&#23618;&#32423;&#65292;&#19988;&#35299;&#37322;&#20102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#23569;&#20381;&#36182;&#20808;&#39564;&#35821;&#35328;&#21644;&#29702;&#35299;&#30693;&#35782;&#36164;&#28304;&#65292;&#24182;&#19988;&#38656;&#35201;&#30340;&#25968;&#25454;&#37327;&#36828;&#23567;&#20110;&#20854;&#23427;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.01685</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#22823;&#33041;&#20013;&#30340;&#35821;&#38899;&#22788;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#22788;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#29305;&#24449;&#19982;&#22823;&#33041;&#31070;&#32463;&#20803;&#23545;&#20110;&#35821;&#38899;&#21050;&#28608;&#30340;&#21453;&#24212;&#33021;&#22815;&#24418;&#25104;&#31867;&#20284;&#30340;&#23618;&#32423;&#65292;&#19988;&#35299;&#37322;&#20102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#23569;&#20381;&#36182;&#20808;&#39564;&#35821;&#35328;&#21644;&#29702;&#35299;&#30693;&#35782;&#36164;&#28304;&#65292;&#24182;&#19988;&#38656;&#35201;&#30340;&#25968;&#25454;&#37327;&#36828;&#23567;&#20110;&#20854;&#23427;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#19968;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#36755;&#20837;&#30340;&#21050;&#28608;&#21453;&#24212;&#33021;&#22815;&#19982;&#20154;&#33041;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#21313;&#20998;&#30456;&#20284;&#12290;&#20294;&#36825;&#20123;&#31639;&#27861;&#23384;&#22312;&#25968;&#25454;&#37327;&#24040;&#22823;&#12289;&#30417;&#30563;&#26631;&#31614;&#38590;&#20197;&#33719;&#21462;&#12289;&#21482;&#33021;&#25509;&#21463;&#25991;&#26412;&#36755;&#20837;&#20197;&#21450;&#38656;&#35201;&#39640;&#26114;&#30340;&#23384;&#20648;&#36164;&#28304;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#38480;&#21046;&#34920;&#26126;&#38656;&#35201;&#23547;&#25214;&#22312;&#36825;&#20123;&#38480;&#21046;&#19979;&#33021;&#22815;&#35299;&#37322;&#34892;&#20026;&#21644;&#22823;&#33041;&#21453;&#24212;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#35821;&#38899;&#22788;&#29702;&#38382;&#39064;&#65292;&#20551;&#35774;&#22522;&#20110;&#21407;&#22987;&#27874;&#24418;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#26696;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26550;&#26500;Wav2Vec 2.0&#21644;412&#21517;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#27721;&#35821;&#21548;&#21462;&#32422;1&#23567;&#26102;&#38899;&#39057;&#20070;&#31821;&#26102;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#22235;&#20010;&#20027;&#35201;&#25104;&#26524;&#65306;&#39318;&#20808;&#65292;&#20316;&#32773;&#21457;&#29616;Wav2Vec 2.0&#21644;&#22823;&#33041;&#31070;&#32463;&#20803;&#20250;&#23558;&#35821;&#38899;&#38899;&#39057;&#20449;&#24687;&#32534;&#30721;&#21040;&#31867;&#20284;&#26102;&#38388;&#21464;&#21270;&#30340;&#23618;&#32423;&#20013;&#12290;&#20854;&#27425;&#65292;&#20316;&#32773;&#35777;&#26126;&#36825;&#31181;&#23545;&#20110;&#38899;&#39057;&#23618;&#32423;&#30340;&#32534;&#30721;&#19981;&#26159;&#30001;&#20110;&#24433;&#21709;&#24120;&#35268;&#22768;&#38899;&#30340;&#34920;&#38754;&#22240;&#32032;&#23548;&#33268;&#12290;&#31532;&#19977;&#65292;&#20316;&#32773;&#36824;&#34920;&#26126;&#65292;&#19982;&#25991;&#26412;&#34920;&#31034;&#27861;&#25110;&#20256;&#32479;&#35821;&#38899;&#29305;&#24449;&#30456;&#27604;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#30340;&#29305;&#24449;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35299;&#37322;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23569;&#30340;&#35821;&#35328;&#32972;&#26223;&#21644;&#29702;&#35299;&#30693;&#35782;&#36164;&#28304;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35299;&#37322;&#26377;&#24847;&#20041;&#30340;&#22823;&#33041;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our results are four-fold. First
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#32479;&#35745;&#29575;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#31454;&#20105;&#21327;&#35758;&#30456;&#27604;&#30340;&#32463;&#39564;&#20248;&#36234;&#24615;&#65292;&#21327;&#35758;&#36890;&#36807;&#20998;&#26742;&#21487;&#20197;&#32467;&#21512;&#38544;&#31169;&#20445;&#38556;&#31243;&#24207;&#20197;&#23545;&#21322;&#35802;&#23454;&#26381;&#21153;&#22120;&#36827;&#34892;&#23433;&#20840;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2205.11765</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#20248;&#32479;&#35745;&#29575;&#21644;&#38544;&#31169;&#20445;&#35777;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Federated Learning with Optimal Statistical Rates and Privacy Guarantees. (arXiv:2205.11765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#32479;&#35745;&#29575;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#31454;&#20105;&#21327;&#35758;&#30456;&#27604;&#30340;&#32463;&#39564;&#20248;&#36234;&#24615;&#65292;&#21327;&#35758;&#36890;&#36807;&#20998;&#26742;&#21487;&#20197;&#32467;&#21512;&#38544;&#31169;&#20445;&#38556;&#31243;&#24207;&#20197;&#23545;&#21322;&#35802;&#23454;&#26381;&#21153;&#22120;&#36827;&#34892;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#32479;&#35745;&#29575;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21327;&#35758;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#25552;&#39640;&#20102;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#24182;&#22312;&#24378;&#20984;&#25439;&#22833;&#30340;&#25152;&#26377;&#21442;&#25968;&#26041;&#38754;&#23454;&#29616;&#20102;&#32039;&#23494;&#30340;&#32479;&#35745;&#29575;&#12290;&#25105;&#20204;&#23545;&#31454;&#20105;&#21327;&#35758;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#21327;&#35758;&#30340;&#32463;&#39564;&#20248;&#36234;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#25105;&#20204;&#30340;&#20998;&#26742;&#21327;&#35758;&#21487;&#20197;&#19982;&#38544;&#31169;&#20445;&#38556;&#31243;&#24207;&#33258;&#28982;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24341;&#20837;&#23545;&#21322;&#35802;&#23454;&#26381;&#21153;&#22120;&#30340;&#23433;&#20840;&#20445;&#38556;&#12290;&#35780;&#20272;&#20195;&#30721;&#20301;&#20110;https://github.com/wanglun1996/secure-robust-federated-learning&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Byzantine-robust federated learning protocols with nearly optimal statistical rates. In contrast to prior work, our proposed protocols improve the dimension dependence and achieve a tight statistical rate in terms of all the parameters for strongly convex losses. We benchmark against competing protocols and show the empirical superiority of the proposed protocols. Finally, we remark that our protocols with bucketing can be naturally combined with privacy-guaranteeing procedures to introduce security against a semi-honest server. The code for evaluation is provided in https://github.com/wanglun1996/secure-robust-federated-learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#39592;&#24178;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35266;&#23519;&#21040;Transformers&#20855;&#26377;&#24456;&#24378;&#30340;&#24418;&#29366;&#20559;&#24046;&#12290;&#24418;&#29366;&#20559;&#24046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#27867;&#21270;&#24615;&#33021;&#65292;&#20316;&#32773;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;CNN-Transformer&#28151;&#21512;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.11083</link><description>&lt;p&gt;
&#28145;&#20837;&#25506;&#35752;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deep Digging into the Generalization of Self-Supervised Monocular Depth Estimation. (arXiv:2205.11083v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#39592;&#24178;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35266;&#23519;&#21040;Transformers&#20855;&#26377;&#24456;&#24378;&#30340;&#24418;&#29366;&#20559;&#24046;&#12290;&#24418;&#29366;&#20559;&#24046;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#27867;&#21270;&#24615;&#33021;&#65292;&#20316;&#32773;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;CNN-Transformer&#28151;&#21512;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26159;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#23558;&#37325;&#28857;&#25918;&#22312;&#25552;&#39640;KITTI&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#65292;&#20294;&#26159;&#23545;&#20110;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#30340;&#23454;&#39564;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#25506;&#31350;&#20102;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#39592;&#24178;&#32593;&#32476;&#65288;&#22914;CNN&#12289;Transformers&#21644;CNN-Transformer&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#22810;&#20803;&#21270;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#32593;&#32476;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#35265;&#36807;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#32441;&#29702;&#20301;&#31227;&#25968;&#25454;&#38598;&#29983;&#25104;&#20102;&#32441;&#29702;&#20559;&#24046;&#21644;&#24418;&#29366;&#20559;&#24046;&#34920;&#31034;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;Transformers&#20855;&#26377;&#24456;&#24378;&#30340;&#24418;&#29366;&#20559;&#24046;&#65292;&#32780;CNN&#21017;&#26377;&#24456;&#24378;&#30340;&#32441;&#29702;&#20559;&#24046;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19982;&#32441;&#29702;&#20559;&#24046;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#24418;&#29366;&#20559;&#24046;&#30340;&#27169;&#22411;&#22312;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;CNN-Transformer&#28151;&#21512;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#24418;&#29366;&#20559;&#24046;&#20998;&#25903;&#65292;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised monocular depth estimation has been widely studied recently. Most of the work has focused on improving performance on benchmark datasets, such as KITTI, but has offered a few experiments on generalization performance. In this paper, we investigate the backbone networks (e.g. CNNs, Transformers, and CNN-Transformer hybrid models) toward the generalization of monocular depth estimation. We first evaluate state-of-the-art models on diverse public datasets, which have never been seen during the network training. Next, we investigate the effects of texture-biased and shape-biased representations using the various texture-shifted datasets that we generated. We observe that Transformers exhibit a strong shape bias and CNNs do a strong texture-bias. We also find that shape-biased models show better generalization performance for monocular depth estimation compared to texture-biased models. Based on these observations, we newly design a CNN-Transformer hybrid network with a mult
&lt;/p&gt;</description></item><item><title>Sionna&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22522;&#20110;TensorFlow&#30340;&#24320;&#28304;&#24211;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#31471;&#21040;&#31471;&#24615;&#33021;&#35780;&#20272;&#12290;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#29983;&#25903;&#25345;&#65292;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#21152;&#19987;&#27880;&#20110;&#33258;&#24049;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2203.11854</link><description>&lt;p&gt;
Sionna&#65306;&#19979;&#19968;&#20195;&#29289;&#29702;&#23618;&#30740;&#31350;&#30340;&#24320;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
Sionna: An Open-Source Library for Next-Generation Physical Layer Research. (arXiv:2203.11854v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11854
&lt;/p&gt;
&lt;p&gt;
Sionna&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22522;&#20110;TensorFlow&#30340;&#24320;&#28304;&#24211;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#31471;&#21040;&#31471;&#24615;&#33021;&#35780;&#20272;&#12290;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#29983;&#25903;&#25345;&#65292;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#21152;&#19987;&#27880;&#20110;&#33258;&#24049;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sionna&#26159;&#19968;&#31181;&#22522;&#20110;TensorFlow&#30340;GPU&#21152;&#36895;&#24320;&#28304;&#24211;&#65292;&#29992;&#20110;&#38142;&#36335;&#23618;&#27169;&#25311;&#12290;&#23427;&#23454;&#29616;&#20102;&#24191;&#27867;&#32780;&#31934;&#24515;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#31471;&#21040;&#31471;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#21407;&#29983;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#12290;&#36825;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#19987;&#27880;&#20110;&#20182;&#20204;&#30340;&#30740;&#31350;&#65292;&#20351;&#20854;&#26356;&#20855;&#24433;&#21709;&#21147;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;&#23454;&#29616;&#20182;&#20204;&#25152;&#19981;&#29087;&#24713;&#30340;&#32452;&#20214;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;Sionna&#65292;&#35299;&#37322;&#20102;&#23427;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#21151;&#33021;&#65292;&#20197;&#21450;&#26410;&#26469;&#30340;&#25193;&#23637;&#65292;&#22914;&#38598;&#25104;&#20809;&#32447;&#36319;&#36394;&#21644;&#33258;&#23450;&#20041;CUDA&#20869;&#26680;&#12290;&#25105;&#20204;&#35748;&#20026;Sionna&#26159;&#30740;&#31350;&#19979;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#65288;&#22914;6G&#65289;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#27426;&#36814;&#26469;&#33258;&#25105;&#20204;&#31038;&#21306;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. It enables the rapid prototyping of complex communication system architectures and provides native support for the integration of neural networks. Sionna implements a wide breadth of carefully tested state-of-the-art algorithms that can be used for benchmarking and end-to-end performance evaluation. This allows researchers to focus on their research, making it more impactful and reproducible, while saving time implementing components outside their area of expertise. This white paper provides a brief introduction to Sionna, explains its design principles and features, as well as future extensions, such as integrated ray tracing and custom CUDA kernels. We believe that Sionna is a valuable tool for research on next-generation communication systems, such as 6G, and we welcome contributions from our community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#33258;&#21160;&#23558;&#19978;&#24093;&#31867;&#25552;&#21462;&#20026;&#26356;&#20855;&#20307;&#32844;&#36131;&#30340;&#36739;&#23567;&#31867;&#21035;&#30340;&#26041;&#27861;&#65292;&#22312;&#20116;&#20010;&#24320;&#28304;&#39033;&#30446;&#19978;&#35780;&#20272;&#21518;&#32467;&#26524;&#34920;&#26126;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2203.08787</link><description>&lt;p&gt;
&#25506;&#31350;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#25552;&#21462;&#20998;&#31867;&#37325;&#26500;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Exploring Variational Graph Auto-Encoders for Extract Class Refactoring Recommendation. (arXiv:2203.08787v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#33258;&#21160;&#23558;&#19978;&#24093;&#31867;&#25552;&#21462;&#20026;&#26356;&#20855;&#20307;&#32844;&#36131;&#30340;&#36739;&#23567;&#31867;&#21035;&#30340;&#26041;&#27861;&#65292;&#22312;&#20116;&#20010;&#24320;&#28304;&#39033;&#30446;&#19978;&#35780;&#20272;&#21518;&#32467;&#26524;&#34920;&#26126;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#24322;&#21619;&#26159;&#36719;&#20214;&#31995;&#32479;&#20013;&#35774;&#35745;&#21644;&#24320;&#21457;&#32570;&#38519;&#30340;&#26631;&#24535;&#65292;&#20250;&#38477;&#20302;&#31995;&#32479;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#12290;&#37325;&#26500;&#26159;&#19968;&#31181;&#25345;&#32493;&#36827;&#34892;&#30340;&#23454;&#36341;&#65292;&#29992;&#20110;&#20174;&#31243;&#24207;&#20195;&#30721;&#20013;&#21435;&#38500;&#20195;&#30721;&#24322;&#21619;&#12290;&#22312;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#20013;&#65292;&#19978;&#24093;&#31867;&#25110;Blob&#26159;&#26368;&#24120;&#35265;&#30340;&#20195;&#30721;&#24322;&#21619;&#20043;&#19968;&#12290;&#19978;&#24093;&#31867;&#21253;&#21547;&#35768;&#22810;&#32844;&#36131;&#65292;&#36829;&#21453;&#20102;&#38754;&#21521;&#23545;&#35937;&#31243;&#24207;&#35774;&#35745;&#30340;&#20302;&#32806;&#21512;&#21644;&#39640;&#20869;&#32858;&#21407;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#21462;&#19978;&#24093;&#31867;&#20026;&#22810;&#20010;&#36739;&#23567;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#20855;&#20307;&#30340;&#32844;&#36131;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#36896;&#20102;&#19968;&#20010;&#26377;&#20851;&#19978;&#24093;&#31867;&#30340;&#26041;&#27861;&#22270;&#65288;&#20316;&#20026;&#33410;&#28857;&#65289;&#12290;&#20219;&#20309;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#36793;&#32536;&#30001;&#23427;&#20204;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#20915;&#23450;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#20041;&#34920;&#31034;&#26041;&#27861;&#21021;&#22987;&#21270;&#27599;&#31181;&#26041;&#27861;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#26041;&#27861;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#23398;&#21040;&#30340;&#21521;&#37327;&#23558;&#26041;&#27861;&#32858;&#31867;&#21040;&#36739;&#23567;&#30340;&#31867;&#21035;&#20013;&#12290;&#35813;&#26041;&#27861;&#22312;&#20116;&#20010;&#24320;&#28304;&#39033;&#30446;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23558;&#19978;&#24093;&#31867;&#25552;&#21462;&#20026;&#26356;&#20855;&#20307;&#32844;&#36131;&#30340;&#36739;&#23567;&#31867;&#21035;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The code smell is a sign of design and development flaws in a software system that reduces the reusability and maintainability of the system. Refactoring is done as an ongoing practice to remove the code smell from the program code. Among different code smells, the God class or Blob is one of the most common code smells. A god class contains too many responsibilities, violating object-oriented programming design's low coupling and high cohesiveness principles. This paper proposes an automatic approach to extracting a God class into multiple smaller classes with more specific responsibilities. To do this, we first construct a graph of methods (as nodes) for the concerning god class. The edge between any two methods is determined by their structural similarity, and the feature for each method is initialized using different semantic representation methods. Then, the variational graph auto-encoder is used to learn a vector representation for each method. Finally, the learned vectors are us
&lt;/p&gt;</description></item><item><title>SuperAnimal&#26159;&#19968;&#31181;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#36229;&#36807;45&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#24494;&#35843;&#27169;&#22411;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2203.07436</link><description>&lt;p&gt;
&#36229;&#32423;&#21160;&#29289;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#21160;&#29289;&#34892;&#20026;&#30340;&#21363;&#25554;&#21363;&#29992;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SuperAnimal models pretrained for plug-and-play analysis of animal behavior. (arXiv:2203.07436v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07436
&lt;/p&gt;
&lt;p&gt;
SuperAnimal&#26159;&#19968;&#31181;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#36229;&#36807;45&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#24494;&#35843;&#27169;&#22411;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#37327;&#21270;&#22312;&#31070;&#32463;&#31185;&#23398;&#12289;&#20861;&#21307;&#21644;&#21160;&#29289;&#20445;&#25252;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34892;&#20026;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#39318;&#20808;&#25552;&#21462;&#19982;&#21160;&#29289;&#30456;&#20851;&#30340;&#20851;&#38190;&#28857;&#65292;&#21363;&#23039;&#21183;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#30340;&#23039;&#21183;&#25512;&#26029;&#30446;&#21069;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#21644;&#25163;&#21160;&#26631;&#27880;&#26469;&#26500;&#24314;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21019;&#26032;&#65292;&#20351;&#19968;&#31181;&#21517;&#20026;SuperAnimal&#30340;&#26032;&#26041;&#27861;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;SuperAnimal&#20801;&#35768;&#23545;45&#22810;&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#21516;&#26102;&#21482;&#20351;&#29992;&#20004;&#31181;&#20840;&#23616;&#21160;&#29289;&#23039;&#21183;&#27169;&#22411;&#12290;&#22914;&#26524;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SuperAnimal&#27169;&#22411;&#20855;&#26377;10&#20493;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#32988;&#36807;&#20808;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#35270;&#39057;&#32454;&#21270;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification of behavior is critical in applications ranging from neuroscience, veterinary medicine and animal conservation efforts. A common key step for behavioral analysis is first extracting relevant keypoints on animals, known as pose estimation. However, reliable inference of poses currently requires domain knowledge and manual labeling effort to build supervised models. We present a series of technical innovations that enable a new method, collectively called SuperAnimal, to develop and deploy deep learning models that require zero additional human labels and model training. SuperAnimal allows video inference on over 45 species with only two global classes of animal pose models. If the models need fine-tuning, we show SuperAnimal models are 10$\times$ more data efficient and outperform prior transfer learning approaches. Moreover, we provide a new video-adaptation method to perform unsupervised refinement of videos, and we illustrate the utility of our model in behavioral clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.06865</link><description>&lt;p&gt;
&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v3 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#37329;&#34701;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23384;&#22312;&#36866;&#21512;&#32473;&#23450;&#19968;&#32452;&#26399;&#26435;&#24066;&#22330;&#20215;&#26684;&#30340;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20351;&#29992;&#30452;&#35273;&#12289;&#29702;&#35770;&#21644;&#32463;&#39564;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#23547;&#25214;&#23454;&#29616;&#31934;&#30830;&#25110;&#36817;&#20284;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#21338;&#24328;&#29702;&#35770;&#24418;&#24335;&#21270;&#38382;&#39064;&#65292;&#20511;&#21161;&#29616;&#20195;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#26377;&#36827;&#23637;&#26469;&#25628;&#32034;&#38543;&#26426;&#36807;&#31243;&#31354;&#38388;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#34987;&#31038;&#21306;&#21033;&#29992;&#21644;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#32852;&#21512;SPX-VIX&#26657;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#22312;&#27874;&#21160;&#29575;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#65292;&#20197;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#31890;&#23376;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;Guyon et Henry-Labordere&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. More importantly, we hope that our techniques can be leveraged and extended by the community to solve important problems in that field, such as the joint SPX-VIX calibration problem. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \`{a} la Guyon et Henry-Labordere where partic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERTSubs&#30340;&#26032;&#22411;&#23376;&#31867;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;OWL&#26412;&#20307;&#31867;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#21253;&#25324;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#25110;&#21478;&#19968;&#20010;&#26412;&#20307;&#30340;&#21629;&#21517;&#31867;&#20197;&#21450;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#30340;&#23384;&#22312;&#38480;&#21046;&#31561;&#22810;&#31181;&#23376;&#31867;&#12290;</title><link>http://arxiv.org/abs/2202.09791</link><description>&lt;p&gt;
&#29992;&#20110;&#26412;&#20307;&#23376;&#31867;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#35821;&#20041;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERTSubs&#30340;&#26032;&#22411;&#23376;&#31867;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;OWL&#26412;&#20307;&#31867;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#21253;&#25324;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#25110;&#21478;&#19968;&#20010;&#26412;&#20307;&#30340;&#21629;&#21517;&#31867;&#20197;&#21450;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#30340;&#23384;&#22312;&#38480;&#21046;&#31561;&#22810;&#31181;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26412;&#20307;&#26500;&#24314;&#21644;&#32500;&#25252;&#26159;&#30693;&#35782;&#24037;&#31243;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#19978;&#19979;&#25991;&#35821;&#20041;&#23884;&#20837;&#65289;&#36827;&#34892;&#39044;&#27979;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#26159;&#30456;&#20851;&#30740;&#31350;&#23588;&#20854;&#26159;&#38024;&#23545;Web&#26412;&#20307;&#35821;&#35328;&#65288;OWL&#65289;&#20013;&#30340;&#34920;&#36798;&#22411;&#26412;&#20307;&#20173;&#22788;&#22312;&#21021;&#27493;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERTSubs&#30340;&#26032;&#22411;&#23376;&#31867;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;OWL&#26412;&#20307;&#31867;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;BERT&#35745;&#31639;&#31867;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#33258;&#23450;&#20041;&#27169;&#26495;&#26469;&#32467;&#21512;&#31867;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#37051;&#36817;&#31867;&#65289;&#21644;&#36923;&#36753;&#23384;&#22312;&#38480;&#21046;&#12290;BERTSubs&#21487;&#20197;&#39044;&#27979;&#22810;&#31181;&#23376;&#31867;&#65292;&#21253;&#25324;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#25110;&#21478;&#19968;&#20010;&#26412;&#20307;&#30340;&#21629;&#21517;&#31867;&#20197;&#21450;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#23376;&#31867;&#20219;&#21153;&#19978;&#23545;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#26412;&#20307;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#34920;&#26126;&#20102;BERTSubs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating ontology construction and curation is an important but challenging task in knowledge engineering and artificial intelligence. Prediction by machine learning techniques such as contextual semantic embedding is a promising direction, but the relevant research is still preliminary especially for expressive ontologies in Web Ontology Language (OWL). In this paper, we present a new subsumption prediction method named BERTSubs for classes of OWL ontology. It exploits the pre-trained language model BERT to compute contextual embeddings of a class, where customized templates are proposed to incorporate the class context (e.g., neighbouring classes) and the logical existential restriction. BERTSubs is able to predict multiple kinds of subsumers including named classes from the same ontology or another ontology, and existential restrictions from the same ontology. Extensive evaluation on five real-world ontologies for three different subsumption tasks has shown the effectiveness of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#24341;&#25806;IDP-Z3&#65292;&#23427;&#36866;&#29992;&#20110;FO(.)&#30693;&#35782;&#34920;&#31034;&#35821;&#35328;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#26500;&#24314;&#22522;&#20110;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#37197;&#32622;&#22120;&#12290;</title><link>http://arxiv.org/abs/2202.00343</link><description>&lt;p&gt;
&#24102;&#26377;FO(.)&#21644;IDP-Z3&#30340;&#20132;&#20114;&#24335;&#37197;&#32622;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interactive configurator with FO(.) and IDP-Z3. (arXiv:2202.00343v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#24341;&#25806;IDP-Z3&#65292;&#23427;&#36866;&#29992;&#20110;FO(.)&#30693;&#35782;&#34920;&#31034;&#35821;&#35328;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#26500;&#24314;&#22522;&#20110;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#37197;&#32622;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#37197;&#32622;&#38382;&#39064;&#22312;&#24037;&#19994;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#35745;&#31639;&#26426;&#36741;&#21161;&#19979;&#30001;&#20154;&#20204;&#20132;&#20114;&#22320;&#35299;&#20915;&#30340;&#32422;&#26463;&#27714;&#35299;&#38382;&#39064;&#12290;&#25152;&#35859;&#30340;&#37197;&#32622;&#31243;&#24207;&#38656;&#35201;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#65288;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65289;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#12290;&#37319;&#29992;&#21629;&#20196;&#24335;&#32534;&#31243;&#26041;&#27861;&#20351;&#24471;&#23454;&#29616;&#21644;&#32500;&#25252;&#27492;&#31867;&#31995;&#32479;&#21464;&#24471;&#21313;&#20998;&#22256;&#38590;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#37197;&#32622;&#31243;&#24207;&#26469;&#24110;&#21161;&#24037;&#31243;&#24072;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IDP-Z3&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;FO(.)&#30693;&#35782;&#34920;&#31034;&#35821;&#35328;&#30340;&#26032;&#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#24182;&#25253;&#21578;&#20102;&#23427;&#22312;&#20174;&#30693;&#35782;&#24211;&#20013;&#33258;&#21160;&#26500;&#24314;&#37197;&#32622;&#31243;&#24207;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry abounds with interactive configuration problems, i.e., constraint solving problems interactively solved by persons with the assistance of a computer. The computer program, called a configurator, needs to perform a variety of reasoning tasks with the (often incomplete) information that the user provides. Imperative programming approaches make such systems difficult to implement and maintain. Knowledge-based configurators have been proposed to help engineers solve such problems, but many challenges remain.  We present IDP-Z3, a new reasoning engine for the FO(.) KR language, and we report on its use for building configurators automatically from a knowledge base.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#26631;&#27880;&#65292;&#22312;&#21333;&#30446;&#35270;&#39057;&#20013;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#23454;&#29616;&#30340;&#19968;&#33268;&#19977;&#32500;&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.09548</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#19968;&#33268;&#19977;&#32500;&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent 3D Hand Reconstruction in Video via self-supervised Learning. (arXiv:2201.09548v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#26631;&#27880;&#65292;&#22312;&#21333;&#30446;&#35270;&#39057;&#20013;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#23454;&#29616;&#30340;&#19968;&#33268;&#19977;&#32500;&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21333;&#30446;&#35270;&#39057;&#20013;&#37325;&#24314;&#20934;&#30830;&#19988;&#19968;&#33268;&#30340;&#19977;&#32500;&#25163;&#37096;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#21457;&#29616;&#26816;&#27979;&#21040;&#30340;&#20108;&#32500;&#25163;&#37096;&#20851;&#38190;&#28857;&#21644;&#22270;&#20687;&#32441;&#29702;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#19977;&#32500;&#25163;&#37096;&#20960;&#20309;&#24418;&#29366;&#21644;&#32441;&#29702;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#20174;&#32780;&#21487;&#20197;&#20943;&#23569;&#25110;&#29978;&#33267;&#28040;&#38500;&#23545;&#19977;&#32500;&#25163;&#37096;&#26631;&#27880;&#30340;&#35201;&#27714;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#19977;&#32500;&#25163;&#37096;&#37325;&#24314;&#27169;&#22411;${\rm {S}^{2}HAND}$&#65292;&#36890;&#36807;&#26131;&#20110;&#33719;&#21462;&#30340;&#20108;&#32500;&#26816;&#27979;&#20851;&#38190;&#28857;&#36827;&#34892;&#30417;&#30563;&#65292;&#22312;&#21333;&#20010;RGB&#36755;&#20837;&#20013;&#20849;&#21516;&#20272;&#35745;&#25163;&#37096;&#23039;&#21183;&#12289;&#24418;&#29366;&#12289;&#32441;&#29702;&#21644;&#30456;&#26426;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Thus we propose ${\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and propose ${\rm {S}^{2}HAND(V)}$, which uses a set of weights shared ${\rm {S}^{2}HAND}$ to process each frame and exploits additional motion, texture, and shape consistency constrains to promote more accurate hand poses and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised approach produces com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#22312;Sunyaev-Zeldovich &#33639;&#20809;$-$&#26143;&#22242;&#36136;&#37327;&#20851;&#31995;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#21464;&#37327;&#65292;&#32467;&#21512;&#20102;$Y_\mathrm{SZ}$&#21644;&#30005;&#31163;&#27668;&#20307;&#27987;&#24230;($c_\mathrm{gas}$)&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20102;&#35813;&#20851;&#31995;&#20013;&#30340;&#25955;&#23556;&#65292;&#25552;&#39640;&#20102;&#23431;&#23449;&#23398;&#20998;&#26512;&#30340;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2201.01305</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#23431;&#23449;&#22825;&#20307;&#29289;&#29702;&#23398;&#30340;&#27604;&#20363;&#20851;&#31995;: &#24212;&#29992;&#20110;&#20943;&#23569; Sunyaev-Zeldovich &#33639;&#20809;&#36136;&#37327;&#25955;&#23556;
&lt;/p&gt;
&lt;p&gt;
Augmenting astrophysical scaling relations with machine learning: application to reducing the Sunyaev-Zeldovich flux-mass scatter. (arXiv:2201.01305v3 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#22312;Sunyaev-Zeldovich &#33639;&#20809;$-$&#26143;&#22242;&#36136;&#37327;&#20851;&#31995;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#21464;&#37327;&#65292;&#32467;&#21512;&#20102;$Y_\mathrm{SZ}$&#21644;&#30005;&#31163;&#27668;&#20307;&#27987;&#24230;($c_\mathrm{gas}$)&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20102;&#35813;&#20851;&#31995;&#20013;&#30340;&#25955;&#23556;&#65292;&#25552;&#39640;&#20102;&#23431;&#23449;&#23398;&#20998;&#26512;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#22825;&#20307;&#31995;&#32479;&#36890;&#24120;&#23637;&#31034;&#20986;&#21487;&#35266;&#27979;&#29305;&#24615;&#65288;&#22914;&#20142;&#24230;&#12289;&#36895;&#24230;&#20998;&#25955;&#12289;&#25391;&#33633;&#21608;&#26399;&#65289;&#20043;&#38388;&#30340;&#20302;&#31163;&#25955;&#27604;&#20363;&#20851;&#31995;&#65292;&#36825;&#20123;&#27604;&#20363;&#20851;&#31995;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#24182;&#20026;&#36136;&#37327;&#21644;&#36317;&#31163;&#20272;&#31639;&#31561;&#25552;&#20379;&#20102;&#35266;&#27979;&#24037;&#20855;&#12290;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#39640;&#32500;&#24230;&#21442;&#25968;&#31354;&#38388;&#20013;&#25552;&#20379;&#24555;&#36895;&#31995;&#32479;&#30340;&#25628;&#32034;&#26032;&#27604;&#20363;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#22238;&#24402;&#8221;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#23558;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#24314;&#27169;&#25104;&#35299;&#26512;&#26041;&#31243;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110; Sunyaev-Zeldovich &#33639;&#20809;$-$&#26143;&#22242;&#36136;&#37327;&#20851;&#31995;($Y_\mathrm{SZ}-M$)&#65292;&#36825;&#20010;&#20851;&#31995;&#20013;&#30340;&#25955;&#23556;&#24433;&#21709;&#20174;&#26143;&#22242;&#20016;&#24230;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30340;&#23431;&#23449;&#23398;&#21442;&#25968;&#12290;&#36890;&#36807;&#22312;Illustris TNG&#27700;&#21147;&#27169;&#25311;&#25968;&#25454;&#19978;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#26143;&#22242;&#36136;&#37327;&#20195;&#29702;&#21464;&#37327;&#65292;&#32467;&#21512;&#20102;$Y_\mathrm{SZ}$&#21644;&#30005;&#31163;&#27668;&#20307;&#27987;&#24230;($c_\mathrm{gas}$)&#65306;$M \propto Y_\mathrm{SZ}(c_\mathrm{gas}/C)^{1.7}$&#65292;&#20854;&#20013;$C$&#26159;&#19968;&#20010;&#24120;&#25968;&#12290;&#36825;&#20010;&#20851;&#31995;&#26174;&#33879;&#20943;&#23567;&#20102;$Y_\mathrm{SZ}-M$&#20013;&#30340;&#25955;&#23556;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26143;&#31995;&#22242;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#23431;&#23449;&#23398;&#20998;&#26512;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex astrophysical systems often exhibit low-scatter relations between observable properties (e.g., luminosity, velocity dispersion, oscillation period). These scaling relations illuminate the underlying physics, and can provide observational tools for estimating masses and distances. Machine learning can provide a fast and systematic way to search for new scaling relations (or for simple extensions to existing relations) in abstract high-dimensional parameter spaces. We use a machine learning tool called symbolic regression (SR), which models patterns in a dataset in the form of analytic equations. We focus on the Sunyaev-Zeldovich flux$-$cluster mass relation ($Y_\mathrm{SZ}-M$), the scatter in which affects inference of cosmological parameters from cluster abundance data. Using SR on the data from the IllustrisTNG hydrodynamical simulation, we find a new proxy for cluster mass which combines $Y_\mathrm{SZ}$ and concentration of ionized gas ($c_\mathrm{gas}$): $M \propto Y_\mathrm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25209;&#27425;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#20197;&#38477;&#20302;&#20107;&#20214;&#27880;&#37322;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#19988;&#24615;&#33021;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2112.03073</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#30340;&#20107;&#20214;&#25277;&#21462;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Event Extraction with Memory-based Loss Prediction Model. (arXiv:2112.03073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25209;&#27425;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#20197;&#38477;&#20302;&#20107;&#20214;&#27880;&#37322;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#19988;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#20197;&#35757;&#32451;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#27880;&#37322;&#25968;&#25454;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39046;&#22495;&#20107;&#20214;&#27880;&#37322;&#65292;&#38656;&#35201;&#30456;&#24212;&#39046;&#22495;&#30340;&#19987;&#23478;&#21442;&#19982;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#26469;&#38477;&#20302;&#20107;&#20214;&#27880;&#37322;&#30340;&#25104;&#26412;&#12290;&#20294;&#26159;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#24456;&#22909;&#22320;&#29992;&#20110;&#20107;&#20214;&#25277;&#21462;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26679;&#26412;&#27744;&#36873;&#25321;&#30340;&#31574;&#30053;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26679;&#26412;&#26377;&#25928;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26679;&#26412;&#37325;&#35201;&#24615;&#35780;&#20272;&#32570;&#20047;&#23545;&#26412;&#22320;&#26679;&#26412;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25209;&#27425;&#30340;&#36873;&#25321;&#31574;&#30053;&#21644;&#19968;&#20010;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#65288;MBLP&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#65292;MBLP&#34987;&#29992;&#26469;&#39044;&#27979;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#33021;&#25439;&#22833;&#65292;&#24182;&#25351;&#23548;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#20449;&#24687;&#22686;&#30410;&#30340;&#26679;&#26412;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20197;&#26174;&#33879;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event extraction (EE) plays an important role in many industrial application scenarios, and high-quality EE methods require a large amount of manual annotation data to train supervised learning models. However, the cost of obtaining annotation data is very high, especially for annotation of domain events, which requires the participation of experts from corresponding domain. So we introduce active learning (AL) technology to reduce the cost of event annotation. But the existing AL methods have two main problems, which make them not well used for event extraction. Firstly, the existing pool-based selection strategies have limitations in terms of computational cost and sample validity. Secondly, the existing evaluation of sample importance lacks the use of local sample information. In this paper, we present a novel deep AL method for EE. We propose a batch-based selection strategy and a Memory-Based Loss Prediction model (MBLP) to select unlabeled samples efficiently. During the selectio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31867;&#20851;&#20110;\L ukasiewicz&#36923;&#36753;&#30340;&#20449;&#24565;&#25193;&#23637;&#65292;&#22522;&#20110;&#19981;&#21516;&#30340;&#20449;&#24565;&#27010;&#24565;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25193;&#23637;&#30340;&#22768;&#38899;&#24615;&#21644;&#23436;&#25972;&#24615;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2111.08564</link><description>&lt;p&gt;
\L ukasiewicz&#36923;&#36753;&#30340;&#20449;&#24565;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Doxastic Extensions of \L ukasiewicz Logic. (arXiv:2111.08564v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31867;&#20851;&#20110;\L ukasiewicz&#36923;&#36753;&#30340;&#20449;&#24565;&#25193;&#23637;&#65292;&#22522;&#20110;&#19981;&#21516;&#30340;&#20449;&#24565;&#27010;&#24565;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25193;&#23637;&#30340;&#22768;&#38899;&#24615;&#21644;&#23436;&#25972;&#24615;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#20851;&#20110;&#20449;&#24565;&#30340;&#25193;&#23637;&#65292;&#23427;&#20204;&#26159;&#27169;&#31946;\L ukasiewicz&#36923;&#36753;&#20851;&#20110;&#22522;&#20110;Kripke&#30340;&#27169;&#22411;&#30340;&#19968;&#20123;&#36866;&#24403;&#31867;&#21035;&#30340;&#22768;&#38899;&#19988;&#23436;&#25972;&#30340;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#19968;&#31867;&#24102;&#26377;&#31867;&#20284;&#20110;&#32463;&#20856;&#20449;&#24565;&#30340;&#20266;&#21476;&#20856;&#20449;&#24565;&#23646;&#24615;&#65292;&#21478;&#19968;&#31867;&#22522;&#20110;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;\textit{skeptical}&#65288;&#24576;&#30097;&#30340;&#65289;&#20449;&#24565;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#20266;&#21476;&#20856;&#20449;&#24565;&#23545;&#27169;&#31946;&#29256;&#26412;&#30340;&#8220;&#27877;&#27870;&#20799;&#31461;&#38382;&#39064;&#8221;&#36827;&#34892;&#24314;&#27169;&#65292;&#20351;&#29992;&#24576;&#30097;&#30340;&#20449;&#24565;&#23545;CPA&#23433;&#20840;&#24615;&#23454;&#39564;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#21518;&#36890;&#36807;&#23637;&#31034;&#20266;&#21476;&#20856;&#20449;&#24565;&#19981;&#36866;&#29992;&#20110;&#27169;&#25311;CPA&#23454;&#39564;&#20013;&#23545;&#25163;&#30340;&#20449;&#24565;&#65292;&#20174;&#32780;&#35777;&#26126;&#25552;&#20986;&#24576;&#30097;&#30340;&#20449;&#24565;&#27010;&#24565;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20123;&#25552;&#20986;&#30340;&#20449;&#24565;&#25193;&#23637;&#30340;&#23436;&#25972;&#24615;&#21644;&#22768;&#38899;&#24615;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two classes of doxastic extensions of fuzzy \L ukasiewicz logic that are sound and complete with respect to some appropriate classes of Kripke-based models in which both atomic propositions and accessibility relations are fuzzy. One class of these extensions is equipped with pseudo-classical belief that has properties similar to the classical belief, and the other class is based on a new notion of belief that we call it \textit{skeptical} belief. We model a fuzzy version of the muddy children problem using pseudo-classical belief and a CPA-security experiment using skeptical belief, then by showing that the pseudo-classical belief is not appropriate for modeling the belief of an adversary in a CPA-experiment we justify proposing the notion of skeptical belief. Furthermore, we prove the soundness and completeness theorems for some of the proposed doxastic extensions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25945;&#23398;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#20869;&#37096;&#21644;&#25945;&#23398;&#20449;&#21495;&#20013;&#21516;&#27493;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#25216;&#33021;&#20064;&#24471;&#30340;&#25928;&#29575;&#65292;&#27492;&#20030;&#26159;&#26500;&#24314;&#20855;&#26377;&#20154;&#31867;&#32423;&#26234;&#33021;&#30340;&#20195;&#29702;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2105.11977</link><description>&lt;p&gt;
&#36861;&#23547;&#21487;&#25945;&#23398;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Teachable Autotelic Agents. (arXiv:2105.11977v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25945;&#23398;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#20869;&#37096;&#21644;&#25945;&#23398;&#20449;&#21495;&#20013;&#21516;&#27493;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#25216;&#33021;&#20064;&#24471;&#30340;&#25928;&#29575;&#65292;&#27492;&#20030;&#26159;&#26500;&#24314;&#20855;&#26377;&#20154;&#31867;&#32423;&#26234;&#33021;&#30340;&#20195;&#29702;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#25506;&#32034;&#21644;&#30452;&#25509;&#25351;&#23548;&#26159;&#20799;&#31461;&#23398;&#20064;&#30340;&#20004;&#20010;&#19981;&#21516;&#26469;&#28304;&#65292;&#20294;&#25945;&#32946;&#31185;&#23398;&#35777;&#26126;&#65292;&#36741;&#21161;&#21457;&#29616;&#25110;&#24341;&#23548;&#28216;&#25103;&#31561;&#28151;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25216;&#33021;&#20064;&#24471;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#36825;&#20123;&#26497;&#31471;&#20998;&#21035;&#26144;&#23556;&#20026;&#33258;&#20027;&#20195;&#29702;&#20174;&#33258;&#24049;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#20197;&#21450;&#23436;&#20840;&#34987;&#25945;&#24072;&#25945;&#25480;&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#20004;&#32773;&#20043;&#38388;&#24212;&#35813;&#31449;&#31435;&#21487;&#25945;&#23398;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#65288;TAA&#65289;&#65306;&#23427;&#20204;&#20174;&#20869;&#37096;&#21644;&#25945;&#23398;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#20197;&#20174;&#36741;&#21161;&#21457;&#29616;&#30340;&#26356;&#39640;&#25928;&#29575;&#20013;&#21463;&#30410;&#12290;&#35774;&#35745;&#36825;&#26679;&#30340;&#20195;&#29702;&#23558;&#20351;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#19987;&#19994;&#29992;&#25143;&#23558;&#20195;&#29702;&#30340;&#23398;&#20064;&#36712;&#36857;&#23450;&#21521;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;&#26356;&#22522;&#26412;&#22320;&#65292;&#36825;&#20063;&#21487;&#33021;&#26159;&#26500;&#24314;&#20855;&#26377;&#20154;&#31867;&#32423;&#26234;&#33021;&#30340;&#20195;&#29702;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#21521;&#21487;&#25945;&#23398;&#33258;&#20027;&#20195;&#29702;&#35774;&#35745;&#30340;&#36335;&#32447;&#22270;&#12290;&#22522;&#20110;&#21457;&#23637;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#31185;&#23398;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous discovery and direct instruction are two distinct sources of learning in children but education sciences demonstrate that mixed approaches such as assisted discovery or guided play result in improved skill acquisition. In the field of Artificial Intelligence, these extremes respectively map to autonomous agents learning from their own signals and interactive learning agents fully taught by their teachers. In between should stand teachable autotelic agents (TAA): agents that learn from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Designing such agents will enable real-world non-expert users to orient the learning trajectories of agents towards their expectations. More fundamentally, this may also be a key step to build agents with human-level intelligence. This paper presents a roadmap towards the design of teachable autonomous agents. Building on developmental psychology and education sciences, we start by identifying key fe
&lt;/p&gt;</description></item></channel></rss>