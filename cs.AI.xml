<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.19852</link><description>&lt;p&gt;
AI&#23545;&#40784;: &#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26088;&#22312;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#12290;&#38543;&#30528;&#25317;&#26377;&#36229;&#20154;&#31867;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38169;&#35823;&#23545;&#40784;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22823;&#35268;&#27169;&#39118;&#38505;&#21464;&#24471;&#26126;&#26174;&#12290;&#25968;&#30334;&#21517;AI&#19987;&#23478;&#21644;&#20844;&#20247;&#20154;&#29289;&#37117;&#23545;AI&#39118;&#38505;&#34920;&#36798;&#20102;&#20851;&#27880;&#65292;&#35748;&#20026;&#20943;&#36731;AI&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#24212;&#35813;&#25104;&#20026;&#20840;&#29699;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#19982;&#22823;&#35268;&#27169;&#31038;&#20250;&#39118;&#38505;&#22914;&#22823;&#27969;&#34892;&#30149;&#21644;&#26680;&#25112;&#20105;&#24182;&#21015;&#12290;&#37492;&#20110;AI&#23545;&#40784;&#39046;&#22495;&#32570;&#20047;&#26368;&#26032;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#23545;&#40784;&#30740;&#31350;&#30340;&#26680;&#24515;&#27010;&#24565;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#30446;&#26631;&#21407;&#21017;&#20316;&#20026;AI&#23545;&#40784;&#30340;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#65288;RICE&#65289;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#23545;&#40784;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#12290;&#21069;&#32773;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02067</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65306;&#26397;&#30528;&#26356;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability. (arXiv:2310.02067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#22270;&#20687;&#21462;&#35777;&#30340;&#32972;&#26223;&#19979;&#65292;&#24456;&#38590;&#30830;&#23450;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20165;&#20165;&#21033;&#29992;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#24120;&#65292;&#26102;&#38388;&#30456;&#36817;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#23646;&#20110;&#21516;&#19968;&#24180;&#40836;&#31867;&#21035;&#30340;&#65289;&#20855;&#26377;&#19968;&#20123;&#20849;&#21516;&#30340;&#20869;&#23481;&#23646;&#24615;&#12290;&#36825;&#31181;&#20869;&#23481;&#20559;&#24046;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#20869;&#23481;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#24102;&#26377;&#23884;&#20837;&#24335;&#24180;&#40836;&#20449;&#21495;&#30340;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#24180;&#40836;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#35757;&#32451;&#30340;&#8220;&#26631;&#20934;&#8221;&#31070;&#32463;&#32593;&#32476;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#20316;&#20026;&#28508;&#22312;&#30340;&#23545;&#31574;&#65292;&#26412;&#25991;&#24212;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#20943;&#36731;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of temporal image forensics, it is not evident that a neural network, trained on images from different time-slots (classes), exploit solely age related features. Usually, images taken in close temporal proximity (e.g., belonging to the same age class) share some common content properties. Such content bias can be exploited by a neural network. In this work, a novel approach that evaluates the influence of image content is proposed. This approach is verified using synthetic images (where content bias can be ruled out) with an age signal embedded. Based on the proposed approach, it is shown that a `standard' neural network trained in the context of age classification is strongly dependent on image content. As a potential countermeasure, two different techniques are applied to mitigate the influence of the image content during training, and they are also evaluated by the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.16741</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#28508;&#31354;&#38388;&#25237;&#24433;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20844;&#21496;&#36890;&#24120;&#22788;&#29702;&#21644;&#23384;&#20648;&#20135;&#29983;&#36830;&#32493;&#19988;&#39640;&#39057;&#30340;&#25968;&#21313;&#20159;&#26465;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25903;&#25345;&#39640;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#26816;&#32034;&#65292;&#20986;&#29616;&#20102;&#19987;&#38376;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#21644;&#31995;&#32479;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25903;&#25345;&#36890;&#36807;&#31867;&#20284;&#20110;&#32422;&#26463;&#21270;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#30340;&#26684;&#24335;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32034;&#24341;&#21644;&#26597;&#35810;&#65292;&#20197;&#23454;&#29616;&#20687;&#8220;&#26376;&#24230;&#20215;&#26684;&#22238;&#25253;&#22823;&#20110;5%&#30340;&#32929;&#31080;&#8221;&#36825;&#26679;&#30340;&#26597;&#35810;&#65292;&#24182;&#20197;&#20005;&#26684;&#30340;&#26684;&#24335;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#19981;&#33021;&#25429;&#25417;&#21040;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#35821;&#35328;&#65288;&#20363;&#22914;&#8220;&#22788;&#20110;&#20302;&#27874;&#21160;&#24615;&#29366;&#24577;&#30340;&#32929;&#31080;&#8221;&#65289;&#26356;&#22909;&#22320;&#25551;&#36848;&#12290;&#32780;&#19988;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#25152;&#38656;&#30340;&#23384;&#20648;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#26816;&#32034;&#22797;&#26434;&#24230;&#24448;&#24448;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#28508;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#25216;&#26415;&#19981;&#20165;&#25509;&#36817;&#20154;&#31867;&#30340;&#33258;&#28982;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#20197;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;&#65292;&#24182;&#19988;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12290;&#24403;&#28982;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#27867;&#28389;&#24341;&#36215;&#20102;&#23545;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#21644;&#27700;&#21360;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;&#27450;&#39575;&#23545;&#31574;&#25361;&#25112;&#65288;ASVspoof&#65289;&#19978;&#65292;&#35813;&#25361;&#25112;&#19987;&#27880;&#20110;&#34987;&#21160;&#23545;&#31574;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22312;&#19981;&#24178;&#25200;&#20154;&#31867;&#21548;&#20247;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36890;&#36807;&#21327;&#21516;&#26426;&#22120;&#26816;&#27979;&#21040;&#29983;&#25104;&#35821;&#38899;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;ASVspoof 2021&#22522;&#32447;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#30340;HiFi-GAN&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.10399</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21033;&#29992;&#22240;&#26524;&#20449;&#21495;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#24102;&#26377;&#23454;&#35777;&#32467;&#26524;&#30340;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#21307;&#23398;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22330;&#26223;&#20013;&#30340;&#24369;&#22240;&#26524;&#20449;&#21495;&#26469;&#24314;&#27169;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#21644;&#22240;&#26524;&#22240;&#23376;&#25552;&#21462;&#27169;&#22359;&#12290;&#21518;&#32773;&#35745;&#31639;&#29305;&#24449;&#22270;&#30340;&#26435;&#37325;&#65292;&#26681;&#25454;&#20854;&#23545;&#22270;&#20687;&#22330;&#26223;&#30340;&#22240;&#26524;&#24433;&#21709;&#22686;&#24378;&#27599;&#20010;&#29305;&#24449;&#22270;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#22806;&#37096;&#20449;&#21495;&#26469;&#20462;&#25913;&#22240;&#26524;&#27169;&#22359;&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#33719;&#24471;&#25105;&#20204;&#26041;&#27861;&#30340;&#19981;&#21516;&#21464;&#20307;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#23454;&#39564;&#12289;&#23450;&#24615;&#35780;&#20272;&#21644;&#21066;&#24369;&#23454;&#39564;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23545;&#21069;&#21015;&#33146;MRI&#22270;&#20687;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#36825;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for automatically classifying medical images that uses weak causal signals in the scene to model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image. Our method consists of two components: a convolutional neural network backbone and a causality-factors extractor module. The latter computes weights for the feature maps to enhance each feature map according to its causal influence in the image's scene. We can modify the functioning of the causality module by using two external signals, thus obtaining different variants of our method. We evaluate our method on a public dataset of prostate MRI images for prostate cancer diagnosis, using quantitative experiments, qualitative assessment, and ablation studies. Our results show that our method improves classification performance and produces more robust predictions, focusing on relevant parts of the image. That is especially important in medic
&lt;/p&gt;</description></item><item><title>SayCanPay&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#65292;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.12682</link><description>&lt;p&gt;
SayCanPay: &#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#24335;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge. (arXiv:2308.12682v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12682
&lt;/p&gt;
&lt;p&gt;
SayCanPay&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#65292;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#20854;&#24222;&#22823;&#30340;"&#19990;&#30028;&#30693;&#35782;"&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#33719;&#24471;&#26082;&#21487;&#34892;&#65288;&#22522;&#20110;&#21487;&#29992;&#24615;&#65289;&#21448;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#65288;&#35745;&#21010;&#38271;&#24230;&#26041;&#38754;&#65289;&#30340;&#35745;&#21010;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#19982;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#21453;&#24046;&#65292;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;(&#22312;&#21160;&#20316;&#27169;&#22411;&#22914;PDDL&#20013;&#24418;&#24335;&#21270;)&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SayCanPay&#21033;&#29992;LLMs&#26469;&#29983;&#25104;&#30001;&#21487;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#24341;&#23548;&#30340;&#21160;&#20316;(Say)&#65292;&#35780;&#20272;&#21160;&#20316;&#30340;&#21487;&#34892;&#24615;(Can)&#21644;&#38271;&#26399;&#22238;&#25253;/&#25910;&#30410;(Pay)&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;(1)&#22312;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLM&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#26500;&#24314;&#65292;(2)&#25972;&#21512;&#20102;&#21487;&#29992;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast "world knowledge". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#12290;</title><link>http://arxiv.org/abs/2307.01530</link><description>&lt;p&gt;
&#24212;&#23545;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#30340;&#35199;&#32418;&#26623;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#30340;&#21367;&#31215;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions. (arXiv:2307.01530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#37319;&#25688;&#23436;&#20840;&#25104;&#29087;&#30340;&#35199;&#32418;&#26623;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#26469;&#33258;&#20110;&#21494;&#23376;&#21644;&#26641;&#26525;&#36896;&#25104;&#30340;&#36974;&#25377;&#65292;&#20197;&#21450;&#22312;&#26524;&#23454;&#21457;&#32946;&#38454;&#27573;&#65292;&#35199;&#32418;&#26623;&#21644;&#21608;&#22260;&#26893;&#34987;&#20043;&#38388;&#30340;&#39068;&#33394;&#30456;&#20284;&#24615;&#12290;&#33258;&#28982;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#35270;&#35282;&#12289;&#36974;&#25377;&#22240;&#32032;&#21644;&#19981;&#21516;&#30340;&#25104;&#29087;&#24230;&#27700;&#24179;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#65292;&#26080;&#35770;&#20854;&#36974;&#25377;&#27700;&#24179;&#12289;&#20809;&#29031;&#26465;&#20214;&#21644;&#25104;&#29087;&#24230;&#22914;&#20309;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32463;&#36807;&#29305;&#21035;&#20026;&#27492;&#30446;&#30340;&#31934;&#24515;&#27880;&#37322;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#35270;&#35282;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#31227;&#21160;&#30456;&#26426;&#20256;&#24863;&#22120;&#19979;&#20934;&#22791;&#65292;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;Laboro&#65289;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13631</link><description>&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#30340;&#33258;&#32452;&#32455;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#35789;&#27719;&#19981;&#20165;&#20256;&#36882;&#20449;&#24687;&#65292;&#36824;&#38543;&#30528;&#25991;&#26126;&#21644;&#20154;&#31867;&#36801;&#31227;&#32780;&#28436;&#21464;&#12290;&#38899;&#20048;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#29702;&#35299;&#38899;&#20048;&#32972;&#21518;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;Essential Element Network (EEN)&#30340;&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#35745;&#31639;&#38899;&#35843;&#12289;&#26102;&#38388;&#21644;&#38899;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#21040;&#65292;&#36890;&#36807;&#20248;&#21270;EEN&#31639;&#27861;&#20197;&#29983;&#25104;Zipf&#23450;&#24459;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35821;&#20041;&#20851;&#31995;&#35270;&#20026;&#35789;&#27719;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32534;&#30721;&#21518;&#30340;&#35789;&#27719;&#26144;&#23556;&#21040;&#38899;&#35843;-&#26102;&#38388;&#31354;&#38388;&#20013;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#31995;&#32479;&#22320;&#32452;&#32455;&#38899;&#20048;&#28145;&#23618;&#32467;&#26500;&#20013;&#30340;&#21477;&#27861;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40657;&#30418;&#23376;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#23545;&#38899;&#20048;&#32972;&#21518;&#22797;&#26434;&#32593;&#32476;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#36807;&#31243;&#31215;&#32047;&#30340;&#32463;&#39564;&#21644;&#23646;&#24615;&#19981;&#20165;&#20026;&#27492;&#31867;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#20026;&#35768;&#22810;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25506;&#32034;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words in a natural language not only transmit information but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind the music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio into text. The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipfs law for the frequency and rank of the clustering coefficient enables us to generate and regard the semantic relationships as words. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions of the complex network behind the music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the applications of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#32508;&#36848;&#20102;NeSy&#21644;StarAI&#20004;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20849;&#26377;&#19971;&#20010;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2108.11451</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#20851;&#31995;&#21040;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Statistical Relational to Neural Symbolic Artificial Intelligence: a Survey. (arXiv:2108.11451v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#32508;&#36848;&#20102;NeSy&#21644;StarAI&#20004;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20849;&#26377;&#19971;&#20010;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#20004;&#20010;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#21644;&#32479;&#35745;&#20851;&#31995;&#20154;&#24037;&#26234;&#33021;&#65288;StarAI&#65289;&#12290;NeSy&#26088;&#22312;&#23558;&#31526;&#21495;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#32780;StarAI&#21017;&#19987;&#27880;&#20110;&#23558;&#36923;&#36753;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#35813;&#35843;&#26597;&#20851;&#27880;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#19971;&#20010;&#20849;&#21516;&#32500;&#24230;&#12290;&#36825;&#20123;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#39046;&#22495;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#27169;&#22411;&#36824;&#26159;&#22522;&#20110;&#35777;&#26126;&#65307;&#65288;2&#65289;&#36923;&#36753;&#29702;&#35770;&#30340;&#35821;&#27861;&#65307;&#65288;3&#65289;&#31995;&#32479;&#30340;&#36923;&#36753;&#35821;&#20041;&#21450;&#20854;&#25193;&#23637;&#20197;&#20419;&#36827;&#23398;&#20064;&#65307;&#65288;4&#65289;&#23398;&#20064;&#30340;&#33539;&#22260;&#65292;&#21253;&#25324;&#20165;&#28041;&#21450;&#21442;&#25968;&#36824;&#26159;&#28041;&#21450;&#25972;&#20010;&#36923;&#36753;&#29702;&#35770;&#65307;&#65288;5&#65289;&#34920;&#31034;&#27861;&#20013;&#31526;&#21495;&#21644;&#23376;&#31526;&#21495;&#25104;&#20998;&#30340;&#23384;&#22312;&#65307;&#65288;6&#65289;&#31995;&#32479;&#25429;&#25417;&#21407;&#22987;&#36923;&#36753;&#12289;&#27010;&#29575;&#21644;&#31070;&#32463;&#33539;&#20363;&#30340;&#31243;&#24230;&#65307;&#21644;&#65288;7&#65289;&#20219;&#21153;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neural-symbolic computation (NeSy) and statistical relational artificial intelligence (StarAI). NeSy aims to integrate symbolic reasoning and neural networks while StarAI focuses on integrating logic with probabilistic graphical models. The survey brings attention to seven shared dimensions between the two approaches. These dimensions are employed to categorize both fields and include: (1) the approach to logic inference, whether model or proof-based; (2) the syntax of logical theories; (3) the logic semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either the parameters alone or the entire logic theory; (5) the presence of symbolic and subsymbolic components in representations; (6) the degree to which the systems can capture the original logic, probabilistic, and neural paradigms; and (7) the classes of tasks the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRUMB&#30340;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#36890;&#36807;&#23384;&#20648;&#20869;&#23384;&#22359;&#30340;&#32034;&#24341;&#26469;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2104.02206</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#27969;&#23398;&#20064;&#30340;&#35843;&#20248;&#32452;&#21512;&#29305;&#24449;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRUMB&#30340;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#36890;&#36807;&#23384;&#20648;&#20869;&#23384;&#22359;&#30340;&#32034;&#24341;&#26469;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#22823;&#33041;&#20174;&#30636;&#26102;&#30340;&#19990;&#30028;&#32463;&#39564;&#20013;&#25552;&#21462;&#20986;&#25345;&#20037;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#30693;&#35782;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36828;&#36828;&#19981;&#33021;&#36798;&#21040;&#30456;&#21516;&#30340;&#27700;&#24179;&#65306;&#24403;&#34987;&#35201;&#27714;&#36890;&#36807;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#35757;&#32451;&#38750;&#37325;&#22797;&#35270;&#39057;&#24103;&#26469;&#23398;&#20064;&#23545;&#35937;&#20998;&#31867;&#26102;&#65288;&#22312;&#32447;&#27969;&#23398;&#20064;&#65289;&#65292;&#37027;&#20123;&#33021;&#22815;&#20174;&#37325;&#26032;&#25490;&#21015;&#30340;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#26032;&#30340;&#21050;&#28608;&#26102;&#20250;&#28798;&#38590;&#24615;&#22320;&#36951;&#24536;&#26087;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Compositional Replay Using Memory Blocks (CRUMB)&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#36890;&#29992;&#37096;&#20998;&#37325;&#24314;&#30340;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20018;&#32852;&#21487;&#35757;&#32451;&#21644;&#21487;&#37325;&#29992;&#30340;&#8220;&#20869;&#23384;&#22359;&#8221;&#21521;&#37327;&#65292;&#20197;&#32452;&#21512;&#26041;&#24335;&#37325;&#24314;&#29305;&#24449;&#22270;&#24352;&#37327;&#65292;&#23601;&#20687;&#38754;&#21253;&#23633;&#32452;&#21512;&#25104;&#19968;&#20010;&#38754;&#21253;&#19968;&#26679;&#12290;CRUMB&#23384;&#20648;&#29992;&#20110;&#37325;&#24314;&#26032;&#21050;&#28608;&#30340;&#20869;&#23384;&#22359;&#32034;&#24341;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#12290;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our brains extract durable, generalizable knowledge from transient experiences of the world. Artificial neural networks come nowhere close: when tasked with learning to classify objects by training on non-repeating video frames in temporal order (online stream learning), models that learn well from shuffled datasets catastrophically forget old knowledge upon learning new stimuli. We propose a new continual learning algorithm, Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB concatenates trainable and re-usable "memory block" vectors to compositionally reconstruct feature map tensors in convolutional neural networks, like crumbs forming a loaf of bread. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also primes the neural network to minimize catastrophic forgetting by for
&lt;/p&gt;</description></item></channel></rss>