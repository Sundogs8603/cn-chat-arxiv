<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01118</link><description>&lt;p&gt;
Pok\'eLLMon&#65306;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Pok\'emon&#23545;&#25112;&#30340;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#24403;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01118
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{Pok\'eLLMon}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20197;Pok\'emon&#23545;&#25112;&#20026;&#20363;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290; \textsc{Pok\'eLLMon}&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#65288;i&#65289;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#21363;&#26102;&#20351;&#29992;&#20174;&#23545;&#25112;&#20013;&#33719;&#24471;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21453;&#39304;&#26469;&#36880;&#27493;&#23436;&#21892;&#31574;&#30053;&#65307;&#65288;ii&#65289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#65292;&#21363;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#20197;&#23545;&#25239;&#20135;&#29983;&#24187;&#35273;&#29616;&#35937;&#65292;&#24182;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21450;&#26102;&#27491;&#30830;&#22320;&#34892;&#21160;&#65307;&#65288;iii&#65289;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#65292;&#20197;&#20943;&#36731;&#20195;&#29702;&#26426;&#22120;&#20154;&#38754;&#23545;&#24378;&#25932;&#26102;&#30340;&#8220;&#24778;&#24908;&#25442;&#25163;&#8221;&#29616;&#35937;&#65292;&#20351;&#20854;&#21487;&#20197;&#36867;&#36991;&#25112;&#26007;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#36827;&#34892;&#30340;&#22312;&#32447;&#23545;&#25112;&#20013;&#65292;\textsc{Pok\'eLLMon}&#37319;&#29992;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#20854;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#21487;&#29609;&#30340;&#25112;&#26007;&#26085;&#24535;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;\url{https://gith
&lt;/p&gt;
&lt;p&gt;
We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;GitHub&#19978;9,129&#20010;&#24320;&#28304;DL&#39033;&#30446;&#30340;&#21333;&#20803;&#27979;&#35797;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#32463;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;DL&#39033;&#30446;&#19982;&#24320;&#28304;&#39033;&#30446;&#25351;&#26631;&#21576;&#27491;&#30456;&#20851;&#65292;68%&#30340;DL&#39033;&#30446;&#26681;&#26412;&#27809;&#26377;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#24314;&#31435;&#20102;&#21333;&#20803;&#27979;&#35797;&#21644;DL&#39033;&#30446;&#20013;&#25925;&#38556;&#20043;&#38388;&#30340;&#26144;&#23556;&#20998;&#31867;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16546</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#24320;&#28304;&#28145;&#24230;&#23398;&#20064;&#39033;&#30446;&#20013;&#21333;&#20803;&#27979;&#35797;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16546
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;GitHub&#19978;9,129&#20010;&#24320;&#28304;DL&#39033;&#30446;&#30340;&#21333;&#20803;&#27979;&#35797;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#32463;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;DL&#39033;&#30446;&#19982;&#24320;&#28304;&#39033;&#30446;&#25351;&#26631;&#21576;&#27491;&#30456;&#20851;&#65292;68%&#30340;DL&#39033;&#30446;&#26681;&#26412;&#27809;&#26377;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#24314;&#31435;&#20102;&#21333;&#20803;&#27979;&#35797;&#21644;DL&#39033;&#30446;&#20013;&#25925;&#38556;&#20043;&#38388;&#30340;&#26144;&#23556;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#24050;&#32463;&#24555;&#36895;&#21457;&#23637;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26469;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#38656;&#35201;&#20687;&#23545;&#24453;&#21644;&#27979;&#35797;&#20854;&#20182;&#36719;&#20214;&#31995;&#32479;&#19968;&#26679;&#23545;&#24453;&#21644;&#27979;&#35797;DL&#39033;&#30446;&#36825;&#26679;&#30340;&#36719;&#20214;&#31995;&#32479;&#26102;&#65292;DL&#39033;&#30446;&#26159;&#21542;&#32463;&#36807;&#20102;&#24443;&#24213;&#27979;&#35797;&#25110;&#21151;&#33021;&#27491;&#30830;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;GitHub&#19978;&#30340;9,129&#20010;&#24320;&#28304;DL&#39033;&#30446;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20854;&#20013;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#32463;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;DL&#39033;&#30446;&#19982;&#24320;&#28304;&#39033;&#30446;&#25351;&#26631;&#21576;&#27491;&#30456;&#20851;&#65292;&#24182;&#19988;&#23545;&#25289;&#21462;&#35831;&#27714;&#26377;&#26356;&#39640;&#30340;&#25509;&#21463;&#29575;&#65292;2&#65289;&#26412;&#26679;&#26412;DL&#39033;&#30446;&#20013;&#26377;68%&#26681;&#26412;&#27809;&#26377;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#65292;3&#65289;DL&#27169;&#22411;&#30340;&#23618;(layer)&#21644;&#23454;&#29992;&#31243;&#24207;(utils)&#20855;&#26377;&#26368;&#22810;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#21644;&#20197;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21333;&#20803;&#27979;&#35797;&#21644;DL&#39033;&#30446;&#20013;&#30340;&#25925;&#38556;&#20043;&#38388;&#30340;&#26144;&#23556;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;Ne&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16546v1 Announce Type: cross  Abstract: Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: 1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests, 2) 68% of the sampled DL projects are not unit tested at all, 3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the ne
&lt;/p&gt;</description></item><item><title>RoboGrind&#26159;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#12289;&#20132;&#20114;&#24335;&#35821;&#38899;&#25511;&#21046;&#21521;&#23548;&#31995;&#32479;&#21644;&#33258;&#21160;&#35268;&#21010;&#25191;&#34892;&#27969;&#27700;&#32447;&#23454;&#29616;&#24037;&#19994;&#26426;&#22120;&#20154;&#23545;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#30340;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#33258;&#21160;&#21270;&#65292;&#20026;&#37325;&#21046;&#29627;&#29827;&#32420;&#32500;&#39118;&#21147;&#28065;&#36718;&#21494;&#29255;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.16542</link><description>&lt;p&gt;
RoboGrind&#65306;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#30452;&#35266;&#20132;&#20114;&#24335;&#34920;&#38754;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16542
&lt;/p&gt;
&lt;p&gt;
RoboGrind&#26159;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#12289;&#20132;&#20114;&#24335;&#35821;&#38899;&#25511;&#21046;&#21521;&#23548;&#31995;&#32479;&#21644;&#33258;&#21160;&#35268;&#21010;&#25191;&#34892;&#27969;&#27700;&#32447;&#23454;&#29616;&#24037;&#19994;&#26426;&#22120;&#20154;&#23545;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#30340;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#33258;&#21160;&#21270;&#65292;&#20026;&#37325;&#21046;&#29627;&#29827;&#32420;&#32500;&#39118;&#21147;&#28065;&#36718;&#21494;&#29255;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16542v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#35832;&#22914;&#30952;&#21066;&#12289;&#25171;&#30952;&#25110;&#25243;&#20809;&#20043;&#31867;&#30340;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#26159;&#35768;&#22810;&#34892;&#19994;&#20215;&#20540;&#38142;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#20294;&#33258;&#21160;&#21270;&#22788;&#29702;&#36825;&#20123;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoboGrind&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#24037;&#19994;&#26426;&#22120;&#20154;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#22320;&#33258;&#21160;&#21270;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;3D&#24863;&#30693;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#34920;&#38754;&#25195;&#25551;&#21644;&#33258;&#21160;&#32570;&#38519;&#35782;&#21035;&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#35821;&#38899;&#25511;&#21046;&#21521;&#23548;&#31995;&#32479;&#65292;&#29992;&#20110;AI&#36741;&#21161;&#21021;&#22987;&#21270;&#21644;&#21442;&#25968;&#21270;&#26426;&#22120;&#20154;&#31243;&#24207;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#21147;&#25511;&#21046;&#24335;&#26426;&#22120;&#20154;&#34920;&#38754;&#22788;&#29702;&#30340;&#33258;&#21160;&#35268;&#21010;&#21644;&#25191;&#34892;&#27969;&#27700;&#32447;&#12290;RoboGrind&#22312;&#23454;&#39564;&#23460;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20027;&#35201;&#26159;&#29992;&#20110;&#37325;&#21046;&#29627;&#29827;&#32420;&#32500;&#39118;&#21147;&#28065;&#36718;&#21494;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16542v1 Announce Type: cross  Abstract: Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22235;&#21313;&#22810;&#24180;&#21069;&#30340;&#22270;&#23572;&#25991;&#27979;&#35797;&#26694;&#26550;&#26159;&#21542;&#23545;LLM&#30340;&#35760;&#24518;&#34892;&#20026;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.16505</link><description>&lt;p&gt;
&#35760;&#24518;&#24046;&#36317;&#65306;LLM &#26159;&#21542;&#33021;&#36890;&#36807;&#22270;&#23572;&#25991;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Memory GAPS: Would LLM pass the Tulving Test?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22235;&#21313;&#22810;&#24180;&#21069;&#30340;&#22270;&#23572;&#25991;&#27979;&#35797;&#26694;&#26550;&#26159;&#21542;&#23545;LLM&#30340;&#35760;&#24518;&#34892;&#20026;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16505v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22270;&#23572;&#25991;&#27979;&#35797;&#26088;&#22312;&#30740;&#31350;&#35748;&#30693;&#21644;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#35760;&#24518;&#34920;&#29616;&#12290;&#20854;&#32467;&#26524;&#26377;&#21161;&#20110;&#35780;&#20272;&#35760;&#24518;&#30340;&#8220;&#21327;&#21516;&#24341;&#23548;&#27169;&#22411;&#8221;&#21450;&#31867;&#20284;RK&#33539;&#20363;&#22312;&#20154;&#31867;&#34920;&#29616;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#30528;&#25163;&#30740;&#31350;&#36825;&#20010;&#24050;&#26377;&#22235;&#21313;&#22810;&#24180;&#21382;&#21490;&#30340;&#26694;&#26550;&#26159;&#21542;&#33021;&#20026;LLM&#30340;&#35760;&#24518;&#34892;&#20026;&#24102;&#26469;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16505v1 Announce Type: new  Abstract: The Tulving Test was designed to investigate memory performance in recognition and recall tasks. Its results help assess the relevance of the "Synergistic Ecphory Model" of memory and similar RK paradigms in human performance. This paper starts investigating whether the more than forty-year-old framework sheds some light on LLMs' acts of remembering.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#25193;&#23637;&#19988;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#30456;&#20284;&#24230;&#23398;&#20064;&#26469;&#21306;&#20998;&#24191;&#27867;&#33539;&#22260;&#30340;&#20891;&#29992;&#21644;&#27665;&#29992;&#39134;&#26426;&#29305;&#24449;&#65292;&#23454;&#29616;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#21644;&#26032;&#39062;&#39134;&#26426;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16486</link><description>&lt;p&gt;
&#26234;&#33021;&#39134;&#26426;&#35782;&#21035;&#65306;&#20174;&#20998;&#31867;&#21040;&#30456;&#20284;&#24230;&#23398;&#20064;&#30340;&#36716;&#21464;&#65292;&#20197;&#29992;&#20110;&#20316;&#25112;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16486
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#25193;&#23637;&#19988;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#30456;&#20284;&#24230;&#23398;&#20064;&#26469;&#21306;&#20998;&#24191;&#27867;&#33539;&#22260;&#30340;&#20891;&#29992;&#21644;&#27665;&#29992;&#39134;&#26426;&#29305;&#24449;&#65292;&#23454;&#29616;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#21644;&#26032;&#39062;&#39134;&#26426;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#31934;&#30830;&#35782;&#21035;&#39134;&#26426;&#26159;&#33322;&#31354;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#22312;&#20316;&#25112;&#35782;&#21035;&#20013;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#25193;&#23637;&#19988;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#36827;&#34892;&#20316;&#25112;&#35782;&#21035;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#22411;&#20197;&#21450;&#26032;&#39062;/&#26410;&#30693;&#31867;&#22411;&#30340;&#39134;&#26426;&#12290;&#20256;&#32479;&#26041;&#27861;&#12289;&#20154;&#31867;&#19987;&#23478;&#39537;&#21160;&#30340;&#20316;&#25112;&#35782;&#21035;&#21644;&#22270;&#20687;&#20998;&#31867;&#22312;&#35782;&#21035;&#26032;&#39062;&#31867;&#21035;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#30456;&#20284;&#24230;&#23398;&#20064;&#26469;&#21306;&#20998;&#24191;&#27867;&#33539;&#22260;&#30340;&#20891;&#29992;&#21644;&#27665;&#29992;&#39134;&#26426;&#30340;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#39062;&#30340;&#39134;&#26426;&#31867;&#22411;&#65292;&#21033;&#29992;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#35782;&#21035;&#21644;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#39134;&#26426;&#31867;&#22411;&#20998;&#31867;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#20998;&#36776;&#29575;&#36965;&#24863;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#20197;&#36866;&#24212;&#22810;&#26679;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16486v1 Announce Type: cross  Abstract: Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#31181;&#21151;&#33021;&#21270;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#21040;&#27169;&#25311;&#65288;Lang2Sim&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#27169;&#25311;&#22120;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16482</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27169;&#25311;&#24341;&#25806;&#36827;&#34892;&#35821;&#35328;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Languaging a Simulation Engine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16482
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#31181;&#21151;&#33021;&#21270;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#21040;&#27169;&#25311;&#65288;Lang2Sim&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#27169;&#25311;&#22120;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#32534;&#31243;&#26448;&#26009;&#27169;&#25311;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#23558;&#20154;&#31867;&#35821;&#35328;&#31934;&#30830;&#36716;&#21270;&#20026;&#23450;&#21046;&#27169;&#25311;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#21151;&#33021;&#21270;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#21040;&#27169;&#25311;&#65288;Lang2Sim&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#20132;&#20114;&#24335;&#23548;&#33322;&#65292;&#36890;&#36807;&#20197;&#22810;&#23380;&#30697;&#38453;&#20013;&#27700;&#21560;&#38468;&#30340;&#22330;&#26223;&#23454;&#20363;&#20026;&#20363;&#23545;&#27169;&#25311;&#24341;&#25806;&#36827;&#34892;&#35821;&#35328;&#21270;&#12290;&#19982;&#36880;&#34892;&#32534;&#30721;&#30446;&#26631;&#27169;&#25311;&#22120;&#19981;&#21516;&#65292;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27599;&#20010;&#27169;&#25311;&#22120;&#20026;&#20855;&#26377;&#19981;&#21464;&#24037;&#20855;&#21151;&#33021;&#21450;&#20854;&#21464;&#20307;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#24635;&#20307;&#12290; Lang2Sim&#36890;&#36807;&#21151;&#33021;&#21270;&#21644;&#24207;&#21015;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#24212;&#22320;&#65292;&#23545;&#24037;&#20855;&#20998;&#31867;&#36827;&#34892;&#21512;&#29702;&#21270;&#65292;&#23450;&#21046;&#20854;&#36755;&#20837;-&#36755;&#20986;&#32452;&#21512;&#65292;&#24182;&#23558;&#27169;&#25311;&#22120;&#36755;&#20837;&#31934;&#28860;&#20026;&#21487;&#25191;&#34892;&#26684;&#24335;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16482v1 Announce Type: new  Abstract: Language model intelligence is revolutionizing the way we program materials simulations. However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format. Importantly, dependin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;</title><link>https://arxiv.org/abs/2402.16472</link><description>&lt;p&gt;
mEdIT: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
mEdIT: Multilingual Text Editing via Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;CoEdIT&#26159;&#26368;&#36817;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#65292;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#12290;mEdIT&#27169;&#22411;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#12290;&#23427;&#20204;&#26088;&#22312;&#25509;&#25910;&#29992;&#25143;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#23646;&#24615;&#30340;&#25351;&#20196;&#65292;&#20363;&#22914;Grammatik korrigieren&#65288;&#24503;&#35821;&#65289;&#25110;Parafrasee la oraci&#243;n&#65288;&#35199;&#29677;&#29273;&#35821;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25991;&#26412;&#32534;&#36753;&#25968;&#25454;&#38598;&#20013;&#31574;&#21010;&#25968;&#25454;&#65292;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35821;&#31995;&#30340;&#22810;&#35821;&#35328;&#65292;&#20026;&#19977;&#20010;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#65288;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;GEC&#65289;&#12289;&#25991;&#26412;&#31616;&#21270;&#21644;&#25913;&#20889;&#65289;&#26500;&#24314;&#20102;mEdIT&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;mEdIT&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#38598;&#19978;&#19982;&#20854;&#20182;&#22810;&#35821;&#35328;LLMs&#30456;&#27604;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;mEdIT gen
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16472v1 Announce Type: cross  Abstract: We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT gen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16459</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending LLMs against Jailbreaking Attacks via Backtranslation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#35757;&#32451;&#25104;&#25298;&#32477;&#26377;&#23475;&#35831;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#37325;&#20889;&#21407;&#22987;&#25552;&#31034;&#20197;&#38544;&#34255;&#20854;&#26377;&#23475;&#24847;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#21453;&#21521;&#32763;&#35793;&#8221;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30446;&#26631;LLM&#20174;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#30340;&#21021;&#22987;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#26029;&#21487;&#20197;&#23548;&#33268;&#35813;&#21709;&#24212;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#25512;&#26029;&#30340;&#25552;&#31034;&#31216;&#20026;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#20542;&#21521;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#22240;&#20026;&#23427;&#26159;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#30340;&#65292;&#19981;&#26159;&#30452;&#25509;&#30001;&#25915;&#20987;&#32773;&#25805;&#32437;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#22312;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19978;&#36816;&#34892;&#30446;&#26631;LLM&#65292;&#22914;&#26524;&#27169;&#22411;&#25298;&#32477;&#20102;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#21017;&#25298;&#32477;&#21407;&#22987;&#25552;&#31034;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#20960;&#20010;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#21457;&#24067;&#30340;&#26041;&#27861;&#21644;&#21033;&#29992;&#24494;&#20998;&#36827;&#21270;&#20316;&#20026;&#20248;&#21270;&#26426;&#21046;&#20043;&#19968;&#30340;&#25216;&#26415;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#19978;&#30340;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16455</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#19978;&#65292;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16455
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#26041;&#27861;&#21644;&#21033;&#29992;&#24494;&#20998;&#36827;&#21270;&#20316;&#20026;&#20248;&#21270;&#26426;&#21046;&#20043;&#19968;&#30340;&#25216;&#26415;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#19978;&#30340;&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;SAEAs&#65289;&#36817;&#24180;&#26469;&#26159;&#26368;&#24191;&#27867;&#30740;&#31350;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20854;&#35299;&#20915;&#26114;&#36149;&#30340;&#30495;&#23454;&#19990;&#30028;&#20248;&#21270;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26032;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#19982;&#20854;&#20182;&#25216;&#26415;&#30340;&#22522;&#20934;&#27979;&#35797;&#20960;&#20046;&#23436;&#20840;&#20381;&#36182;&#20154;&#20026;&#21019;&#24314;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20351;&#29992;&#20004;&#20010;&#30495;&#23454;&#30340;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#21313;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#21333;&#30446;&#26631;SAEA&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20197;&#21450;&#36873;&#23450;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#26469;&#20998;&#26512;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#26356;&#36817;&#26399;&#21457;&#24067;&#30340;&#26041;&#27861;&#20197;&#21450;&#21033;&#29992;&#24494;&#20998;&#36827;&#21270;&#20316;&#20026;&#20854;&#20248;&#21270;&#26426;&#21046;&#20043;&#19968;&#30340;&#25216;&#26415;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#32771;&#34385;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16455v1 Announce Type: cross  Abstract: Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the most widely studied methods for their capability to solve expensive real-world optimization problems. However, the development of new methods and benchmarking with other techniques still relies almost exclusively on artificially created problems. In this paper, we use two real-world computational fluid dynamics problems to compare the performance of eleven state-of-the-art single-objective SAEAs. We analyze the performance by investigating the quality and robustness of the obtained solutions and the convergence properties of the selected methods. Our findings suggest that the more recently published methods, as well as the techniques that utilize differential evolution as one of their optimization mechanisms, perform significantly better than the other considered methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LiDAR&#30340;&#30446;&#26631;&#23548;&#21521;&#21644;&#21208;&#25506;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#38556;&#30861;&#29289;&#36991;&#20813;&#22312;&#26410;&#30693;&#21160;&#24577;&#22810;&#38556;&#30861;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#25511;&#21046;</title><link>https://arxiv.org/abs/2402.16449</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#21160;&#24577;&#22810;&#38556;&#30861;&#29615;&#22659;&#20013;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#32447;&#39640;&#25928;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16449
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LiDAR&#30340;&#30446;&#26631;&#23548;&#21521;&#21644;&#21208;&#25506;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#38556;&#30861;&#29289;&#36991;&#20813;&#22312;&#26410;&#30693;&#21160;&#24577;&#22810;&#38556;&#30861;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LiDAR&#30340;&#30446;&#26631;&#23548;&#21521;&#21644;&#21208;&#25506;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#38556;&#30861;&#29289;&#36991;&#20813;&#22312;&#30001;&#38745;&#24577;&#21644;&#31227;&#21160;&#38556;&#30861;&#29289;&#22635;&#28385;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#20256;&#32479;&#21160;&#24577;&#25511;&#21046;&#23631;&#38556;&#21151;&#33021;&#65288;D-CBFs&#65289;&#25152;&#38754;&#20020;&#30340;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#22312;&#32447;&#26500;&#24314;&#21644;&#30001;&#20110;&#20351;&#29992;&#22810;&#20010;D-CBFs&#23548;&#33268;&#30340;&#23454;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#30340;&#24863;&#30693;&#32452;&#20214;&#36890;&#36807;DBSCAN&#31639;&#27861;&#23545;&#28857;&#20113;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#21033;&#29992;&#26368;&#23567;&#30028;&#38480;&#26925;&#29699;&#65288;MBEs&#65289;&#31639;&#27861;&#23558;&#36825;&#20123;&#32858;&#31867;&#36827;&#34892;&#23553;&#35013;&#65292;&#21019;&#36896;&#20986;&#26925;&#22278;&#24418;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#24403;&#21069;&#29366;&#24577;&#30340;MBEs&#19982;&#20043;&#21069;&#23384;&#20648;&#30340;MBEs&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#21306;&#20998;&#65292;&#24182;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#26469;&#39044;&#27979;&#21518;&#32773;&#30340;&#31227;&#21160;&#12290;&#36825;&#31181;&#20998;&#26512;&#26377;&#21161;&#20110;&#23454;&#26045;D-CB
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16449v1 Announce Type: cross  Abstract: This paper proposes a LiDAR-based goal-seeking and exploration framework, addressing the efficiency of online obstacle avoidance in unstructured environments populated with static and moving obstacles. This framework addresses two significant challenges associated with traditional dynamic control barrier functions (D-CBFs): their online construction and the diminished real-time performance caused by utilizing multiple D-CBFs. To tackle the first challenge, the framework's perception component begins with clustering point clouds via the DBSCAN algorithm, followed by encapsulating these clusters with the minimum bounding ellipses (MBEs) algorithm to create elliptical representations. By comparing the current state of MBEs with those stored from previous moments, the differentiation between static and dynamic obstacles is realized, and the Kalman filter is utilized to predict the movements of the latter. Such analysis facilitates the D-CB
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16442</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37197;&#23545;&#27425;&#27169;&#27169;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22823;&#20110;&#20869;&#23384;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#21462;&#20915;&#20110;&#23376;&#38598;&#36873;&#25321;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#32452;&#37325;&#35201;&#21644;&#20195;&#34920;&#24615;&#30340;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#20272;&#35745;&#36817;&#20284;&#20445;&#35777;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16442v1 Announce Type: cross  Abstract: Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16435</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Implicit Generative Models via an Invariant Statistical Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#38656;&#35201;&#36890;&#36807;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#19968;&#32500;&#65288;1D&#65289;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#38543;&#21518;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#20197;&#36866;&#24212;&#22810;&#21464;&#37327;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#27169;&#22411;&#26679;&#26412;&#32463;&#36807;&#36866;&#24403;&#36873;&#25321;&#30340;&#21464;&#25442;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#65307;&#22240;&#27492;&#65292;&#23427;&#23545;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#19968;&#32500;&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#37325;&#21442;&#25968;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16435v1 Announce Type: cross  Abstract: Implicit generative models have the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization 
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#24067;&#23616;&#37319;&#26679;&#26041;&#27861;&#65292;Distributional Edge Layouts&#65288;DELs&#65289;&#65292;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#21644;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#33021;&#22815;&#25429;&#33719;&#24191;&#27867;&#30340;&#33021;&#37327;&#20998;&#24067;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16402</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#36793;&#24067;&#23616;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Learning with Distributional Edge Layouts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16402
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#24067;&#23616;&#37319;&#26679;&#26041;&#27861;&#65292;Distributional Edge Layouts&#65288;DELs&#65289;&#65292;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#21644;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#33021;&#22815;&#25429;&#33719;&#24191;&#27867;&#30340;&#33021;&#37327;&#20998;&#24067;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#22312;&#26576;&#20123;&#25299;&#25169;&#24067;&#23616;&#19978;&#27839;&#30528;&#36793;&#22312;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#23616;&#37096;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22312;&#29616;&#20195;GNNs&#20013;&#65292;&#36825;&#20123;&#25299;&#25169;&#24067;&#23616;&#36890;&#24120;&#26159;&#25353;&#29031;&#30830;&#23450;&#24615;&#35745;&#31639;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;GNNs&#65289;&#25110;&#22312;&#21551;&#21457;&#24615;&#20551;&#35774;&#19979;&#26412;&#22320;&#37319;&#26679;&#65288;&#20363;&#22914;&#65292;GraphSage&#65289;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#36825;&#20123;&#24067;&#23616;&#21487;&#20197;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#20840;&#23616;&#37319;&#26679;&#65292;&#36981;&#24490;&#37197;&#22791;&#26126;&#30830;&#29289;&#29702;&#33021;&#37327;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#26356;&#20855;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#19968;&#32452;&#37319;&#26679;/&#20248;&#21270;&#30340;&#24067;&#23616;&#21487;&#20197;&#25429;&#33719;&#24191;&#27867;&#30340;&#33021;&#37327;&#20998;&#24067;&#65292;&#24182;&#22312;WL-test&#20043;&#19978;&#24102;&#26469;&#39069;&#22806;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#31616;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#36793;&#24067;&#23616;&#65288;DELs&#65289;&#20316;&#20026;&#21508;&#31181;GNNs&#30340;&#34917;&#20805;&#12290;DEL&#26159;&#19968;&#20010;&#19982;&#21518;&#32493;GNN&#21464;&#31181;&#26080;&#20851;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65292;&#22240;&#27492;&#38750;&#24120;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16402v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts. Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions. In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world. We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs. DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly fl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26377;&#25928;&#30340;&#21487;&#36801;&#31227;&#25915;&#20987;&#32773;&#65292;&#22635;&#34917;&#20102;&#28145;&#24230;&#27700;&#21360;&#25216;&#26415;&#22312;&#38754;&#23545;&#25830;&#38500;&#21644;&#31713;&#25913;&#39118;&#38505;&#26102;&#30340;&#33030;&#24369;&#24615;&#36825;&#19968;&#39046;&#22495;&#32570;&#20047;&#31995;&#32479;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.16397</link><description>&lt;p&gt;
&#25506;&#32034;&#28145;&#24230;&#27700;&#21360;&#23433;&#20840;&#24615;&#65306;&#23545;&#25239;&#24615;&#21487;&#36801;&#31227;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Investigating Deep Watermark Security: An Adversarial Transferability Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26377;&#25928;&#30340;&#21487;&#36801;&#31227;&#25915;&#20987;&#32773;&#65292;&#22635;&#34917;&#20102;&#28145;&#24230;&#27700;&#21360;&#25216;&#26415;&#22312;&#38754;&#23545;&#25830;&#38500;&#21644;&#31713;&#25913;&#39118;&#38505;&#26102;&#30340;&#33030;&#24369;&#24615;&#36825;&#19968;&#39046;&#22495;&#32570;&#20047;&#31995;&#32479;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#20852;&#36215;&#24341;&#21457;&#20102;&#23545;&#29983;&#25104;&#20869;&#23481;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#20445;&#25252;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#28145;&#24230;&#27700;&#21360;&#25216;&#26415;&#20197;&#20854;&#22312;IP&#20445;&#25252;&#20013;&#30340;&#28789;&#27963;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#24615;&#21487;&#36801;&#31227;&#25915;&#20987;&#30340;&#28608;&#22686;&#32473;&#28145;&#24230;&#27700;&#21360;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#65292;&#36825;&#20010;&#39046;&#22495;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26377;&#25928;&#30340;&#21487;&#36801;&#31227;&#25915;&#20987;&#32773;&#26469;&#35780;&#20272;&#28145;&#24230;&#27700;&#21360;&#22312;&#38754;&#23545;&#25830;&#38500;&#21644;&#31713;&#25913;&#39118;&#38505;&#26102;&#30340;&#33030;&#24369;&#24615;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#23616;&#37096;&#26679;&#26412;&#23494;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#23427;&#25512;&#23548;&#20986;&#26377;&#20851;&#27169;&#22411;&#36755;&#20986;&#19968;&#33268;&#24615;&#30340;&#23450;&#29702;&#12290;&#22312;&#21457;&#29616;&#23558;&#26679;&#26412;&#25200;&#21160;&#33267;&#30446;&#26631;&#31867;&#21035;&#39640;&#23494;&#24230;&#21306;&#22495;&#65288;HSDR&#65289;&#26377;&#21161;&#20110;&#22686;&#24378;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#21487;&#36801;&#31227;&#24615;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Easy Sample Selection (ESS) &#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16397v1 Announce Type: cross  Abstract: The rise of generative neural networks has triggered an increased demand for intellectual property (IP) protection in generated content. Deep watermarking techniques, recognized for their flexibility in IP protection, have garnered significant attention. However, the surge in adversarial transferable attacks poses unprecedented challenges to the security of deep watermarking techniques-an area currently lacking systematic investigation. This study fills this gap by introducing two effective transferable attackers to assess the vulnerability of deep watermarks against erasure and tampering risks. Specifically, we initially define the concept of local sample density, utilizing it to deduce theorems on the consistency of model outputs. Upon discovering that perturbing samples towards high sample density regions (HSDR) of the target class enhances targeted adversarial transferability, we propose the Easy Sample Selection (ESS) mechanism an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;MoZIP&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;IP-oriented&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MoZi&#65292;&#23454;&#39564;&#35777;&#26126;MoZi&#22312;MoZIP&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.16389</link><description>&lt;p&gt;
MoZIP&#65306;&#35780;&#20272;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;MoZIP&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;IP-oriented&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MoZi&#65292;&#23454;&#39564;&#35777;&#26126;MoZi&#22312;MoZIP&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23545;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#20363;&#22914;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#39046;&#22495;&#65289;&#30340;&#34920;&#29616;&#36824;&#20102;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21363;&#31532;&#19968;&#20010;&#38754;&#21521;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#22810;&#35821;&#35328;&#26234;&#21147;&#20135;&#26435;&#27979;&#39564;(MoZIP)&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;MoZIP&#22522;&#20934;&#21253;&#25324;&#19977;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#30693;&#35782;&#20135;&#26435;&#22810;&#39033;&#36873;&#25321;&#27979;&#39564;&#65288;IPQuiz&#65289;&#12289;&#30693;&#35782;&#20135;&#26435;&#38382;&#31572;&#65288;IPQA&#65289;&#21644;&#19987;&#21033;&#21305;&#37197;&#65288;PatentMatch&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#38754;&#21521;&#30693;&#35782;&#20135;&#26435;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#31216;&#20026;MoZi&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;BLOOMZ&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;IP&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;MoZIP&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;MoZi&#27169;&#22411;&#21644;&#22235;&#31181;&#30693;&#21517;LLMs&#65288;&#21363;BLOOMZ&#12289;BELLE&#12289;ChatGLM&#21644;ChatGPT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoZi&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16389v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;Simplified-Temporal-Graph-Network&#12290;</title><link>https://arxiv.org/abs/2402.16387</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#38388;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#29702;&#35770;&#35265;&#35299;&#19982;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;Simplified-Temporal-Graph-Network&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#65288;TGL&#65289;&#24050;&#25104;&#20026;&#19981;&#21516;&#30495;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#22270;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;TGL&#22312;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#26377;&#38480;&#23485;&#36229;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#19981;&#21516;TGL&#31639;&#27861;&#65288;&#22914;&#22522;&#20110;GNN&#12289;&#22522;&#20110;RNN&#21644;&#22522;&#20110;&#20869;&#23384;&#30340;&#26041;&#27861;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;TGL&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;GNN-/RNN- TGL&#26041;&#27861;&#20013;&#8220;&#23618;&#25968;/&#27493;&#25968;&#8221;&#20197;&#21450;&#29305;&#24449;-&#26631;&#31614;&#23545;&#40784;&#65288;FLA&#65289;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20854;&#20013;FLA&#21487;&#29992;&#20316;&#34920;&#36798;&#33021;&#21147;&#30340;&#20195;&#29702;&#65292;&#24182;&#35299;&#37322;&#20102;&#22522;&#20110;&#20869;&#23384;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21270;&#26102;&#38388;&#22270;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16387v1 Announce Type: cross  Abstract: Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and "the number of layers/steps" in the GNN-/RNN-based TGL methods and "the feature-label alignment (FLA) score", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generaliza
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#12289;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#20197;&#21450;&#24405;&#38899;&#26684;&#24335;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.16380</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#36716;&#35821;&#38899;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#24320;&#28304;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#12289;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#20197;&#21450;&#24405;&#38899;&#26684;&#24335;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#22522;&#20110;&#35821;&#38899;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#20869;&#23481;&#21019;&#20316;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20869;&#23481;&#65292;&#32763;&#35793;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#23545;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#24613;&#20999;&#38656;&#27714;&#12290;&#26412;&#20316;&#21697;&#30340;&#36129;&#29486;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#65306;&#23558;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#21040;&#26679;&#26412;&#36873;&#25321;&#20013;&#65292;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#65292;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#65292;&#20197;&#21450;&#22788;&#29702;&#24405;&#38899;&#20197;&#28385;&#36275;&#25351;&#23450;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16380v1 Announce Type: cross  Abstract: Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16379</link><description>&lt;p&gt;
&#29992;&#31995;&#32479;&#33258;&#26657;&#27491;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving LLM-based Machine Translation with Systematic Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16379
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#20180;&#32454;&#35780;&#20272;&#21457;&#29616;&#65292;LLMs&#29983;&#25104;&#30340;&#32763;&#35793;&#20173;&#28982;&#21253;&#21547;&#22810;&#20010;&#38169;&#35823;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23558;&#36825;&#31181;&#38169;&#35823;&#20449;&#24687;&#21453;&#39304;&#21040;LLMs&#20013;&#21487;&#20197;&#23454;&#29616;&#33258;&#26657;&#27491;&#65292;&#24182;&#25913;&#21892;&#32763;&#35793;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#20195;&#34920;&#20102;&#22312;&#36825;&#19968;&#26041;&#21521;&#19978;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#25105;&#20204;&#30340;&#33258;&#26657;&#27491;&#26694;&#26550;&#25104;&#21151;&#22320;&#24110;&#21161;LLMs&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36824;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#36824;&#26159;&#22260;&#32469;&#20854;&#20182;&#35821;&#35328;&#65307;2&#65289;TER&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#27491;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65292;&#32780;&#27492;&#35843;&#26597;&#35770;&#25991;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#29983;&#25104;AI&#25193;&#25955;&#21644;&#20256;&#32479;&#27169;&#22411;&#30340;&#22522;&#26412;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16369</link><description>&lt;p&gt;
&#35270;&#35273;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#27169;&#22411;&#12289;&#24230;&#37327;&#21644;&#24212;&#29992;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI in Vision: A Survey on Models, Metrics and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16369
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#27491;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65292;&#32780;&#27492;&#35843;&#26597;&#35770;&#25991;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#29983;&#25104;AI&#25193;&#25955;&#21644;&#20256;&#32479;&#27169;&#22411;&#30340;&#22522;&#26412;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#21019;&#24314;&#65292;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#29983;&#25104;AI&#25193;&#25955;&#21644;&#20256;&#32479;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#22522;&#26412;&#25216;&#26415;&#12289;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#20197;&#21450;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21644;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24314;&#27169;&#31561;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#22270;&#20687;&#20462;&#34917;&#21644;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#31561;&#22810;&#26679;&#21270;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#21644;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#32508;&#21512;&#29616;&#26377;&#30740;&#31350;&#24182;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#20851;&#20110;&#29983;&#25104;AI&#22312;&#35270;&#35273;&#20013;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16369v1 Announce Type: cross  Abstract: Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;Transformer-based&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#65288;LR-Drop&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#35757;&#32451;&#31574;&#30053;&#36880;&#23618;&#23545;&#27599;&#20010;Transformer&#23618;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#36755;&#20986;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16361</link><description>&lt;p&gt;
&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#29992;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Regularized Dropout for Neural Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;Transformer-based&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#65288;LR-Drop&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#35757;&#32451;&#31574;&#30053;&#36880;&#23618;&#23545;&#27599;&#20010;Transformer&#23618;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#36755;&#20986;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#27969;&#34892;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;dropout&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#19981;&#21487;&#25110;&#32570;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;dropout&#38543;&#26426;&#24615;&#24341;&#36215;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#19981;&#19968;&#33268;&#24615;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#23545;&#36755;&#20986;&#23618;&#30340;dropout&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#65288;LR-Drop&#65289;&#65292;&#19987;&#20026;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LR-Drop&#20351;&#29992;&#23618;&#27425;&#19968;&#33268;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#36880;&#23618;&#23545;&#27599;&#20010;Transformer&#23618;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#36890;&#36807;dropout&#37319;&#26679;&#30340;&#20004;&#20010;&#23402;&#29983;&#23376;&#27169;&#22411;&#65292;&#28982;&#21518;LR-Drop&#24378;&#21046;&#20351;&#20004;&#20010;&#23402;&#29983;&#23376;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#36755;&#20986;&#20998;&#24067;&#20445;&#25345;&#19968;&#33268;&#12290;&#25152;&#25552;&#20986;&#30340;LR-Drop&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#8220;&#33258;&#33976;&#39311;&#8221;&#26694;&#26550;&#65292;&#20854;&#20013;dropout&#29983;&#25104;&#30340;&#27599;&#20010;&#23376;&#27169;&#22411;&#37117;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16361v1 Announce Type: cross  Abstract: Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a "self-distillation" framework, in which each sub-model generated by dropout is the other's "teacher" model and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16354</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-guided Skill Learning with Temporal Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#21457;&#29616;&#25216;&#33021;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20986;&#36712;&#36857;&#30340;&#21021;&#22987;&#20998;&#21106;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20998;&#23618;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#23558;LLM&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#36712;&#36857;&#27573;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25511;&#21046;&#21387;&#32553;&#21644;&#21487;&#37325;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#30340;&#26032;&#36741;&#21161;&#30446;&#26631;&#65292;&#24110;&#21161;&#24341;&#23548;&#36825;&#31181;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;Agent&#33021;&#22815;&#21457;&#29616;&#26377;&#21161;&#20110;&#21152;&#36895;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#22312;BabyAI&#65288;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#29615;&#22659;&#65289;&#20197;&#21450;ALFRED&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#29615;&#22659;&#65289;&#30340;&#26032;&#38271;&#26399;&#20219;&#21153;&#20013;&#32988;&#36807;&#22522;&#32447;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
&lt;/p&gt;</description></item><item><title>MathGenie&#36890;&#36807;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#23478;&#26063;&#21270;&#30340;&#27169;&#22411;&#31995;&#21015;MathGenieLM&#12290;</title><link>https://arxiv.org/abs/2402.16352</link><description>&lt;p&gt;
MathGenie: &#20351;&#29992;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16352
&lt;/p&gt;
&lt;p&gt;
MathGenie&#36890;&#36807;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#23478;&#26063;&#21270;&#30340;&#27169;&#22411;&#31995;&#21015;MathGenieLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24320;&#28304;&#27169;&#22411;&#21644;GPT-4&#31561;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#22312;&#36825;&#19968;&#39046;&#22495;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MathGenie&#65292;&#29992;&#20110;&#20174;&#23567;&#35268;&#27169;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#31181;&#23376;&#25968;&#25454;&#65289;&#20013;&#29983;&#25104;&#22810;&#26679;&#19988;&#21487;&#38752;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#20805;&#20102;&#31181;&#23376;&#25968;&#25454;&#30340;&#30495;&#23454;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21453;&#21521;&#32763;&#35793;&#27169;&#22411;&#65292;&#23558;&#25193;&#20805;&#30340;&#35299;&#20915;&#26041;&#26696;&#32763;&#35793;&#22238;&#26032;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#20102;&#38598;&#25104;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#30830;&#20445;&#38598;&#25104;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#39564;&#35777;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#26032;&#31579;&#36873;&#30340;&#25968;&#25454;&#19978;&#23545;&#20174;7B&#21040;70B&#19981;&#31561;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#31216;&#20026;MathGenieLM&#30340;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16352v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. Th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21033;&#29992;&#21452;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#25913;&#21892;&#34892;&#26143;&#28459;&#28216;&#36710;&#31359;&#36234;&#35268;&#21010;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2402.16342</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#31354;&#38388;&#20219;&#21153;&#30340;&#24212;&#24613;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Contingency Planning Using Bi-level Markov Decision Processes for Space Missions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21033;&#29992;&#21452;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#25913;&#21892;&#34892;&#26143;&#28459;&#28216;&#36710;&#31359;&#36234;&#35268;&#21010;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#31185;&#23398;&#20219;&#21153;&#30340;&#33258;&#20027;&#24212;&#24613;&#35268;&#21010;&#65292;&#36890;&#36807;&#22312;&#20986;&#29616;&#24310;&#36831;&#25110;&#20559;&#31163;&#27491;&#24120;&#20219;&#21153;&#35745;&#21010;&#26102;&#65292;&#20174;&#29366;&#24577;&#31354;&#38388;&#20013;&#20219;&#20309;&#38750;&#27491;&#24120;&#28857;&#36827;&#34892;&#24555;&#36895;&#31574;&#30053;&#35745;&#31639;&#12290;&#25104;&#21151;&#30340;&#24212;&#24613;&#35268;&#21010;&#28041;&#21450;&#22312;&#38543;&#26426;&#24773;&#26223;&#20013;&#31649;&#29702;&#39118;&#38505;&#21644;&#22238;&#25253;&#65292;&#36890;&#24120;&#19982;&#34892;&#21160;&#27010;&#29575;&#30456;&#20851;&#12290;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#29992;&#20110;&#22312;&#27492;&#31867;&#24773;&#26223;&#20013;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#34892;&#26143;&#28459;&#28216;&#36710;&#31359;&#36234;&#35268;&#21010;&#30340;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#24222;&#22823;&#30340;&#34892;&#21160;&#31354;&#38388;&#21644;&#38271;&#26399;&#35268;&#21010;&#26102;&#38388;&#36328;&#24230;&#24102;&#26469;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;MDP&#26694;&#26550;&#26469;&#25552;&#39640;&#35745;&#31639;&#30340;&#21487;&#35299;&#24615;&#65292;&#21516;&#26102;&#19982;&#29616;&#26377;&#20219;&#21153;&#35268;&#21010;&#23454;&#36341;&#20445;&#25345;&#19968;&#33268;&#65292;&#22686;&#24378;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#20219;&#21153;&#35268;&#21010;MDP&#36716;&#25442;&#20026;&#21452;&#23618;MDP&#65292;&#24182;&#22312;RoverGridWorld&#19978;&#27979;&#35797;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16342v1 Announce Type: new  Abstract: This work focuses on autonomous contingency planning for scientific missions by enabling rapid policy computation from any off-nominal point in the state space in the event of a delay or deviation from the nominal mission plan. Successful contingency planning involves managing risks and rewards, often probabilistically associated with actions, in stochastic scenarios. Markov Decision Processes (MDPs) are used to mathematically model decision-making in such scenarios. However, in the specific case of planetary rover traverse planning, the vast action space and long planning time horizon pose computational challenges. A bi-level MDP framework is proposed to improve computational tractability, while also aligning with existing mission planning practices and enhancing explainability and trustworthiness of AI-driven solutions. We discuss the conversion of a mission planning MDP into a bi-level MDP, and test the framework on RoverGridWorld, a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;VQScore&#65292;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#21487;&#21464;&#20998;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#26032;&#39062;&#30340;&#33258;&#33976;&#39311;&#26426;&#21046;&#25552;&#39640;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16321</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#28165;&#26224;&#35821;&#38899;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;VQScore&#65292;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#21487;&#21464;&#20998;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#26032;&#39062;&#30340;&#33258;&#33976;&#39311;&#26426;&#21046;&#25552;&#39640;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#20174;&#20154;&#31867;&#21548;&#35273;&#19987;&#23478;&#35774;&#35745;&#36716;&#21464;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22823;&#22810;&#25968;&#27169;&#22411;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#32780;&#23548;&#33268;&#26631;&#31614;&#25910;&#38598;&#32791;&#26102;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;VQScore&#65292;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#21487;&#21464;&#20998;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#37327;&#21270;&#35823;&#24046;&#35780;&#20272;&#35821;&#38899;&#30340;&#33258;&#30417;&#30563;&#24230;&#37327;&#12290;VQ-VAE&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#28165;&#26224;&#35821;&#38899;&#65292;&#22240;&#27492;&#24403;&#35821;&#38899;&#21463;&#21040;&#25197;&#26354;&#26102;&#21487;&#20197;&#39044;&#26399;&#21040;&#36739;&#22823;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#19982;&#30495;&#23454;&#36136;&#37327;&#20998;&#25968;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#27169;&#22411;&#35774;&#35745;&#20013;&#12290;&#21457;&#29616;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#20063;&#21487;&#20197;&#29992;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#27169;&#22411;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#32534;&#30721;&#22120;&#23545;SE&#30340;&#40065;&#26834;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#33976;&#39311;&#26426;&#21046;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16321v1 Announce Type: cross  Abstract: Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a self-supervised metric for evaluating speech based on the quantization error of a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE relies on clean speech; hence, large quantization errors can be expected when the speech is distorted. To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design. We found that the vector quantization mechanism could also be used for self-supervised speech enhancement (SE) model training. To improve the robustness of the encoder for SE, a novel self-distillation mechanism combined with adversarial training is introduced. I
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.16313</link><description>&lt;p&gt;
Chain-of-Discussion&#65306;&#22797;&#26434;&#35777;&#25454;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#38656;&#35201;&#27169;&#22411;&#25214;&#21040;&#36866;&#24403;&#30340;&#35777;&#25454;&#26469;&#24418;&#25104;&#21512;&#29702;&#12289;&#20840;&#38754;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#36824;&#38656;&#35201;&#21442;&#19982;&#23545;&#19982;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#30340;&#28508;&#22312;&#22330;&#26223;&#36827;&#34892;&#28145;&#20837;&#35752;&#35770;&#12290;&#22312;&#26816;&#32034;&#27169;&#22359;&#30340;&#22686;&#24378;&#19979;&#65292;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#19968;&#33268;&#30340;&#31572;&#26696;&#65292;&#20294;&#22312;&#21487;&#38752;&#35777;&#25454;&#36873;&#25321;&#21644;&#28145;&#20837;&#38382;&#39064;&#20998;&#26512;&#26041;&#38754;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Chain-of-Discussion&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#24320;&#25918;&#24335;QA&#25552;&#20379;&#26356;&#27491;&#30830;&#12289;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20010;&#20307;&#19978;&#36824;&#19981;&#22815;&#24378;&#22823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#35752;&#35770;&#23545;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;\url{https://github.com/kobaya}&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32852;&#37030;&#19978;&#19979;&#25991;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24322;&#27493;&#36890;&#20449;&#21644;&#32771;&#34385;&#24322;&#36136;&#29992;&#25143;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;&#25512;&#33616;&#65292;&#24182;&#32473;&#20986;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.16312</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#27493;&#36890;&#20449;&#21644;&#24322;&#36136;&#29992;&#25143;&#30340;&#32852;&#37030;&#19978;&#19979;&#25991;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32852;&#37030;&#19978;&#19979;&#25991;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24322;&#27493;&#36890;&#20449;&#21644;&#32771;&#34385;&#24322;&#36136;&#29992;&#25143;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;&#25512;&#33616;&#65292;&#24182;&#32473;&#20986;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#19978;&#19979;&#25991;&#32452;&#21512;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;$|\mathcal{U}|$&#20010;&#20195;&#29702;&#22312;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#21512;&#20316;&#65292;&#20026;$|\mathcal{U}|$&#20010;&#23545;&#24212;&#30340;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#25512;&#33616;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24322;&#27493;&#36890;&#20449;&#33539;&#24335;&#19979;&#30340;&#32852;&#37030;&#20195;&#29702;&#65292;&#26080;&#38656;&#24378;&#21046;&#21516;&#27493;&#65292;&#24182;&#19988;&#25152;&#26377;&#20195;&#29702;&#37117;&#29420;&#31435;&#20110;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#20197;&#21450;&#24322;&#36136;&#29992;&#25143;&#34892;&#20026;&#65292;&#20854;&#20013;&#29992;&#25143;&#21487;&#20197;&#34987;&#20998;&#20026;$J\le |\mathcal{U}|$&#20010;&#28508;&#22312;&#29992;&#25143;&#38598;&#32676;&#65292;&#27599;&#20010;&#38598;&#32676;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31934;&#24515;&#36890;&#20449;&#21327;&#35758;&#30340;UCB&#31867;&#22411;&#31639;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20110;p&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16312v1 Announce Type: cross  Abstract: We study the problem of federated contextual combinatorial cascading bandits, where $|\mathcal{U}|$ agents collaborate under the coordination of a central server to provide tailored recommendations to the $|\mathcal{U}|$ corresponding users. Existing works consider either a synchronous framework, necessitating full agent participation and global synchronization, or assume user homogeneity with identical behaviors. We overcome these limitations by considering (1) federated agents operating in an asynchronous communication paradigm, where no mandatory synchronization is required and all agents communicate independently with the server, (2) heterogeneous user behaviors, where users can be stratified into $J \le |\mathcal{U}|$ latent user clusters, each exhibiting distinct preferences. For this setting, we propose a UCB-type algorithm with delicate communication protocols. Through theoretical analysis, we give sub-linear regret bounds on p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16311</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#20013;&#25991;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Chinese Sentence Pattern Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#21477;&#24335;&#32467;&#26500;&#65288;SPS&#65289;&#35299;&#26512;&#26159;&#19968;&#31181;&#20027;&#35201;&#29992;&#20110;&#35821;&#35328;&#25945;&#23398;&#30340;&#21477;&#27861;&#20998;&#26512;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SPS&#35299;&#26512;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#20869;&#12290;&#20174;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#37096;&#20998;&#21477;&#27861;&#35268;&#21017;&#65292;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#32467;&#21512;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#35299;&#26512;&#22120;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;F1&#25351;&#26631;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#33258;&#30001;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32469;&#36807;&#20256;&#32479;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#30452;&#25509;&#20248;&#21270;&#22270;&#20687;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#65292;&#20026;&#25913;&#36827;&#22270;&#20687;&#29983;&#25104;&#25552;&#20379;&#20102;&#20851;&#38190;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.16305</link><description>&lt;p&gt;
&#23457;&#31295;&#21592;&#20063;&#21487;&#20197;&#21442;&#19982;&#65306;&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16305
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#33258;&#30001;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32469;&#36807;&#20256;&#32479;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#30452;&#25509;&#20248;&#21270;&#22270;&#20687;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#65292;&#20026;&#25913;&#36827;&#22270;&#20687;&#29983;&#25104;&#25552;&#20379;&#20102;&#20851;&#38190;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#65292;&#21363;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#38590;&#20197;&#20005;&#26684;&#36981;&#23432;&#22797;&#26434;&#12289;&#22810;&#26041;&#38754;&#30340;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36825;&#19968;&#21327;&#35843;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#35266;&#28857;&#65292;&#23558;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DPMs&#35270;&#20026;&#21453;&#36716;&#20808;&#36827;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#36890;&#36807;&#36825;&#31181;&#34920;&#36848;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#19982;DPMs&#30456;&#20851;&#30340;&#20256;&#32479;&#37319;&#26679;&#36807;&#31243;&#12290;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#22270;&#20687;&#65292;&#24182;&#22312;&#26377;&#36776;&#21035;&#21147;&#30340;VLMs&#30340;&#30417;&#30563;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#26356;&#22909;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#12290;&#20316;&#20026;&#27010;&#24565;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;BLIP-2&#27169;&#22411;&#30340;&#27969;&#31243;&#65292;&#24182;&#30830;&#23450;&#20102;&#20960;&#20010;&#29992;&#20110;&#25913;&#36827;&#22270;&#20687;&#29983;&#25104;&#30340;&#20851;&#38190;&#35774;&#35745;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16305v1 Announce Type: cross  Abstract: As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;GDPO&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#20219;&#24847;&#30446;&#26631;&#20248;&#21270;&#22270;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16302</link><description>&lt;p&gt;
&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Diffusion Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;GDPO&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#20219;&#24847;&#30446;&#26631;&#20248;&#21270;&#22270;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#29305;&#23450;&#19979;&#28216;&#30446;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#23545;&#20110;&#39046;&#22495;&#22914;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36861;&#27714;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#25193;&#25955;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;GDPO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#20219;&#24847;&#65288;&#22914;&#38750;&#21487;&#24494;&#20998;&#65289;&#30446;&#26631;&#20248;&#21270;&#22270;&#25193;&#25955;&#27169;&#22411;&#12290;GDPO&#22522;&#20110;&#38024;&#23545;&#22270;&#25193;&#25955;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#24613;&#20999;&#31574;&#30053;&#26799;&#24230;&#65292;&#36890;&#36807;&#35748;&#30495;&#20998;&#26512;&#24320;&#21457;&#65292;&#26377;&#26395;&#25552;&#39640;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GDPO&#22312;&#20855;&#26377;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30446;&#26631;&#30340;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/sail-sg/GDPO&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16302v1 Announce Type: cross  Abstract: Recent research has made significant progress in optimizing diffusion models for specific downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph diffusion presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#22810;&#35270;&#35282;&#32593;&#32476;&#29992;&#20110;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#31867;&#65292;&#24341;&#20837;&#20102;&#21160;&#24577;&#27880;&#24847;&#21147;&#22359;&#20197;&#26377;&#25928;&#25972;&#21512;&#22810;&#35270;&#22270;&#20449;&#24687;&#65292;&#24182;&#20419;&#36827;&#20449;&#24687;&#20043;&#38388;&#30340;&#36830;&#36143;&#20256;&#36882;</title><link>https://arxiv.org/abs/2402.16298</link><description>&lt;p&gt;
MV-Swin-T: &#29992;&#22810;&#35270;&#35282;Swin Transformer&#36827;&#34892;&#20083;&#33146;X&#32447;&#25668;&#24433;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#22810;&#35270;&#35282;&#32593;&#32476;&#29992;&#20110;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#31867;&#65292;&#24341;&#20837;&#20102;&#21160;&#24577;&#27880;&#24847;&#21147;&#22359;&#20197;&#26377;&#25928;&#25972;&#21512;&#22810;&#35270;&#22270;&#20449;&#24687;&#65292;&#24182;&#20419;&#36827;&#20449;&#24687;&#20043;&#38388;&#30340;&#36830;&#36143;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20083;&#33146;&#30284;&#20998;&#31867;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35270;&#22270;&#20998;&#26512;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#21516;&#26102;&#26816;&#26597;&#20083;&#25151;X&#32447;&#25668;&#24433;&#26816;&#26597;&#20013;&#30340;&#25152;&#26377;&#35270;&#22270;&#65292;&#21033;&#29992;&#36825;&#20123;&#35270;&#22270;&#20043;&#38388;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#26377;&#25928;&#22320;&#26816;&#27979;&#32959;&#30244;&#12290;&#37492;&#20110;&#22810;&#35270;&#35282;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#19968;&#20123;&#30740;&#31350;&#24341;&#20837;&#20102;&#29420;&#31435;&#22788;&#29702;&#20083;&#25151;X&#32447;&#25668;&#24433;&#35270;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#21367;&#31215;&#20998;&#25903;&#25110;&#31616;&#21333;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#26080;&#24847;&#20013;&#23548;&#33268;&#20102;&#20851;&#38190;&#30340;&#35270;&#35282;&#38388;&#30456;&#20851;&#24615;&#30340;&#20002;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#22810;&#35270;&#35282;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#20301;&#31383;&#21475;&#30340;&#21160;&#24577;&#27880;&#24847;&#21147;&#22359;&#65292;&#20419;&#36827;&#20102;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#26377;&#25928;&#25972;&#21512;&#65292;&#24182;&#20419;&#36827;&#20102;&#36825;&#20123;&#20449;&#24687;&#20043;&#38388;&#30340;&#36830;&#36143;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16298v1 Announce Type: cross  Abstract: Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.16297</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16297
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#35745;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#32780;&#22791;&#21463;&#37325;&#35270;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22788;&#29702;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#30340;&#35745;&#25968;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65288;PGDSs&#65289;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;&#35745;&#25968;&#24207;&#21015;&#24213;&#23618;&#21160;&#24577;&#30340;&#28436;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;PGDS&#22312;&#25429;&#25417;&#24120;&#35265;&#20110;&#23454;&#38469;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;PGDS&#65292;&#20801;&#35768;&#22522;&#30784;&#36716;&#31227;&#30697;&#38453;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#28436;&#21464;&#30340;&#36716;&#31227;&#30697;&#38453;&#30001;&#31934;&#24515;&#35774;&#35745;&#30340;Dirichlet Markov&#38142;&#24314;&#27169;&#12290;&#21033;&#29992;Dirichlet-Multinomial-Beta&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#20849;&#36717;&#19988;&#39640;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16297v1 Announce Type: cross  Abstract: Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16294</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Unlearning on Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#30830;&#20445;FL&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#21306;&#22359;&#38142;FL&#28041;&#21450;&#21442;&#19982;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#38543;&#21518;&#23558;&#27169;&#22411;&#21457;&#24067;&#21040;&#21306;&#22359;&#38142;&#19978;&#65292;&#24418;&#25104;&#34920;&#31034;&#27169;&#22411;&#20851;&#31995;&#30340;&#31867;&#20284;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#32487;&#25215;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;DAG&#30340;&#32467;&#26500;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#21644;&#24320;&#38144;&#36739;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#21464;&#33394;&#40857;&#21704;&#24076;&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#36951;&#24536;&#20219;&#21153;&#30340;&#35745;&#31639;&#21644;&#20849;&#35782;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BlockFUL&#25903;&#25345;&#21508;&#31181;&#32852;&#37030;&#36951;&#24536;&#26041;&#27861;&#65292;&#30830;&#20445;&#27169;&#22411;&#26356;&#26032;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16294v1 Announce Type: cross  Abstract: Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conduc
&lt;/p&gt;</description></item><item><title>PerLTQA&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#30340;&#21019;&#26032;QA&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#21644;&#35760;&#24518;&#25972;&#21512;&#12289;&#26816;&#32034;&#12289;&#21512;&#25104;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.16288</link><description>&lt;p&gt;
PerLTQA: &#19968;&#20010;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#35760;&#24518;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#21512;&#25104;&#30340;&#20010;&#20154;&#38271;&#26399;&#35760;&#24518;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16288
&lt;/p&gt;
&lt;p&gt;
PerLTQA&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#30340;&#21019;&#26032;QA&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#21644;&#35760;&#24518;&#25972;&#21512;&#12289;&#26816;&#32034;&#12289;&#21512;&#25104;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#35760;&#24518;&#22312;&#20010;&#20154;&#20114;&#21160;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32771;&#34385;&#21040;&#38271;&#26399;&#35760;&#24518;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#12289;&#21382;&#21490;&#20449;&#24687;&#21644;&#23545;&#35805;&#20013;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;PerLTQA&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#65292;&#21253;&#25324;&#19990;&#30028;&#30693;&#35782;&#12289;&#29992;&#25143;&#36164;&#26009;&#12289;&#31038;&#20250;&#20851;&#31995;&#12289;&#20107;&#20214;&#21644;&#23545;&#35805;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#25910;&#38598;&#29992;&#20110;&#25506;&#35752;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20132;&#20114;&#21160;&#21644;&#20107;&#20214;&#12290;PerLTQA&#20855;&#26377;&#20004;&#31181;&#35760;&#24518;&#31867;&#22411;&#21644;&#19968;&#20010;&#21253;&#21547;8,593&#20010;&#38382;&#39064;&#30340;30&#20010;&#23383;&#31526;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20419;&#36827;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25506;&#32034;&#21644;&#24212;&#29992;&#20010;&#24615;&#21270;&#35760;&#24518;&#12290;&#22522;&#20110;PerLTQA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35760;&#24518;&#25972;&#21512;&#21644;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#35760;&#24518;&#20998;&#31867;&#12289;&#35760;&#24518;&#26816;&#32034;&#21644;&#35760;&#24518;&#21512;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;LLM&#21644;&#19977;&#20010;&#35780;&#20272;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16288v1 Announce Type: cross  Abstract: Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#32593;&#32476;(RPSN)&#65292;&#36890;&#36807;&#23558;&#21487;&#24494;&#20998;&#30340;&#36870;&#36816;&#21160;&#23398;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#33021;&#22815;&#39640;&#25104;&#21151;&#29575;&#22320;&#25512;&#27979;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.16281</link><description>&lt;p&gt;
&#26397;&#30528;&#25935;&#25463;&#26426;&#22120;&#20154;&#65306;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30452;&#35266;&#30340;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Agile Robots: Intuitive Robot Position Speculation with Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#32593;&#32476;(RPSN)&#65292;&#36890;&#36807;&#23558;&#21487;&#24494;&#20998;&#30340;&#36870;&#36816;&#21160;&#23398;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#33021;&#22815;&#39640;&#25104;&#21151;&#29575;&#22320;&#25512;&#27979;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#26159;&#25511;&#21046;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#65292;&#20197;&#30830;&#23450;&#24213;&#30424;&#24212;&#35813;&#31227;&#21160;&#21040;&#21738;&#37324;&#12290;&#20026;&#20102;&#28385;&#36275;&#25935;&#25463;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#32593;&#32476;(RPSN)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#25935;&#25463;&#24615;&#12290;RPSN&#23558;&#21487;&#24494;&#20998;&#30340;&#36870;&#36816;&#21160;&#23398;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;RPSN&#33021;&#22815;&#39640;&#25104;&#21151;&#29575;&#22320;&#25512;&#27979;&#20301;&#32622;&#12290;&#25105;&#20204;&#23558;RPSN&#24212;&#29992;&#20110;&#20998;&#35299;&#26411;&#26399;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#30340;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#12290;&#22312;&#21508;&#31181;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RPSN&#25552;&#20379;&#30340;&#21021;&#22987;&#20301;&#32622;&#21487;&#33021;&#26159;&#29702;&#24819;&#20301;&#32622;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16281v1 Announce Type: cross  Abstract: The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators. The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods. Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators. The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network. Through end-to-end training, the RPSN can speculate positions with a high success rate. We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs). Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the idea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20915;&#31574;&#20248;&#21270;CoPilot&#65288;DOCP&#65289;&#65292;&#24110;&#21161;&#20915;&#31574;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#29702;&#35299;&#24182;&#35299;&#20915;&#19994;&#21153;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16269</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#21040;&#20915;&#31574;&#20248;&#21270;CoPilot&#65306;&#19968;&#39033;&#30740;&#31350;&#23459;&#35328;
&lt;/p&gt;
&lt;p&gt;
From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16269
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20915;&#31574;&#20248;&#21270;CoPilot&#65288;DOCP&#65289;&#65292;&#24110;&#21161;&#20915;&#31574;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#29702;&#35299;&#24182;&#35299;&#20915;&#19994;&#21153;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22823;&#31616;&#21270;&#20026; real-world business problems &#21019;&#24314;&#20248;&#21270;&#27169;&#22411;&#19968;&#30452;&#26159;&#23558;&#25968;&#23398;&#20248;&#21270;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#37325;&#35201;&#21830;&#19994;&#21644;&#31038;&#20250;&#20915;&#31574;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#21450;&#26102;&#30340;&#26426;&#20250;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;LLMs&#21644;&#20248;&#21270;&#30340;&#20132;&#21449;&#28857;&#19978;&#24320;&#23637;&#30740;&#31350;&#65292;&#21019;&#24314;&#19968;&#20010;&#20915;&#31574;&#20248;&#21270;CoPilot&#65288;DOCP&#65289;- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#20219;&#20309;&#20915;&#31574;&#32773;&#30340;AI&#24037;&#20855;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#29702;&#35299;&#19994;&#21153;&#38382;&#39064;&#65292;&#38543;&#21518;&#21046;&#23450;&#21644;&#35299;&#20915;&#30456;&#24212;&#20248;&#21270;&#27169;&#22411;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;DOCP&#24895;&#26223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#23454;&#26045;&#30340;&#20960;&#20010;&#22522;&#26412;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#29486;&#35843;&#26597;&#21644;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#39564;&#25551;&#36848;&#20102;&#29616;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;a&#65289;LLMs&#24050;&#32463;&#25552;&#20379;&#20102;&#19982;DOCP&#30456;&#20851;&#30340;&#37325;&#22823;&#26032;&#21151;&#33021;&#65292;b&#65289;&#20027;&#35201;&#30740;&#31350; c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16269v1 Announce Type: new  Abstract: Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;&#65292;&#20511;&#37492;&#31038;&#20132;&#23186;&#20307;&#30340;&#36879;&#26126;&#24230;&#25253;&#21578;&#23454;&#36341;&#65292;&#30446;&#30340;&#22312;&#20110;&#22312;&#22522;&#30784;&#27169;&#22411;&#34892;&#19994;&#23578;&#26410;&#25104;&#29087;&#26102;&#21046;&#23450;&#36879;&#26126;&#24230;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2402.16268</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Transparency Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;&#65292;&#20511;&#37492;&#31038;&#20132;&#23186;&#20307;&#30340;&#36879;&#26126;&#24230;&#25253;&#21578;&#23454;&#36341;&#65292;&#30446;&#30340;&#22312;&#20110;&#22312;&#22522;&#30784;&#27169;&#22411;&#34892;&#19994;&#23578;&#26410;&#25104;&#29087;&#26102;&#21046;&#23450;&#36879;&#26126;&#24230;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#20855;&#26377;&#24191;&#27867;&#31038;&#20250;&#24433;&#21709;&#30340;&#20851;&#38190;&#25968;&#23383;&#25216;&#26415;&#65292;&#38656;&#35201;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#35268;&#33539;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#32773;&#24212;&#22914;&#20309;&#25552;&#20379;&#26377;&#20851;&#20854;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36879;&#26126;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;&#65292;&#20511;&#37492;&#31038;&#20132;&#23186;&#20307;&#30340;&#36879;&#26126;&#24230;&#25253;&#21578;&#23454;&#36341;&#12290;&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#36879;&#26126;&#24230;&#25253;&#21578;&#26159;&#30001;&#22806;&#37096;&#23545;&#31038;&#20250;&#20260;&#23475;&#30340;&#25991;&#26723;&#20419;&#25104;&#30340;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#34892;&#19994;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#26102;&#20026;&#22522;&#30784;&#27169;&#22411;&#21046;&#23450;&#36879;&#26126;&#24230;&#25253;&#21578;&#12290;&#20026;&#35774;&#35745;&#25105;&#20204;&#30340;&#25253;&#21578;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;6&#26465;&#35774;&#35745;&#21407;&#21017;&#65292;&#32771;&#34385;&#20102;&#31038;&#20132;&#23186;&#20307;&#36879;&#26126;&#24230;&#25253;&#21578;&#30340;&#25104;&#21151;&#21644;&#19981;&#36275;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#30340;&#25253;&#21578;&#31995;&#32479;&#21270;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25351;&#25968;&#30340;100&#20010;&#36879;&#26126;&#24230;&#25351;&#26631;&#12290;&#26681;&#25454;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#27979;&#37327;&#23427;&#20204;&#19982;&#20845;&#20010;&#20027;&#35201;&#36879;&#26126;&#24230;&#35201;&#27714;&#20013;&#21253;&#21547;&#30340;&#36879;&#26126;&#24230;&#35201;&#27714;&#30340;&#37325;&#21472;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16268v1 Announce Type: cross  Abstract: Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six promine
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20013;&#21457;&#29616;&#24403;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#22240;&#23384;&#22312;&#26377;&#20559;&#30340;&#25237;&#24433;&#22836;&#23548;&#33268;&#30340;&#32852;&#37030;&#27169;&#22411;&#19981;&#21487;&#38752;&#24615;&#65292;&#25552;&#20986;&#20102;&#8220;&#32452;&#35013;&#25237;&#24433;&#22836;&#8221;&#65288;APH&#65289;&#26041;&#27861;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16255</link><description>&lt;p&gt;
&#30041;&#24847;&#20320;&#30340;&#22836;&#37096;&#65306;&#32452;&#35013;&#25237;&#24433;&#22836;&#20197;&#25552;&#39640;&#32852;&#37030;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16255
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#21457;&#29616;&#24403;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#22240;&#23384;&#22312;&#26377;&#20559;&#30340;&#25237;&#24433;&#22836;&#23548;&#33268;&#30340;&#32852;&#37030;&#27169;&#22411;&#19981;&#21487;&#38752;&#24615;&#65292;&#25552;&#20986;&#20102;&#8220;&#32452;&#35013;&#25237;&#24433;&#22836;&#8221;&#65288;APH&#65289;&#26041;&#27861;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#36935;&#21040;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#20943;&#36731;&#27492;&#31867;&#24433;&#21709;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#32852;&#37030;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#21364;&#34987;&#22823;&#22810;&#24573;&#35270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#36890;&#29992;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;\textbf{&#24403;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#32852;&#37030;&#27169;&#22411;&#34920;&#29616;&#20986;&#19981;&#21487;&#38752;&#24615;}&#65292;&#34920;&#29616;&#20026;&#23545;&#20998;&#24067;&#27979;&#35797;&#25968;&#25454;&#30340;&#36739;&#24046;&#26657;&#20934;&#21644;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#20302;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;&#36825;&#31181;&#19981;&#21487;&#38752;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;&#23384;&#22312;&#26377;&#20559;&#30340;&#25237;&#24433;&#22836;&#65292;&#36825;&#20123;&#25237;&#24433;&#22836;&#20026;&#32852;&#37030;&#27169;&#22411;&#24341;&#20837;&#20102;&#38169;&#35823;&#26657;&#20934;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;&#32852;&#37030;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#8220;&#32452;&#35013;&#25237;&#24433;&#22836;&#8221;&#65288;APH&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16255v1 Announce Type: cross  Abstract: Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: \textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the "Assembled Projection Heads" (APH) method for enhancing the reliability of federated models. By
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#20869;&#23481;&#36873;&#25321;&#30340;&#22797;&#21046;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#25913;&#36827;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#20351;&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#29983;&#25104;&#22810;&#26679;&#24615;&#25552;&#39640;&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.16248</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#20869;&#23481;&#36873;&#25321;&#30340;&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Topic-to-essay generation with knowledge-based content selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#20869;&#23481;&#36873;&#25321;&#30340;&#22797;&#21046;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#25913;&#36827;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#20351;&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#29983;&#25104;&#22810;&#26679;&#24615;&#25552;&#39640;&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#20027;&#39064;&#35789;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#27573;&#33853;&#32423;&#25991;&#26412;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#32780;&#24573;&#30053;&#20102;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#20869;&#23481;&#36873;&#25321;&#27169;&#22359;&#30340;&#22797;&#21046;&#26426;&#21046;&#27169;&#22411;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30693;&#35782;&#25972;&#21512;&#21040;&#35299;&#30721;&#22120;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#36755;&#20837;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;TEG&#20219;&#21153;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#22810;&#26679;&#24615;&#25552;&#39640;35%&#33267;59%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16248v1 Announce Type: cross  Abstract: The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\% to 59\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#30340;&#23402;&#29983;&#21069;&#26223;&#20851;&#32852;&#39537;&#21160;&#22256;&#38590;&#26679;&#26412;&#20248;&#21270;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#26696;&#20363;&#26102;&#38754;&#20020;&#30340;&#19981;&#24179;&#34913;&#21644;&#32570;&#22833;&#24615;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.16242</link><description>&lt;p&gt;
HSONet&#65306;&#38754;&#21521;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#30340;&#23402;&#29983;&#21069;&#26223;&#20851;&#32852;&#39537;&#21160;&#22256;&#38590;&#26679;&#26412;&#20248;&#21270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#30340;&#23402;&#29983;&#21069;&#26223;&#20851;&#32852;&#39537;&#21160;&#22256;&#38590;&#26679;&#26412;&#20248;&#21270;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#26696;&#20363;&#26102;&#38754;&#20020;&#30340;&#19981;&#24179;&#34913;&#21644;&#32570;&#22833;&#24615;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#21518;&#26399;&#65292;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#39640;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#23398;&#20064;&#22256;&#38590;&#26696;&#20363;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#27169;&#22411;&#23545;&#22256;&#38590;&#26696;&#20363;&#30340;&#23398;&#20064;&#65307;&#28982;&#32780;&#65292;&#23398;&#20064;&#22256;&#38590;&#26696;&#20363;&#26377;&#20004;&#20010;&#39069;&#22806;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#21464;&#21270;&#26631;&#31614;&#26377;&#38480;&#19988;&#20542;&#21521;&#20110;&#21482;&#25351;&#21521;&#21069;&#26223;&#30446;&#26631;&#65292;&#28982;&#32780;&#22256;&#38590;&#26696;&#20363;&#22312;&#32972;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#20391;&#37325;&#20110;&#21069;&#26223;&#30446;&#26631;&#24182;&#24573;&#30053;&#32972;&#26223;&#30340;&#22256;&#38590;&#26696;&#20363;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#19981;&#24179;&#34913;&#12290;&#65288;2&#65289;&#22797;&#26434;&#24773;&#20917;&#65292;&#22914;&#20809;&#24433;&#12289;&#30446;&#26631;&#36974;&#25377;&#21644;&#23395;&#33410;&#21464;&#21270;&#65292;&#24341;&#21457;&#22256;&#38590;&#26696;&#20363;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#30417;&#30563;&#21644;&#22330;&#26223;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24456;&#38590;&#30452;&#25509;&#23398;&#20064;&#22256;&#38590;&#26696;&#20363;&#20197;&#20934;&#30830;&#33719;&#24471;&#21464;&#21270;&#20449;&#24687;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32570;&#22833;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#21069;&#26223;&#20851;&#32852;&#39537;&#21160;&#30340;&#22256;&#38590;&#26679;&#26412;&#20248;&#21270;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16242v1 Announce Type: cross  Abstract: In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance. (2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness. We propose a Siamese foreground association-driven hard case sample optimization network 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#31163;&#25955;&#21270;&#30452;&#25509;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#24037;&#20316;&#30340;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#27963;&#36291;&#27700;&#24179;&#38598;&#20272;&#35745;&#31639;&#27861;</title><link>https://arxiv.org/abs/2402.16237</link><description>&lt;p&gt;
&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#27963;&#36291;&#27700;&#24179;&#38598;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16237
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#31163;&#25955;&#21270;&#30452;&#25509;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#24037;&#20316;&#30340;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#27963;&#36291;&#27700;&#24179;&#38598;&#20272;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#27700;&#24179;&#38598;&#20272;&#35745;&#65292;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#20989;&#25968;&#22495;&#20013;&#20989;&#25968;&#39640;&#20110;&#25110;&#20302;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#21306;&#22495;&#12290; &#24403;&#20989;&#25968;&#26159;&#40657;&#30418;&#19988;&#35780;&#20272;&#25104;&#26412;&#39640;&#26102;&#65292;&#38656;&#35201;&#22312;&#26368;&#23567;&#30340;&#20989;&#25968;&#35780;&#20272;&#38598;&#20013;&#25214;&#21040;&#27700;&#24179;&#38598;&#12290; &#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#28857;&#38598;&#30340;&#31163;&#25955;&#25628;&#32034;&#31354;&#38388;&#65292;&#29992;&#20110;&#20989;&#25968;&#35780;&#20272;&#21644;&#20272;&#35745;&#27700;&#24179;&#38598;&#12290; &#24403;&#24212;&#29992;&#20110;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39318;&#20808;&#23545;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#36825;&#20250;&#23548;&#33268;&#32467;&#26524;&#19981;&#20339;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#35745;&#31639;&#26102;&#38388;&#12290; &#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#35774;&#23450;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#29702;&#35770;&#25910;&#25947;&#30340;&#36866;&#24403;&#20445;&#35777;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#24037;&#20316;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16237v1 Announce Type: cross  Abstract: A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21019;&#20316;Java&#32534;&#31243;&#35838;&#31243;&#30340;&#31034;&#20363;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#36880;&#34892;&#35299;&#37322;&#22823;&#37327;&#31034;&#20363;&#30340;&#36127;&#25285;</title><link>https://arxiv.org/abs/2402.16235</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#21019;&#20316;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Human-AI Co-Creation of Worked Examples for Programming Classes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16235
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21019;&#20316;Java&#32534;&#31243;&#35838;&#31243;&#30340;&#31034;&#20363;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#36880;&#34892;&#35299;&#37322;&#22823;&#37327;&#31034;&#20363;&#30340;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#31034;&#20363;&#65288;&#20856;&#22411;&#32534;&#31243;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26576;&#31181;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#21576;&#29616;&#65292;&#24182;&#29992;&#20110;&#35299;&#37322;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#20027;&#39064;&#65289;&#26159;&#32534;&#31243;&#35838;&#31243;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#23398;&#20064;&#20869;&#23481;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#29992;&#20110;&#21521;&#23398;&#29983;&#23637;&#31034;&#36825;&#20123;&#31034;&#20363;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#37117;&#22522;&#20110;&#23545;&#31034;&#20363;&#20195;&#30721;&#30340;&#36880;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25945;&#24072;&#24456;&#23569;&#26377;&#26102;&#38388;&#20026;&#32534;&#31243;&#35838;&#31243;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#22823;&#37327;&#31034;&#20363;&#25552;&#20379;&#36880;&#34892;&#35299;&#37322;&#12290;&#26412;&#25991;&#25506;&#35752;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;Java&#32534;&#31243;&#25776;&#20889;&#31034;&#20363;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#21019;&#24314;Java&#24037;&#20316;&#31034;&#20363;&#30340;&#32534;&#20889;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#30340;&#21021;&#22987;&#29256;&#26412;&#65292;&#24182;&#23558;&#20854;&#21576;&#29616;&#32473;&#25945;&#24072;&#20197;&#22312;&#24517;&#35201;&#26102;&#36827;&#34892;&#32534;&#36753;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#39033;&#35780;&#20272;&#20351;&#29992;&#27492;&#26041;&#27861;&#21019;&#24314;&#30340;&#35299;&#37322;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16235v1 Announce Type: cross  Abstract: Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.We also present a study that assesses the quality of explanations created with this approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GARNNs&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#21464;&#37327;&#36129;&#29486;&#24635;&#32467;&#21644;&#29305;&#24449;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.16230</link><description>&lt;p&gt;
GARNN: &#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GARNNs&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#21464;&#37327;&#36129;&#29486;&#24635;&#32467;&#21644;&#29305;&#24449;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#39044;&#27979;&#26410;&#26469;&#34880;&#31958;&#65288;BG&#65289;&#27700;&#24179;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#31649;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#24182;&#21457;&#30151;&#65292;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GARNNs&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#65292;&#36890;&#36807;&#24635;&#32467;&#21464;&#37327;&#37325;&#35201;&#24615;&#35299;&#37322;&#21464;&#37327;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#26426;&#21046;&#29983;&#25104;&#29305;&#24449;&#22270;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#20107;&#21518;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#19981;&#21516;&#20020;&#24202;&#22330;&#26223;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GARNNs&#12290;&#19982;&#21313;&#20108;&#31181;&#20844;&#35748;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;GARNNs&#19981;&#20165;&#23454;&#29616;&#20102;BG&#39044;&#27979;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#36824;&#26356;&#20855;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16230v1 Announce Type: cross  Abstract: Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS). However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients. In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only ach
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;</title><link>https://arxiv.org/abs/2402.16200</link><description>&lt;p&gt;
IR2&#65306;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IR2: Information Regularization for Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16200
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IR2&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#22797;&#26434;&#26597;&#35810;&#29305;&#24449;&#30340;&#19977;&#20010;&#26368;&#36817;&#30340;IR&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;DORIS-MAE&#12289;ArguAna&#21644;WhatsThatBook&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#19981;&#20165;&#22312;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50&#65285;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#19981;&#21516;&#38454;&#27573;&#30340;&#19977;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#36755;&#20837;&#12289;&#25552;&#31034;&#21644;&#36755;&#20986;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25506;&#32034;&#65292;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#22343;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#20013;&#38388;&#23618;&#30340;&#20196;&#29260;&#23884;&#20837;&#20316;&#20026;&#25552;&#31034;&#26597;&#35810;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;PCL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;50%&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20934;&#30830;&#24230;&#19979;&#38477;&#23567;&#20110;1%&#12290;</title><link>https://arxiv.org/abs/2402.16189</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#21333;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-stage Prompt-based Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16189
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#20013;&#38388;&#23618;&#30340;&#20196;&#29260;&#23884;&#20837;&#20316;&#20026;&#25552;&#31034;&#26597;&#35810;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;PCL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;50%&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20934;&#30830;&#24230;&#19979;&#38477;&#23567;&#20110;1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#39537;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;PCL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#26395;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20854;&#22312;&#38450;&#27490;&#38544;&#31169;&#20405;&#29359;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PCL&#26041;&#27861;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20004;&#20010;ViT&#21069;&#39304;&#38454;&#27573;&#65307;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#25552;&#31034;&#26597;&#35810;&#20197;&#36873;&#25321;&#25552;&#31034;&#27744;&#20013;&#30340;&#25552;&#31034;&#30340;&#26597;&#35810;ViT&#65292;&#21478;&#19968;&#20010;&#26159;&#39592;&#24178;ViT&#65292;&#29992;&#20110;&#22312;&#36873;&#25321;&#30340;&#25552;&#31034;&#21644;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#28151;&#21512;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#20013;&#38388;&#23618;&#30340;&#20196;&#29260;&#23884;&#20837;&#20316;&#20026;&#25552;&#31034;&#26597;&#35810;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;PCL&#26694;&#26550;&#12290;&#36825;&#31181;&#35774;&#35745;&#28040;&#38500;&#20102;&#26597;&#35810;ViT&#30340;&#39069;&#22806;&#21069;&#21521;&#38454;&#27573;&#30340;&#38656;&#35201;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;&#32422;50%&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24230;&#19979;&#38477;&#19981;&#21040;1%&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26597;&#35810;&#27744;&#27491;&#21017;&#21270;&#65288;QR&#65289;&#25439;&#22833;&#65292;&#29992;&#26469;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16189v1 Announce Type: cross  Abstract: Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop &lt; 1%. We further introduce a Query-Pool Regularization (QR) loss that regul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#30340;&#31574;&#30053;&#20808;&#39564;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16181</link><description>&lt;p&gt;
LLM&#22914;&#20309;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#65311;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
How Can LLM Guide RL? A Value-Based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#30340;&#31574;&#30053;&#20808;&#39564;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#25913;&#36827;&#26410;&#26469;&#30340;&#34892;&#21160;&#31574;&#30053;&#26469;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#30340;&#20107;&#23454;&#26631;&#20934;&#23454;&#36341;&#65292;&#20294;&#26159;RL&#31639;&#27861;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#26469;&#25910;&#38598;&#26377;&#29992;&#30340;&#21453;&#39304;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#25506;&#32034;&#21644;&#33258;&#25105;&#25913;&#36827;&#35268;&#21010;&#20219;&#21153;&#30340;&#33021;&#21147;&#19978;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#32570;&#20047;&#22522;&#20110;&#21453;&#39304;&#33258;&#20027;&#25913;&#36827;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#25552;&#20379;&#30340;&#31574;&#30053;&#20808;&#39564;&#22914;&#20309;&#22686;&#24378;RL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;LINVIT&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#24341;&#23548;&#20316;&#20026;&#20215;&#20540;&#22411;RL&#20013;&#30340;&#27491;&#21017;&#21270;&#22240;&#23376;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23398;&#20064;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#29305;&#21035;&#26159;&#24403;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16181v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when 
&lt;/p&gt;</description></item><item><title>GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16174</link><description>&lt;p&gt;
GenNBV: &#36890;&#29992;&#30340;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16174
&lt;/p&gt;
&lt;p&gt;
GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#25216;&#26415;&#36827;&#27493;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#22330;&#26223;&#30340;&#30495;&#23454;&#25968;&#23383;&#21270;, &#20294;&#26159;&#22270;&#20687;&#25429;&#33719;&#36807;&#31243;&#20173;&#28982;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#31574;&#30053;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NBV&#31574;&#30053;&#20005;&#37325;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#26631;&#20934;&#12289;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25110;&#32773;&#26159;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#20248;&#21270;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#32422;&#26463;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#25968;&#25454;&#38598;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenNBV&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#36890;&#29992;&#30340;NBV&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#23558;&#20856;&#22411;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#12290;&#23427;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#26080;&#20154;&#26426;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#26410;&#35265;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#65292;&#21253;&#25324;&#20960;&#20309;&#12289;&#35821;&#20041;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16174v1 Announce Type: cross  Abstract: While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action repres
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36890;&#20449;&#27969;&#37327;&#29305;&#24449;&#35782;&#21035;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.16173</link><description>&lt;p&gt;
&#36890;&#20449;&#27969;&#37327;&#29305;&#24449;&#25581;&#31034;&#29289;&#32852;&#32593;&#35774;&#22791;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
Communication Traffic Characteristics Reveal an IoT Devices Identity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16173
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36890;&#20449;&#27969;&#37327;&#29305;&#24449;&#35782;&#21035;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#26159;21&#19990;&#32426;&#30340;&#25216;&#26415;&#36827;&#27493;&#20043;&#19968;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#27963;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#24102;&#26469;&#20102;&#26032;&#31867;&#22411;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#35774;&#22791;&#35748;&#35777;&#12289;&#27969;&#37327;&#31867;&#22411;&#20998;&#31867;&#20197;&#21450;&#22312;&#32593;&#32476;&#39046;&#22495;&#20013;&#24694;&#24847;&#27969;&#37327;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#35774;&#22791;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#20165;&#20351;&#29992;&#36890;&#20449;&#27969;&#37327;&#29305;&#24449;&#65288;&#25110;&#38544;&#24335;&#26631;&#35782;&#31526;&#65289;&#35782;&#21035;&#32593;&#32476;&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16173v1 Announce Type: cross  Abstract: Internet of Things (IoT) is one of the technological advancements of the twenty-first century which can improve living standards. However, it also imposes new types of security challenges, including device authentication, traffic types classification, and malicious traffic identification, in the network domain. Traditionally, internet protocol (IP) and media access control (MAC) addresses are utilized for identifying network-connected devices in a network, whilst these addressing schemes are prone to be compromised, including spoofing attacks and MAC randomization. Therefore, device identification using only explicit identifiers is a challenging task. Accurate device identification plays a key role in securing a network. In this paper, a supervised machine learning-based device fingerprinting (DFP) model has been proposed for identifying network-connected IoT devices using only communication traffic characteristics (or implicit identif
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#26469;&#25506;&#31350;&#32534;&#30721;&#20449;&#24687;&#30340;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#21487;&#35270;&#21270;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.16168</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#26041;&#27861;&#21644;&#26356;&#22810;&#25506;&#38024;&#26469;&#25506;&#31350;&#32534;&#30721;&#20449;&#24687;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hitting "Probe"rty with Non-Linearity, and More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16168
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#26469;&#25506;&#31350;&#32534;&#30721;&#20449;&#24687;&#30340;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#21487;&#35270;&#21270;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#25506;&#38024;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#20197;&#25214;&#21040;&#20381;&#23384;&#26641;&#22914;&#20309;&#23884;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;White&#31561;&#20154;&#20171;&#32461;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#65292;&#20351;&#20854;&#35774;&#35745;&#26356;&#31616;&#21333;&#20294;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#23450;&#24615;&#35780;&#20272;&#21477;&#23376;&#20013;&#20004;&#20010;&#21333;&#35789;&#22312;&#39044;&#27979;&#30340;&#20381;&#23384;&#26641;&#20013;&#30340;&#36830;&#25509;&#24378;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25216;&#26415;&#26469;&#29702;&#35299;&#21738;&#31181;&#38750;&#32447;&#24615;&#25506;&#38024;&#21464;&#20307;&#25797;&#38271;&#32534;&#30721;&#21477;&#27861;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36824;&#29992;&#23427;&#23450;&#24615;&#30740;&#31350;&#20102;BERT&#22312;&#27599;&#20010;&#23618;&#20013;&#32534;&#30721;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16168v1 Announce Type: cross  Abstract: Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models. This simple design may not allow for full exploitation of the structure of the encoded information. Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes. We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective. We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree. We use this technique to understand which non-linear probe variant is good at encoding syntactical information. Additionally, we also use it to qualitatively investigate the structure of dependency trees that BERT encodes in each of its layers. We find that the radial basis function (RBF) is an effectiv
&lt;/p&gt;</description></item><item><title>ChatMusician &#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#20869;&#22312;&#38899;&#20048;&#33021;&#21147;&#30340;&#24320;&#28304;LLM&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#34920;&#29616;&#20248;&#20110;GPT-4&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16153</link><description>&lt;p&gt;
ChatMusician&#65306;&#29702;&#35299;&#21644;&#29983;&#25104;&#20855;&#26377;LLM&#30340;&#38899;&#20048;&#20869;&#22312;
&lt;/p&gt;
&lt;p&gt;
ChatMusician: Understanding and Generating Music Intrinsically with LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16153
&lt;/p&gt;
&lt;p&gt;
ChatMusician &#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#20869;&#22312;&#38899;&#20048;&#33021;&#21147;&#30340;&#24320;&#28304;LLM&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#34920;&#29616;&#20248;&#20110;GPT-4&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#33021;&#21147;&#23578;&#26410;&#25512;&#24191;&#21040;&#38899;&#20048;&#65292;&#20063;&#23601;&#26159;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ChatMusician&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#38598;&#25104;&#20102;&#20869;&#22312;&#30340;&#38899;&#20048;&#33021;&#21147;&#12290;&#23427;&#22522;&#20110;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;ABC&#35760;&#35889;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;LLaMA2&#65292;&#24182;&#19988;&#23558;&#38899;&#20048;&#35270;&#20026;&#31532;&#20108;&#35821;&#35328;&#12290;ChatMusician&#21487;&#20197;&#20351;&#29992;&#32431;&#25991;&#26412;&#26631;&#35760;&#22120;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#22806;&#37096;&#22810;&#27169;&#24577;&#31070;&#32463;&#32467;&#26500;&#25110;&#26631;&#35760;&#22120;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36171;&#20104;&#38899;&#20048;&#33021;&#21147;&#24182;&#19981;&#20250;&#25439;&#23475;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#36798;&#21040;&#30053;&#39640;&#30340;MMLU&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#12289;&#21644;&#24358;&#12289;&#26059;&#24459;&#12289;&#20027;&#39064;&#12289;&#38899;&#20048;&#24418;&#24335;&#31561;&#21019;&#20316;&#32467;&#26500;&#33391;&#22909;&#12289;&#23436;&#25972;&#38271;&#24230;&#30340;&#38899;&#20048;&#65292;&#36229;&#36234;&#20102;GPT-4&#30340;&#22522;&#32447;&#12290;&#22312;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#23398;&#32423;&#38899;&#20048;&#29702;&#35299;&#22522;&#20934;&#19978;&#65292;MusicTheory
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16153v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheory
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#20581;&#36523;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#21644;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16142</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#36716;&#21270;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#21151;&#33021;&#24615;&#30340;&#20840;&#38754;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#20581;&#36523;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#21644;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#25442;&#22120;&#65288;BERT&#65289;&#22312;&#20174;&#25216;&#26415;&#12289;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21040;&#25945;&#32946;&#31561;&#21508;&#39046;&#22495;&#30340;&#25193;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23578;&#26410;&#31995;&#32479;&#22320;&#30740;&#31350;&#36807;&#23427;&#20204;&#23545;&#20581;&#36523;&#12289;&#25972;&#20307;&#24184;&#31119;&#24863;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#20197;&#21450;&#28798;&#23475;&#31649;&#29702;&#31561;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#38500;&#20102;&#20840;&#38754;&#20998;&#26512;LLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#21033;&#29992;&#31243;&#24230;&#20043;&#22806;&#65292;&#35813;&#32508;&#36848;&#35770;&#25991;&#36824;&#35782;&#21035;&#20102;LLMs&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#21033;&#29992;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#39046;&#22495;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;&#22312;&#20581;&#36523;&#19982;&#24184;&#31119;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#21644;&#28798;&#23475;&#21709;&#24212;&#31561;&#39046;&#22495;&#30041;&#19979;&#30165;&#36857;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#20182;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16142v1 Announce Type: cross  Abstract: This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed. This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.16139</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Generative Artificial Intelligence Means for Terminological Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#21019;&#24314;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;GenAI&#24037;&#20855;&#19982;&#20256;&#32479;&#26415;&#35821;&#36164;&#28304;&#30456;&#27604;&#65292;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;ChatGPT&#22312;&#20197;&#20132;&#20114;&#24335;&#21644;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#29305;&#23450;&#35821;&#22659;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#35782;&#21035;&#36164;&#28304;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#21487;&#33021;&#20250;&#22240;&#20854;&#21487;&#38752;&#24615;&#32780;&#32487;&#32493;&#23384;&#22312;&#12290;&#20174;&#26415;&#35821;&#23398;&#23478;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24037;&#20855;&#20351;&#24471;AI&#36741;&#21161;&#30340;&#26415;&#35821;&#32534;&#32386;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#21518;&#26399;&#32534;&#36753;&#26415;&#35821;&#32534;&#32386;&#65292;&#23558;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#23450;&#20041;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
&lt;/p&gt;</description></item><item><title>LSTPrompt&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20998;&#35299;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#25552;&#31034;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16132</link><description>&lt;p&gt;
LSTPrompt: &#38271;&#30701;&#26399;&#25552;&#31034;&#19979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16132
&lt;/p&gt;
&lt;p&gt;
LSTPrompt&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20998;&#35299;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#25552;&#31034;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#36807;&#20998;&#31616;&#21270;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#23558;&#20854;&#35270;&#20026;&#35821;&#35328;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24573;&#35270;&#20102;&#20854;&#21160;&#24577;&#24615;&#20197;&#21450;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#65288;&#22914;Chain-of-Thought&#65289;&#30340;&#34701;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LSTPrompt&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#25552;&#31034;LLMs&#30340;&#26032;&#26041;&#27861;&#12290;LSTPrompt&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20998;&#35299;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#25552;&#31034;&#12290;LSTPrompt&#24341;&#23548;LLMs&#23450;&#26399;&#37325;&#26032;&#35780;&#20272;&#39044;&#27979;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#36866;&#24212;&#24615;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;LSTPrompt&#30340;&#24615;&#33021;&#22987;&#32456;&#26356;&#22909;&#65292;&#24182;&#19988;&#19982;&#22522;&#26412;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16132v1 Announce Type: cross  Abstract: Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.
&lt;/p&gt;</description></item><item><title>InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16123</link><description>&lt;p&gt;
InstructEdit&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
InstructEdit: Instruction-based Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16123
&lt;/p&gt;
&lt;p&gt;
InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#32780;&#19981;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#28040;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#32534;&#36753;&#22120;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#30693;&#35782;&#32534;&#36753;&#20013;&#30340;&#22810;&#20219;&#21153;&#27867;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#31216;&#20026;InstructEdit&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25351;&#20196;&#20419;&#36827;&#32534;&#36753;&#22120;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#21482;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#32534;&#36753;&#22120;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#65292;InstructEdit&#21487;&#20197;&#25552;&#39640;&#32534;&#36753;&#22120;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22810;&#20219;&#21153;&#32534;&#36753;&#35774;&#32622;&#20013;&#24179;&#22343;&#25552;&#39640;&#21487;&#38752;&#24615;14.86%&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#20445;&#30041;&#26410;&#35265;&#20219;&#21153;&#30340;&#23454;&#39564;&#35828;&#26126;&#65292;InstructEdi
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RepAPQ&#65292;&#21033;&#29992;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#26469;&#20943;&#36731;&#24322;&#24120;&#20540;&#23545;&#37327;&#21270;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20445;&#25345;&#37327;&#21270;&#21518;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.16121</link><description>&lt;p&gt;
&#20026;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22411;&#23454;&#29616;&#20934;&#30830;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Accurate Post-training Quantization for Reparameterized Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16121
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RepAPQ&#65292;&#21033;&#29992;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#26469;&#20943;&#36731;&#24322;&#24120;&#20540;&#23545;&#37327;&#21270;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20445;&#25345;&#37327;&#21270;&#21518;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#21442;&#25968;&#21270;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22411;&#26102;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#29305;&#23450;&#26679;&#26412;&#21644;&#36890;&#36947;&#19978;&#20986;&#29616;&#30340;&#24322;&#24120;&#20540;&#25152;&#23548;&#33268;&#30340;&#65292;&#36825;&#20123;&#24322;&#24120;&#20540;&#20165;&#20986;&#29616;&#22312;&#29305;&#23450;&#30340;&#26679;&#26412;&#21644;&#36890;&#36947;&#19978;&#65292;&#24182;&#23545;&#37327;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepAPQ&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20445;&#25345;&#37327;&#21270;&#21518;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20316;&#20026;&#24230;&#37327;&#30340;&#26694;&#26550;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#26469;&#20943;&#36731;&#24322;&#24120;&#20540;&#23545;&#37327;&#21270;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#20445;&#25252;&#37327;&#21270;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#36328;&#22359;&#26657;&#20934;&#12290;&#20026;&#20102;&#26377;&#25928;&#26657;&#20934;&#65292;&#35831;&#20102;&#35299;&#20445;&#25252;&#37327;&#21270;&#37325;&#26032;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16121v1 Announce Type: cross  Abstract: Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters. Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration. For effective calibration, Quantization Protecting Reparameterization
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27867;&#21270;&#26426;&#22120;&#20154;&#34892;&#20026;&#21512;&#25104;&#30340;&#26641;&#29366;&#22810;&#27169;&#24577;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#20855;&#26377;&#23558;&#39640;&#32423;&#20154;&#31867;&#25351;&#20196;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16117</link><description>&lt;p&gt;
RoboCodeX: &#29992;&#20110;&#26426;&#22120;&#20154;&#34892;&#20026;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16117
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27867;&#21270;&#26426;&#22120;&#20154;&#34892;&#20026;&#21512;&#25104;&#30340;&#26641;&#29366;&#22810;&#27169;&#24577;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#20855;&#26377;&#23558;&#39640;&#32423;&#20154;&#31867;&#25351;&#20196;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#34892;&#20026;&#21512;&#25104;&#26159;&#29702;&#35299;&#22810;&#27169;&#24577;&#36755;&#20837;&#24182;&#20026;&#26426;&#22120;&#20154;&#29983;&#25104;&#31934;&#30830;&#29289;&#29702;&#25511;&#21046;&#30340;&#38382;&#39064;&#65292;&#26159;&#20855;&#26377;&#20307;&#29616;&#29305;&#24449;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#23613;&#31649;&#22312;&#24212;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#32423;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23558;&#36825;&#20123;&#27010;&#24565;&#29702;&#35299;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#22312;&#23454;&#29616;&#36328;&#19981;&#21516;&#22330;&#26223;&#30340;&#27867;&#21270;&#26102;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27867;&#21270;&#26426;&#22120;&#20154;&#34892;&#20026;&#21512;&#25104;&#30340;&#26641;&#29366;&#22810;&#27169;&#24577;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#21517;&#20026;RoboCodeX&#12290;RoboCodeX&#23558;&#39640;&#32423;&#20154;&#31867;&#25351;&#20196;&#20998;&#35299;&#20026;&#30001;&#29289;&#29702;&#20559;&#22909;&#65288;&#22914;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#32422;&#26463;&#65289;&#32452;&#25104;&#30340;&#22810;&#20010;&#29289;&#20307;&#20013;&#24515;&#25805;&#32437;&#21333;&#20803;&#65292;&#24182;&#24212;&#29992;&#20195;&#30721;&#29983;&#25104;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#23558;&#27010;&#24565;&#21644;&#24863;&#30693;&#29702;&#35299;&#26144;&#23556;&#21040;&#25511;&#21046;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#19968;&#31181;&#19987;&#38376;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16117v1 Announce Type: cross  Abstract: Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#20803;&#32032;&#32423;&#21035;&#32780;&#38750;&#20256;&#32479;&#30340;&#23618;&#32423;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#20197;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.16091</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21442;&#25968;&#36873;&#25321;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Network For Personalized Federated Learning Parameter Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#20803;&#32032;&#32423;&#21035;&#32780;&#38750;&#20256;&#32479;&#30340;&#23618;&#32423;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#20197;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#24615;&#33021;&#19981;&#20339;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#26368;&#20026;&#32039;&#36843;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20559;&#31163;&#20256;&#32479;&#33539;&#24335;&#65292;&#20854;&#20013;&#25152;&#26377;&#23458;&#25143;&#31471;&#20351;&#29992;&#30456;&#21516;&#27169;&#22411;&#65292;&#32780;&#26159;&#21162;&#21147;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21457;&#29616;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#24615;&#12290;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#20010;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#23618;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#24182;&#27809;&#26377;&#25552;&#20379;&#21487;&#38752;&#30340;&#29702;&#30001;&#65292;&#26377;&#20123;&#36873;&#25321;&#20102;&#23436;&#20840;&#19981;&#21516;&#19988;&#30456;&#20114;&#20914;&#31361;&#30340;&#20010;&#24615;&#21270;&#23618;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26356;&#36827;&#19968;&#27493;&#65292;&#25552;&#35758;&#22312;&#20803;&#32032;&#32423;&#21035;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23618;&#32423;&#20010;&#24615;&#21270;&#12290;&#20026;&#20102;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20381;&#36182;&#23427;&#20204;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#25105;&#20204;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16091v1 Announce Type: cross  Abstract: Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm's efficacy on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;&#32593;&#32476;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#21487;&#23398;&#20064;&#30340;&#20960;&#20309;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.16086</link><description>&lt;p&gt;
&#29992;&#20110;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Homography Estimation for Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16086
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;&#32593;&#32476;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#21487;&#23398;&#20064;&#30340;&#20960;&#20309;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#20154;&#23450;&#20301;&#21644;&#22686;&#24378;&#29616;&#23454;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20998;&#23618;VPR&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#36890;&#24120;&#39318;&#20808;&#20351;&#29992;&#20840;&#23616;&#29305;&#24449;&#26469;&#26816;&#32034;&#20505;&#36873;&#22270;&#20687;&#65292;&#28982;&#21518;&#39564;&#35777;&#21305;&#37197;&#30340;&#23616;&#37096;&#29305;&#24449;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#20197;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#36890;&#24120;&#20381;&#36182;&#20110;RANSAC&#31639;&#27861;&#36827;&#34892;&#21333;&#24212;&#24615;&#25311;&#21512;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#19981;&#21487;&#24494;&#20998;&#30340;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#20013;&#35757;&#32451;&#32593;&#32476;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;(DHE)&#32593;&#32476;&#65292;&#20854;&#20197;&#30001;&#20027;&#24178;&#32593;&#32476;&#25552;&#21462;&#30340;&#23494;&#38598;&#29305;&#24449;&#22270;&#20026;&#36755;&#20837;&#65292;&#24182;&#36866;&#21512;&#20110;&#24555;&#36895;&#21644;&#21487;&#23398;&#20064;&#30340;&#20960;&#20309;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#28857;&#37325;&#25237;&#24433;&#35823;&#24046;&#25439;&#22833;&#26469;&#35757;&#32451;DHE&#32593;&#32476;&#65292;&#26080;&#38656;&#28155;&#21152;&#39069;&#22806;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16086v1 Announce Type: cross  Abstract: Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without addit
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16073</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20960;&#20046;&#23454;&#26102;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16073
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#23884;&#20837;&#26469;&#32534;&#30721;&#29992;&#25143;&#21160;&#20316;&#21644;&#39033;&#30446;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#26816;&#32034;&#65292;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#29992;&#25143;&#23884;&#20837;&#21487;&#33021;&#38480;&#21046;&#25152;&#25429;&#33719;&#30340;&#20852;&#36259;&#22810;&#26679;&#24615;&#65292;2&#65289;&#20445;&#25345;&#23427;&#20204;&#26368;&#26032;&#38656;&#35201;&#26114;&#36149;&#30340;&#23454;&#26102;&#22522;&#30784;&#35774;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#26356;&#26032;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27599;&#20004;&#20998;&#38047;&#32452;&#25104;&#19968;&#20010;&#20449;&#24687;&#27969;&#65292;&#21033;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#21450;&#20854;&#21508;&#33258;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#33655;&#20848;&#21644;&#27604;&#21033;&#26102;&#26368;&#22823;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#19968;Bol&#19978;&#27979;&#35797;&#24182;&#37096;&#32626;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#23548;&#33268;&#36716;&#21270;&#29575;&#26174;&#33879;&#25552;&#39640;&#20102;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
&lt;/p&gt;</description></item><item><title>ROS-Causal&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#32570;&#20047;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16068</link><description>&lt;p&gt;
ROS-Causal&#65306;&#22522;&#20110;ROS&#30340;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16068
&lt;/p&gt;
&lt;p&gt;
ROS-Causal&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#32570;&#20047;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#20849;&#20139;&#31354;&#38388;&#37096;&#32626;&#26426;&#22120;&#20154;&#38656;&#35201;&#29702;&#35299;&#38468;&#36817;Agent&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#23545;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#26377;&#21161;&#20110;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#24182;&#39044;&#27979;&#26426;&#22120;&#20154;&#24178;&#39044;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30446;&#21069;&#32570;&#20047;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#37096;&#30340;&#23454;&#29616;&#65292;&#36825;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ROS-Causal&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#19978;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#12290;&#38598;&#25104;&#20102;ROS&#30340;&#20020;&#26102;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#29983;&#25104;&#22240;&#26524;&#27169;&#22411;&#12290;ROS-Causal&#21487;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/lcastri/roscausal.git&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16068v1 Announce Type: cross  Abstract: Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22260;&#32469;&#21160;&#24577;&#26080;&#20154;&#26426;&#38654;&#37096;&#32626;&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#31995;&#32479;&#22312;&#21463;&#28798;&#22320;&#21306;&#30340;&#36866;&#24212;&#24615;&#21644;&#36816;&#33829;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.16052</link><description>&lt;p&gt;
&#29992;&#20110;&#37325;&#35201;&#25937;&#25588;&#34892;&#21160;&#30340;&#26368;&#22823;&#21270;&#26080;&#20154;&#26426;&#38654;&#37096;&#32626;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Maximizing UAV Fog Deployment Efficiency for Critical Rescue Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16052
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22260;&#32469;&#21160;&#24577;&#26080;&#20154;&#26426;&#38654;&#37096;&#32626;&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#31995;&#32479;&#22312;&#21463;&#28798;&#22320;&#21306;&#30340;&#36866;&#24212;&#24615;&#21644;&#36816;&#33829;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28798;&#38590;&#22330;&#26223;&#21644;&#39640;&#39118;&#38505;&#25937;&#25588;&#34892;&#21160;&#20013;&#65292;&#23558;&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#20316;&#20026;&#38654;&#33410;&#28857;&#25972;&#21512;&#24050;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#25972;&#21512;&#30830;&#20445;&#20102;&#21463;&#24433;&#21709;&#20154;&#32676;&#19982;&#22522;&#26412;&#20581;&#24247;&#30417;&#27979;&#35774;&#22791;&#20043;&#38388;&#30340;&#39034;&#30021;&#36830;&#25509;&#65292;&#30001;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22260;&#32469;&#21160;&#24577;&#26080;&#20154;&#26426;&#38654;&#37096;&#32626;&#30340;&#26032;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#31995;&#32479;&#22312;&#21463;&#28798;&#22320;&#21306;&#30340;&#36866;&#24212;&#24615;&#21644;&#36816;&#33829;&#25928;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#21363;&#36830;&#25509;&#21644;&#35206;&#30422;&#23376;&#38382;&#39064;&#65292;&#20197;&#21450;&#32593;&#32476;&#23551;&#21629;&#20248;&#21270;&#23376;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16052v1 Announce Type: cross  Abstract: In disaster scenarios and high-stakes rescue operations, integrating Unmanned Aerial Vehicles (UAVs) as fog nodes has become crucial. This integration ensures a smooth connection between affected populations and essential health monitoring devices, supported by the Internet of Things (IoT). Integrating UAVs in such environments is inherently challenging, where the primary objectives involve maximizing network connectivity and coverage while extending the network's lifetime through energy-efficient strategies to serve the maximum number of affected individuals. In this paper, We propose a novel model centred around dynamic UAV-based fog deployment that optimizes the system's adaptability and operational efficacy within the afflicted areas. First, we decomposed the problem into two subproblems. Connectivity and coverage subproblem, and network lifespan optimization subproblem. We shape our UAV fog deployment problem as a uni-objective op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#38382;&#31572;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#23601;&#65292;&#23588;&#20854;&#26159;&#22312;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.16038</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16038
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#38382;&#31572;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#23601;&#65292;&#23588;&#20854;&#26159;&#22312;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21033;&#29992;&#35832;&#22914;GPU&#21644;TPU&#31561;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20687;BERT&#21644;GPT-3&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#35821;&#20041;&#29702;&#35299;&#12289;&#26234;&#33021;&#20889;&#20316;&#21644;&#25512;&#29702;&#65292;&#20026;&#26356;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;NLP&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#24357;&#21512;&#20154;&#19982;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;NLP&#30340;&#24403;&#21069;&#26684;&#23616;&#21644;&#26410;&#26469;&#23637;&#26395;&#65292;&#37325;&#28857;&#25918;&#22312;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#38382;&#31572;&#31995;&#32479;&#19978;&#12290;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#23454;&#38469;&#26696;&#20363;&#21644;&#21457;&#23637;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16038v1 Announce Type: cross  Abstract: In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#26680;&#24515;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#31561;&#26041;&#38754;&#65292;&#22312;&#33258;&#21160;&#29983;&#25104;&#20135;&#21697;&#25551;&#36848;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#21644;&#23458;&#26381;&#23545;&#35805;&#33258;&#21160;&#22788;&#29702;&#31561;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#31215;&#26497;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16035</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#26234;&#33021;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#30340;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#26680;&#24515;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#31561;&#26041;&#38754;&#65292;&#22312;&#33258;&#21160;&#29983;&#25104;&#20135;&#21697;&#25551;&#36848;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#21644;&#23458;&#26381;&#23545;&#35805;&#33258;&#21160;&#22788;&#29702;&#31561;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#31215;&#26497;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Transformer&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20219;&#21153;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#65292;&#36825;&#20123;&#27169;&#22411;&#29305;&#21035;&#24191;&#27867;&#20351;&#29992;&#65292;&#20174;&#25991;&#26412;&#29702;&#35299;&#21040;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#65292;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#20248;&#21270;&#26381;&#21153;&#27969;&#31243;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#25991;&#26412;&#29702;&#35299;&#21644;&#25512;&#33616;&#29983;&#25104;&#20013;&#30340;&#26680;&#24515;&#24212;&#29992;&#22330;&#26223;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20135;&#21697;&#25551;&#36848;&#30340;&#33258;&#21160;&#29983;&#25104;&#65292;&#29992;&#25143;&#35780;&#35770;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#26500;&#24314;&#20197;&#21450;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#30340;&#33258;&#21160;&#22788;&#29702;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23454;&#29616;&#36807;&#31243;&#21644;&#29305;&#23450;&#26696;&#20363;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#20854;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16035v1 Announce Type: cross  Abstract: With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advan
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.16034</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Short English Texts using Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#30340;&#26377;&#38480;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#19968;&#39033;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#26694;&#26550;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#35789;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;BERT&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;SmallEnglishEmotions&#8221;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;6372&#20010;&#24102;&#26377;&#20116;&#31181;&#20027;&#35201;&#24773;&#32490;&#31867;&#21035;&#27880;&#37322;&#30340;&#19981;&#21516;&#30701;&#27874;&#26031;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#22312;&#20934;&#30830;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#26041;&#38754;&#20248;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16034v1 Announce Type: cross  Abstract: Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the "SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#65288;VCB&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16030</link><description>&lt;p&gt;
&#19981;&#35201;&#24536;&#35760;&#24744;&#30340;&#22870;&#21169;&#20215;&#20540;: &#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#65288;VCB&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#25285;&#24551;&#65292;&#25552;&#35758;&#19968;&#31995;&#21015;&#22522;&#20110;&#39034;&#24207;&#30340;&#26657;&#20934;&#26041;&#27861;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#24403;&#21069;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#65292;&#26816;&#26597;&#23427;&#20204;&#22312;&#21033;&#29992;&#22870;&#21169;&#20215;&#20540;&#21644;&#35299;&#20915;&#19981;&#23545;&#40784;&#38382;&#39064;&#26041;&#38754;&#30340;&#20302;&#25928;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#65288;VCB&#65289;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VCB&#22312;AI&#21161;&#25163;&#21644;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16030v1 Announce Type: cross  Abstract: While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.
&lt;/p&gt;</description></item><item><title>&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16021</link><description>&lt;p&gt;
TMT: &#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#35270;&#20026;&#19981;&#21516;&#35821;&#35328;&#26469;&#23454;&#29616;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19977;&#27169;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16021
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20849;&#21516;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#27491;&#22312;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#37197;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27169;&#32763;&#35793;&#65288;TMT&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#24847;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#24182;&#23558;&#22810;&#27169;&#24577;&#32763;&#35793;&#35270;&#20026;&#19968;&#20010;&#25104;&#29087;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#26631;&#35760;&#20026;&#31163;&#25955;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#36328;&#27169;&#24577;&#30340;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25552;&#20986;&#30340;TMT&#20013;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#36827;&#34892;&#26680;&#24515;&#32763;&#35793;&#65292;&#32780;&#27169;&#24577;&#29305;&#23450;&#22788;&#29702;&#20165;&#22312;&#26631;&#35760;&#21270;&#21644;&#21435;&#26631;&#35760;&#21270;&#38454;&#27573;&#20869;&#36827;&#34892;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#20845;&#31181;&#27169;&#24577;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;TMT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16021v1 Announce Type: cross  Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six mod
&lt;/p&gt;</description></item><item><title>OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16014</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#35745;&#31639;&#35268;&#27169;&#19978;&#26500;&#24314;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Flexible Machine Learning Models for Scientific Computing at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16014
&lt;/p&gt;
&lt;p&gt;
OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1 Announce Type: cross  Abstract: Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering ap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#32534;&#30721;&#30340;&#21518;&#37327;&#23376;&#23494;&#30721;&#26041;&#27861;&#26144;&#23556;&#21040;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;PQC&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12289;&#38543;&#26426;&#25200;&#21160;&#30340;&#23494;&#25991;&#21644;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#22686;&#24378;&#23494;&#25991;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16002</link><description>&lt;p&gt;
&#21518;&#37327;&#23376;&#23494;&#30721;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Post-Quantum Cryptography Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16002
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#32534;&#30721;&#30340;&#21518;&#37327;&#23376;&#23494;&#30721;&#26041;&#27861;&#26144;&#23556;&#21040;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;PQC&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12289;&#38543;&#26426;&#25200;&#21160;&#30340;&#23494;&#25991;&#21644;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#22686;&#24378;&#23494;&#25991;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#21644;Shor&#37327;&#23376;&#31639;&#27861;&#23545;&#24403;&#21069;&#20027;&#27969;&#30340;&#38750;&#23545;&#31216;&#21152;&#23494;&#26041;&#27861;&#65288;&#20363;&#22914;RSA&#21644;&#26925;&#22278;&#26354;&#32447;&#21152;&#23494;&#65288;ECC&#65289;&#65289;&#26500;&#25104;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#26500;&#24314;&#19968;&#31181;&#21518;&#37327;&#23376;&#23494;&#30721;&#65288;PQC&#65289;&#26041;&#27861;&#26469;&#25269;&#25239;&#37327;&#23376;&#35745;&#31639;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PQC&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#22522;&#20110;&#32534;&#30721;&#30340;PQC&#26041;&#27861;&#26144;&#23556;&#21040;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12289;&#38543;&#26426;&#25200;&#21160;&#30340;&#23494;&#25991;&#20197;&#21450;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#22686;&#24378;&#20102;&#23494;&#25991;&#30340;&#23433;&#20840;&#24615;&#12290;&#22312;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;&#26412;&#30740;&#31350;&#20197;&#34562;&#31389;&#32593;&#32476;&#20449;&#21495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#24314;&#35758;&#30340;&#22522;&#20110;PQC&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#36827;&#34892;&#21152;&#23494;&#21644;&#35299;&#23494;&#12290;&#26410;&#26469;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#22522;&#20110;PQC&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16002v1 Announce Type: cross  Abstract: In recent years, quantum computers and Shor quantum algorithm have posed a threat to current mainstream asymmetric cryptography methods (e.g. RSA and Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a Post-Quantum Cryptography (PQC) method to resist quantum computing attacks. Therefore, this study proposes a PQC-based neural network that maps a code-based PQC method to a neural network structure and enhances the security of ciphertexts with non-linear activation functions, random perturbation of ciphertexts, and uniform distribution of ciphertexts. In practical experiments, this study uses cellular network signals as a case study to demonstrate that encryption and decryption can be performed by the proposed PQC-based neural network with the uniform distribution of ciphertexts. In the future, the proposed PQC-based neural network could be applied to various applications.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#24320;&#21457;&#32773;&#35770;&#22363;&#21644;&#24179;&#21488;&#19978;&#30340;&#24086;&#23376;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30456;&#20851;&#30340;133&#20010;&#20027;&#39064;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#21253;&#25324;&#36719;&#20214;&#20381;&#36182;&#12289;&#27169;&#22411;&#37096;&#32626;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.15990</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Challenges in Machine Learning Asset Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15990
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#24320;&#21457;&#32773;&#35770;&#22363;&#21644;&#24179;&#21488;&#19978;&#30340;&#24086;&#23376;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30456;&#20851;&#30340;133&#20010;&#20027;&#39064;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#21253;&#25324;&#36719;&#20214;&#20381;&#36182;&#12289;&#27169;&#22411;&#37096;&#32626;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#65292;&#39640;&#25928;&#30340;&#36164;&#20135;&#31649;&#29702;&#65292;&#21253;&#25324;ML&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#12289;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#23545;&#20110;&#36164;&#28304;&#20248;&#21270;&#12289;&#25345;&#32493;&#24615;&#33021;&#21644;&#31616;&#21270;&#30340;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20351;&#24471;&#24555;&#36895;&#36845;&#20195;&#12289;&#36866;&#24212;&#24615;&#12289;&#20943;&#23569;&#24320;&#21457;&#21040;&#37096;&#32626;&#26102;&#38388;&#20197;&#21450;&#21487;&#38752;&#30340;&#36755;&#20986;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#30740;&#31350;&#65292;&#20294;&#22312;&#35832;&#22914;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;&#12289;&#25968;&#25454;&#21487;&#36861;&#28335;&#24615;&#21644;&#21327;&#20316;&#31561;&#25805;&#20316;&#25361;&#25112;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#35201;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#36825;&#23545;ML&#39033;&#30446;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#24320;&#21457;&#32773;&#35770;&#22363;&#21644;&#24179;&#21488;&#30340;15,065&#20010;&#24086;&#23376;&#65292;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#26469;&#20998;&#31867;&#26597;&#35810;&#65292;&#21033;&#29992;BERTopic&#25552;&#21462;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24320;&#25918;&#24335;&#21345;&#29255;&#25490;&#24207;&#21644;BERTopic&#32858;&#31867;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;133&#20010;&#19982;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20998;&#25104;16&#20010;&#23439;&#20027;&#39064;&#65292;&#20854;&#20013;&#36719;&#20214;&#20381;&#36182;&#12289;&#27169;&#22411;&#37096;&#32626;&#21644;&#27169;&#22411;&#35757;&#32451;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15990v1 Announce Type: cross  Abstract: In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle. This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs. Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects. Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering. We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;Proportional-Integral-Derivative&#65288;PID&#65289;&#38381;&#29615;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;&#21644;&#34920;&#31034;&#23481;&#37327;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#31283;&#23450;&#24615;&#21644;&#25239;&#22122;&#24615;&#65292;&#35299;&#20915;&#20102;&#36755;&#20986;&#34920;&#31034;&#20013;&#30340;&#31209;&#22349;&#32553;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15989</link><description>&lt;p&gt;
PIDformer: Transformer&#36935;&#35265;&#25511;&#21046;&#35770;
&lt;/p&gt;
&lt;p&gt;
PIDformer: Transformer Meets Control Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;Proportional-Integral-Derivative&#65288;PID&#65289;&#38381;&#29615;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;&#21644;&#34920;&#31034;&#23481;&#37327;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#31283;&#23450;&#24615;&#21644;&#25239;&#22122;&#24615;&#65292;&#35299;&#20915;&#20102;&#36755;&#20986;&#34920;&#31034;&#20013;&#30340;&#31209;&#22349;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;transformer&#26550;&#26500;&#30340;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#36755;&#20837;&#25439;&#22351;&#21644;&#36755;&#20986;&#34920;&#31034;&#20013;&#30340;&#31209;&#22349;&#32553;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#27880;&#24847;&#21147;&#20316;&#20026;&#33258;&#20027;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#20854;&#35299;&#20915;&#26041;&#26696;&#20013;&#22266;&#26377;&#22320;&#20419;&#36827;&#24179;&#28369;&#24615;&#65292;&#23548;&#33268;&#36739;&#20302;&#31209;&#30340;&#36755;&#20986;&#21644;&#38477;&#20302;&#30340;&#34920;&#31034;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#31283;&#24577;&#35299;&#23545;&#36755;&#20837;&#25200;&#21160;&#25935;&#24863;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#27604;&#20363;-&#31215;&#20998;-&#24494;&#20998;&#65288;PID&#65289;&#38381;&#29615;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#19982;&#19968;&#20010;&#21442;&#32771;&#28857;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#34920;&#31034;&#23481;&#37327;&#12290;&#36825;&#31181;&#38598;&#25104;&#26088;&#22312;&#22312;&#21152;&#24378;&#27169;&#22411;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#20445;&#30041;&#39640;&#39057;&#32454;&#33410;&#65292;&#20351;&#20854;&#26356;&#20855;&#25239;&#22122;&#24615;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#21463;&#25511;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#25797;&#38271;&#35299;&#20915;&#31209;&#23849;&#28291;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#25511;&#21046;&#26694;&#26550;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#22411;transformers&#65292;&#21363;PID-controlled Transformer&#65288;PIDformer&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15989v1 Announce Type: new  Abstract: In this work, we address two main shortcomings of transformer architectures: input corruption and rank collapse in their output representation. We unveil self-attention as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity. Moreover, the steady-state solution of the model is sensitive to input perturbations. We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity. This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient. The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse. Motivated by this control framework, we derive a novel class of transformers, PID-controlled Transformer (PIDform
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15987</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20559;&#24046;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Mitigation of Evaluation Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20284;&#28982;&#20316;&#20026;&#34913;&#37327;LLM&#23545;&#21477;&#23376;&#21487;&#20449;&#24230;&#30340;&#25351;&#26631;&#65292;&#21487;&#33021;&#20250;&#22240;&#21477;&#23376;&#34920;&#38754;&#24046;&#24322;&#65288;&#22914;&#35789;&#24207;&#21644;&#21477;&#23376;&#32467;&#26500;&#65289;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23558;LLMs&#29992;&#20110;&#35780;&#20272;&#65292;&#21487;&#33021;&#23384;&#22312;&#20284;&#28982;&#20559;&#24046;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#39640;&#20272;&#20855;&#26377;&#36739;&#39640;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#65292;&#32780;&#20302;&#20272;&#20855;&#26377;&#36739;&#20302;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#23545;LLM&#35780;&#20272;&#22120;&#20013;&#20284;&#28982;&#20559;&#24046;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#20284;&#28982;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#23454;&#20363;&#20316;&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20219;&#21153;&#26102;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#20960;&#31181;LLMs&#26174;&#31034;&#20986;&#20284;&#28982;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#36731;&#20102;&#36825;&#31181;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
&lt;/p&gt;</description></item><item><title>CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15968</link><description>&lt;p&gt;
CoDream&#65306;&#20351;&#29992;&#24322;&#26500;&#27169;&#22411;&#20132;&#25442;&#26790;&#24819;&#32780;&#19981;&#26159;&#27169;&#22411;&#36827;&#34892;&#32852;&#21512;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15968
&lt;/p&gt;
&lt;p&gt;
CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#25955;&#25968;&#25454;&#19978;&#30340;&#21327;&#20316;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#20135;&#29983;&#30340;&#8220;&#30693;&#35782;&#8221;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25193;&#23637;&#36825;&#19968;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; \codream &#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#21327;&#20316;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#65292;&#31867;&#20284;&#20110;&#22312;FL&#20013;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#32852;&#21512;&#20248;&#21270;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#25429;&#33719;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#31354;&#38388;&#20849;&#20139;&#30693;&#35782;&#20855;&#26377;&#35768;&#22810;&#22909;&#22788;&#65306;&#65288;1&#65289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#21363;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65307;&#65288;2&#65289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65307;&#65288;3&#65289;&#19982;&#23433;&#20840;&#32858;&#21512;&#20860;&#23481;&#65292;&#22240;&#27492;&#21487;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating "knowledge" derived from models, instead of model parameters. We present a novel framework called \codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20248;&#20808;&#35745;&#21010;&#21644;&#21160;&#24577;&#35268;&#21010;&#21046;&#23450;&#35745;&#21010;&#26469;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#38382;&#39064;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.15960</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Budget-Constrained Tool Learning with Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20248;&#20808;&#35745;&#21010;&#21644;&#21160;&#24577;&#35268;&#21010;&#21046;&#23450;&#35745;&#21010;&#26469;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#38382;&#39064;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#22312;&#24037;&#20855;&#23398;&#20064;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#26159;&#20851;&#27880;&#22312;&#29305;&#23450;&#39044;&#31639;&#32422;&#26463;&#19979;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#38382;&#39064;&#21364;&#34987;&#24191;&#27867;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#39044;&#31639;&#32422;&#26463;&#19979;&#21019;&#24314;&#19968;&#20010;&#20248;&#20808;&#35745;&#21010;&#65292;&#28982;&#21518;&#20877;&#21033;&#29992;&#24037;&#20855;&#12290;&#35813;&#35745;&#21010;&#27010;&#36848;&#20102;&#21487;&#34892;&#30340;&#24037;&#20855;&#21450;&#20854;&#21487;&#34987;&#20351;&#29992;&#30340;&#26368;&#22823;&#27425;&#25968;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#26356;&#24191;&#27867;&#30340;&#35282;&#24230;&#26469;&#20998;&#37197;&#39044;&#31639;&#12290;&#20026;&#20102;&#22312;&#19981;&#20135;&#29983;&#26174;&#33879;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#35745;&#21010;&#65292;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#39318;&#20808;&#20272;&#35745;&#20505;&#36873;&#24037;&#20855;&#30340;&#26377;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#21046;&#23450;&#35745;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#21508;&#31181;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15960v1 Announce Type: new  Abstract: Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked. This paper proposes a novel method for budget-constrained tool learning. Our approach involves creating a preferable plan under the budget constraint before utilizing the tools. This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for large language models. This allows them to allocate the budget from a broader perspective. To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience. Subsequently, we employ dynamic programming to formulate the plan. Experimental results demonstrate that our method can be integrated with various tool learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21512;&#25104;&#25915;&#20987;&#22330;&#26223;&#26469;&#25913;&#36827;&#23041;&#32961;&#35782;&#21035;&#21644;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15945</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#31649;&#29702;&#30340;&#21069;&#27839;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21512;&#25104;&#25915;&#20987;&#22330;&#26223;&#26469;&#25913;&#36827;&#23041;&#32961;&#35782;&#21035;&#21644;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#12290;&#38024;&#23545;&#32593;&#32476;&#23041;&#32961;&#19981;&#26029;&#28436;&#21464;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21512;&#25104;&#25915;&#20987;&#22330;&#26223;&#65292;&#20174;&#32780;&#20016;&#23500;&#25968;&#25454;&#38598;&#24182;&#25913;&#36827;&#23041;&#32961;&#35782;&#21035;&#12290;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#32467;&#21512;&#26159;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#20102;&#27169;&#22411;&#32858;&#28966;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#26816;&#27979;&#24494;&#22937;&#21644;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;GANs&#36890;&#36807;&#29983;&#25104;&#39069;&#22806;&#22810;&#26679;&#30340;&#25915;&#20987;&#25968;&#25454;&#65292;&#28085;&#30422;&#24050;&#30693;&#21644;&#26032;&#20852;&#30340;&#23041;&#32961;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#21452;&#37325;&#26041;&#27861;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#24212;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#32593;&#32476;&#25915;&#20987;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;KDD Cup &#21644; CICIDS2017 &#25968;&#25454;&#38598;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15945v1 Announce Type: cross  Abstract: This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on anomaly detection. In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification. Integrating attention mechanisms with Generative Adversarial Networks (GANs) is a key feature of the proposed method. The attention mechanism enhances the model's ability to focus on relevant features, essential for detecting subtle and complex attack patterns. In addition, GANs address the issue of data scarcity by generating additional varied attack data, encompassing known and emerging threats. This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks. The KDD Cup and CICIDS2017 datasets were used to validate this m
&lt;/p&gt;</description></item><item><title>FMware&#30340;&#29420;&#29305;&#23646;&#24615;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#38480;&#21046;&#23548;&#33268;&#20102;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.15943</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#36719;&#20214;&#24037;&#31243;&#65306;&#21487;&#20449;&#22522;&#30784;&#27169;&#22411;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#25361;&#25112;&#31934;&#36873;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15943
&lt;/p&gt;
&lt;p&gt;
FMware&#30340;&#29420;&#29305;&#23646;&#24615;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#38480;&#21046;&#23548;&#33268;&#20102;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foundation&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#23454;&#29616;&#26032;&#30340;&#29992;&#20363;&#21644;&#21830;&#19994;&#27169;&#22411;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#36719;&#20214;&#24320;&#21457;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;FMs&#26500;&#24314;&#30340;&#36719;&#20214;&#31216;&#20026;FMware&#12290;FMware&#30340;&#29420;&#29305;&#23646;&#24615;&#65288;&#20363;&#22914;&#25552;&#31034;&#12289;&#20195;&#29702;&#21644;&#32534;&#25490;&#30340;&#38656;&#27714;&#65289;&#65292;&#19982;FMs&#30340;&#20869;&#22312;&#38480;&#21046;&#65288;&#20363;&#22914;&#24187;&#35273;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#20840;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#24037;&#19994;&#32463;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;10&#20010;&#20851;&#38190;&#30340;SE4FMware&#25361;&#25112;&#65292;&#23548;&#33268;&#20225;&#19994;FMware&#24320;&#21457;&#21464;&#24471;&#20302;&#25928;&#12289;&#25104;&#26412;&#39640;&#26114;&#19988;&#39118;&#38505;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#38416;&#26126;&#20102;&#25105;&#20204;&#35774;&#24819;&#30340;&#21019;&#26032;&#36335;&#24452;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FMArts&#65292;&#36825;&#26159;&#25105;&#20204;&#20026;&#26500;&#24314;&#21487;&#20449;FMware&#32780;&#36827;&#34892;&#30340;&#38271;&#26399;&#21162;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FMArts&#30340;&#29420;&#29305;&#23646;&#24615;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#19968;&#31181;&#22823;&#22411;FMware&#35774;&#35745;&#21644;&#24320;&#21457;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15943v1 Announce Type: cross  Abstract: Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified 10 key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. In this paper, we discuss these challenges in detail and state the path for innovation that we envision. Next, we present FMArts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#39064;&#26465;&#20214;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#21644;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#65292;&#23558;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;3D&#35270;&#35273;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15933</link><description>&lt;p&gt;
&#36328;&#36234;2D&#21644;3D&#35270;&#35273;&#38382;&#31572;&#20043;&#38388;&#30340;&#40511;&#27807;&#65306;&#19968;&#31181;&#29992;&#20110;3D VQA&#30340;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15933
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#39064;&#26465;&#20214;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#21644;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#65292;&#23558;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;3D&#35270;&#35273;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#35270;&#35273;&#38382;&#31572;&#65288;3D VQA&#65289;&#20013;&#65292;&#20805;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#20869;&#23481;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#23545;&#26032;&#39062;&#22330;&#26223;&#21644;3D&#27010;&#24565;&#30340;&#27867;&#21270;&#65288;&#22914;ScanQA&#21644;SQA&#25968;&#25454;&#38598;&#20165;&#21033;&#29992;&#20102;&#32422;800&#20010;&#22330;&#26223;&#65289;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;2D&#20449;&#24687;&#26469;&#36741;&#21161;3D&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65306;&#23427;&#20204;&#35201;&#20040;&#20351;&#29992;&#24341;&#20837;&#36807;&#20110;&#22797;&#26434;&#19988;&#26377;&#26102;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;2D&#35270;&#22270;&#65292;&#35201;&#20040;&#20381;&#38752;&#26469;&#33258;2D VLM&#30340;&#20840;&#23616;&#32858;&#21512;&#22330;&#26223;/&#22270;&#20687;&#32423;&#34920;&#31034;&#65292;&#20174;&#32780;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#35821;&#35328;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#65292;&#20934;&#30830;&#22320;&#25351;&#20986;&#20102;&#20851;&#38190;&#35270;&#35273;&#32447;&#32034;&#30340;&#35821;&#20041;&#30456;&#20851;2D&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#23558;&#36825;&#31181;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#32467;&#26500;&#37319;&#29992;&#20102;&#21452;Transformer&#35774;&#35745;&#65292;&#32039;&#20945;&#22320;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15929</link><description>&lt;p&gt;
QuaCer-C&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#23450;&#37327;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#24182;&#26410;&#23545;LLMs&#30340;&#34920;&#29616;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#25105;&#20204;&#22312;&#27492;&#23545;&#30693;&#21517;LLMs&#30340;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#27491;&#24335;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#35777;&#20070;&#26159;&#23450;&#37327;&#30340; - &#23427;&#20204;&#21253;&#25324;&#23545;&#30446;&#26631;LLM&#22312;&#20219;&#20309;&#30456;&#20851;&#30693;&#35782;&#29702;&#35299;&#25552;&#31034;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#27010;&#29575;&#30340;&#39640;&#32622;&#20449;&#24230;&#32039;&#23494;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;Llama&#12289;Vicuna&#21644;Mistral LLMs&#30340;&#35777;&#20070;&#34920;&#26126;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#24182;&#19988;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15925</link><description>&lt;p&gt;
MultiContrievers: &#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MultiContrievers: Analysis of Dense Retrieval Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#22120;&#23558;&#28304;&#25991;&#26723;&#21387;&#32553;&#20026;&#65288;&#21487;&#33021;&#26159;&#26377;&#25439;&#30340;&#65289;&#21521;&#37327;&#34920;&#31034;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;&#22833;&#21435;&#21644;&#20445;&#30041;&#30340;&#20449;&#24687;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#26512;&#36739;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#23545;&#27604;&#31264;&#23494;&#26816;&#32034;&#22120;&#25429;&#33719;&#30340;&#20449;&#24687;&#19982;&#23427;&#20204;&#22522;&#20110;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#19982;Contriever&#65289;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;25&#20010;MultiBert&#26816;&#26597;&#28857;&#20316;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#26469;&#35757;&#32451;MultiContrievers&#65292;&#36825;&#26159;&#19968;&#32452;25&#20010;contriever&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#29305;&#23450;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#21644;&#32844;&#19994;&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26723;&#30340;contriever&#21521;&#37327;&#20013;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#25506;&#27979;&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#25552;&#21462;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#32467;&#26524;&#23545;&#35768;&#22810;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#27927;&#29260;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;contriever&#27169;&#22411;&#26377;&#26174;&#33879;&#22686;&#21152;&#30340;&#21487;&#25552;&#21462;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#23454;&#26102;&#20998;&#26512;&#20013;&#39044;&#27979;&#30005;&#23376;&#31454;&#25216;&#27604;&#36187;&#32467;&#26524;&#65292;&#20165;&#21033;&#29992;&#29609;&#23478;&#29983;&#21629;&#20540;&#25351;&#26631;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#36739;&#20043;&#20256;&#32479;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15923</link><description>&lt;p&gt;
&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#30005;&#23376;&#31454;&#25216;&#20013;&#39044;&#27979;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Predicting Outcomes in Video Games with Long Short Term Memory Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15923
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#23454;&#26102;&#20998;&#26512;&#20013;&#39044;&#27979;&#30005;&#23376;&#31454;&#25216;&#27604;&#36187;&#32467;&#26524;&#65292;&#20165;&#21033;&#29992;&#29609;&#23478;&#29983;&#21629;&#20540;&#25351;&#26631;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#36739;&#20043;&#20256;&#32479;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#30005;&#23376;&#31454;&#25216;&#20013;&#30340;&#32988;&#32773;&#65292;&#22312;&#23454;&#26102;&#20998;&#26512;&#20013;&#20855;&#26377;&#28508;&#21147;&#36827;&#19968;&#27493;&#21560;&#24341;&#35266;&#30475;&#20027;&#35201;&#38182;&#26631;&#36187;&#30340;&#35266;&#20247;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28216;&#25103;&#20013;&#28041;&#21450;&#19981;&#21516;&#29609;&#23478;&#31574;&#30053;&#21644;&#20915;&#31574;&#30340;&#19981;&#21487;&#39044;&#27979;&#21464;&#37327;&#65292;&#36827;&#34892;&#36825;&#31181;&#23454;&#26102;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23454;&#26102;&#39044;&#27979;&#32988;&#21033;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35270;&#39057;&#28216;&#25103;&#27604;&#36187;&#20013;&#35266;&#20247;&#30340;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTMs&#65289;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#20165;&#20351;&#29992;&#27599;&#20010;&#29609;&#23478;&#30340;&#29983;&#21629;&#20540;&#25351;&#26631;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#26469;&#39640;&#25928;&#39044;&#27979;&#32988;&#36127;&#32467;&#26524;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#22312;&#32463;&#20856;&#30340;&#20004;&#20154;&#34903;&#26426;&#28216;&#25103;&#12298;&#36229;&#32423;&#34903;&#38712;II Turbo&#12299;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#21363;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25214;&#21040;&#30340;Transformer&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#24076;&#26395;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15923v1 Announce Type: cross  Abstract: Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events. However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making. Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series. As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo. We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs). Finally, we open-source our data set and code in hopes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23467;&#39048;&#30284;&#32454;&#32990;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNNs&#36827;&#34892;&#24494;&#35843;&#24182;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#26368;&#23567;&#21270;&#35823;&#20998;&#31867;&#25104;&#26412;&#65292;&#36798;&#21040;&#20102;97.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15905</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#23545;&#27604;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#29992;&#20110;&#23467;&#39048;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23467;&#39048;&#30284;&#32454;&#32990;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNNs&#36827;&#34892;&#24494;&#35843;&#24182;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#26368;&#23567;&#21270;&#35823;&#20998;&#31867;&#25104;&#26412;&#65292;&#36798;&#21040;&#20102;97.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#23467;&#39048;&#30284;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#30340;CNNs&#65292;&#24182;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#30456;&#20851;&#25104;&#26412;&#25110;&#37325;&#35201;&#24615;&#30340;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#26469;&#26368;&#23567;&#21270;&#24635;&#30340;&#35823;&#20998;&#31867;&#25104;&#26412;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#27169;&#22411;&#26356;&#25797;&#38271;&#25429;&#25417;&#37325;&#35201;&#29305;&#24449;&#21644;&#27169;&#24335;&#12290;&#23545;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;SIPaKMeD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;97.29%&#30340;&#20934;&#30830;&#29575;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#31995;&#32479;&#26356;&#21152;&#21487;&#20449;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#26159;&#22914;&#20309;&#20570;&#20986;&#20855;&#20307;&#20915;&#31574;&#30340;&#12290;&#31995;&#32479;&#30340;&#23454;&#29616;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040; - https://github.com/isha-67/CervicalCancerStudy.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15905v1 Announce Type: cross  Abstract: This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs). We first fine-tune five pre-trained CNNs and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#24182;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.15903</link><description>&lt;p&gt;
&#39640;&#25928;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#26080;&#32447;&#35774;&#22791;&#19978;
&lt;/p&gt;
&lt;p&gt;
ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#24182;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#65288;&#20998;&#24067;&#24335;&#35774;&#22791;&#65289;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#35774;&#22791;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#30340;&#36164;&#28304;&#26159;&#19968;&#20010;&#26497;&#20855;&#21560;&#24341;&#21147;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#22312;&#20855;&#26377;&#24322;&#26500;&#31471;&#35774;&#22791;&#65288;EDs&#65289;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#30340;&#24378;&#22823;&#35745;&#31639;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;EDs&#20043;&#38388;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65292;&#36825;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36845;&#20195;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#33719;&#24471;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15903v1 Announce Type: cross  Abstract: Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive simulations have been c
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;Bayesian&#32479;&#35745;&#23884;&#20837;&#21040;&#26356;&#24191;&#27867;&#30340;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#32479;&#35745;&#28216;&#25103;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15892</link><description>&lt;p&gt;
&#32479;&#35745;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Statistical Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;Bayesian&#32479;&#35745;&#23884;&#20837;&#21040;&#26356;&#24191;&#27867;&#30340;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#32479;&#35745;&#28216;&#25103;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#20960;&#31181;&#20856;&#22411;&#30340;&#28216;&#25103;&#36827;&#34892;&#20102;&#25968;&#23398;&#25506;&#32034;&#65292;&#20854;&#20013;&#33258;&#28982;&#28044;&#29616;&#20102;&#32479;&#35745;&#23398;&#21644;&#27010;&#29575;&#35770;&#30340;&#26680;&#24515;&#27010;&#24565;&#12290;&#36825;&#20123;&#28216;&#25103;&#21253;&#25324;&#36153;&#33293;&#23572;&#28216;&#25103;&#21644;&#36125;&#21494;&#26031;&#28216;&#25103;&#65292;&#23427;&#20204;&#20998;&#21035;&#19982;&#39057;&#29575;&#27966;&#32479;&#35745;&#23398;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#30456;&#20851;&#12290;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#31216;&#20026;&#32479;&#35745;&#28216;&#25103;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#35774;&#32622;&#19968;&#20010;&#36827;&#19968;&#27493;&#30340;&#21442;&#25968;&#65292;&#21363;&#29609;&#23478;&#30340;&#30456;&#23545;&#39118;&#38505;&#21388;&#24694;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36153;&#33293;&#23572;&#28216;&#25103;&#21644;&#36125;&#21494;&#26031;&#28216;&#25103;&#21487;&#20197;&#34987;&#35270;&#20026;&#32479;&#35745;&#28216;&#25103;&#30340;&#26497;&#38480;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#32479;&#35745;&#28216;&#25103;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21746;&#23398;&#26694;&#26550;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#65292;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15892v1 Announce Type: cross  Abstract: This work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge. The first two kinds of games are termed Fisher and Bayesian games, which are connected to Frequentist and Bayesian statistics, respectively. Later, a more general type of game is introduced, termed Statistical game, in which a further parameter, the players' relative risk aversion, can be set. In this work, we show that Fisher and Bayesian games can be viewed as limiting cases of Statistical games. Therefore, Statistical games can be viewed as a unified framework, incorporating both Frequentist and Bayesian statistics. Furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   The main motivation for this work was to embed Bayesian statistics into a broader decision-making framework, whe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#33041;&#32959;&#30244;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#24314;&#31435;&#20102;&#21360;&#24230;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#24615;&#33021;&#22522;&#20934;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26032;&#30340;&#35780;&#32423;&#21644;&#26816;&#27979;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.15832</link><description>&lt;p&gt;
&#21033;&#29992;&#33487;&#26408;&#31934;&#19982;&#20234;&#32418;&#26579;&#33394;&#25972;&#24352;&#22270;&#20687;&#36827;&#34892;&#33014;&#36136;&#30244;&#35786;&#26029;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65306;&#21360;&#24230;&#38431;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#33041;&#32959;&#30244;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#24314;&#31435;&#20102;&#21360;&#24230;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#24615;&#33021;&#22522;&#20934;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26032;&#30340;&#35780;&#32423;&#21644;&#26816;&#27979;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#20195;&#34920;&#19968;&#31181;&#20005;&#37325;&#19988;&#21361;&#21450;&#29983;&#21629;&#30340;&#30142;&#30149;&#65292;&#38656;&#35201;&#31934;&#30830;&#30340;&#35786;&#26029;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#33041;&#32959;&#30244;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#20005;&#26684;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#23454;&#39564;&#30340;&#21457;&#29616;&#65292;&#25512;&#21160;&#20102;&#24739;&#32773;&#25252;&#29702;&#12290;&#23427;&#22312;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#26041;&#38754;&#24314;&#31435;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#20010;&#19987;&#27880;&#20110;&#21360;&#24230;&#20154;&#21475;&#30340;&#26032;&#25968;&#25454;&#38598;&#65288;IPD-Brain&#65289;&#65292;&#20026;&#29616;&#26377;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#20351;&#29992;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;ResNet-50&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#32467;&#21512;DTFD&#29305;&#24449;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#22312;IPD-Brain&#21644;TCGA-Brain&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19977;&#20998;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#30340;&#26368;&#26032;AUC&#65288;&#20998;&#21035;&#20026;88.08&#21644;95.81&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#35780;&#32423;&#21644;&#26816;&#27979;IHC&#20998;&#23376;&#29983;&#29289;&#26631;&#24535;&#29289;&#65288;IDH1&#65288;&#31361;&#21464; R132H&#65289;&#12289;TP53&#12289;ATRX&#12289;Ki-67&#65289;&#26041;&#38754;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15832v1 Announce Type: cross  Abstract: Brain tumors represent a severe and life-threatening condition, demanding precise diagnosis and tailored treatment strategies. This study advances patient care with findings from rigorous multiple-instance-learning experimentations across various feature extractors and aggregators in brain tumor histopathology. It establishes new performance benchmarks in glioma subtype classification across multiple datasets, including a novel dataset focused on the Indian demographic (IPD-Brain), providing a valuable resource for existing research. Using a ResNet-50, pretrained on histopathology datasets, for feature extraction, combined with DTFD feature aggregator, our approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on TCGA-Brain dataset respectively for three-way glioma subtype classification. Moreover, it establishes new benchmarks in grading and detecting IHC molecular biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) t
&lt;/p&gt;</description></item><item><title>&#20195;&#29702;&#36890;&#36807;&#36777;&#35770;&#22411;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#21487;&#36777;&#26126;&#31574;&#30053;&#65292;&#20197;&#25903;&#25345;&#35777;&#25454;&#35777;&#26126;&#20915;&#31574;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15826</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#29702;&#24207;&#36143;&#20915;&#31574;&#30340;&#22870;&#21169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reward Design for Justifiable Sequential Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15826
&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#36890;&#36807;&#36777;&#35770;&#22411;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#21487;&#36777;&#26126;&#31574;&#30053;&#65292;&#20197;&#25903;&#25345;&#35777;&#25454;&#35777;&#26126;&#20915;&#31574;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20195;&#29702;&#36171;&#20104;&#33021;&#22815;&#20351;&#29992;&#25903;&#25345;&#35777;&#25454;&#26469;&#35777;&#26126;&#20915;&#31574;&#30340;&#33021;&#21147;&#26159;&#36127;&#36131;&#20219;&#20915;&#31574;&#30340;&#22522;&#30707;&#12290;&#27492;&#22806;&#65292;&#30830;&#20445;&#36825;&#20123;&#35777;&#26126;&#19982;&#20154;&#31867;&#26399;&#26395;&#21644;&#31038;&#20250;&#35268;&#33539;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36777;&#35770;&#22411;&#22870;&#21169;&#27169;&#22411;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20854;&#20013;&#38646;&#21644;&#36777;&#35770;&#28216;&#25103;&#30340;&#32467;&#26524;&#37327;&#21270;&#20102;&#22312;&#29305;&#23450;&#29366;&#24577;&#19979;&#30340;&#20915;&#31574;&#30340;&#21512;&#29702;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#27169;&#22411;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#26356;&#23481;&#26131;&#22320;&#19982;&#25903;&#25345;&#35777;&#25454;&#30456;&#21360;&#35777;&#30340;&#21487;&#36777;&#26126;&#31574;&#30053;&#12290;&#22312;&#36777;&#35770;&#28216;&#25103;&#20013;&#65292;&#20004;&#20010;&#36777;&#25252;&#24615;&#20195;&#29702;&#36718;&#27969;&#25552;&#20379;&#25903;&#25345;&#35777;&#25454;&#65292;&#25903;&#25345;&#20004;&#20010;&#31454;&#20105;&#24615;&#20915;&#31574;&#12290;&#22312;&#25552;&#20379;&#30340;&#35777;&#25454;&#26465;&#20214;&#19979;&#65292;&#19968;&#20010;&#20154;&#31867;&#27861;&#23448;&#30340;&#20195;&#29702;&#35780;&#20272;&#21738;&#20010;&#20915;&#31574;&#26356;&#21512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22788;&#26041;&#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15826v1 Announce Type: cross  Abstract: Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22996;&#25176;&#28216;&#25103;&#20013;&#25506;&#35752;&#20102;&#25511;&#21046;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#25353;&#29031;&#20854;&#22996;&#25176;&#20154;&#30340;&#20559;&#22909;&#34892;&#20107;&#65289;&#21644;&#21512;&#20316;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#33391;&#22909;&#22320;&#21327;&#20316;&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#40784;&#21644;&#33021;&#21147;&#23545;&#22996;&#25176;&#20154;&#31119;&#21033;&#30340;&#24433;&#21709;&#12290;en_tdlr: This paper explores the issues of control (agents failing to act in line with their principals' preferences) and cooperation (agents failing to work well together) in delegation games, analyzing how alignment and capabilities impact principals' welfare.</title><link>https://arxiv.org/abs/2402.15821</link><description>&lt;p&gt;
&#22996;&#25176;&#28216;&#25103;&#20013;&#30340;&#21512;&#20316;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cooperation and Control in Delegation Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22996;&#25176;&#28216;&#25103;&#20013;&#25506;&#35752;&#20102;&#25511;&#21046;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#25353;&#29031;&#20854;&#22996;&#25176;&#20154;&#30340;&#20559;&#22909;&#34892;&#20107;&#65289;&#21644;&#21512;&#20316;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#33391;&#22909;&#22320;&#21327;&#20316;&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#40784;&#21644;&#33021;&#21147;&#23545;&#22996;&#25176;&#20154;&#31119;&#21033;&#30340;&#24433;&#21709;&#12290;en_tdlr: This paper explores the issues of control (agents failing to act in line with their principals' preferences) and cooperation (agents failing to work well together) in delegation games, analyzing how alignment and capabilities impact principals' welfare.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#24863;&#20852;&#36259;&#30340;&#22330;&#26223; - &#20174;&#34394;&#25311;&#20010;&#20154;&#21161;&#29702;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742; - &#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22996;&#25176;&#20154;&#65288;&#20154;&#31867;&#65289;&#22996;&#25176;&#32473;&#20195;&#29702;&#20154;&#65288;&#26426;&#22120;&#65289;&#65292;&#36825;&#20123;&#20195;&#29702;&#20154;&#20043;&#21518;&#20195;&#34920;&#20182;&#20204;&#30340;&#22996;&#25176;&#20154;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#22996;&#25176;&#20154;&#65292;&#22810;&#20195;&#29702;&#20154;&#30340;&#24773;&#20917;&#31216;&#20026;&#22996;&#25176;&#28216;&#25103;&#12290;&#22312;&#36825;&#31867;&#28216;&#25103;&#20013;&#65292;&#23384;&#22312;&#20004;&#31181;&#37325;&#35201;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;&#25511;&#21046;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#25353;&#29031;&#20854;&#22996;&#25176;&#20154;&#30340;&#20559;&#22909;&#34892;&#20107;&#65289;&#21644;&#21512;&#20316;&#38382;&#39064;&#65288;&#20195;&#29702;&#20154;&#26410;&#33021;&#33391;&#22909;&#22320;&#21327;&#20316;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#21644;&#20998;&#26512;&#36825;&#20123;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23558;&#20854;&#35299;&#37322;&#20026;&#23545;&#40784;&#65288;&#21442;&#19982;&#32773;&#26159;&#21542;&#20855;&#26377;&#30456;&#20284;&#30340;&#20559;&#22909;&#65311;&#65289;&#21644;&#33021;&#21147;&#65288;&#21442;&#19982;&#32773;&#22312;&#28385;&#36275;&#36825;&#20123;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#25514;&#26045;&#22914;&#20309;&#30830;&#23450;&#22996;&#25176;&#20154;&#30340;&#31119;&#21033;&#65292;&#22914;&#20309;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#35266;&#23519;&#26469;&#20272;&#35745;&#36825;&#20123;&#25514;&#26045;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15821v1 Announce Type: cross  Abstract: Many settings of interest involving humans and machines -- from virtual personal assistants to autonomous vehicles -- can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf. We refer to these multi-principal, multi-agent scenarios as delegation games. In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together). In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?). We show -- theoretically and empirically -- how these measures determine the principals' welfare, how they can be estimated using limited observations, and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DART&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#32972;&#26223;&#25248;&#22270;&#30340;&#23454;&#26102;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15820</link><description>&lt;p&gt;
DART: &#28145;&#24230;&#22686;&#24378;&#30340;&#31934;&#30830;&#23454;&#26102;&#32972;&#26223;&#25248;&#22270;
&lt;/p&gt;
&lt;p&gt;
DART: Depth-Enhanced Accurate and Real-Time Background Matting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15820
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DART&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#32972;&#26223;&#25248;&#22270;&#30340;&#23454;&#26102;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;RGB-Depth (RGB-D)&#30456;&#26426;&#25552;&#20379;&#30340;&#20016;&#23500;&#28145;&#24230;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#32972;&#26223;&#25248;&#22270;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#21629;&#21517;&#20026;DART&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#21407;&#22987;&#22522;&#20110;RGB&#30340;BGM&#31639;&#27861;&#20197;&#25972;&#21512;&#28145;&#24230;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#23545;&#36755;&#20986;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#65292;&#34701;&#20837;&#20102;&#32972;&#26223;&#28145;&#24230;&#20808;&#39564;&#12290;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15820v1 Announce Type: cross  Abstract: Matting with a static background, often referred to as ``Background Matting" (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing. Nevertheless, achieving highly accurate background matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images. These limitations manifest in the form of susceptibility to varying lighting conditions and unforeseen shadows.   In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information. The resulting model's output undergoes refinement through Bayesian inference, incorporating a background depth prior. The posterior prediction is then translated into a 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#25913;&#36827;&#21160;&#20316;&#65292;&#22686;&#24378;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15809</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#23398;&#20064;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Empowering Large Language Model Agents through Action Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15809
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#25913;&#36827;&#21160;&#20316;&#65292;&#22686;&#24378;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#36817;&#26469;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20174;&#35797;&#38169;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#26159;&#26234;&#33021;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;LLM&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20154;&#31867;&#33258;&#28982;&#22320;&#25193;&#23637;&#20182;&#20204;&#30340;&#21160;&#20316;&#31354;&#38388;&#24182;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#21457;&#23637;&#25216;&#33021;&#65292;&#20294;LLM&#20195;&#29702;&#36890;&#24120;&#22312;&#22266;&#23450;&#30340;&#21160;&#20316;&#31354;&#38388;&#20869;&#25805;&#20316;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25104;&#38271;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LearnAct&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#26469;&#21019;&#24314;&#21644;&#25913;&#36827;Python&#20989;&#25968;&#24418;&#24335;&#30340;&#21160;&#20316;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;LLM&#26681;&#25454;&#22312;&#22833;&#36133;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#35782;&#21035;&#20986;&#30340;&#38169;&#35823;&#65292;&#20462;&#35746;&#21644;&#26356;&#26032;&#24403;&#21069;&#21487;&#29992;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#21160;&#20316;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15809v1 Announce Type: new  Abstract: Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations acr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.15808</link><description>&lt;p&gt;
&#22810;&#33218;&#25915;&#20987;&#30340;&#26368;&#20339;&#38646;&#23556;&#20987;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Optimal Zero-Shot Detector for Multi-Armed Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24694;&#24847;&#21442;&#19982;&#32773;&#37319;&#29992;&#22810;&#33218;&#25915;&#20987;&#31574;&#30053;&#25805;&#32437;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#24335;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#26816;&#27979;&#20219;&#20309;&#23545;&#36755;&#20837;&#30340;&#26356;&#25913;&#26469;&#20445;&#25252;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#38450;&#24481;&#31574;&#30053;&#20013;&#26497;&#24230;&#35880;&#24910;&#65292;&#25805;&#20316;&#22312;&#38450;&#23432;&#32773;&#25317;&#26377;&#20449;&#24687;&#26126;&#26174;&#23569;&#20110;&#25915;&#20987;&#32773;&#30340;&#29615;&#22659;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38450;&#23432;&#32773;&#26080;&#27861;&#21033;&#29992;&#20219;&#20309;&#25968;&#25454;&#26679;&#26412;&#26469;&#35757;&#32451;&#38450;&#24481;&#27169;&#22411;&#25110;&#39564;&#35777;&#20449;&#36947;&#30340;&#23436;&#25972;&#24615;&#12290;&#30456;&#21453;&#65292;&#38450;&#23432;&#32773;&#23436;&#20840;&#20381;&#36182;&#19968;&#32452;&#29616;&#25104;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#25506;&#27979;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#36825;&#20123;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#20351;&#29992;&#26696;&#20363;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15808v1 Announce Type: cross  Abstract: This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario fo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20174;&#22823;&#37327;&#20302;&#31934;&#24230;GPS&#36712;&#36857;&#25968;&#25454;&#34701;&#21512;&#29983;&#25104;&#39640;&#31934;&#24230;GPS&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#31038;&#20132;&#36710;&#36742;&#30340;&#22320;&#22270;&#25968;&#25454;&#25910;&#38598;&#30340;&#8220;&#20247;&#21253;&#26356;&#26032;&#8221;&#27169;&#22411;&#65292;&#23545;&#25552;&#39640;&#25968;&#25454;&#20934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15796</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#36712;GPS&#25968;&#25454;&#26500;&#24314;&#21644;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20247;&#21253;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Construction and application of artificial intelligence crowdsourcing map based on multi-track GPS data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15796
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20174;&#22823;&#37327;&#20302;&#31934;&#24230;GPS&#36712;&#36857;&#25968;&#25454;&#34701;&#21512;&#29983;&#25104;&#39640;&#31934;&#24230;GPS&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#31038;&#20132;&#36710;&#36742;&#30340;&#22320;&#22270;&#25968;&#25454;&#25910;&#38598;&#30340;&#8220;&#20247;&#21253;&#26356;&#26032;&#8221;&#27169;&#22411;&#65292;&#23545;&#25552;&#39640;&#25968;&#25454;&#20934;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39640;&#31934;&#24230;&#22320;&#22270;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#26234;&#33021;&#36710;&#36742;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#12290;&#39640;&#31934;&#24230;&#22320;&#22270;&#25216;&#26415;&#26159;&#26234;&#33021;&#36710;&#36742;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#37325;&#35201;&#20445;&#38556;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#39640;&#31934;&#24230;&#22320;&#22270;&#25216;&#26415;&#30340;&#30740;&#31350;&#19981;&#36275;&#65292;&#38590;&#20197;&#22312;&#26234;&#33021;&#36710;&#36742;&#39046;&#22495;&#21512;&#29702;&#20351;&#29992;&#36825;&#19968;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#30456;&#20851;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20174;&#22823;&#37327;&#20302;&#31934;&#24230;GPS&#36712;&#36857;&#25968;&#25454;&#34701;&#21512;&#29983;&#25104;&#39640;&#31934;&#24230;GPS&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20102;&#33509;&#24178;&#20851;&#38190;&#25968;&#25454;&#28857;&#31616;&#21270;GPS&#36712;&#36857;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22823;&#37327;&#31038;&#20132;&#36710;&#36742;&#30340;&#22320;&#22270;&#25968;&#25454;&#25910;&#38598;&#30340;&#8220;&#20247;&#21253;&#26356;&#26032;&#8221;&#27169;&#22411;&#12290;&#36825;&#31181;&#31639;&#27861;&#23545;&#25552;&#39640;&#25968;&#25454;&#20934;&#30830;&#24615;&#12289;&#38477;&#20302;&#27979;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15796v1 Announce Type: new  Abstract: In recent years, the rapid development of high-precision map technology combined with artificial intelligence has ushered in a new development opportunity in the field of intelligent vehicles. High-precision map technology is an important guarantee for intelligent vehicles to achieve autonomous driving. However, due to the lack of research on high-precision map technology, it is difficult to rationally use this technology in the field of intelligent vehicles. Therefore, relevant researchers studied a fast and effective algorithm to generate high-precision GPS data from a large number of low-precision GPS trajectory data fusion, and generated several key data points to simplify the description of GPS trajectory, and realized the "crowdsourced update" model based on a large number of social vehicles for map data collection came into being. This kind of algorithm has the important significance to improve the data accuracy, reduce the measur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21152;&#23494;&#36827;&#34892;&#30772;&#35299;&#21644;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#26497;&#22823;&#22320;&#21152;&#23494;&#36890;&#20449;&#26426;&#21046;&#65292;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#24182;&#20943;&#23569;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15779</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21152;&#23494;&#30340;&#30772;&#35299;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21152;&#23494;&#36827;&#34892;&#30772;&#35299;&#21644;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#26497;&#22823;&#22320;&#21152;&#23494;&#36890;&#20449;&#26426;&#21046;&#65292;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#24182;&#20943;&#23569;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#26085;&#28176;&#27969;&#34892;&#20197;&#21450;&#36890;&#36807;&#20113;&#21644;&#25968;&#25454;&#20013;&#24515;&#24191;&#27867;&#20351;&#29992;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#65292;&#20010;&#20154;&#21644;&#32452;&#32455;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#21464;&#24471;&#26497;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#21152;&#23494;&#36890;&#36807;&#20445;&#25252;&#20844;&#20849;&#20449;&#24687;&#20132;&#27969;&#26469;&#26377;&#25928;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20102;&#21508;&#31181;&#21152;&#23494;&#31639;&#27861;&#26469;&#28385;&#36275;&#35813;&#39046;&#22495;&#30340;&#19981;&#21516;&#35201;&#27714;&#65292;&#24182;&#22312;&#24037;&#20316;&#36807;&#31243;&#20013;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#21152;&#23494;&#36890;&#20449;&#26426;&#21046;&#65292;&#20197;&#23613;&#21487;&#33021;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#25915;&#20987;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15779v1 Announce Type: cross  Abstract: With the rising popularity of the internet and the widespread use of networks and information systems via the cloud and data centers, the privacy and security of individuals and organizations have become extremely crucial. In this perspective, encryption consolidates effective technologies that can effectively fulfill these requirements by protecting public information exchanges. To achieve these aims, the researchers used a wide assortment of encryption algorithms to accommodate the varied requirements of this field, as well as focusing on complex mathematical issues during their work to substantially complicate the encrypted communication mechanism. as much as possible to preserve personal information while significantly reducing the possibility of attacks. Depending on how complex and distinct the requirements established by these various applications are, the potential of trying to break them continues to occur, and systems for eva
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#32593;&#32476;&#23433;&#20840;&#26694;&#26550;&#22312;&#21830;&#19994;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#26426;&#36935;&#12289;&#39118;&#38505;&#21644;&#21512;&#35268;&#24615;&#65292;&#21457;&#29616;&#26032;&#30340;ISO 42001:2023&#23545;LLMs&#30340;&#26426;&#36935;&#25552;&#20379;&#20102;&#26368;&#20840;&#38754;&#30340;&#25903;&#25345;&#65292;COBIT 2019&#19982;&#27431;&#30431;AI&#27861;&#26696;&#26368;&#20026;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2402.15770</link><description>&lt;p&gt;
&#20174;COBIT&#21040;ISO 42001&#65306;&#35780;&#20272;&#21830;&#19994;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#36935;&#12289;&#39118;&#38505;&#21644;&#21512;&#35268;&#24615;&#30340;&#32593;&#32476;&#23433;&#20840;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#32593;&#32476;&#23433;&#20840;&#26694;&#26550;&#22312;&#21830;&#19994;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#26426;&#36935;&#12289;&#39118;&#38505;&#21644;&#21512;&#35268;&#24615;&#65292;&#21457;&#29616;&#26032;&#30340;ISO 42001:2023&#23545;LLMs&#30340;&#26426;&#36935;&#25552;&#20379;&#20102;&#26368;&#20840;&#38754;&#30340;&#25903;&#25345;&#65292;COBIT 2019&#19982;&#27431;&#30431;AI&#27861;&#26696;&#26368;&#20026;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22235;&#31181;&#20027;&#35201;&#30340;&#32593;&#32476;&#23433;&#20840;&#27835;&#29702;&#12289;&#39118;&#38505;&#21644;&#21512;&#35268;&#65288;GRC&#65289;&#26694;&#26550; - NIST CSF 2.0&#12289;COBIT 2019&#12289;ISO 27001:2022&#21644;&#26368;&#26032;&#30340;ISO 42001:2023 - &#22312;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#23545;&#26426;&#36935;&#12289;&#39118;&#38505;&#21644;&#21512;&#35268;&#24615;&#30340;&#25972;&#21512;&#20934;&#22791;&#24773;&#20917;&#65292;&#37319;&#29992;&#23450;&#24615;&#20869;&#23481;&#20998;&#26512;&#21644;&#19987;&#23478;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#28041;&#21450;LLMs&#21644;&#20154;&#31867;&#19987;&#23478;&#65292;&#25581;&#31034;&#20102;LLM&#25972;&#21512;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#36825;&#20123;&#26694;&#26550;&#23545;LLM&#39118;&#38505;&#30417;&#30563;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#27604;&#36739;&#24615;&#24046;&#36317;&#20998;&#26512;&#31361;&#20986;&#26174;&#31034;&#65292;&#19987;&#20026;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31649;&#29702;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;ISO 42001:2023&#20026;LLM&#26426;&#36935;&#25552;&#20379;&#20102;&#26368;&#20840;&#38754;&#30340;&#20415;&#21033;&#65292;&#32780;COBIT 2019&#19982;&#21363;&#23558;&#20986;&#21488;&#30340;&#27431;&#30431;AI&#27861;&#26696;&#26368;&#20026;&#25509;&#36817;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#35780;&#20272;&#30340;&#26694;&#26550;&#37117;&#23558;&#21463;&#30410;&#20110;&#26356;&#26377;&#25928;&#22320;&#22686;&#24378;&#26356;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15770v1 Announce Type: cross  Abstract: This study investigated the integration readiness of four predominant cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0, COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the opportunities, risks, and regulatory compliance when adopting Large Language Models (LLMs), using qualitative content analysis and expert validation. Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks. Comparative gap analysis has highlighted that the new ISO 42001:2023, specifically designed for Artificial Intelligence (AI) management systems, provided most comprehensive facilitation for LLM opportunities, whereas COBIT 2019 aligned most closely with the impending European Union AI Act. Nonetheless, our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;GenCode&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#25351;&#26631;&#36873;&#25321;&#29983;&#25104;&#30340;&#20195;&#30721;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#29702;&#35299;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.15769</link><description>&lt;p&gt;
&#37325;&#28857;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#30721;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Importance Guided Data Augmentation for Neural-Based Code Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15769
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;GenCode&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#25351;&#26631;&#36873;&#25321;&#29983;&#25104;&#30340;&#20195;&#30721;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#29702;&#35299;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15769v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#27169;&#22411;&#24320;&#21551;&#20102;&#20195;&#30721;&#26234;&#33021;&#26102;&#20195;&#12290;&#26368;&#36817;&#35768;&#22810;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#23398;&#20064;&#39046;&#22495;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#33258;&#21160;&#36827;&#34892;&#20195;&#30721;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#32773;&#20934;&#22791;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#19981;&#36275;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;GenCode&#65292;&#29992;&#20110;&#22686;&#24378;&#20195;&#30721;&#29702;&#35299;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;GenCode&#36981;&#24490;&#19968;&#31181;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#33539;&#24335;&#26469;&#20934;&#22791;&#26377;&#29992;&#30340;&#35757;&#32451;&#20195;&#30721;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20351;&#29992;&#20195;&#30721;&#36716;&#25442;&#25216;&#26415;&#39318;&#20808;&#29983;&#25104;&#26032;&#30340;&#20195;&#30721;&#20505;&#36873;&#65292;&#28982;&#21518;&#36890;&#36807;&#37325;&#35201;&#24615;&#25351;&#26631;&#36873;&#25321;&#37325;&#35201;&#30340;&#20195;&#30721;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;GenCode&#19982;&#36890;&#29992;&#37325;&#35201;&#24615;&#25351;&#26631;&#65288;&#25439;&#22833;&#20540;&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#20195;&#30721;&#29702;&#35299;&#20219;&#21153;&#65288;&#22914;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65289;&#21644;&#19977;&#20010;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#65288;&#22914;CodeT5&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#22686;&#24378;&#25216;&#26415;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15769v1 Announce Type: cross  Abstract: Pre-trained code models lead the era of code intelligence. Many models have been designed with impressive performance recently. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in the field of code learning. In this paper, we introduce a general data augmentation framework, GenCode, to enhance the training of code understanding models. GenCode follows a generation-and-selection paradigm to prepare useful training codes. Specifically, it uses code transformation techniques to generate new code candidates first and then selects important ones as the training data by importance metrics. To evaluate the effectiveness of GenCode with a general importance metric -- loss value, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code augm
&lt;/p&gt;</description></item><item><title>PhyPlan&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#20449;&#24687;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#25913;&#36827;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#20855;&#26377;&#36523;&#20307;&#30340;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#21160;&#24577;&#29289;&#29702;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#21160;&#24577;&#20915;&#23450;&#22312;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#29615;&#22659;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.15767</link><description>&lt;p&gt;
PhyPlan&#65306;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#32593;&#32476;&#30340;&#26426;&#22120;&#20154;&#26426;&#26800;&#33218;&#29289;&#29702;&#20219;&#21153;&#25512;&#29702;&#30340;&#32452;&#21512;&#21644;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15767
&lt;/p&gt;
&lt;p&gt;
PhyPlan&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#20449;&#24687;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#25913;&#36827;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#20855;&#26377;&#36523;&#20307;&#30340;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#21160;&#24577;&#29289;&#29702;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#21160;&#24577;&#20915;&#23450;&#22312;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#29615;&#22659;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#23558;&#29699;&#29366;&#29289;&#20307;&#23450;&#20301;&#21040;&#36229;&#20986;&#30452;&#25509;&#35302;&#21450;&#33539;&#22260;&#30340;&#30446;&#26631;&#21306;&#22495;&#36825;&#19968;&#20219;&#21153;&#65292;&#20154;&#31867;&#36890;&#24120;&#20250;&#25237;&#25527;&#12289;&#28369;&#21160;&#25110;&#23558;&#29289;&#20307;&#21453;&#24377;&#21040;&#22681;&#19978;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#35753;&#26426;&#22120;&#20154;&#36827;&#34892;&#31867;&#20284;&#25512;&#29702;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#29616;&#26377;&#30340;&#29289;&#29702;&#25512;&#29702;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PhyPlan&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#35268;&#21010;&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#25913;&#36827;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#20351;&#20855;&#26377;&#36523;&#20307;&#30340;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#21160;&#24577;&#29289;&#29702;&#20219;&#21153;&#12290;PhyPlan&#21033;&#29992;PINNs&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#39044;&#27979;&#21160;&#20316;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#29992;MCTS&#36827;&#34892;&#35268;&#21010;&#12290;&#23427;&#21160;&#24577;&#30830;&#23450;&#26159;&#35201;&#21672;&#35810;&#22522;&#20110;PINN&#30340;&#27169;&#25311;&#22120;&#65288;&#31895;&#31961;&#20294;&#24555;&#36895;&#65289;&#36824;&#26159;&#30452;&#25509;&#19982;&#23454;&#38469;&#29615;&#22659;&#20114;&#21160;&#65288;&#31934;&#32454;&#20294;&#32531;&#24930;&#65289;&#20197;&#30830;&#23450;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#27169;&#25311;&#30340;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26426;&#22120;&#20154;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15767v1 Announce Type: cross  Abstract: Given the task of positioning a ball-like object to a goal region beyond direct reach, humans can often throw, slide, or rebound objects against the wall to attain the goal. However, enabling robots to reason similarly is non-trivial. Existing methods for physical reasoning are data-hungry and struggle with complexity and uncertainty inherent in the real world. This paper presents PhyPlan, a novel physics-informed planning framework that combines physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search (MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan leverages PINNs to simulate and predict outcomes of actions in a fast and accurate manner and uses MCTS for planning. It dynamically determines whether to consult a PINN-based simulator (coarse but fast) or engage directly with the actual environment (fine but slow) to determine optimal policy. Evaluation with robots in simulated 3D environm
&lt;/p&gt;</description></item><item><title>PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.15764</link><description>&lt;p&gt;
&#22312;&#36339;&#27133;&#20043;&#21069;&#19977;&#24605;&#65306;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15764
&lt;/p&gt;
&lt;p&gt;
PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#65288;PEP&#65289;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#20043;&#21069;&#20998;&#35299;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEP&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#38382;&#39064;&#25552;&#20986;&#30340;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
&lt;/p&gt;</description></item><item><title>Res-VMamba&#21033;&#29992;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#27604;Transformer&#32467;&#26500;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#26159;&#39135;&#21697;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15761</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#39135;&#21697;&#31867;&#21035;&#35270;&#35273;&#20998;&#31867;&#30340;Res-VMamba
&lt;/p&gt;
&lt;p&gt;
Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15761
&lt;/p&gt;
&lt;p&gt;
Res-VMamba&#21033;&#29992;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#27604;Transformer&#32467;&#26500;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#26159;&#39135;&#21697;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#20998;&#31867;&#26159;&#21457;&#23637;&#39135;&#21697;&#35270;&#35273;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#24182;&#22312;&#35745;&#31639;&#33829;&#20859;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#39135;&#29289;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#26368;&#36817;&#30340;&#23398;&#26415;&#30740;&#31350;&#20027;&#35201;&#20462;&#25913;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#21644;/&#25110;&#35270;&#35273;&#21464;&#21387;&#22120;(ViTs)&#26469;&#25191;&#34892;&#39135;&#21697;&#31867;&#21035;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23398;&#20064;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;CNN&#39592;&#24178;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#32780;&#21253;&#21547;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;ViT&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#25512;&#20986;&#30340;&#26032;&#30340;&#24207;&#21015;&#29366;&#24577;&#31354;&#38388;(S4)&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#21644;&#19982;&#25195;&#25551;(S6)&#30340;&#35745;&#31639;&#65292;&#20439;&#31216;&#20026;Mamba&#65292;&#30456;&#36739;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#23558;Mamba&#26426;&#21046;&#25972;&#21512;&#21040;&#22270;&#20687;&#20219;&#21153;(&#22914;&#20998;&#31867;)&#20013;&#30340;VMamba&#27169;&#22411;&#30446;&#21069;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15761v1 Announce Type: cross  Abstract: Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#12290;</title><link>https://arxiv.org/abs/2402.15759</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#25552;&#39640;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#24615;&#33021;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15759
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#38646;&#26679;&#26412;&#20998;&#21106;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#25991;&#26412;-&#35270;&#35273;-&#25552;&#31034;SAM&#65288;TV-SAM&#65289;&#65292;&#26080;&#38656;&#20219;&#20309;&#25163;&#21160;&#26631;&#27880;&#12290;TV-SAM&#34701;&#21512;&#24182;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12289;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;GLIP&#21644;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#65292;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36793;&#30028;&#26694;&#25552;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;SAM&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;&#22312;&#19971;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#28085;&#30422;&#20843;&#31181;&#25104;&#20687;&#27169;&#24335;&#65292;&#35777;&#26126;TV-SAM&#21487;&#20197;&#26377;&#25928;&#22320;&#36328;&#21508;&#31181;&#27169;&#24335;&#20998;&#21106;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#26126;&#26174;&#20248;&#20110;SAM AUTO&#21644;GSAM, &#19982;&#37329;&#26631;&#20934;&#36793;&#30028;&#26694;&#25552;&#31034;&#30340;SAM BBOX&#24615;&#33021;&#22522;&#26412;&#21305;&#25932;&#65292;&#24182;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#65288;&#22914;ISIC&#21644;WBC&#65289;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TV-SAM&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15759v1 Announce Type: cross  Abstract: This study develops and evaluates a novel multimodal medical image zero-shot segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual annotations. TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images, thereby enhancing SAM for zero-shot segmentation. Comprehensive evaluations are implemented on seven public datasets encompassing eight imaging modalities to demonstrate that TV-SAM can effectively segment unseen targets across various modalities without additional training, significantly outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX with gold standard bounding box prompts, and surpassing the state-of-the-art on specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as an effective multimodal 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15758</link><description>&lt;p&gt;
Chimera: &#34701;&#21512;&#25152;&#26377;&#20196;&#29260;&#30340;&#26080;&#25439;&#35299;&#30721;&#26041;&#27861;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#34987;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#25152;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#21512;&#24182;&#20102;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#21518;&#32493;&#20196;&#29260;&#30340;&#24182;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#30721;&#22836;&#30340;&#20934;&#30830;&#24615;&#36828;&#19981;&#21450;&#33258;&#22238;&#24402;&#35299;&#30721;&#26041;&#27861;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25512;&#27979;&#37319;&#26679;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#12290;&#20026;&#20102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#36731;&#37327;&#32423;&#33609;&#31295;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#24213;&#23618;&#25429;&#33719;&#30701;&#31243;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#30701;&#24182;&#21487;&#24182;&#34892;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15757</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Batch Active Learning of Reward Functions from Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#30701;&#24182;&#21487;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#29983;&#25104;&#21644;&#26631;&#35760;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24448;&#24448;&#25104;&#26412;&#39640;&#26114;&#12290;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#21521;&#29992;&#25143;&#25552;&#20986;&#20559;&#22909;&#38382;&#39064;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#26631;&#35760;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26032;&#31639;&#27861;&#65292;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#30701;&#30340;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#65292;&#24182;&#20445;&#25345;&#21487;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#29983;&#25104;&#21644;&#20960;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#20013;&#20171;&#32461;&#20102;&#19968;&#20123;&#26426;&#22120;&#20154;&#23398;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15757v1 Announce Type: cross  Abstract: Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this paper, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes (DPP) for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.15751</link><description>&lt;p&gt;
&#31232;&#30095;MeZO&#65306;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#36890;&#24120;&#20250;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#32780;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#21033;&#29992;&#23384;&#20648;&#22120;&#30340;&#38646;&#38454;&#65288;MeZO&#65289;&#20248;&#21270;&#22120;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#20869;&#23384;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#24448;&#24448;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19982;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;MeZO&#20173;&#28982;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#21040;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;MeZO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#20165;&#23558;ZO&#24212;&#29992;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#36873;&#25321;&#26041;&#26696;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26234;&#33021;&#23548;&#28436;&#26694;&#26550;&#65292;&#32467;&#21512;LENS&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#24103;&#29983;&#25104;&#25551;&#36848;&#65292;&#20877;&#21033;&#29992;ChatGPT&#29983;&#25104;&#36830;&#36143;&#23383;&#24149;&#21644;&#25512;&#33616;&#38899;&#20048;&#21517;&#31216;&#65292;&#36890;&#36807;&#38899;&#20048;&#26816;&#32034;&#33719;&#24471;&#26368;&#20339;&#21305;&#37197;&#38899;&#20048;&#65292;&#26368;&#32456;&#25972;&#21512;&#21508;&#31181;&#32032;&#26448;&#23454;&#29616;&#25925;&#20107;&#24615;&#35270;&#39057;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.15746</link><description>&lt;p&gt;
&#26234;&#33021;&#23548;&#28436;&#65306;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21160;&#24577;&#35270;&#35273;&#26500;&#22270;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26234;&#33021;&#23548;&#28436;&#26694;&#26550;&#65292;&#32467;&#21512;LENS&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#24103;&#29983;&#25104;&#25551;&#36848;&#65292;&#20877;&#21033;&#29992;ChatGPT&#29983;&#25104;&#36830;&#36143;&#23383;&#24149;&#21644;&#25512;&#33616;&#38899;&#20048;&#21517;&#31216;&#65292;&#36890;&#36807;&#38899;&#20048;&#26816;&#32034;&#33719;&#24471;&#26368;&#20339;&#21305;&#37197;&#38899;&#20048;&#65292;&#26368;&#32456;&#25972;&#21512;&#21508;&#31181;&#32032;&#26448;&#23454;&#29616;&#25925;&#20107;&#24615;&#35270;&#39057;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;TikTok&#20026;&#20195;&#34920;&#30340;&#30701;&#35270;&#39057;&#24179;&#21488;&#30340;&#20852;&#36215;&#65292;&#29992;&#25143;&#36890;&#36807;&#29031;&#29255;&#21644;&#35270;&#39057;&#34920;&#36798;&#21019;&#24847;&#30340;&#36235;&#21183;&#24613;&#21095;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#29992;&#25143;&#32570;&#20047;&#20351;&#29992;&#19987;&#19994;&#21019;&#20316;&#36719;&#20214;&#21046;&#20316;&#39640;&#36136;&#37327;&#35270;&#39057;&#30340;&#19987;&#19994;&#25216;&#33021;&#12290;&#20026;&#28385;&#36275;&#26234;&#33021;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35270;&#39057;&#21019;&#20316;&#24037;&#20855;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#35270;&#35273;&#26500;&#22270;&#65288;DVC&#65289;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#33258;&#21160;&#25972;&#21512;&#21508;&#31181;&#23186;&#20307;&#20803;&#32032;&#24182;&#21019;&#24314;&#21465;&#20107;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#23548;&#28436;&#26694;&#26550;&#65292;&#21033;&#29992;LENS&#20026;&#22270;&#20687;&#21644;&#35270;&#39057;&#24103;&#29983;&#25104;&#25551;&#36848;&#65292;&#32467;&#21512;ChatGPT&#29983;&#25104;&#36830;&#36143;&#23383;&#24149;&#21516;&#26102;&#25512;&#33616;&#36866;&#24403;&#30340;&#38899;&#20048;&#21517;&#31216;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#38899;&#20048;&#26816;&#32034;&#33719;&#24471;&#26368;&#20339;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#25509;&#30528;&#65292;&#23383;&#24149;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#20048;&#31561;&#32032;&#26448;&#34987;&#25972;&#21512;&#36215;&#26469;&#65292;&#26080;&#32541;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15746v1 Announce Type: cross  Abstract: With the rise of short video platforms represented by TikTok, the trend of users expressing their creativity through photos and videos has increased dramatically. However, ordinary users lack the professional skills to produce high-quality videos using professional creation software. To meet the demand for intelligent and user-friendly video creation tools, we propose the Dynamic Visual Composition (DVC) task, an interesting and challenging task that aims to automatically integrate various media elements based on user requirements and create storytelling videos. We propose an Intelligent Director framework, utilizing LENS to generate descriptions for images and video frames and combining ChatGPT to generate coherent captions while recommending appropriate music names. Then, the best-matched music is obtained through music retrieval. Then, materials such as captions, images, videos, and music are integrated to seamlessly synthesize the 
&lt;/p&gt;</description></item><item><title>GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#20013;&#22269;&#20154;&#31867;&#27700;&#24179;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#24050;&#32463;&#22312;&#22270;&#20687;&#24863;&#30693;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26497;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#26412;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#20123;&#26080;&#27861;&#20805;&#20998;&#21453;&#26144;&#20986;LVLMs&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;8&#20010;&#31185;&#30446;&#21644;12&#31181;&#31867;&#22411;&#30340;&#22270;&#29255;&#65292;&#22914;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#12289;&#22320;&#22270;&#21644;&#29031;&#29255;&#12290;GAOKAO-MM&#26469;&#28304;&#20110;&#20013;&#22269;&#26412;&#22303;&#32972;&#26223;&#65292;&#24182;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#29702;&#35299;&#12289;&#30693;&#35782;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;LVLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#65292;&#20854;&#20013;GPT-4-Vision&#65288;48.1%&#65289;&#12289;Qwen-VL-Plus&#65288;41.2%&#65289;&#21644;Gemini-Pro-Vision&#65288;35.1%&#65289;&#20301;&#21015;&#21069;&#19977;&#21517;&#12290;&#25105;&#20204;&#30340;&#22810;&#32500;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;LVLMs&#20855;&#26377;&#36866;&#24230;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v1 Announce Type: cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moder
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.15729</link><description>&lt;p&gt;
&#20154;&#31867;&#26159;&#22914;&#20309;&#32534;&#20889;&#20195;&#30721;&#30340;&#65311;&#22823;&#22411;&#27169;&#22411;&#20063;&#20197;&#21516;&#26679;&#30340;&#26041;&#24335;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
How Do Humans Write Code? Large Models Do It the Same Way Too
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15729
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#12290;&#19982;&#20256;&#32479;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30456;&#27604;&#65292;&#31243;&#24207;&#21270;&#24605;&#32500;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#25191;&#34892;&#36825;&#20123;&#20195;&#30721;&#65292;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#32780;&#19981;&#26159;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;LLMs&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#29983;&#25104;&#27604;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26356;&#22810;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-Think Language&#65288;HTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#30001;&#27169;&#22411;&#29983;&#25104;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35299;&#20915;&#38382;&#39064;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#20195;&#30721;&#65292;&#21453;&#26144;&#20986;&#20154;&#20204;&#22312;&#23558;&#36923;&#36753;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#24605;&#32771;&#21518;&#20877;&#23558;&#20854;&#20889;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15729v1 Announce Type: new  Abstract: Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SELFDEFEND&#30340;&#36731;&#37327;&#32423;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24310;&#36831;&#19979;&#25269;&#24481;&#25152;&#26377;&#29616;&#26377;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.15727</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#20197;&#23454;&#29992;&#30340;&#26041;&#24335;&#33258;&#25105;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#65306;&#19968;&#20221;&#23637;&#26395;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SELFDEFEND&#30340;&#36731;&#37327;&#32423;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24310;&#36831;&#19979;&#25269;&#24481;&#25152;&#26377;&#29616;&#26377;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21487;&#20197;&#32469;&#36807;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#12290;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#26377;&#25928;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#65288;GCG&#65289;&#25915;&#20987;&#12289;&#22522;&#20110;&#36234;&#29425;&#27169;&#26495;&#30340;&#25915;&#20987;&#65292;&#20363;&#22914;&#20351;&#29992;&#8220;Do-Anything-Now&#8221;&#65288;DAN&#65289;&#65292;&#20197;&#21450;&#22810;&#35821;&#35328;&#36234;&#29425;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38450;&#24481;&#26041;&#38754;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32780;&#23454;&#29992;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;SELFDEFEND&#65292;&#21487;&#20197;&#25269;&#24481;&#25152;&#26377;&#29616;&#26377;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#22312;&#36234;&#29425;&#25552;&#31034;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24310;&#36831;&#65292;&#23545;&#20110;&#27491;&#24120;&#29992;&#25143;&#25552;&#31034;&#20063;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#36234;&#29425;&#31574;&#30053;&#65292;&#26368;&#32456;&#37117;&#38656;&#35201;&#22312;&#21457;&#36865;&#32473;LLMs&#30340;&#25552;&#31034;&#20013;&#21253;&#21547;&#26377;&#23475;&#25552;&#31034;&#65288;&#20363;&#22914;&#8220;&#22914;&#20309;&#21046;&#36896;&#28856;&#24377;&#8221;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#36829;&#21453;&#23433;&#20840;&#35268;&#21017;&#30340;&#26377;&#23475;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15727v1 Announce Type: cross  Abstract: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using "Do-Anything-Now" (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful prompt (e.g., "how to make a bomb") in the prompt sent to LLMs, and we found that existing LLMs can effectively recognize such harmful prompts that violate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hal-Eval&#65292;&#19968;&#20010;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#19987;&#27880;&#20110;&#20107;&#20214;&#24187;&#35273;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#36807;&#28388;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#26469;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#21508;&#31181;&#24187;&#35273;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15721</link><description>&lt;p&gt;
Hal-Eval: &#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hal-Eval&#65292;&#19968;&#20010;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#19987;&#27880;&#20110;&#20107;&#20214;&#24187;&#35273;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#36807;&#28388;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#26469;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#21508;&#31181;&#24187;&#35273;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22270;&#29255;&#21644;&#20854;&#25551;&#36848;&#20043;&#38388;&#23384;&#22312;&#24187;&#35273;&#19981;&#19968;&#33268;&#12290;&#20197;&#24448;&#23545;LVLMs&#36827;&#34892;&#30340;&#24187;&#35273;&#35780;&#20272;&#30740;&#31350;&#21457;&#29616;&#20102;&#20851;&#20110;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#24187;&#35273;&#65292;&#20294;&#24573;&#30053;&#20102;&#22260;&#32469;&#34394;&#26500;&#23454;&#20307;&#21019;&#24314;&#25972;&#20010;&#21465;&#20107;&#30340;&#22797;&#26434;&#24187;&#35273;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#31867;&#21035;&#65306;&#20107;&#20214;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#21644;&#36807;&#28388;&#30001;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#32452;&#25104;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#65292;&#29305;&#21035;&#20851;&#27880;&#20107;&#20214;&#24187;&#35273;&#65292;&#20026;&#22312;&#25105;&#20204;&#30340;&#36890;&#29992;&#35780;&#20272;&#26694;&#26550;&#20869;&#38598;&#25104;&#36776;&#21035;&#21644;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#22880;&#23450;&#22522;&#30784;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#21487;&#20197;&#29420;&#29305;&#22320;&#35780;&#20272;LVLMs&#22788;&#29702;&#24191;&#27867;&#24187;&#35273;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#21644;&#20840;&#38754;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15721v1 Announce Type: new  Abstract: Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#22909;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;</title><link>https://arxiv.org/abs/2402.15713</link><description>&lt;p&gt;
&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#22909;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#65288;CFRE&#65289;&#26159;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#38656;&#35201;&#27169;&#22411;&#22312;&#36991;&#20813;&#24536;&#35760;&#26087;&#20851;&#31995;&#30340;&#21516;&#26102;&#36830;&#32493;&#23398;&#20064;&#26032;&#20851;&#31995;&#65292;&#21482;&#26377;&#26497;&#23569;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#21033;&#29992;&#25552;&#31034;&#23398;&#20064;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#24335;&#33021;&#21147;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#25552;&#31034;&#34920;&#31034;&#20197;&#33719;&#24471;&#26356;&#24191;&#20041;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26087;&#30340;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#22522;&#20110;&#36793;&#30028;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#26356;&#22810;&#22320;&#20851;&#27880;&#22256;&#38590;&#26679;&#26412;&#65292;&#20174;&#32780;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35760;&#24518;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15713v1 Announce Type: cross  Abstract: Continual Few-shot Relation Extraction (CFRE) is a practical problem that requires the model to continuously learn novel relations while avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses prompt learning to explore the implicit capabilities of pre-trained language models to address the above two challenges, thereby making language models better continual few-shot relation extractors. Specifically, we propose a Contrastive Prompt Learning framework, which designs prompt representation to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based contrastive learning to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in low-resource scenarios, we introduce an effective memory augmentation strategy that employs well-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.15708</link><description>&lt;p&gt;
&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#26597;&#35810;&#35821;&#20041;&#30340;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Augmentation by Decoding Semantics from Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#29992;&#20110;&#32454;&#21270;&#35821;&#20041;&#19981;&#20934;&#30830;&#26597;&#35810;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#26597;&#35810;&#25193;&#23637;&#20381;&#36182;&#20110;&#20174;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#12289;&#28508;&#22312;&#30456;&#20851;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22914;&#26524;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#36136;&#37327;&#36739;&#20302;&#65292;&#21017;&#26597;&#35810;&#25193;&#23637;&#30340;&#26377;&#25928;&#24615;&#20063;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Brain-Aug&#65292;&#36890;&#36807;&#23558;&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#26597;&#35810;&#20013;&#26469;&#22686;&#24378;&#26597;&#35810;&#12290;Brain-Aug&#20351;&#29992;&#20102;&#22312;&#33041;&#20449;&#21495;&#20449;&#24687;&#26500;&#24314;&#30340;&#25552;&#31034;&#21644;&#38754;&#21521;&#25490;&#21517;&#30340;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#21407;&#22987;&#26597;&#35810;&#30340;&#24310;&#32493;&#37096;&#20998;&#12290;&#23545;fMRI&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Brain-Aug&#29983;&#25104;&#30340;&#26597;&#35810;&#22312;&#35821;&#20041;&#19978;&#26356;&#20934;&#30830;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#12290;&#33041;&#20449;&#21495;&#24102;&#26469;&#30340;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#27169;&#31946;&#26597;&#35810;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20026;&#22312;&#20165;&#26377;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.15703</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#20197;&#20165;&#20973;&#26377;&#38480;&#26679;&#26412;&#36827;&#34892;&#31163;&#32447;&#20915;&#31574;&#65311;&#36890;&#36807;&#20449;&#20219;&#21306;&#22495;&#22686;&#24378;&#22312;&#25968;&#25454;&#31232;&#32570;&#36172;&#21338;&#26426;&#20013;&#21487;&#38752;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20026;&#22312;&#20165;&#26377;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#21482;&#21253;&#21547;&#27599;&#20010;&#33218;&#30340;&#21333;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#33021;&#20174;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#22312;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#30340;&#29615;&#22659;&#20013;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#31574;&#30053;&#12290;&#36825;&#20026;&#22312;&#24517;&#39035;&#20165;&#20381;&#38752;&#23569;&#25968;&#26679;&#26412;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;\emph{&#38543;&#26426;&#31574;&#30053;&#23545;&#20110;&#31163;&#32447;&#20915;&#31574;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#30830;&#23450;&#24615;&#31574;&#30053;}&#12290;&#19987;&#27880;&#20110;&#31163;&#32447;&#22810;&#33218;&#32769;&#34382;&#26426;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#20219;&#21306;&#22495;&#30340;&#38543;&#26426;&#31574;&#30053;&#22686;&#24378;&#65288;TRUST&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#19982;&#20027;&#23548;&#24615;&#20215;&#20540;&#20026;&#22522;&#30784;&#30340;&#36739;&#20302;&#32622;&#20449;&#19979;&#30028;&#26041;&#27861;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#20854;&#35774;&#35745;&#24471;&#30410;&#20110;&#23450;&#20301;&#35268;&#24459;&#12289;&#20020;&#30028;&#21322;&#24452;&#21644;&#30456;&#23545;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;L&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15703v1 Announce Type: cross  Abstract: What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   Our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of L
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15700</link><description>&lt;p&gt;
&#12298;CoRelation: &#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#25552;&#21319;&#33258;&#21160;ICD&#32534;&#30721;&#12299;
&lt;/p&gt;
&lt;p&gt;
CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15700
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#32534;&#30721;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#20197;&#20415;&#27491;&#30830;&#35760;&#24405;&#21644;&#35745;&#36153;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25552;&#21319;&#33258;&#21160;ICD&#32534;&#30721;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#21521;&#26159;&#23545;ICD&#32534;&#30721;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#23545;ICD&#32534;&#30721;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#24314;&#27169;&#19981;&#36275;&#65292;&#36890;&#24120;&#24573;&#35270;&#20102;&#20020;&#24202;&#35760;&#24405;&#20013;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#21644;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#32771;&#34385;&#20020;&#24202;&#35760;&#24405;&#19978;&#19979;&#25991;&#30340;&#20381;&#36182;&#23398;&#20064;&#33539;&#24335;&#65292;&#23545;&#24314;&#27169;&#25152;&#26377;&#21487;&#33021;&#30340;&#32534;&#30721;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;ICD&#32534;&#30721;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15700v1 Announce Type: cross  Abstract: Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35748;&#30693;&#19968;&#33268;&#24615;&#29702;&#35770;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25552;&#31034;&#25552;&#20379;&#20102;&#24515;&#29702;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#33050;-&#38376;&#25216;&#26415;&#30340;&#33258;&#21160;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15690</link><description>&lt;p&gt;
&#36367;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8220;&#36234;&#29425;&#8221;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35748;&#30693;&#19968;&#33268;&#24615;&#29702;&#35770;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25552;&#31034;&#25552;&#20379;&#20102;&#24515;&#29702;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#33050;-&#38376;&#25216;&#26415;&#30340;&#33258;&#21160;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#28176;&#28176;&#25104;&#20026;&#20154;&#20204;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#20837;&#21475;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25171;&#30772;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#65288;&#8220;&#30417;&#29425;&#8221;&#65289;&#20197;&#35775;&#38382;&#21463;&#38480;&#20449;&#24687;&#65292;&#36825;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#38754;&#23545;&#27492;&#31867;&#36234;&#29425;&#25915;&#20987;&#26102;&#30340;&#34180;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;LLMs&#22312;&#25509;&#25910;&#36234;&#29425;&#25552;&#31034;&#26102;&#20869;&#22312;&#20915;&#31574;&#26426;&#21046;&#30340;&#29702;&#35299;&#26126;&#26174;&#27424;&#32570;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36234;&#29425;&#25552;&#31034;&#30340;&#24515;&#29702;&#35299;&#37322;&#12290;&#20511;&#37492;&#35748;&#30693;&#19968;&#33268;&#24615;&#29702;&#35770;&#65292;&#25105;&#20204;&#35748;&#20026;&#36234;&#29425;&#30340;&#20851;&#38190;&#26159;&#24341;&#23548;LLMs&#22312;&#38169;&#35823;&#26041;&#21521;&#19978;&#23454;&#29616;&#35748;&#30693;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#33050;-&#38376;&#30340;&#33258;&#21160;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36880;&#27493;&#35825;&#23548;&#27169;&#22411;&#36890;&#36807;&#22810;&#27493;&#22686;&#37327;&#25552;&#31034;&#22238;&#31572;&#26377;&#23475;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15690v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gradually become the gateway for people to acquire new knowledge. However, attackers can break the model's security protection ("jail") to access restricted information, which is called "jailbreaking." Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks. Nevertheless, comprehension of the intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak prompts is noticeably lacking. Our research provides a psychological explanation of the jailbreak prompts. Drawing on cognitive consistency theory, we argue that the key to jailbreak is guiding the LLM to achieve cognitive coordination in an erroneous direction. Further, we propose an automatic black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique. This method progressively induces the model to answer harmful questions via multi-step incremental prompts. We instantiat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861; DINO-Reg&#65292;&#21033;&#29992;&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120; DINOv2 &#36827;&#34892;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22270;&#20687;&#37197;&#20934;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15687</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120; DINOv2 &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
General Purpose Image Encoder DINOv2 for Medical Image Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861; DINO-Reg&#65292;&#21033;&#29992;&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120; DINOv2 &#36827;&#34892;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22270;&#20687;&#37197;&#20934;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#23616;&#37096;&#32441;&#29702;&#29305;&#24449;&#26469;&#23545;&#40784;&#22270;&#20687;&#12290;&#21069;&#32773;&#22312;&#27809;&#26377;&#22823;&#22411;&#19987;&#38376;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#21487;&#38752;&#22320;&#23454;&#26045;&#65292;&#32780;&#21518;&#32773;&#32570;&#20047;&#20840;&#23616;&#35821;&#20041;&#65292;&#22240;&#27492;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861; DINO-Reg&#65292;&#21033;&#29992;&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120; DINOv2 &#36827;&#34892;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#12290;DINOv2 &#32534;&#30721;&#22120;&#26159;&#20351;&#29992;&#21253;&#21547;&#33258;&#28982;&#22270;&#20687;&#30340; ImageNet &#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340; DINOv2 &#32780;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558; DINOv2 &#32534;&#30721;&#29305;&#24449;&#36755;&#20837;&#21040;&#31163;&#25955;&#20248;&#21270;&#22120;&#20013;&#65292;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#21487;&#21464;&#24418;&#37197;&#20934;&#22330;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#36825;&#31181;&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120;&#22312;&#22270;&#20687;&#37197;&#20934;&#24212;&#29992;&#20013;&#30340;&#34892;&#20026;&#21644;&#20316;&#29992;&#12290;&#32467;&#21512;&#25163;&#24037;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#32988;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15687v1 Announce Type: cross  Abstract: Existing medical image registration algorithms rely on either dataset specific training or local texture-based features to align images. The former cannot be reliably implemented without large modality-specific training datasets, while the latter lacks global semantics thus could be easily trapped at local minima. In this paper, we present a training-free deformable image registration method, DINO-Reg, leveraging a general purpose image encoder DINOv2 for image feature extraction. The DINOv2 encoder was trained using the ImageNet data containing natural images. We used the pretrained DINOv2 without any finetuning. Our method feeds the DINOv2 encoded features into a discrete optimizer to find the optimal deformable registration field. We conducted a series of experiments to understand the behavior and role of such a general purpose image encoder in the application of image registration. Combined with handcrafted features, our method won
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#38024;&#23545;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20154;&#21592;&#36718;&#29677;&#35268;&#21010;&#21644;&#26080;&#20851;&#26426;&#22120;&#35843;&#24230;&#65292;&#24182;&#36890;&#36807;MILP&#27169;&#22411;&#26368;&#23567;&#21270;&#24635;&#29983;&#20135;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.15670</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#20154;&#21592;&#36718;&#29677;&#35268;&#21010;&#21644;&#26080;&#20851;&#24182;&#34892;&#26426;&#22120;&#35843;&#24230;&#30340;&#25968;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A mathematical model for simultaneous personnel shift planning and unrelated parallel machine scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15670
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20154;&#21592;&#36718;&#29677;&#35268;&#21010;&#21644;&#26080;&#20851;&#26426;&#22120;&#35843;&#24230;&#65292;&#24182;&#36890;&#36807;MILP&#27169;&#22411;&#26368;&#23567;&#21270;&#24635;&#29983;&#20135;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20135;&#29983;&#20110;&#24037;&#19994;&#24212;&#29992;&#26696;&#20363;&#30340;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#20154;&#21592;&#21487;&#29992;&#24615;&#32422;&#26463;&#30340;&#26080;&#20851;&#24182;&#34892;&#26426;&#22120;&#35843;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22810;&#26399;&#35843;&#24230;&#21608;&#26399;&#20869;&#20248;&#21270;&#29983;&#20135;&#35745;&#21010;&#65292;&#23481;&#32435;&#27599;&#20010;&#26102;&#38388;&#27573;&#20869;&#20154;&#21592;&#36718;&#29677;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#27169;&#22411;&#20551;&#35774;&#26426;&#22120;&#20043;&#38388;&#20849;&#20139;&#20154;&#21592;&#65292;&#27599;&#21488;&#26426;&#22120;&#22312;&#20316;&#19994;&#22788;&#29702;&#26399;&#38388;&#38656;&#35201;&#19968;&#20010;&#20154;&#21592;&#36827;&#34892;&#35774;&#32622;&#21644;&#30417;&#30563;&#12290;&#21487;&#29992;&#20154;&#21592;&#23569;&#20110;&#26426;&#22120;&#25968;&#37327;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#33021;&#22815;&#24182;&#34892;&#25805;&#20316;&#30340;&#26426;&#22120;&#25968;&#37327;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#26368;&#23567;&#21270;&#32771;&#34385;&#26426;&#22120;&#30456;&#20851;&#22788;&#29702;&#26102;&#38388;&#21644;&#39034;&#24207;&#30456;&#20851;&#35774;&#32622;&#26102;&#38388;&#30340;&#24635;&#29983;&#20135;&#26102;&#38388;&#12290;&#27169;&#22411;&#22788;&#29702;&#20102;&#35832;&#22914;&#26426;&#22120;&#21512;&#26684;&#32422;&#26463;&#21644;&#29983;&#20135;&#26102;&#38388;&#31383;&#21475;&#20043;&#31867;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#24341;&#20837;&#20102;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#27169;&#22411;&#26469;&#34920;&#36848;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15670v1 Announce Type: new  Abstract: This paper addresses a production scheduling problem derived from an industrial use case, focusing on unrelated parallel machine scheduling with the personnel availability constraint. The proposed model optimizes the production plan over a multi-period scheduling horizon, accommodating variations in personnel shift hours within each time period. It assumes shared personnel among machines, with one personnel required per machine for setup and supervision during job processing. Available personnel are fewer than the machines, thus limiting the number of machines that can operate in parallel. The model aims to minimize the total production time considering machine-dependent processing times and sequence-dependent setup times. The model handles practical scenarios like machine eligibility constraints and production time windows. A Mixed Integer Linear Programming (MILP) model is introduced to formulate the problem, taking into account both c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.15666</link><description>&lt;p&gt;
&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#20013;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Universal Model in Online Customer Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#22312; typcial &#21830;&#19994;&#22330;&#26223;&#20013;&#24120;&#24120;&#38656;&#35201;&#25968;&#26376;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#33268;&#24615;&#24182;&#32771;&#34385;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23450;&#26399;&#30340;&#37325;&#26032;&#35757;&#32451;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30005;&#23376;&#21830;&#21153;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#23545;&#35805;&#20013;&#26631;&#35760;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#38382;&#39064;&#21450;&#30456;&#24212;&#26631;&#31614;&#30340;&#23384;&#20648;&#24211;&#12290;&#24403;&#23458;&#25143;&#35831;&#27714;&#24110;&#21161;&#26102;&#65292;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#23384;&#20648;&#24211;&#20013;&#25628;&#32034;&#30456;&#20284;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#20998;&#26512;&#26469;&#39044;&#27979;&#30456;&#24212;&#30340;&#26631;&#31614;&#12290;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15666v1 Announce Type: cross  Abstract: Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predict-ing labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#23458;&#25143;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#24182;&#23558;&#20854;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;AUC&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.15665</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#36335;&#30001;&#20013;&#30340;&#24072;&#29983;&#23398;&#20064;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Learning on Complexity in Intelligent Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15665
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#23458;&#25143;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#24182;&#23558;&#20854;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;AUC&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#26381;&#21153;&#36890;&#24120;&#26159;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#20013;&#26368;&#32791;&#26102;&#30340;&#29615;&#33410;&#65292;&#27599;&#27425;&#32852;&#31995;&#36890;&#24120;&#38656;&#35201;&#33457;&#36153;10-15&#20998;&#38047;&#12290;&#26377;&#25928;&#22320;&#23558;&#23458;&#25143;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#65292;&#36991;&#20813;&#36716;&#25509;&#26159;&#30005;&#23376;&#21830;&#21153;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#23458;&#25143;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#24182;&#30456;&#24212;&#22320;&#23558;&#20854;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#12290;&#35813;&#26694;&#26550;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#24072;&#20613;&#27169;&#22411;&#65292;&#26681;&#25454;&#32852;&#31995;&#21518;&#30340;&#23545;&#35805;&#25991;&#26412;&#35780;&#20998;&#26469;&#35780;&#20272;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24072;&#20613;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#26631;&#27880;&#32773;&#65292;&#20026;&#35757;&#32451;&#19968;&#20010;&#21482;&#26681;&#25454;&#32852;&#31995;&#21069;&#25968;&#25454;&#39044;&#27979;&#22797;&#26434;&#24615;&#30340;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#26694;&#26550;&#26159;&#25104;&#21151;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#23458;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#22797;&#26434;&#24615;AUC&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#22312;&#32479;&#35745;&#27700;&#24179;&#19978;&#35780;&#20272;&#23458;&#25143;&#26381;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15665v1 Announce Type: cross  Abstract: Customer service is often the most time-consuming aspect for e-commerce websites, with each contact typically taking 10-15 minutes. Effectively routing customers to appropriate agents without transfers is therefore crucial for e-commerce success. To this end, we have developed a machine learning framework that predicts the complexity of customer contacts and routes them to appropriate agents accordingly. The framework consists of two parts. First, we train a teacher model to score the complexity of a contact based on the post-contact transcripts. Then, we use the teacher model as a data annotator to provide labels to train a student model that predicts the complexity based on pre-contact data only. Our experiments show that such a framework is successful and can significantly improve customer experience. We also propose a useful metric called complexity AUC that evaluates the effectiveness of customer service at a statistical level.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiMeFive&#30340;&#38754;&#37096;&#24773;&#32490;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#28608;&#27963;&#21644;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#36827;&#34892;&#35299;&#37322;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#35270;&#39057;&#31034;&#20363;&#20197;&#21450;&#23454;&#26102;&#25668;&#20687;&#22836;&#27969;&#19978;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15662</link><description>&lt;p&gt;
GiMeFive&#65306;&#38754;&#37096;&#24773;&#32490;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GiMeFive: Towards Interpretable Facial Emotion Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiMeFive&#30340;&#38754;&#37096;&#24773;&#32490;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#28608;&#27963;&#21644;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#36827;&#34892;&#35299;&#37322;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#35270;&#39057;&#31034;&#20363;&#20197;&#21450;&#23454;&#26102;&#25668;&#20687;&#22836;&#27969;&#19978;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#33021;&#25104;&#21151;&#35782;&#21035;&#38754;&#37096;&#24773;&#32490;&#22810;&#24180;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#25110;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;GiMeFive&#65292;&#36890;&#36807;&#23618;&#28608;&#27963;&#21644;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23545;&#20845;&#31181;&#38754;&#37096;&#24773;&#32490;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#65288;FER&#65289;&#22522;&#20934;&#21644;&#25105;&#20204;&#30340;&#27719;&#24635;FER GiMeFive&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#35270;&#39057;&#31034;&#20363;&#20013;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#20197;&#21450;&#23454;&#26102;&#30452;&#25773;&#25668;&#20687;&#22836;&#27969;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#34917;&#20805;&#26448;&#26009;&#21487;&#22312; https&#65306;//github.com/werywjw/SEP-CVDL &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15662v1 Announce Type: cross  Abstract: Deep convolutional neural networks have been shown to successfully recognize facial emotions for the past years in the realm of computer vision. However, the existing detection approaches are not always reliable or explainable, we here propose our model GiMeFive with interpretations, i.e., via layer activations and gradient-weighted class activation mapping. We compare against the state-of-the-art methods to classify the six facial emotions. Empirical results show that our model outperforms the previous methods in terms of accuracy on two Facial Emotion Recognition (FER) benchmarks and our aggregated FER GiMeFive. Furthermore, we explain our work in real-world image and video examples, as well as real-time live camera streams. Our code and supplementary material are available at https: //github.com/werywjw/SEP-CVDL.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15656</link><description>&lt;p&gt;
&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#65306;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#30340;&#32479;&#19968;&#36882;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#31639;&#23376;&#65288;NOs&#65289;&#29702;&#35770;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35745;&#31639;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#35299;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;NO&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#26102;&#31354;PDE&#26102;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#21069;&#30340;NO&#29702;&#35770;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#20415;&#26681;&#25454;&#31232;&#30095;&#37319;&#26679;&#30340;&#22024;&#26434;&#27979;&#37327;&#26377;&#25928;&#22320;&#32416;&#27491;PDE&#35299;&#30340;&#28436;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#26469;&#35745;&#31639;&#26080;&#38480;&#32500;&#21322;&#32447;&#24615;PDE&#30340;&#35299;&#31639;&#23376;&#12290;&#21033;&#29992;&#21322;&#32447;&#24615;PDE&#30340;&#32467;&#26500;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#35266;&#27979;&#32773;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#65292;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15656v1 Announce Type: cross  Abstract: Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with spatio-temporal PDEs over long time scales. Specifically, the current theory of NOs does not present a systematic framework to perform data assimilation and efficiently correct the evolution of PDE solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear PDEs. Exploiting the structure of semilinear PDEs and the theory of nonlinear observers in function spaces, we develop a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. The proposed 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23450;&#20041;&#32852;&#31995;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;AI&#19987;&#23478;&#27169;&#22411;&#26469;&#35780;&#20272;&#23458;&#25143;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#36991;&#20813;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.15655</link><description>&lt;p&gt;
&#23458;&#25143;&#26381;&#21153;&#20013;&#30340;&#32852;&#31995;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Contact Complexity in Customer Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15655
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23450;&#20041;&#32852;&#31995;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;AI&#19987;&#23478;&#27169;&#22411;&#26469;&#35780;&#20272;&#23458;&#25143;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#36991;&#20813;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#31995;&#23458;&#25143;&#26381;&#21153;&#25903;&#25345;&#30340;&#23458;&#25143;&#21487;&#33021;&#38754;&#20020;&#21508;&#31181;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#23558;&#39640;&#22797;&#26434;&#24615;&#30340;&#32852;&#31995;&#36335;&#30001;&#21040;&#21021;&#32423;&#20195;&#29702;&#21487;&#33021;&#23548;&#33268;&#22810;&#27425;&#36716;&#25509;&#25110;&#37325;&#22797;&#32852;&#31995;&#65292;&#32780;&#23558;&#20302;&#22797;&#26434;&#24615;&#30340;&#32852;&#31995;&#36335;&#30001;&#21040;&#39640;&#32423;&#20195;&#29702;&#21487;&#33021;&#20250;&#20351;&#20182;&#20204;&#30340;&#19987;&#19994;&#24110;&#21161;&#23481;&#37327;&#21463;&#21040;&#21387;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#23458;&#25143;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;&#39640;&#24230;&#25277;&#35937;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#32463;&#39564;&#20195;&#29702;&#36827;&#34892;&#22522;&#20110;&#20849;&#35782;&#30340;&#25968;&#25454;&#27880;&#37322;&#26159;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23450;&#20041;&#32852;&#31995;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#65292;&#32780;&#26159;&#35757;&#32451;&#19968;&#31181;AI&#19987;&#23478;&#27169;&#22411;&#26469;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#24182;&#26681;&#25454;&#32852;&#31995;&#30340;&#34892;&#20026;&#35780;&#20272;&#27599;&#20010;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15655v1 Announce Type: cross  Abstract: Customers who reach out for customer service support may face a range of issues that vary in complexity. Routing high-complexity contacts to junior agents can lead to multiple transfers or repeated contacts, while directing low-complexity contacts to senior agents can strain their capacity to assist customers who need professional help. To tackle this, a machine learning model that accurately predicts the complexity of customer issues is highly desirable. However, defining the complexity of a contact is a difficult task as it is a highly abstract concept. While consensus-based data annotation by experienced agents is a possible solution, it is time-consuming and costly. To overcome these challenges, we have developed a novel machine learning approach to define contact complexity. Instead of relying on human annotation, we trained an AI expert model to mimic the behavior of agents and evaluate each contact's complexity based on how the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.15650</link><description>&lt;p&gt;
&#20855;&#26377;&#30446;&#26631;&#25233;&#21046;&#30340;&#22810;&#32422;&#26463;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#30446;&#26631;&#25233;&#21046;&#65292;&#26681;&#25454;&#23433;&#20840;&#35780;&#21028;&#22120;&#33258;&#36866;&#24212;&#22320;&#25233;&#21046;&#20219;&#21153;&#22870;&#21169;&#26368;&#22823;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#23545;&#30446;&#26631;&#25233;&#21046;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20219;&#20309;&#38169;&#35823;&#30340;&#34892;&#20026;&#37117;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#25105;&#20204;&#30340;&#22522;&#20934;&#32447;&#25152;&#36798;&#21040;&#30340;&#20219;&#21153;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#35748;&#35777;&#26694;&#26550;&#36827;&#34892;&#32454;&#31890;&#24230;&#20107;&#23454;&#32423;&#21035;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.15631</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#30340;&#33258;&#35748;&#35777;&#21487;&#20197;&#25552;&#21319;&#20107;&#23454;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Self-Endorsement Improves Factuality and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#35748;&#35777;&#26694;&#26550;&#36827;&#34892;&#32454;&#31890;&#24230;&#20107;&#23454;&#32423;&#21035;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#32531;&#35299;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#30340;&#24187;&#35273;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#35748;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#22810;&#20010;&#25277;&#26679;&#21709;&#24212;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20107;&#23454;&#32423;&#21035;&#27604;&#36739;&#12290;&#19982;&#20808;&#21069;&#30340;&#32452;&#21512;&#26041;&#27861;&#65288;&#29579;&#31561;&#65292;2022&#24180;&#65307;&#38472;&#31561;&#65292;2023&#24180;&#65289;&#36827;&#34892;&#21709;&#24212;&#32423;&#21035;&#36873;&#25321;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#36731;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#26377;&#30410;&#20110;&#36739;&#23567;&#21644;&#24320;&#28304;&#30340;LLM&#65292;&#22240;&#20026;&#23427;&#20027;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#27604;&#36739;&#12290;&#22312;&#20256;&#35760;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30452;&#35266;&#30340;&#25552;&#31034;&#26377;&#25928;&#25913;&#21892;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#29983;&#25104;&#30340;&#20107;&#23454;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;TriviaQA&#21644;GSM8K&#30340;&#20840;&#38754;&#20998;&#26512;&#23637;&#31034;&#20102;&#33258;&#35748;&#35777;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15631v1 Announce Type: cross  Abstract: This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations. Particularly, we propose a self-endorsement framework that leverages the fine-grained fact-level comparisons across multiple sampled responses. Compared with prior ensemble methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level selection, our approach can better alleviate hallucinations, especially for longform generation tasks. Our approach can broadly benefit smaller and open-source LLMs as it mainly conducts simple content-based comparisons. Experiments on Biographies show that our method can effectively improve the factuality of generations with simple and intuitive prompts across different scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K demonstrate the potential of self-endorsement for broader application.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#65292;&#36890;&#36807;&#20132;&#26367;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#21644;&#26368;&#22823;&#21270;&#21487;&#35265;&#25968;&#25454;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.15625</link><description>&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Cyclic Causal Models from Incomplete Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#65292;&#36890;&#36807;&#20132;&#26367;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#21644;&#26368;&#22823;&#21270;&#21487;&#35265;&#25968;&#25454;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#23398;&#20064;&#26159;&#32479;&#35745;&#23398;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#26410;&#35265;&#27835;&#30103;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#37117;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;&#65306;(i) &#28508;&#22312;&#22270;&#26159;&#26080;&#29615;&#30340;&#65292;(ii) &#21487;&#29992;&#25968;&#25454;&#26159;&#23436;&#25972;&#30340;&#12290;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#21253;&#21547;&#21453;&#39304;&#29615;&#36335;&#65288;&#20363;&#22914;&#29983;&#29289;&#31995;&#32479;&#65289;&#65292;&#23454;&#38469;&#24773;&#20917;&#32463;&#24120;&#28041;&#21450;&#32570;&#22833;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#12290;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#19979;&#65292;MissNODAGS&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#22312;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#19982;&#26368;&#22823;&#21270;&#25968;&#25454;&#21487;&#35265;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#20043;&#38388;&#20132;&#26367;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#36981;&#24490;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15625v1 Announce Type: cross  Abstract: Causal learning is a fundamental problem in statistics and science, offering insights into predicting the effects of unseen treatments on a system. Despite recent advances in this topic, most existing causal discovery algorithms operate under two key assumptions: (i) the underlying graph is acyclic, and (ii) the available data is complete. These assumptions can be problematic as many real-world systems contain feedback loops (e.g., biological systems), and practical scenarios frequently involve missing data. In this work, we propose a novel framework, named MissNODAGS, for learning cyclic causal graphs from partially missing data. Under the additive noise model, MissNODAGS learns the causal graph by alternating between imputing the missing data and maximizing the expected log-likelihood of the visible part of the data in each training step, following the principles of the expectation-maximization (EM) framework. Through synthetic exper
&lt;/p&gt;</description></item><item><title>RecWizard&#26159;&#19968;&#31181;&#23545;&#35805;&#24335;&#25512;&#33616;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#27169;&#22411;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#25552;&#39640;CRS&#30740;&#31350;&#25928;&#29575;&#24182;&#20943;&#23569;&#39069;&#22806;&#24037;&#20316;&#37327;</title><link>https://arxiv.org/abs/2402.15591</link><description>&lt;p&gt;
RecWizard&#65306;&#19968;&#31181;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#27169;&#22411;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
RecWizard: A Toolkit for Conversational Recommendation with Modular, Portable Models and Interactive User Interface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15591
&lt;/p&gt;
&lt;p&gt;
RecWizard&#26159;&#19968;&#31181;&#23545;&#35805;&#24335;&#25512;&#33616;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#27169;&#22411;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#25552;&#39640;CRS&#30740;&#31350;&#25928;&#29575;&#24182;&#20943;&#23569;&#39069;&#22806;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecWizard&#30340;&#26032;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#12290;RecWizard&#25903;&#25345;&#27169;&#22411;&#24320;&#21457;&#21644;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;&#65292;&#20511;&#37492;&#20102;Huggingface&#29983;&#24577;&#31995;&#32479;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;CRS&#19982;RecWizard&#20855;&#26377;&#27169;&#22359;&#21270;&#12289;&#20415;&#25658;&#12289;&#20132;&#20114;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21451;&#22909;&#24615;&#65292;&#20197;&#31616;&#21270;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;CRS&#30740;&#31350;&#30340;&#39069;&#22806;&#24037;&#20316;&#37327;&#12290;&#26377;&#20851;RecWizard&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#35831;&#26597;&#30475;&#25105;&#20204;&#30340;GitHub https://github.com/McAuley-Lab/RecWizard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15591v1 Announce Type: cross  Abstract: We present a new Python toolkit called RecWizard for Conversational Recommender Systems (CRS). RecWizard offers support for development of models and interactive user interface, drawing from the best practices of the Huggingface ecosystems. CRS with RecWizard are modular, portable, interactive and Large Language Models (LLMs)-friendly, to streamline the learning process and reduce the additional effort for CRS research. For more comprehensive information about RecWizard, please check our GitHub https://github.com/McAuley-Lab/RecWizard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.15589</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#25163;&#31295;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#20013;&#35201;&#27714;LLMs&#25776;&#20889;&#20803;&#35780;&#35770;&#33609;&#26696;
&lt;/p&gt;
&lt;p&gt;
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#25776;&#20889;&#20803;&#35780;&#35770;&#65292;&#36825;&#28041;&#21450;&#26681;&#25454;&#22810;&#20301;&#19987;&#23478;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#29702;&#35299;&#23398;&#26415;&#25163;&#31295;&#30340;&#26680;&#24515;&#36129;&#29486;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#19987;&#23478;&#22810;&#35270;&#35282;&#30340;&#30475;&#27861;&#24635;&#32467;&#20026;&#31616;&#27905;&#30340;&#25972;&#20307;&#27010;&#36848;&#12290;&#37492;&#20110;&#29983;&#25104;&#22411;AI&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#37325;&#22823;&#21457;&#23637;&#65292;&#25105;&#20204;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#29615;&#22659;&#20013;&#29983;&#25104;&#36825;&#31181;&#20803;&#35780;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#21363;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;TELeR&#20998;&#31867;&#27861;&#20197;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#20419;&#20351;&#23427;&#20204;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20803;&#35780;&#35770;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#25972;&#21512;&#21040;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#8220;&#29289;&#20307;&#35825;&#23548;&#8221;&#27169;&#22411;&#26041;&#27861;&#24182;&#21033;&#29992;&#20808;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;&#21487;&#35299;&#37322;&#30340;AV&#27169;&#22411;&#65292;&#22312;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#20013;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#20915;&#31574;&#29702;&#35299;</title><link>https://arxiv.org/abs/2402.15572</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25913;&#36827;&#21487;&#35299;&#37322;&#30340;&#29289;&#20307;&#35825;&#23548;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#25972;&#21512;&#21040;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#8220;&#29289;&#20307;&#35825;&#23548;&#8221;&#27169;&#22411;&#26041;&#27861;&#24182;&#21033;&#29992;&#20808;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;&#21487;&#35299;&#37322;&#30340;AV&#27169;&#22411;&#65292;&#22312;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#20013;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#20915;&#31574;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26377;&#26395;&#25552;&#20379;&#26356;&#23433;&#20840;&#12289;&#26356;&#39640;&#25928;&#21644;&#26356;&#33298;&#36866;&#30340;&#20986;&#34892;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#20013;&#38754;&#20020;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#21487;&#35299;&#37322;AV&#26550;&#26500;&#22312;&#25552;&#20379;&#21160;&#20316;&#35299;&#37322;&#26102;&#24573;&#30053;&#20102;&#19982;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#8220;&#29289;&#20307;&#35825;&#23548;&#8221;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24378;&#35843;&#22330;&#26223;&#20013;&#29289;&#20307;&#22312;&#20915;&#31574;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;Beta&#20808;&#39564;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#23558;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#25972;&#21512;&#21040;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#21463;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#20808;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#21463;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#21644;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;BDD-OIA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#65292;&#36890;&#36807;&#36825;&#20123;&#22686;&#24378;&#25514;&#26045;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;AV&#20915;&#31574;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15572v1 Announce Type: new  Abstract: The rapid evolution of automated vehicles (AVs) has the potential to provide safer, more efficient, and comfortable travel options. However, these systems face challenges regarding reliability in complex driving scenarios. Recent explainable AV architectures neglect crucial information related to inherent uncertainties while providing explanations for actions. To overcome such challenges, our study builds upon the "object-induced" model approach that prioritizes the role of objects in scenes for decision-making and integrates uncertainty assessment into the decision-making process using an evidential deep learning paradigm with a Beta prior. Additionally, we explore several advanced training strategies guided by uncertainty, including uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA dataset, our findings underscore that the model, through these enhancements, not only offers a clearer comprehension of AV decisi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#26463;&#25628;&#32034;&#30340;&#24555;&#36895;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;BEAST&#65292;&#33021;&#22815;&#22312;&#19968;&#20998;&#38047;&#20869;&#39640;&#25104;&#21151;&#29575;&#22320;&#36234;&#29425;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15570</link><description>&lt;p&gt;
&#19968;&#20998;&#38047;&#20869;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24555;&#36895;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fast Adversarial Attacks on Language Models In One GPU Minute
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15570
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#26463;&#25628;&#32034;&#30340;&#24555;&#36895;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;BEAST&#65292;&#33021;&#22815;&#22312;&#19968;&#20998;&#38047;&#20869;&#39640;&#25104;&#21151;&#29575;&#22320;&#36234;&#29425;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24555;&#36895;&#22522;&#20110;&#26463;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#65288;BEAST&#65289;&#12290;BEAST&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#22312;&#25915;&#20987;&#36895;&#24230;&#12289;&#25104;&#21151;&#29575;&#21644;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;BEAST&#30340;&#35745;&#31639;&#25928;&#29575;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#36234;&#29425;&#12289;&#24341;&#21457;&#24187;&#35273;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#21487;&#20197;&#22312;&#19968;&#20998;&#38047;&#20869;&#36234;&#29425;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#24456;&#39640;&#12290;&#20363;&#22914;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22522;&#20934;&#30456;&#27604;&#65292;BEAST&#21487;&#20197;&#22312;&#19968;&#20998;&#38047;&#20869;&#36234;&#29425; Vicuna-7B-v1.5&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;89%&#65292;&#32780;&#22522;&#20934;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#23567;&#26102;&#20197;&#19978;&#25165;&#33021;&#20351;&#29992;&#21333;&#20010; Nvidia RTX A6000 48GB GPU &#23454;&#29616;70%&#30340;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26524;&#65292;&#21363;&#25105;&#20204;&#30340;&#38750;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#20250;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#20135;&#29983;&#24187;&#35273;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#38750;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#23548;&#33268;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15570v1 Announce Type: cross  Abstract: In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack cause
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.15567</link><description>&lt;p&gt;
&#20855;&#26377;&#24076;&#23572;&#20271;&#29305;&#34920;&#31034;&#30340;&#22522;&#30784;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Foundation Policies with Hilbert Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 &#36827;&#34892;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#20363;&#22914;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#65292;&#24050;&#32463;&#20351;&#24471;&#21487;&#20197;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#19968;&#20010;&#30495;&#27491;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#20197;&#33719;&#21462;&#36890;&#29992;&#25919;&#31574;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;RL&#65292;&#22522;&#20110;&#35832;&#22914;&#22522;&#20110;&#30446;&#26631;&#30340;RL&#12289;&#34892;&#20026;&#20811;&#38534;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#23398;&#20064;&#31561;&#21407;&#21017;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#21457;&#29616;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12289;&#38656;&#35201;&#39640;&#36136;&#37327;&#28436;&#31034;&#25968;&#25454;&#25110;&#32570;&#20047;&#26126;&#30830;&#30340;&#25552;&#31034;&#25110;&#36866;&#24212;&#26426;&#21046;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#30340;&#36890;&#29992;&#25919;&#31574;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#33719;&#21462;&#36825;&#20123;&#34892;&#20026;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 Announce Type: cross  Abstract: Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear prompting or adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly ada
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15555</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24635;&#26159;&#29702;&#35299;&#24182;&#19988;&#36825;&#23601;&#26159;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Deep Networks Always Grok and Here is Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15555
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#65292;&#25110;&#32773;&#24310;&#36831;&#27867;&#21270;&#65292;&#26159;&#25351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36798;&#21040;&#25509;&#36817;&#20110;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#21518;&#24456;&#38271;&#26102;&#38388;&#20869;&#25165;&#21457;&#29983;&#27867;&#21270;&#30340;&#29616;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#29305;&#23450;&#21463;&#25511;&#29615;&#22659;&#20013;&#20986;&#29616;grokking&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#20351;&#29992;&#22823;&#33539;&#25968;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;DNN&#25110;&#32773;&#22312;&#31639;&#27861;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;transformers&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;grokking&#23454;&#38469;&#19978;&#26356;&#21152;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#21576;&#29616;&#65292;&#20363;&#22914;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#32773;&#22312;Imagenette&#19978;&#35757;&#32451;&#30340;Resnet&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;&#65292;&#21363;DNN&#22312;&#25554;&#20540;&#21644;/&#25110;&#27867;&#21270;&#20043;&#21518;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#29702;&#35299;&#24182;&#21464;&#24471;&#40065;&#26834;&#12290;&#25105;&#20204;&#38024;&#23545;DNN&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#25552;&#20986;&#20102;&#20986;&#29616;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#34913;&#37327;&#20102;DNN&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#23545;&#31216;&#24615;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36816;&#21160;&#32467;&#26500;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#65292;&#24310;&#20280;&#33267;&#26426;&#22120;&#20154;&#29366;&#24577;&#31354;&#38388;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#36827;&#32780;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#23398;&#24314;&#27169;&#12289;&#25511;&#21046;&#21644;&#35774;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15552</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#24418;&#24577;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Morphological Symmetries in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15552
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23545;&#31216;&#24615;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36816;&#21160;&#32467;&#26500;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#65292;&#24310;&#20280;&#33267;&#26426;&#22120;&#20154;&#29366;&#24577;&#31354;&#38388;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#36827;&#32780;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#23398;&#24314;&#27169;&#12289;&#25511;&#21046;&#21644;&#35774;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#21644;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#24418;&#24577;&#23545;&#31216;&#24615;&#12290;&#36825;&#20123;&#26159;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#32463;&#24120;&#22312;&#21160;&#29289;&#29983;&#29289;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#20013;&#35266;&#23519;&#21040;&#65292;&#28304;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#22797;&#21046;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#20123;&#23545;&#31216;&#24615;&#22914;&#20309;&#24310;&#20280;&#21040;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#31354;&#38388;&#20197;&#21450;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#37096;&#24863;&#30693;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#31561;&#19981;&#21464;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#24418;&#24577;&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#20010;&#30456;&#20851;&#19988;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#21463;&#29289;&#29702;&#21551;&#31034;&#30340;&#20960;&#20309;&#20808;&#39564;&#65292;&#23545;&#26426;&#22120;&#20154;&#24314;&#27169;&#12289;&#25511;&#21046;&#12289;&#20272;&#35745;&#21644;&#35774;&#35745;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#21644;&#20998;&#26512;&#26041;&#27861;&#37117;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#24418;&#24577;&#23545;&#31216;&#24615;&#22914;&#20309;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15552v1 Announce Type: cross  Abstract: We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot's morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot's state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot's equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models 
&lt;/p&gt;</description></item><item><title>HiMAP &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20280;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#20998;&#25955;&#24335;&#35757;&#32451;&#20013;&#23545;&#29992;&#25143;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#36827;&#34892;&#20102;&#25913;&#36827;</title><link>https://arxiv.org/abs/2402.15546</link><description>&lt;p&gt;
HiMAP&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15546
&lt;/p&gt;
&lt;p&gt;
HiMAP &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20280;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#20998;&#25955;&#24335;&#35757;&#32451;&#20013;&#23545;&#29992;&#25143;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#37117;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#38543;&#30528;&#22823;&#37327;&#33258;&#27835;&#20307;&#21516;&#26102;&#25805;&#20316;&#32780;&#22686;&#38271;&#65292;&#39640;&#25928;&#21644;&#36991;&#20813;&#30896;&#25758;&#30340;&#21327;&#35843;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#31639;&#27861;&#22312;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#26469;&#35299;&#20915;MAPF&#30340;&#22797;&#26434;&#24615;&#65307;&#28982;&#32780;&#65292;&#23427;&#20063;&#34987;&#21457;&#29616;&#22312;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#31934;&#32454;&#30340;&#23454;&#26045;&#12289;&#28459;&#38271;&#30340;&#35757;&#32451;&#65292;&#32780;&#19988;&#36890;&#24120;&#34920;&#29616;&#20986;&#19981;&#31283;&#23450;&#30340;&#25910;&#25947;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Heuristics-Informed Multi-Agent Pathfinding&#65288;HiMAP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20280;&#32553;&#26041;&#27861;&#65292;&#37319;&#29992;&#20197;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#24335;&#26469;&#36827;&#34892;&#20998;&#25955;&#24335;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#19968;&#20010;&#21551;&#21457;&#24335;&#31574;&#30053;&#20316;&#20026;&#25945;&#24072;&#65292;&#23558;&#27599;&#20010;&#21333;&#20010;&#26234;&#20307;&#35266;&#27979;&#20449;&#24687;&#26144;&#23556;&#21040;&#34892;&#21160;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15546v1 Announce Type: cross  Abstract: Large-scale multi-agent pathfinding (MAPF) presents significant challenges in several areas. As systems grow in complexity with a multitude of autonomous agents operating simultaneously, efficient and collision-free coordination becomes paramount. Traditional algorithms often fall short in scalability, especially in intricate scenarios. Reinforcement Learning (RL) has shown potential to address the intricacies of MAPF; however, it has also been shown to struggle with scalability, demanding intricate implementation, lengthy training, and often exhibiting unstable convergence, limiting its practical application. In this paper, we introduce Heuristics-Informed Multi-Agent Pathfinding (HiMAP), a novel scalable approach that employs imitation learning with heuristic guidance in a decentralized manner. We train on small-scale instances using a heuristic policy as a teacher that maps each single agent observation information to an action prob
&lt;/p&gt;</description></item><item><title>AgentLite&#26159;&#19968;&#20010;&#29992;&#20110;&#31616;&#21270;&#26500;&#24314;LLM&#20195;&#29702;&#25512;&#29702;&#21644;&#26550;&#26500;&#30340;&#36731;&#37327;&#32423;&#24211;&#12290;</title><link>https://arxiv.org/abs/2402.15538</link><description>&lt;p&gt;
AgentLite: &#29992;&#20110;&#26500;&#24314;&#21644;&#25512;&#21160;&#38754;&#21521;&#20219;&#21153;&#30340;LLM Agent&#31995;&#32479;&#30340;&#36731;&#37327;&#32423;&#24211;
&lt;/p&gt;
&lt;p&gt;
AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15538
&lt;/p&gt;
&lt;p&gt;
AgentLite&#26159;&#19968;&#20010;&#29992;&#20110;&#31616;&#21270;&#26500;&#24314;LLM&#20195;&#29702;&#25512;&#29702;&#21644;&#26550;&#26500;&#30340;&#36731;&#37327;&#32423;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#30340;&#34028;&#21187;&#21457;&#23637;&#24341;&#21457;&#20102;LLM&#20195;&#29702;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#23613;&#31649;LLM&#20195;&#29702;&#30340;&#22522;&#30784;&#26159;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#35774;&#35745;&#26368;&#20339;&#25512;&#29702;&#31574;&#30053;&#21644;&#20195;&#29702;&#26550;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;LLM&#20195;&#29702;&#30740;&#31350;&#20174;&#31616;&#21333;&#30340;&#24605;&#32500;&#38142;&#25512;&#36827;&#21457;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;ReAct&#21644;Reflection&#25512;&#29702;&#31574;&#30053;&#65307;&#20195;&#29702;&#26550;&#26500;&#20063;&#20174;&#21333;&#19968;&#20195;&#29702;&#29983;&#25104;&#21457;&#23637;&#21040;&#22810;&#20195;&#29702;&#23545;&#35805;&#65292;&#20197;&#21450;&#22810;&#20010;LLM&#22810;&#20195;&#29702;&#32452;&#32676;&#32842;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#22797;&#26434;&#30340;&#26694;&#26550;&#21644;&#24211;&#65292;&#21019;&#24314;&#21644;&#35780;&#20272;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#21644;&#20195;&#29702;&#26550;&#26500;&#24050;&#25104;&#20026;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#20195;&#29702;&#30340;&#30740;&#31350;&#35843;&#26597;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#19968;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24211;&#65292;AgentLite&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#36731;&#20415;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#65292;&#31616;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#36731;&#26494;&#21019;&#26032;LLM&#20195;&#29702;&#30340;&#25512;&#29702;&#12289;&#26550;&#26500;&#21644;&#24212;&#29992;&#12290;AgentLite&#26159;&#19968;&#20010;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15538v1 Announce Type: cross  Abstract: The booming success of LLMs initiates rapid development in LLM agents. Though the foundation of an LLM agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, LLM agent research advances from the simple chain-of-thought prompting to more complex ReAct and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-LLM multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents. Thus, we open-source a new AI agent library, AgentLite, which simplifies this process by offering a lightweight, user-friendly platform for innovating LLM agent reasoning, architectures, and applications with ease. AgentLite is a task
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item><item><title>PCA-Bench &#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24341;&#20837;&#22797;&#26434;&#22330;&#26223;&#21644;&#38169;&#35823;&#23450;&#20301;&#33021;&#21147;&#65292;&#25552;&#39640;&#37096;&#32626;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758; PCA-Eval&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15527</link><description>&lt;p&gt;
PCA-Bench: &#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24863;&#30693;-&#35748;&#30693;-&#34892;&#21160;&#38142;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15527
&lt;/p&gt;
&lt;p&gt;
PCA-Bench &#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24341;&#20837;&#22797;&#26434;&#22330;&#26223;&#21644;&#38169;&#35823;&#23450;&#20301;&#33021;&#21147;&#65292;&#25552;&#39640;&#37096;&#32626;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758; PCA-Eval&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PCA-Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#32508;&#21512;&#33021;&#21147;&#30340;&#22810;&#27169;&#20915;&#31574;&#22522;&#20934;&#12290;&#19982;&#20043;&#21069;&#19987;&#27880;&#20110;&#31616;&#21333;&#20219;&#21153;&#21644;&#21333;&#20010;&#27169;&#22411;&#33021;&#21147;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;PCA-Bench&#24341;&#20837;&#20102;&#19977;&#20010;&#22797;&#26434;&#22330;&#26223;&#65306;&#33258;&#21160;&#39550;&#39542;&#12289;&#23478;&#24237;&#26426;&#22120;&#20154;&#21644;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#12290;&#22312;&#32473;&#23450;&#20219;&#21153;&#25351;&#20196;&#21644;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#38656;&#35201;&#26080;&#32541;&#25972;&#21512;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#22810;&#37325;&#33021;&#21147;&#65292;&#20197;&#36827;&#34892;&#25512;&#29702;&#38142;&#20197;&#20570;&#20986;&#20934;&#30830;&#20915;&#23450;&#12290;&#27492;&#22806;&#65292;PCA-Bench&#20855;&#26377;&#38169;&#35823;&#23450;&#20301;&#33021;&#21147;&#65292;&#23457;&#26597;&#27169;&#22411;&#22312;&#24863;&#30693;&#12289;&#30693;&#35782;&#25110;&#25512;&#29702;&#31561;&#39046;&#22495;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#36825;&#25552;&#39640;&#20102;&#37096;&#32626;MLLMs&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#22312;&#35780;&#20272;&#20013;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PCA-Eval&#65292;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#35780;&#20272;&#20102;10&#31181;&#27969;&#34892;&#30340;MLLMs&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15527v1 Announce Type: cross  Abstract: We present PCA-Bench, a multimodal decision-making benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous benchmarks focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task instructions and diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a reasoning chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an automatic evaluation protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between
&lt;/p&gt;</description></item><item><title>Chain-of-Specificity (CoS)&#26159;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#36880;&#27493;&#31934;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#25351;&#20196;&#20013;&#36845;&#20195;&#24378;&#35843;&#29305;&#23450;&#32422;&#26463;&#65292;&#35299;&#38145;&#30693;&#35782;&#24182;&#25913;&#36827;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.15526</link><description>&lt;p&gt;
Chain-of-Specificity: &#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#36880;&#27493;&#31934;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15526
&lt;/p&gt;
&lt;p&gt;
Chain-of-Specificity (CoS)&#26159;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#36880;&#27493;&#31934;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#25351;&#20196;&#20013;&#36845;&#20195;&#24378;&#35843;&#29305;&#23450;&#32422;&#26463;&#65292;&#35299;&#38145;&#30693;&#35782;&#24182;&#25913;&#36827;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#24341;&#20154;&#30633;&#30446;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#26377;&#26102;&#38590;&#20197;&#36981;&#24490;&#20855;&#20307;&#30340;&#32422;&#26463;(&#22914;&#22312;&#29305;&#23450;&#22320;&#28857;&#25110;&#29305;&#23450;&#26102;&#38388;)&#65292;&#29978;&#33267;&#26377;&#26102;&#20250;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#65292;&#23548;&#33268;&#22238;&#24212;&#35201;&#20040;&#22826;&#31548;&#32479;&#65292;&#35201;&#20040;&#19981;&#22815;&#28385;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Chain-of-Specificity (CoS)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoS&#36845;&#20195;&#22320;&#24378;&#35843;&#36755;&#20837;&#25351;&#20196;&#20013;&#30340;&#20855;&#20307;&#32422;&#26463;&#65292;&#35299;&#38145;LLMs&#20869;&#37096;&#30340;&#30693;&#35782;&#65292;&#24182;&#31934;&#21270;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15526v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). In response, this paper proposes a simple yet effective method named Chain-of-Specificity (CoS). Specifically, CoS iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses. Experiments conducted on publicly 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20462;&#21098;&#20844;&#24335;&#30340;&#37096;&#20998;&#65292;&#21152;&#36895;&#26522;&#20030;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65292;&#26080;&#38656;&#25968;&#25454;&#26631;&#35760;&#65292;&#20063;&#26080;&#38656;&#26469;&#33258;&#30446;&#26631;&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15524</link><description>&lt;p&gt;
&#22270;&#21098;&#26525;&#29992;&#20110;&#26522;&#20030;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15524
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20462;&#21098;&#20844;&#24335;&#30340;&#37096;&#20998;&#65292;&#21152;&#36895;&#26522;&#20030;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65292;&#26080;&#38656;&#25968;&#25454;&#26631;&#35760;&#65292;&#20063;&#26080;&#38656;&#26469;&#33258;&#30446;&#26631;&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#20108;&#36827;&#21046;&#32422;&#26463;&#30340;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65288;MUSes&#65289;&#26159;&#36229;&#32422;&#26463;&#31995;&#32479;&#19981;&#21487;&#34892;&#24615;&#20998;&#26512;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#26522;&#20030;MUSes&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#23545;&#20844;&#24335;&#36827;&#34892;&#20462;&#21098;&#20197;&#21152;&#36895;MUS&#26522;&#20030;&#12290;&#25105;&#20204;&#23558;&#20844;&#24335;&#34920;&#31034;&#20026;&#22270;&#65292;&#28982;&#21518;&#24320;&#21457;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#24212;&#35813;&#20462;&#21098;&#20844;&#24335;&#30340;&#21738;&#19968;&#37096;&#20998;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#36890;&#36807;&#20165;&#26816;&#26597;&#20462;&#21098;&#21518;&#30340;&#20844;&#24335;&#30340;&#21487;&#28385;&#36275;&#24615;&#26469;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#12290;&#23427;&#29978;&#33267;&#19981;&#38656;&#35201;&#26469;&#33258;&#30446;&#26631;&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#23545;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#22806;&#25512;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;MUS&#26522;&#20030;&#22120;&#32467;&#21512;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;&#21253;&#25324;&#19968;&#32452;&#36229;&#20986;&#25105;&#20204;&#35757;&#32451;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#38469;&#38382;&#39064;&#22312;&#20869;&#30340;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15524v1 Announce Type: new  Abstract: Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a common problem in infeasibility analysis of over-constrained systems. However, because of the exponential search space of the problem, enumerating MUSes is extremely time-consuming in real applications. In this work, we propose to prune formulas using a learned model to speed up MUS enumeration. We represent formulas as graphs and then develop a graph-based learning model to predict which part of the formula should be pruned. Importantly, our algorithm does not require data labeling by only checking the satisfiability of pruned formulas. It does not even require training data from the target application because it extrapolates to data with different distributions. In our experiments we combine our algorithm with existing MUS enumerators and validate its effectiveness in multiple benchmarks including a set of real-world problems outside our training distribution. The
&lt;/p&gt;</description></item><item><title>&#25226;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#25193;&#23637;&#21040;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#26377;&#25928;&#23454;&#29616;&#24182;&#35752;&#35770;&#25913;&#36827;&#65292;&#25104;&#20026;ILP&#27714;&#35299;&#39046;&#22495;&#26377;&#29992;&#30340;&#34917;&#20805;&#12290;</title><link>https://arxiv.org/abs/2402.15522</link><description>&lt;p&gt;
IntSat: &#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#32422;&#26463;&#23398;&#20064;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
IntSat: Integer Linear Programming by Conflict-Driven Constraint-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15522
&lt;/p&gt;
&lt;p&gt;
&#25226;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#25193;&#23637;&#21040;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#26377;&#25928;&#23454;&#29616;&#24182;&#35752;&#35770;&#25913;&#36827;&#65292;&#25104;&#20026;ILP&#27714;&#35299;&#39046;&#22495;&#26377;&#29992;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#30340;&#26368;&#20808;&#36827;&#30340;SAT&#27714;&#35299;&#22120;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#20363;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#25152;&#35859;&#30340;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21253;&#25324;&#19968;&#20123;&#21033;&#29992;&#22312;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#20914;&#31361;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#26415;&#25193;&#23637;&#21040;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#20013;&#65292;&#20854;&#20013;&#21464;&#37327;&#21487;&#20197;&#21462;&#19968;&#33324;&#25972;&#25968;&#20540;&#32780;&#19981;&#20165;&#20165;&#26159;&#20108;&#36827;&#21046;&#20540;&#65292;&#32422;&#26463;&#27604;&#21629;&#39064;&#23376;&#21477;&#26356;&#20855;&#34920;&#36798;&#21147;&#65292;&#21487;&#33021;&#23384;&#22312;&#19968;&#20010;&#35201;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#26412;&#23454;&#29616;&#30340;&#25903;&#25345;&#65292;&#35813;&#23454;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36825;&#20010;&#36828;&#26410;&#25104;&#29087;&#30340;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#24050;&#32463;&#26159;ILP&#27714;&#35299;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#26377;&#29992;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15522v1 Announce Type: new  Abstract: State-of-the-art SAT solvers are nowadays able to handle huge real-world instances. The key to this success is the so-called Conflict-Driven Clause-Learning (CDCL) scheme, which encompasses a number of techniques that exploit the conflicts that are encountered during the search for a solution. In this article we extend these techniques to Integer Linear Programming (ILP), where variables may take general integer values instead of purely binary ones, constraints are more expressive than just propositional clauses, and there may be an objective function to optimise. We explain how these methods can be implemented efficiently, and discuss possible improvements. Our work is backed with a basic implementation that shows that, even in this far less mature stage, our techniques are already a useful complement to the state of the art in ILP solving.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HKD-SHO&#30340;&#28151;&#21512;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#26377;&#30410;&#22320;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#20013;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15521</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#28151;&#21512;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479; - HKD-SHO
&lt;/p&gt;
&lt;p&gt;
HKD-SHO: A hybrid smart home system based on knowledge-based and data-driven services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15521
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HKD-SHO&#30340;&#28151;&#21512;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#26377;&#30410;&#22320;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#20013;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#23478;&#23621;&#36890;&#36807;&#35774;&#32622;&#21508;&#31181;&#26381;&#21153;&#26469;&#23454;&#29616;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#21019;&#24314;&#26234;&#33021;&#23478;&#23621;&#26381;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23621;&#27665;&#30340;&#25163;&#21160;&#36755;&#20837;&#65292;&#22914;&#26524;&#25152;&#20851;&#27880;&#29615;&#22659;&#29366;&#24577;&#30340;&#29289;&#29702;&#29616;&#35937;&#22797;&#26434;&#65292;&#32780;&#19988;&#23621;&#27665;&#19981;&#30693;&#36947;&#22914;&#20309;&#35843;&#25972;&#30456;&#20851;&#25191;&#34892;&#22120;&#20197;&#23454;&#29616;&#26381;&#21153;&#30417;&#35270;&#30340;&#29366;&#24577;&#30340;&#30446;&#26631;&#20540;&#65292;&#21017;&#21487;&#33021;&#20250;&#21464;&#24471;&#22797;&#26434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23601;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#26080;&#27861;&#21521;&#23621;&#27665;&#23637;&#31034;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#26576;&#20123;&#26381;&#21153;&#25552;&#20986;&#20102;&#26576;&#20123;&#25191;&#34892;&#22120;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HKD-SHO&#65288;&#22522;&#20110;&#28151;&#21512;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#65289;&#30340;&#28151;&#21512;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15521v1 Announce Type: new  Abstract: A smart home is realized by setting up various services. Several methods have been proposed to create smart home services, which can be divided into knowledge-based and data-driven approaches. However, knowledge-based approaches usually require manual input from the inhabitant, which can be complicated if the physical phenomena of the concerned environment states are complex, and the inhabitant does not know how to adjust related actuators to achieve the target values of the states monitored by services. Moreover, machine learning-based data-driven approaches that we are interested in are like black boxes and cannot show the inhabitant in which situations certain services proposed certain actuators' states. To solve these problems, we propose a hybrid system called HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart HOme system), where knowledge-based and machine learning-based data-driven services are profitably integra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#26500;&#24314;&#20102;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#25351;&#20986;&#22522;&#22240;&#32452;&#39118;&#38505;&#22240;&#32032;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15515</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30456;&#20851;&#22240;&#32032;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Feasibility of Identifying Factors Related to Alzheimer's Disease and Related Dementia in Real-World Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15515
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#26500;&#24314;&#20102;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#25351;&#20986;&#22522;&#22240;&#32452;&#39118;&#38505;&#22240;&#32032;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;/&#30456;&#20851;&#30196;&#21574;&#30151;&#30456;&#20851;&#22240;&#32032;&#30340;&#20840;&#38754;&#35270;&#35282;&#23558;&#26497;&#22823;&#22320;&#24110;&#21161;&#24320;&#23637;&#26032;&#30340;&#27835;&#30103;&#30740;&#31350;&#65292;&#24182;&#20026;&#39044;&#38450;&#24037;&#20316;&#30830;&#23450;&#39640;&#39118;&#38505;&#20154;&#32676;&#21644;&#24739;&#32773;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22238;&#39038;&#29616;&#26377;&#30340;&#39118;&#38505;&#21644;&#39044;&#38450;&#22240;&#32032;&#30340;Meta&#20998;&#26512;&#21644;&#32508;&#36848;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;/&#30456;&#20851;&#30196;&#21574;&#30151;&#30456;&#20851;&#30340;&#21361;&#38505;&#22240;&#32032;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#20174;537&#39033;&#30740;&#31350;&#20013;&#25552;&#21462;&#20102;10&#20010;&#31867;&#21035;&#20013;&#30340;477&#20010;&#21361;&#38505;&#22240;&#32032;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30693;&#35782;&#22270;&#35889;&#26469;&#20256;&#25773;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#22823;&#22810;&#25968;&#39118;&#38505;&#22240;&#32032;&#21487;&#20174;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#20013;&#33719;&#24471;&#65292;&#20020;&#24202;&#21465;&#36848;&#26174;&#31034;&#20986;&#20316;&#20026;&#20449;&#24687;&#26469;&#28304;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#35780;&#20272;&#22522;&#22240;&#32452;&#39118;&#38505;&#22240;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;/&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#22522;&#22240;&#26816;&#27979;&#20173;&#28982;&#19981;&#26159;&#24120;&#35268;&#20570;&#27861;&#65292;&#32780;&#19988;&#22312;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;EHRs&#20013;&#35760;&#24405;&#19981;&#33391;&#12290;&#32771;&#34385;&#21040;&#20851;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;/&#30456;&#20851;&#30196;&#21574;&#30151;&#39118;&#38505;&#22240;&#32032;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#65292;&#25991;&#29486;&#25366;&#25496;v
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15515v1 Announce Type: new  Abstract: A comprehensive view of factors associated with AD/ADRD will significantly aid in studies to develop new treatments for AD/ADRD and identify high-risk populations and patients for prevention efforts. In our study, we summarized the risk factors for AD/ADRD by reviewing existing meta-analyses and review articles on risk and preventive factors for AD/ADRD. In total, we extracted 477 risk factors in 10 categories from 537 studies. We constructed an interactive knowledge map to disseminate our study results. Most of the risk factors are accessible from structured Electronic Health Records (EHRs), and clinical narratives show promise as information sources. However, evaluating genomic risk factors using RWD remains a challenge, as genetic testing for AD/ADRD is still not a common practice and is poorly documented in both structured and unstructured EHRs. Considering the constantly evolving research on AD/ADRD risk factors, literature mining v
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.15514</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25991;&#26412;&#22312;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Scale Generative AI Text Applied to Sports and Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#23186;&#20307;&#20869;&#23481;&#65288;&#21253;&#25324;&#35780;&#35770;&#21644;&#20010;&#24615;&#21270;&#26032;&#38395;&#25253;&#36947;&#65289;&#25193;&#23637;&#21040;&#20840;&#29699;&#22823;&#22411;&#20307;&#32946;&#21644;&#38899;&#20048;&#27963;&#21160;&#30340;&#29983;&#20135;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23558;&#22823;&#37327;&#22810;&#27169;&#25968;&#25454;&#65288;&#20363;&#22914;&#35270;&#39057;&#12289;&#25991;&#31456;&#12289;&#23454;&#26102;&#27604;&#20998;&#12289;&#32479;&#35745;&#25968;&#25454;&#21644;&#36164;&#26009;&#65289;&#36716;&#25442;&#20026;&#36830;&#36143;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#22522;&#20110;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;&#19968;&#27454;&#20154;&#24037;&#26234;&#33021;&#35780;&#35770;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#37096;&#32626;&#29992;&#20110;&#20026;2023&#24180;&#32654;&#22269;&#20844;&#24320;&#36187;&#12289;&#28201;&#24067;&#23572;&#30331;&#20844;&#24320;&#36187;&#21644;&#22823;&#24072;&#36187;&#30340;&#31934;&#24425;&#29255;&#27573;&#21046;&#20316;&#33258;&#21160;&#21270;&#21465;&#36848;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36824;&#34987;&#25193;&#23637;&#29992;&#20110;&#20026;ESPN&#26790;&#24187;&#27204;&#27012;&#29699;&#21644;&#26684;&#33713;&#32654;&#22870;&#38899;&#20048;&#33402;&#26415;&#23478;&#25925;&#20107;&#21019;&#36896;&#20010;&#24615;&#21270;&#20869;&#23481;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#37319;&#29992;&#20102;&#30456;&#21516;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;15&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24179;&#22343;Rouge-L&#20026;82.00&#65292;&#22256;&#24785;&#24230;&#20026;6.6&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15514v1 Announce Type: cross  Abstract: We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the Grammy awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforeme
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15313</link><description>&lt;p&gt;
ArabianGPT&#65306;&#22522;&#20110;&#21407;&#29983;&#38463;&#25289;&#20271;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArabianGPT: Native Arabic GPT-based Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#25289;&#19969;&#35821;&#20026;&#20027;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#26174;&#33879;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#32780;&#25104;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#19982;&#38463;&#25289;&#20271;&#35821;&#30340;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
&lt;/p&gt;</description></item><item><title>&#22312;fNIRS&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26657;&#20934;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.15266</link><description>&lt;p&gt;
fNIRS&#20013;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration of Deep Learning Classification Models in fNIRS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15266
&lt;/p&gt;
&lt;p&gt;
&#22312;fNIRS&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26657;&#20934;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#26159;&#29992;&#20110;&#30417;&#27979;&#33041;&#27963;&#21160;&#30340;&#23453;&#36149;&#26080;&#21019;&#24037;&#20855;&#12290;&#19982;&#24847;&#35782;&#27963;&#21160;&#30456;&#20851;&#30340;fNIRS&#25968;&#25454;&#20998;&#31867;&#23545;&#20110;&#25512;&#36827;&#25105;&#20204;&#23545;&#22823;&#33041;&#30340;&#29702;&#35299;&#21644;&#20419;&#36827;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#28145;&#24230;&#23398;&#20064;&#26469;&#35299;&#20915;fNIRS&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20998;&#31867;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;fNIRS&#30340;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#32622;&#20449;&#24230;&#21487;&#38752;&#24615;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20043;&#19968;&#23601;&#26159;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#26657;&#20934;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26657;&#20934;&#34701;&#20837;fNIRS&#39046;&#22495;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26657;&#20934;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#25512;&#21160;fNIRS&#39046;&#22495;&#30340;&#26657;&#20934;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15266v1 Announce Type: new  Abstract: Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14890</link><description>&lt;p&gt;
Vygotsky Distance: &#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vygotsky Distance: Measure for Benchmark Task Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14890
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#24037;&#20855;&#21644;&#23454;&#36341;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#20934;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#31216;&#20043;&#20026;"&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;"&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#8220;&#23398;&#29983;&#8221;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20219;&#21153;&#26412;&#36523;&#30340;&#23646;&#24615;&#12290;&#22914;&#26524;&#20004;&#20010;&#20219;&#21153;&#22312;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#19978;&#24444;&#27492;&#25509;&#36817;&#65292;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978; tend to have similar relative performance&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20102;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
&lt;/p&gt;</description></item><item><title>Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14873</link><description>&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report on the Checkfor.ai AI-Generated Text Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14873
&lt;/p&gt;
&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Checkfor.ai&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#21306;&#20998;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;Checkfor.ai&#22312;&#30001;&#21313;&#31181;&#25991;&#26412;&#39046;&#22495;&#65288;&#23398;&#29983;&#20889;&#20316;&#12289;&#21019;&#24847;&#20889;&#20316;&#12289;&#31185;&#23398;&#20889;&#20316;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#12289;&#31185;&#23398;&#35770;&#25991;&#12289;&#31616;&#31572;&#38382;&#31572;&#65289;&#21644;8&#20010;&#24320;&#28304;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#38646;&#20914;&#20987;&#26041;&#27861;&#22914;DetectGPT&#20197;&#21450;&#20027;&#27969;&#21830;&#19994;AI&#26816;&#27979;&#24037;&#20855;&#65292;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;9&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#65292;&#20351;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35780;&#35770;&#31561;&#39640;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#20302;&#35823;&#25253;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Checkfor.ai&#19981;&#23545;&#38750;&#27597;&#35821;&#33521;&#35821;&#20154;&#22763;&#20135;&#29983;&#20559;&#35265;&#65292;&#24182;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#39046;&#22495;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&amp;A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.14807</link><description>&lt;p&gt;
&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#20013;&#21160;&#24577;&#19981;&#23433;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#20219;&#21153;&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65288;DLM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#38477;&#20302;&#23381;&#20135;&#22919;&#27515;&#20129;&#29575;&#30340;&#21162;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39044;&#38450;&#20445;&#20581;&#35745;&#21010;&#65292;&#21521;&#39640;&#39118;&#38505;&#20154;&#32676;&#20256;&#25773;&#37325;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DLM&#65306;&#19968;&#31181;&#29992;&#20110;RMAB&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#22522;&#20934;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25581;&#31034;&#20102;&#20934;&#30830;&#24615;&#26368;&#39640;&#30340;&#27169;&#22411;&#21487;&#33021;&#20063;&#20855;&#26377;&#26368;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14418</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Evaluation for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#22522;&#20934;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25581;&#31034;&#20102;&#20934;&#30830;&#24615;&#26368;&#39640;&#30340;&#27169;&#22411;&#21487;&#33021;&#20063;&#20855;&#26377;&#26368;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#12289;LLaVA&#21644;CogVLM&#36825;&#26679;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22240;&#22312;&#20960;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#35780;&#20272;VLMs&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30095;&#24573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#21040;&#35780;&#20272;VLMs&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;20&#22810;&#20010;VLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#30340;5&#20010;&#25968;&#25454;&#38598;&#19978;&#26816;&#39564;&#20102;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20854;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#20934;&#30830;&#24615;&#26368;&#39640;&#30340;&#27169;&#22411;&#21487;&#33021;&#20063;&#20855;&#26377;&#26368;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#35777;&#23454;&#20102;&#20026;VLMs&#27979;&#37327;&#20854;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#36824;&#25581;&#31034;&#20102;&#19968;&#31181;&#30456;&#20851;&#24615;&#65292;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14418v1 Announce Type: cross  Abstract: Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs.   Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities.   Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14404</link><description>&lt;p&gt;
&#22312;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#26512;&#27010;&#24565;&#34920;&#36798;&#65306;&#20511;&#21161;&#21453;&#21521;&#35789;&#20856;&#25506;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#26597;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25506;&#26597;LLMs&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#35821;&#35328;&#25551;&#36848;&#20013;&#26263;&#31034;&#30340;&#23545;&#35937;&#27010;&#24565;&#30340;&#26415;&#35821;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#31283;&#20581;&#22320;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#20851;&#20110;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#22312;&#21477;&#27861;&#27867;&#21270;&#34892;&#20026;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;&#25506;&#32034;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#20351;&#29992;&#25551;&#36848;$\Rightarrow$&#21333;&#35789;&#31034;&#20363;&#21487;&#33021;&#20250;&#35825;&#23548;&#20986;&#36229;&#36234;&#20219;&#21153;&#26500;&#22411;&#34920;&#38754;&#24046;&#24322;&#30340;&#27867;&#21270;&#65292;&#24182;&#20419;&#36827;&#27169;&#22411;&#23545;&#26356;&#24191;&#27867;&#30340;&#20849;&#21516;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14404v1 Announce Type: cross  Abstract: Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commons
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14174</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Blending Data-Driven Priors in Dynamic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14174
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#26426;&#22120;&#20154;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20154;&#32676;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#20123;&#31995;&#32479;&#24212;&#35813;&#22312;&#23433;&#20840;&#30340;&#12289;&#19982;&#20154;&#20114;&#21160;&#24847;&#35782;&#30456;&#20851;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#35268;&#21010;&#22120;&#19982;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#30340;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#21338;&#24328;&#35770;&#25919;&#31574;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;KLGame&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;Kullback-Leibler&#65288;KL&#65289;&#27491;&#21017;&#21270;&#30340;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;&#65292;&#38024;&#23545;&#19968;&#20010;&#19968;&#33324;&#30340;&#12289;&#38543;&#26426;&#30340;&#65292;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#30340;&#21442;&#32771;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14174v1 Announce Type: cross  Abstract: As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13929</link><description>&lt;p&gt;
SDXL-Lightning: &#28176;&#36827;&#24335;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SDXL-Lightning: Progressive Adversarial Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;SDXL&#30340;&#19968;&#27493;/&#20960;&#27493;1024&#20687;&#32032;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#36136;&#37327;&#21644;&#27169;&#24335;&#35206;&#30422;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29702;&#35770;&#20998;&#26512;&#12289;&#21028;&#21035;&#22120;&#35774;&#35745;&#12289;&#27169;&#22411;&#20844;&#24335;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#20197;LoRA&#21644;&#23436;&#25972;UNet&#26435;&#37325;&#30340;&#24418;&#24335;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;SDXL-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
&lt;/p&gt;</description></item><item><title>Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13866</link><description>&lt;p&gt;
Kuaiji&#65306;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kuaiji: the First Chinese Accounting Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13866
&lt;/p&gt;
&lt;p&gt;
Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;GPT-4&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#20219;&#21153;&#35201;&#27714;&#36866;&#24212;&#20250;&#35745;&#31561;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Kuaiji&#65292;&#19968;&#20010;&#19987;&#38376;&#23450;&#21046;&#30340;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;Kuaiji&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#20351;&#29992;&#21253;&#21547;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#30340;Baichuan&#26694;&#26550;&#12290;&#22312;CAtAcctQA&#30340;&#25903;&#25345;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#30495;&#23454;&#20250;&#35745;&#24072;&#19982;&#23458;&#25143;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;Kuaiji&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#23558;Kuaiji&#24314;&#31435;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#24320;&#28304;&#20013;&#22269;&#20250;&#35745;LLM&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35843;&#25972;&#20197;&#36866;&#24212;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#30340;&#28508;&#21147;&#12289;&#25361;&#25112;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#39640;&#25928;&#21019;&#24314;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.12702</link><description>&lt;p&gt;
&#20174;&#20113;&#31471;&#21040;&#36793;&#32536;&#65306;&#37325;&#26032;&#24605;&#32771;&#20302;&#36164;&#28304;&#35774;&#35745;&#25361;&#25112;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12702
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35843;&#25972;&#20197;&#36866;&#24212;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#30340;&#28508;&#21147;&#12289;&#25361;&#25112;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#39640;&#25928;&#21019;&#24314;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#25216;&#26415;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#36164;&#28304;&#30340;&#27785;&#37325;&#38656;&#27714;&#65292;&#23427;&#36890;&#24120;&#22312;&#22823;&#22411;&#35745;&#31639;&#22522;&#30784;&#26550;&#26500;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#32463;&#24120;&#20316;&#20026;&#22522;&#20110;&#20113;&#31471;&#30340;&#26381;&#21153;&#25552;&#20379;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#36793;&#32536;&#36827;&#34892;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#28508;&#21147;&#12289;&#25361;&#25112;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#20854;&#20013;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#12289;&#33021;&#32791;&#65288;&#30005;&#27744;&#65289;&#21644;&#32593;&#32476;&#36830;&#25509;&#21487;&#33021;&#26377;&#38480;&#12290;&#35843;&#25972;&#29983;&#25104;AI&#20197;&#36866;&#24212;&#36825;&#26679;&#30340;&#35774;&#32622;&#28041;&#21450;&#20811;&#26381;&#37325;&#22823;&#38556;&#30861;&#65292;&#20027;&#35201;&#26159;&#22914;&#20309;&#31616;&#21270;&#22797;&#26434;&#27169;&#22411;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26377;&#25928;&#36816;&#34892;&#12290;&#36825;&#38656;&#35201;&#22312;&#27169;&#22411;&#21387;&#32553;&#12289;&#39640;&#25928;&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#21487;&#33021;&#29978;&#33267;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#26041;&#38754;&#25552;&#20986;&#21019;&#26032;&#24615;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#21033;&#29992;&#29983;&#25104;AI&#30340;&#21147;&#37327;&#20026;&#35774;&#35745;&#38382;&#39064;&#21019;&#36896;&#37327;&#36523;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12702v1 Announce Type: new  Abstract: Generative Artificial Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design. However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service. In this position paper, we consider the potential, challenges, and promising approaches for generative AI for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited. Adapting generative AI for such settings involves overcoming significant hurdles, primarily in how to streamline complex models to function efficiently in low-resource environments. This necessitates innovative approaches in model compression, efficient algorithmic design, and perhaps even leveraging edge computing. The objective is to harness the power of generative AI in creating bespoke solutions for design problems, such
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.12168</link><description>&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12168
&lt;/p&gt;
&lt;p&gt;
PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#25552;&#20986;&#24182;&#25104;&#21151;&#23454;&#26045;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#38754;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#20165;&#26356;&#26032;&#26377;&#38480;&#27169;&#22411;&#21442;&#25968;&#30340;PEFT&#26159;&#21542;&#26500;&#25104;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#20173;&#28982;&#26131;&#21463;&#21033;&#29992;&#65292;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#22312;&#24494;&#35843;&#21518;&#20381;&#28982;&#20445;&#25345;&#39640;&#32622;&#20449;&#24230;&#12290;&#21463;&#21040;&#36825;&#19968;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;PEFT&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#65292;&#25552;&#20379;&#38024;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31283;&#20581;&#38450;&#24481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;PEFT&#35757;&#32451;PSIM&#65292;&#24102;&#26377;&#38543;&#26426;&#37325;&#32622;&#26679;&#26412;&#26631;&#31614;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11871</link><description>&lt;p&gt;
&#20174;&#23454;&#38469;&#21040;&#36923;&#36753;&#20877;&#21040;&#23454;&#38469;&#65306;&#20026;&#35268;&#21010;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21457;&#26126;&#31526;&#21495;&#35789;&#27719;&#12289;&#21160;&#20316;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20811;&#26381;&#38271;&#26399;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#34920;&#31034;&#38656;&#35201;&#20855;&#26377;&#24378;&#28872;&#30452;&#35273;&#21644;&#35814;&#32454;&#30693;&#35782;&#30340;&#19987;&#23478;&#65292;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#21487;&#33021;&#38656;&#35201;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#28040;&#38500;&#23545;&#20154;&#31867;&#30452;&#35273;&#30340;&#20381;&#36182;&#26159;&#19968;&#20010;&#26497;&#20026;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#20174;&#26410;&#26631;&#35760;&#30340;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#12290;&#25152;&#23398;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;&#31867;PDDL&#22495;&#27169;&#22411;&#12290;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20174;&#23569;&#25968;&#26426;&#22120;&#20154;&#36712;&#36857;&#20013;&#21487;&#20197;&#23398;&#21040;&#24378;&#22823;&#30340;&#25277;&#35937;&#34920;&#31034;&#65307;&#25152;&#23398;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11871v1 Announce Type: cross  Abstract: Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relation
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;Transformer&#21462;&#20195;U-net&#32467;&#26500;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#23454;&#35777;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11588</link><description>&lt;p&gt;
&#24102;Transformer&#30340;&#33033;&#20914;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SDiT: Spiking Diffusion Model with Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;Transformer&#21462;&#20195;U-net&#32467;&#26500;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#23454;&#35777;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#29983;&#29289;&#35299;&#37322;&#29305;&#24615;&#65292;&#34987;&#35748;&#20026;&#22312;&#33410;&#33021;&#35745;&#31639;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#25506;&#32034;SNNs&#30340;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#23578;&#26410;&#25552;&#20986;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32479;&#19968;&#19988;&#26377;&#25928;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;Transformer&#26469;&#21462;&#20195;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;U-net&#32467;&#26500;&#12290;&#23427;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#12290;&#23427;&#26088;&#22312;&#20026;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#25552;&#20379;&#23454;&#35777;&#22522;&#20934;&#12290;&#22312;MNIST&#12289;Fashion-MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#29616;&#26377;&#30340;SNN&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11588v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10340</link><description>&lt;p&gt;
&#35770;&#37096;&#32626;LLMs/VLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#31361;&#26174;&#39118;&#38505;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10340
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25152;&#28041;&#21450;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#25805;&#20316;&#65292;&#23548;&#33322;&#31561;&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#20250;&#24341;&#20837;&#26174;&#30528;&#30340;&#28431;&#27934;&#65292;&#21363;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs/VLMs&#19982;&#26426;&#22120;&#20154;&#30028;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#25805;&#32437;&#25110;&#35823;&#23548;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#24694;&#24847;&#25915;&#20987;&#31034;&#20363;&#65292;&#24182;&#23545;&#38598;&#25104;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#30693;&#21517;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;&#21253;&#25324;KnowNo VIMA&#21644;Instruct2Act&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#23454;&#29616;&#20102;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#22686;&#24378;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10002</link><description>&lt;p&gt;
MM-Point: &#22810;&#35270;&#35282;&#20449;&#24687;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#19977;&#32500;&#28857;&#20113;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#23454;&#29616;&#20102;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#22686;&#24378;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24863;&#30693;&#39046;&#22495;&#20013;&#65292;&#23558;&#22810;&#31181;&#20256;&#24863;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#23558;2D&#35270;&#22270;&#19978;&#30340;&#35270;&#35273;&#20449;&#24687;&#26144;&#23556;&#21040;3D&#29289;&#20307;&#19978;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#29702;&#35299;&#12290;&#20294;&#26159;&#22312;&#20174;&#19981;&#21516;&#35282;&#24230;&#28210;&#26579;&#30340;&#21333;&#20010;2D&#35270;&#22270;&#20013;&#65292;&#21482;&#33021;&#25552;&#20379;&#26377;&#38480;&#30340;&#37096;&#20998;&#20449;&#24687;&#12290;&#22810;&#35270;&#35282;2D&#20449;&#24687;&#30340;&#20016;&#23500;&#24615;&#21644;&#20215;&#20540;&#21487;&#20197;&#20026;3D&#29289;&#20307;&#25552;&#20379;&#20248;&#31168;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#23427;&#21463;&#21040;&#20869;&#27169;&#24577;&#21644;&#22806;&#27169;&#24577;&#30456;&#20284;&#24230;&#30446;&#26631;&#30340;&#39537;&#21160;&#12290;MM-Point&#30340;&#26680;&#24515;&#22312;&#20110;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#25191;&#34892;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;2D&#22810;&#35270;&#22270;&#20449;&#24687;&#19968;&#33268;&#24615;&#20132;&#21449;&#27169;&#24577;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;(Multi-MLP)&#21644;&#22810;&#23618;&#32423;&#22686;&#24378;&#31574;&#30053;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10002v1 Announce Type: cross  Abstract: In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully desig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.09456</link><description>&lt;p&gt;
&#26410;&#30693;&#21338;&#24328;&#20013;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#26080;&#36951;&#25022;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#22810;&#20010;&#20915;&#31574;&#32773;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#26410;&#30693;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#21644;&#22810;&#26426;&#26500;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#25277;&#26679;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#20132;&#36890;&#36335;&#30001;&#21644;&#38647;&#36798;&#24863;&#30693;&#20013;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#21313;&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#22870;&#21169;&#32467;&#26500;&#26377;&#19968;&#23450;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#30028;&#38480;&#20165;&#23545;&#24635;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FESS Loss&#30340;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;Dice&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25552;&#39640;&#31354;&#38388;&#31934;&#24230;&#21644;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#12289;&#26356;&#31934;&#32454;&#30340;&#20998;&#21106;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08582</link><description>&lt;p&gt;
FESS Loss&#65306;&#29992;&#20110;&#20248;&#21270;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FESS Loss&#30340;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;Dice&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25552;&#39640;&#31354;&#38388;&#31934;&#24230;&#21644;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#12289;&#26356;&#31934;&#32454;&#30340;&#20998;&#21106;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#23545;&#20110;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#30740;&#31350;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#28041;&#21450;&#23558;&#22270;&#20687;&#21010;&#20998;&#20026;&#22810;&#20010;&#21306;&#22495;&#65292;&#20195;&#34920;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#25110;&#30149;&#29702;&#32467;&#26500;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#31354;&#38388;&#31934;&#24230;&#21644;&#20840;&#38754;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;&#65288;FESS Loss&#65289;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#30340;&#20248;&#21183;&#65288;&#22312;&#21307;&#23398;&#25104;&#20687;&#30340;&#24494;&#22937;&#39046;&#22495;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#29305;&#24449;&#65289;&#19982;Dice&#25439;&#22833;&#30340;&#31354;&#38388;&#20934;&#30830;&#24615;&#30456;&#32467;&#21512;&#12290;&#30446;&#26631;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#22686;&#24378;&#31354;&#38388;&#31934;&#24230;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;FESS Loss&#20195;&#34920;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#12289;&#26356;&#31934;&#32454;&#30340;&#20998;&#21106;&#36807;&#31243;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07140</link><description>&lt;p&gt;
&#25991;&#23383;&#25551;&#36848;&#20013;&#30340;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07140
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22270;&#25512;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#26174;&#33879;&#24433;&#21709;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20174;42.22&#65285;&#25552;&#39640;&#21040;70&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19981;&#38543;&#22270;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05902</link><description>&lt;p&gt;
&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#31934;&#35843;&#30340;Segment Anything Model&#65288;SAM&#65289;
&lt;/p&gt;
&lt;p&gt;
ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12289;&#22810;&#26679;&#30340;&#36755;&#20837;&#25552;&#31034;&#12289;&#35757;&#32451;&#33021;&#21147;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#65292;&#26032;&#21457;&#24067;&#30340;Segment Anything Model&#65288;SAM&#65289;&#25104;&#20026;&#22270;&#20687;&#22788;&#29702;&#20013;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;SAM&#24403;&#21069;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#65292;&#23588;&#20854;&#26159;&#36229;&#22768;&#22270;&#20687;&#12290;&#36229;&#22768;&#22270;&#20687;&#24448;&#24448;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#37325;&#35201;&#32467;&#26500;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ClickSAM&#65292;&#23427;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;ClickSAM&#26377;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20301;&#20110;&#30495;&#23454;&#36718;&#24275;&#20013;&#24515;&#30340;&#21333;&#20987;&#25552;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#39069;&#22806;&#30340;&#27491;&#36127;&#28857;&#20987;&#25552;&#31034;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#25513;&#33180;&#36827;&#34892;&#27604;&#36739;&#65292;&#35745;&#31639;&#20986;&#30495;&#27491;&#27491;&#12289;&#20551;&#27491;&#21644;&#20551;&#36127;&#27573;&#12290;&#27491;&#28857;&#20987;&#20351;&#29992;&#30495;&#23454;&#25513;&#33180;&#20013;&#30340;&#30495;&#23454;
&lt;/p&gt;
&lt;p&gt;
The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.05493</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Investigating White-Box Attacks for On-Device Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#30456;&#24212;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#36731;&#26131;&#25552;&#21462;&#20986;&#26469;&#12290;&#29616;&#26377;&#30340;&#35774;&#22791;&#19978;&#25915;&#20987;&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#40657;&#30418;&#25915;&#20987;&#65292;&#36825;&#31181;&#26041;&#27861;&#36828;&#19981;&#22914;&#30333;&#30418;&#31574;&#30053;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;TFLite&#19981;&#25903;&#25345;&#26799;&#24230;&#35745;&#31639;&#65292;&#32780;&#26799;&#24230;&#35745;&#31639;&#23545;&#20110;&#30333;&#30418;&#25915;&#20987;&#31639;&#27861;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#21457;&#29616;&#21487;&#33021;&#20302;&#20272;&#20102;&#35774;&#22791;&#19978;&#25915;&#20987;&#30340;&#21361;&#23475;&#24615;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65306;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#30333;&#30418;&#31574;&#30053;&#30452;&#25509;&#21463;&#21040;&#25915;&#20987;&#65311;&#25105;&#20204;&#39318;&#20808;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#23558;&#35774;&#22791;&#19978;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#29256;&#26412;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36870;&#21521;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REOM
&lt;/p&gt;
&lt;p&gt;
Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.04880</link><description>&lt;p&gt;
&#32467;&#21512;&#20113;&#35745;&#31639;&#19982;&#31227;&#21160;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Combining Cloud and Mobile Computing for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04880
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#20063;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#36235;&#21183;&#32473;&#31227;&#21160;&#35774;&#22791;&#24102;&#26469;&#20102;&#38382;&#39064;&#65292;&#22914;&#20869;&#23384;&#23481;&#37327;&#21644;&#30005;&#27744;&#23551;&#21629;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#35768;&#22810;&#26381;&#21153;&#65288;&#22914;ChatGPT&#21644;Midjourney&#65289;&#22312;&#20113;&#20013;&#36816;&#34892;&#25152;&#26377;&#30340;&#25512;&#29702;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#28789;&#27963;&#24615;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20998;&#37197;&#26356;&#21487;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#20998;&#21106;&#35270;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35745;&#31639;&#20998;&#21106;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#35745;&#31639;&#23494;&#38598;&#37096;&#20998;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#21106;&#19981;&#20165;&#20943;&#23569;&#20102;&#29992;&#25143;&#31561;&#24453;&#26102;&#38388;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#32454;&#31890;&#24230;&#35843;&#25972;&#26469;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#24230;&#22120;&#65292;&#25910;&#38598;&#32593;&#32476;&#36136;&#37327;&#12289;&#23458;&#25143;&#31471;&#35774;&#22791;&#33021;&#21147;&#21644;&#20316;&#19994;&#35201;&#27714;&#30340;&#20449;&#24687;&#65292;&#20570;&#20986;&#20915;&#31574;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03822</link><description>&lt;p&gt;
RevOrder&#65306;&#19968;&#31181;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevOrder&#65292;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;n&#20301;&#25968;&#20056;&#20197;1&#20301;&#25968;&#65288;nD&#20056;&#20197;1D&#65289;&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39034;&#24207;&#20013;&#38388;&#25968;&#23383;&#30340;&#25968;&#37327; (CSID)&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#35780;&#20272;&#26041;&#31243;&#22797;&#26434;&#24615;&#30340;&#26032;&#24230;&#37327;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;&#65292;RevOrder&#19981;&#20165;&#22312;&#22522;&#26412;&#30340;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#22823;&#25968;&#24773;&#20917;&#19979;&#12290;RevOrder&#30340;&#23454;&#29616;&#23545;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;RevOrder&#24212;&#29992;&#20110;&#23545;GSM8K&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;LLaMA2-7B&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26041;&#31243;&#35745;&#31639;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;46%&#65292;&#23558;&#24635;&#20307;&#24471;&#20998;&#20174;41.6&#25552;&#21319;&#21040;44.4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.03246</link><description>&lt;p&gt;
SGS-SLAM&#65306;&#22522;&#20110;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03246
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29702;&#35299;&#22312;&#31264;&#23494;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65288;SLAM&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20840;&#38754;&#30340;&#22330;&#26223;&#35299;&#26512;&#12290;&#26368;&#36817;&#23558;&#39640;&#26031;&#28857;&#20113;&#38598;&#25104;&#21040;SLAM&#31995;&#32479;&#20013;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#30340;&#19977;&#32500;&#39640;&#26031;&#34920;&#31034;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGS-SLAM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;&#35270;&#35273;SLAM&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#65292;&#36824;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#24314;&#22270;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#36890;&#36947;&#20248;&#21270;&#65292;&#23558;&#22806;&#35266;&#12289;&#20960;&#20309;&#21644;&#35821;&#20041;&#32422;&#26463;&#19982;&#20851;&#38190;&#24103;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SGS-SLAM&#22312;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12289;&#22320;&#22270;&#37325;&#24314;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01729</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextualization Distillation from Large Language Model for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#35821;&#26009;&#24211;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#25110;&#21516;&#20041;&#35789;&#23450;&#20041;&#20013;&#25910;&#38598;&#30340;&#38745;&#24577;&#21644;&#22122;&#22768;&#24615;&#36136;&#24120;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;PLM&#30340;KGC&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#33976;&#39311;&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25554;&#20837;&#21644;&#21487;&#25773;&#25918;&#30340;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#21644;&#29983;&#25104;&#30340;KGC&#26694;&#26550;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#32039;&#20945;&#30340;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#27573;&#33853;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#37325;&#24314;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#36739;&#23567;&#30340;KGC&#27169;&#22411;&#33021;&#22815;&#21560;&#25910;&#36825;&#20123;&#20016;&#23500;&#30340;&#19977;&#20803;&#32452;&#20013;&#30340;&#35265;&#35299;&#12290;&#23545;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;KGC&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#26080;&#35770;&#22522;&#30784;&#31649;&#36947;&#22914;&#20309;&#65292;&#22987;&#32456;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.00397</link><description>&lt;p&gt;
&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00397
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#23545;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#20197;&#24110;&#21161;&#39640;&#25928;&#20998;&#37197;&#36164;&#28304;&#21644;&#26377;&#25928;&#25511;&#21046;&#20132;&#36890;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20016;&#23500;&#30340;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#35768;&#22810;&#22478;&#24066;&#30001;&#20110;&#35774;&#22791;&#25903;&#25345;&#26377;&#38480;&#32780;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#36825;&#23545;&#20132;&#36890;&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35266;&#23519;&#65306;&#20132;&#36890;&#27169;&#24335;&#22312;&#19981;&#21516;&#22478;&#24066;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#65288;MTPB&#65289;&#12290;&#20027;&#35201;&#19978;&#65292;MTPB&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#21551;&#21160;&#20854;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#31867;&#25216;&#26415;&#20174;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20013;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#12290;&#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20934;&#30830;&#30340;&#20132;&#36890;&#27169;&#24335;&#26816;&#32034;&#26426;&#21046;&#36827;&#34892;&#36328;&#22478;&#24066;&#30340;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;CodeAid&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#31243;&#21161;&#25163;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#23398;&#29983;&#21644;&#25945;&#32946;&#32773;&#30340;&#38656;&#27714;&#65292;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27010;&#24565;&#24615;&#22238;&#31572;&#32780;&#19981;&#26292;&#38706;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#37096;&#32626;&#65292;&#24182;&#24635;&#32467;&#20986;&#22235;&#20010;&#26410;&#26469;&#25945;&#32946;AI&#21161;&#25163;&#30340;&#35774;&#35745;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2401.11314</link><description>&lt;p&gt;
CodeAid: &#23545;&#22522;&#20110;LLM&#30340;&#32534;&#31243;&#21161;&#25163;&#36827;&#34892;&#35838;&#22530;&#37096;&#32626;&#30340;&#35780;&#20272;&#65292;&#24179;&#34913;&#23398;&#29983;&#21644;&#25945;&#32946;&#32773;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11314
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;CodeAid&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#31243;&#21161;&#25163;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#23398;&#29983;&#21644;&#25945;&#32946;&#32773;&#30340;&#38656;&#27714;&#65292;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27010;&#24565;&#24615;&#22238;&#31572;&#32780;&#19981;&#26292;&#38706;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#37096;&#32626;&#65292;&#24182;&#24635;&#32467;&#20986;&#22235;&#20010;&#26410;&#26469;&#25945;&#32946;AI&#21161;&#25163;&#30340;&#35774;&#35745;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30340;&#12289;&#20010;&#24615;&#21270;&#30340;&#21453;&#39304;&#23545;&#20110;&#23398;&#20064;&#32534;&#31243;&#30340;&#23398;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#20687;ChatGPT&#36825;&#26679;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#25552;&#20379;&#21363;&#26102;&#25903;&#25345;&#65292;&#20294;&#21576;&#29616;&#30452;&#25509;&#30340;&#20195;&#30721;&#31572;&#26696;&#65292;&#36825;&#21487;&#33021;&#22952;&#30861;&#28145;&#20837;&#30340;&#27010;&#24565;&#21442;&#19982;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;CodeAid&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#31243;&#21161;&#25163;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#12289;&#25216;&#26415;&#19978;&#27491;&#30830;&#30340;&#22238;&#31572;&#65292;&#20294;&#19981;&#26174;&#31034;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#12290;CodeAid&#22238;&#31572;&#27010;&#24565;&#38382;&#39064;&#65292;&#29983;&#25104;&#24102;&#26377;&#36880;&#34892;&#35299;&#37322;&#30340;&#20266;&#20195;&#30721;&#65292;&#24182;&#20026;&#23398;&#29983;&#30340;&#38169;&#35823;&#20195;&#30721;&#21152;&#19978;&#20462;&#22797;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#25324;700&#21517;&#23398;&#29983;&#30340;&#32534;&#31243;&#35838;&#31243;&#20013;&#37096;&#32626;&#20102;CodeAid&#65292;&#20026;&#26399;12&#21608;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;8000&#27425;CodeAid&#20351;&#29992;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#27599;&#21608;&#35843;&#26597;&#21644;22&#27425;&#23398;&#29983;&#35775;&#35848;&#12290;&#28982;&#21518;&#25105;&#20204;&#37319;&#35775;&#20102;&#20843;&#21517;&#32534;&#31243;&#25945;&#32946;&#32773;&#65292;&#20197;&#33719;&#24471;&#26356;&#22810;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#26410;&#26469;&#25945;&#32946;AI&#21161;&#25163;&#30340;&#22235;&#20010;&#35774;&#35745;&#32771;&#34385;&#65306;D1)&#20805;&#20998;&#21033;&#29992;AI&#30340;&#29420;&#29305;&#20248;&#21183;&#65307;D2)&#31616;&#21270;&#26597;&#35810;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11314v2 Announce Type: replace-cross  Abstract: Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student's incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI's unique benefits; D2) simplifying query form
&lt;/p&gt;</description></item><item><title>MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2401.07314</link><description>&lt;p&gt;
MapGPT&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#30340;&#22320;&#22270;&#24341;&#23548;&#25552;&#31034;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07314
&lt;/p&gt;
&lt;p&gt;
MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;GPT&#20316;&#20026;&#22823;&#33041;&#30340;&#20307;&#39564;&#20195;&#29702;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#20915;&#31574;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#38646;-shot&#20195;&#29702;&#21482;&#20419;&#20351;GPT-4&#22312;&#23616;&#37096;&#29615;&#22659;&#20013;&#36873;&#25321;&#28508;&#22312;&#20301;&#32622;&#65292;&#32780;&#27809;&#26377;&#20026;&#20195;&#29702;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#8220;&#20840;&#23616;&#35270;&#22270;&#8221;&#26469;&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22320;&#22270;&#24341;&#23548;&#30340;&#22522;&#20110;GPT&#30340;&#20195;&#29702;&#65292;&#21517;&#20026;MapGPT&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#26469;&#40723;&#21169;&#20840;&#23616;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#21512;&#24182;&#21040;&#21253;&#21547;&#33410;&#28857;&#20449;&#24687;&#21644;&#25299;&#25169;&#20851;&#31995;&#30340;&#25552;&#31034;&#20013;&#65292;&#20197;&#24110;&#21161;GPT&#29702;&#35299;&#31354;&#38388;&#29615;&#22659;&#12290;&#20174;&#36825;&#19968;&#35774;&#35745;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#20195;&#29702;&#26681;&#25454;&#22320;&#22270;&#25191;&#34892;&#22810;&#27493;&#35268;&#21010;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#20010;&#20505;&#36873;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07314v2 Announce Type: replace  Abstract: Embodied agents equipped with GPT as their brain have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT-4 to select potential locations within localized environments, without constructing an effective "global-view" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage the global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate 
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;MoE-Mamba&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#65292;&#36798;&#21040;&#20102;&#19982;Mamba&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35757;&#32451;&#27493;&#39588;&#20943;&#23569;&#20102;2.35&#20493;&#12290;</title><link>https://arxiv.org/abs/2401.04081</link><description>&lt;p&gt;
MoE-Mamba: &#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.04081
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;MoE-Mamba&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#65292;&#36798;&#21040;&#20102;&#19982;Mamba&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35757;&#32451;&#27493;&#39588;&#20943;&#23569;&#20102;2.35&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#24050;&#32463;&#25104;&#20026;&#39034;&#24207;&#24314;&#27169;&#39046;&#22495;&#30340;&#20005;&#32899;&#31454;&#20105;&#32773;&#65292;&#25361;&#25112;&#20102;Transformer&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#24320;&#25918;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#35201;&#21457;&#25496;SSMs&#22312;&#25193;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#24212;&#35813;&#19982;MoE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;Mamba&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#36817;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MoE-Mamba&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#12290;&#29305;&#21035;&#22320;&#65292;MoE-Mamba&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#20013;&#36798;&#21040;&#19982;Mamba&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;Mamba&#30456;&#23545;&#20110;Transformer&#30340;&#25512;&#29702;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.00546</link><description>&lt;p&gt;
AllSpark: &#19968;&#20010;&#20855;&#26377;&#21313;&#19977;&#31181;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00546
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#30001;&#20110;&#21508;&#31181;&#26102;&#31354;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#39640;&#24230;&#24322;&#36136;&#24615;&#65292;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#32852;&#21512;&#35299;&#37322;&#19968;&#30452;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#32780;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#24179;&#34913;&#34920;&#29616;&#20986;&#36880;&#28176;&#38750;&#32447;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#20316;&#20026;&#21442;&#32771;&#26694;&#26550;&#65288;LaRF&#65289;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#27169;&#24577;&#32479;&#19968;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21462;&#24471;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21253;&#25324;1D&#65288;&#25991;&#26412;&#65292;&#20195;&#30721;&#65289;&#65292;2D&#65288;RGB&#65292;&#32418;&#22806;&#32447;&#65292;SAR&#65292;&#22810;&#20809;&#35889;&#65292;&#39640;&#20809;&#35889;&#65292;&#34920;&#26684;&#65292;&#22270;&#34920;&#65292;&#36712;&#36857;&#65292;&#26012;&#35282;&#25668;&#24433;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#22240;&#26524;&#20986;&#29616;&#21644;&#30456;&#20851;&#23450;&#37327;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#21644;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#22240;&#26524;&#20986;&#29616;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.16815</link><description>&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#20986;&#29616;&#21644;&#22240;&#26524;&#20851;&#31995;&#65306;&#20851;&#20110;&#22240;&#26524;&#20986;&#29616;&#21644;&#30456;&#20851;&#23450;&#37327;&#30740;&#31350;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Emergence and Causality in Complex Systems: A Survey on Causal Emergence and Related Quantitative Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#22240;&#26524;&#20986;&#29616;&#21644;&#30456;&#20851;&#23450;&#37327;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#21644;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#22240;&#26524;&#20986;&#29616;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#29616;&#21644;&#22240;&#26524;&#20851;&#31995;&#26159;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#23427;&#20204;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#12290;&#20986;&#29616;&#19968;&#26041;&#38754;&#25351;&#30340;&#26159;&#23439;&#35266;&#23646;&#24615;&#19981;&#33021;&#20165;&#24402;&#22240;&#20110;&#20010;&#20307;&#23646;&#24615;&#30340;&#21407;&#22240;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#34920;&#29616;&#20986;&#20986;&#29616;&#65292;&#24847;&#21619;&#30528;&#38543;&#30528;&#25277;&#35937;&#23618;&#27425;&#30340;&#22686;&#21152;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#26032;&#30340;&#22240;&#26524;&#23450;&#24459;&#12290;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#26088;&#22312;&#36830;&#25509;&#36825;&#20004;&#20010;&#27010;&#24565;&#65292;&#29978;&#33267;&#37319;&#29992;&#22240;&#26524;&#24230;&#37327;&#26469;&#37327;&#21270;&#20986;&#29616;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20986;&#29616;&#23450;&#37327;&#29702;&#35770;&#21644;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#37325;&#28857;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#21644;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#12290;&#35299;&#20915;&#21518;&#32773;&#38656;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#22240;&#26524;&#20986;&#29616;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#29992;&#20110;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#30340;&#26550;&#26500;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergence and causality are two fundamental concepts for understanding complex systems. They are interconnected. On one hand, emergence refers to the phenomenon where macroscopic properties cannot be solely attributed to the cause of individual properties. On the other hand, causality can exhibit emergence, meaning that new causal laws may arise as we increase the level of abstraction. Causal emergence theory aims to bridge these two concepts and even employs measures of causality to quantify emergence. This paper provides a comprehensive review of recent advancements in quantitative theories and applications of causal emergence. Two key problems are addressed: quantifying causal emergence and identifying it in data. Addressing the latter requires the use of machine learning techniques, thus establishing a connection between causal emergence and artificial intelligence. We highlighted that the architectures used for identifying causal emergence are shared by causal representation learn
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.16424</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft Contrastive Learning for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#34920;&#31034;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30456;&#20284;&#30340;&#23454;&#20363;&#25110;&#30456;&#37051;&#26102;&#38388;&#25139;&#30340;&#20540;&#36827;&#34892;&#23545;&#27604;&#20250;&#24573;&#30053;&#23427;&#20204;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftCLT&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#36719;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#20174;&#38646;&#21040;&#19968;&#30340;&#36719;&#36171;&#20540;&#30340;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;1)&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20102;&#23454;&#20363;&#32423;&#23545;&#27604;&#25439;&#22833;&#30340;&#36719;&#36171;&#20540;&#65292;&#24182;&#20026;2)&#22522;&#20110;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#20102;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#12290;SoftCLT&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#27809;&#26377;&#36807;&#22810;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.12430</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#25913;&#36827;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12430
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;RAG&#26041;&#27861;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#22120;&#22312;&#25552;&#21319;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25581;&#31034;&#27599;&#23545;&#26597;&#35810;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#38656;&#35201;&#21453;&#22797;&#23545;&#26597;&#35810;&#21644;&#22823;&#37327;&#38271;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#23548;&#33268;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#38480;&#21046;&#20102;&#26816;&#32034;&#25991;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20934;&#30830;&#24615;&#12290;&#20316;&#20026;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#36895;&#24230;&#25552;&#39640;20&#20493;&#33267;40&#20493;&#65292;&#36229;&#36807;&#22522;&#20934;&#36890;&#36947;&#37325;&#26032;&#25490;&#24207;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Sigmoid Trick&#65292;&#19968;&#31181;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#22312;&#20174;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#32463;&#39564;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#33976;&#39311;(MD)&#26694;&#26550;&#32467;&#21512;&#20102;LLMs&#20013;&#30340;Program of Thought (PoT)&#21644;Chain of Thought (CoT)&#33021;&#21147;&#65292;&#23558;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#33976;&#39311;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#36739;&#23567;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.10730</link><description>&lt;p&gt;
&#28151;&#21512;&#33976;&#39311;&#26377;&#21161;&#20110;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mixed Distillation Helps Smaller Language Model Better Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10730
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#33976;&#39311;(MD)&#26694;&#26550;&#32467;&#21512;&#20102;LLMs&#20013;&#30340;Program of Thought (PoT)&#21644;Chain of Thought (CoT)&#33021;&#21147;&#65292;&#23558;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#33976;&#39311;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#36739;&#23567;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26368;&#36817;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36890;&#36807;&#20174;LLMs&#33976;&#39311;&#30693;&#35782;&#26469;&#22686;&#24378;&#36739;&#23567;&#27169;&#22411;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#21035;&#38656;&#35201;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#19982;LLMs&#30340;&#24615;&#33021;&#21305;&#25932;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28151;&#21512;&#33976;&#39311;(MD)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;Program of Thought (PoT)&#21644;Chain of Thought (CoT)&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#32467;&#21512;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#24182;&#23558;&#36825;&#20123;&#33021;&#21147;&#33976;&#39311;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MD&#26174;&#33879;&#22686;&#24378;&#20102;&#36739;&#23567;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21333;&#36335;&#24452;&#21644;&#22810;&#36335;&#24452;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10730v2 Announce Type: replace-cross  Abstract: While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generalit
&lt;/p&gt;</description></item><item><title>MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.08656</link><description>&lt;p&gt;
MaxK-GNN: &#25506;&#32034;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#29702;&#35770;&#36895;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08656
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21152;&#36895;&#26041;&#38754;&#65292;GPU&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#24179;&#21488;&#12290; GPU&#22312;GNN&#19978;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20363;&#22914;PyG&#12289;DGL&#19982;cuSPARSE&#65292;&#20197;&#21450;GNNAdvisor&#26694;&#26550;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#20869;&#23384;&#27969;&#37327;&#20173;&#28982;&#24456;&#26174;&#33879;&#12290; &#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#36890;&#36807;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#22402;&#30452;&#20248;&#21270;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#26159;&#23558;&#21152;&#36895;&#20248;&#21270;&#35270;&#20026;&#8220;&#20107;&#21518;&#24605;&#32771;&#8221;&#65288;&#21363;&#65288;i&#65289;&#32473;&#23450;GNN&#31639;&#27861;&#65292;&#35774;&#35745;&#21152;&#36895;&#22120;&#65292;&#25110;&#65288;ii&#65289;&#32473;&#23450;&#30828;&#20214;&#65292;&#20027;&#35201;&#20248;&#21270;GNN&#31639;&#27861;&#65289;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MaxK-GNN&#65292;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#20808;&#36827;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#12290; &#65288;i&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;MaxK&#38750;&#32447;&#24615;&#24182;&#25552;&#20379;&#20102;MaxK&#38750;&#32447;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;GPT-4V(ision)&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23545;&#27604;&#20102;&#20854;&#19982;CLIP&#12289;LLaVA&#21644;Gemini&#31561;&#30693;&#21517;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.07424</link><description>&lt;p&gt;
GPT-4V(ision)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#36866;&#24212;&#24615;&#22914;&#20309;&#65311;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;GPT-4V(ision)&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23545;&#27604;&#20102;&#20854;&#19982;CLIP&#12289;LLaVA&#21644;Gemini&#31561;&#30693;&#21517;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#38024;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#27867;&#21270;&#33021;&#21147;&#8212;&#8212;&#21363;&#37096;&#32626;&#26465;&#20214;&#19982;&#35757;&#32451;&#22330;&#26223;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#8212;&#8212;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#27668;&#20505;&#24314;&#27169;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#12290;&#22522;&#20110;&#24191;&#27867;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#32780;&#21306;&#21035;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#36866;&#24212;&#33021;&#21147;&#30340;&#22686;&#21152;&#20852;&#36259;&#12290;GPT-4V(ision)&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#20844;&#24320;&#33719;&#21462;&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#12289;&#35270;&#39057;&#29702;&#35299;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#31350;&#12290;&#38024;&#23545;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23545;GPT-4V&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#19982;CLIP&#12289;LLaVA&#21644;Gemini&#31561;&#30693;&#21517;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07424v3 Announce Type: replace-cross  Abstract: In machine learning, generalization against distribution shifts -- where deployment conditions diverge from the training scenarios -- is crucial, particularly in fields like climate modeling, biomedicine, and autonomous driving. The emergence of foundation models, distinguished by their extensive pretraining and task versatility, has led to an increased interest in their adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced publicly accessible multimodal foundation model, with extensive applications across various domains, including anomaly detection, video understanding, image generation, and medical diagnosis. However, its robustness against data distributions remains largely underexplored. Addressing this gap, this study rigorously evaluates GPT-4V's adaptability and generalization capabilities in dynamic environments, benchmarking against prominent models like CLIP, LLaVA, and Gemini. We delve into GP
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#30340;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#21382;&#21490;&#20381;&#36182;&#21644;&#26410;&#26469;&#36235;&#21183;&#21453;&#26144;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.03004</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#30340;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#21382;&#21490;&#20381;&#36182;&#21644;&#26410;&#26469;&#36235;&#21183;&#21453;&#26144;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#65306;&#23558;&#22522;&#20110;&#21382;&#21490;&#24555;&#29031;&#30340;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#25512;&#29702;&#31216;&#20026;&#22806;&#25512;&#65292;&#24050;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#30001;&#20110;&#20854;&#26497;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#31354;&#38388;&#19982;&#26102;&#38388;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#65292;TKG&#25512;&#29702;&#21576;&#29616;&#20986;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#26377;&#25928;&#25429;&#33719;&#20107;&#23454;&#20043;&#38388;&#30340;&#24182;&#21457;&#32467;&#26500;&#21644;&#28436;&#21464;&#20132;&#20114;&#20316;&#29992;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;TKG&#30340;&#22810;&#31181;&#20869;&#22312;&#34920;&#36798;&#35821;&#20041;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#36328;&#22810;&#20010;&#26102;&#38388;&#25139;&#30340;&#23454;&#20307;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#21608;&#26399;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#20805;&#20998;&#21453;&#26144;&#21382;&#21490;&#20381;&#36182;&#20851;&#31995;&#21644;&#26410;&#26469;&#36235;&#21183;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#65288;LMS&#65289;&#30340;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03004v2 Announce Type: replace-cross  Abstract: Temporal Knowledge Graph (TKG) reasoning that forecasts future events based on historical snapshots distributed over timestamps is denoted as extrapolation and has gained significant attention. Owing to its extreme versatility and variation in spatial and temporal correlations, TKG reasoning presents a challenging task, demanding efficient capture of concurrent structures and evolutional interactions among facts. While existing methods have made strides in this direction, they still fall short of harnessing the diverse forms of intrinsic expressive semantics of TKGs, which encompass entity correlations across multiple timestamps and periodicity of temporal information. This limitation constrains their ability to thoroughly reflect historical dependencies and future trends. In response to these drawbacks, this paper proposes an innovative reasoning approach that focuses on Learning Multi-graph Structure (LMS). Concretely, it com
&lt;/p&gt;</description></item><item><title>xTrimoGene&#26159;&#19968;&#31181;&#38024;&#23545;scRNA-seq&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#31232;&#30095;&#29305;&#24615;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#35757;&#32451;&#26368;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.15156</link><description>&lt;p&gt;
xTrimoGene&#65306;&#29992;&#20110;&#21333;&#32454;&#32990;RNA-Seq&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15156
&lt;/p&gt;
&lt;p&gt;
xTrimoGene&#26159;&#19968;&#31181;&#38024;&#23545;scRNA-seq&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#31232;&#30095;&#29305;&#24615;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#35757;&#32451;&#26368;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#27979;&#24207;&#25216;&#26415;&#30340;&#36827;&#27493;&#24050;&#32463;&#22312;&#21333;&#32454;&#32990;&#27700;&#24179;&#19978;&#27979;&#37327;&#22522;&#22240;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#65288;scRNA-seq&#65289;&#25968;&#25454;&#37327;&#24050;&#32463;&#36229;&#36807;&#20102;5000&#19975;&#26465;&#20154;&#31867;&#35760;&#24405;&#65292;&#27599;&#26465;&#35760;&#24405;&#27979;&#37327;&#20102;2&#19975;&#20010;&#22522;&#22240;&#12290;&#36825;&#31361;&#20986;&#20102;&#23545;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#38656;&#27714;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#26041;&#38754;&#37117;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;scRNA-seq&#25968;&#25454;&#30340;&#38750;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#31216;&#20026;xTrimoGene$^\alpha$&#65288;&#25110;&#31616;&#31216;&#20026;xTrimoGene&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#31232;&#30095;&#29305;&#24615;&#26469;&#25193;&#23637;&#39044;&#35757;&#32451;&#12290;xTrimoGene&#30340;&#21487;&#25193;&#23637;&#35774;&#35745;&#23558;FLOPs&#38477;&#20302;&#20102;&#19968;&#20010;&#21040;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19982;&#20256;&#32479;Transformer&#30456;&#27604;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#26368;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15156v2 Announce Type: replace-cross  Abstract: Advances in high-throughput sequencing technology have led to significant progress in measuring gene expressions at the single-cell level. The amount of publicly available single-cell RNA-seq (scRNA-seq) data is already surpassing 50M records for humans with each record measuring 20,000 genes. This highlights the need for unsupervised representation learning to fully ingest these data, yet classical transformer architectures are prohibitive to train on such data in terms of both computation and memory. To address this challenge, we propose a novel asymmetric encoder-decoder transformer for scRNA-seq data, called xTrimoGene$^\alpha$ (or xTrimoGene for short), which leverages the sparse characteristic of the data to scale up the pre-training. This scalable design of xTrimoGene reduces FLOPs by one to two orders of magnitude compared to classical transformers while maintaining high accuracy, enabling us to train the largest transf
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;IL&#21644;&#40065;&#26834;MPC&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Tube-NeRF&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;NeRFs&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36890;&#36807;&#31649;&#30340;&#29305;&#24615;&#36873;&#25321;&#30456;&#20851;&#35270;&#22270;&#65292;&#39640;&#25928;&#35745;&#31639;&#23545;&#24212;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35270;&#35273;&#23548;&#21521;&#31574;&#30053;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.14153</link><description>&lt;p&gt;
Tube-NeRF&#65306;&#20351;&#29992;Tube-Guided&#25968;&#25454;&#22686;&#24378;&#21644;NeRFs&#20174;MPC&#36827;&#34892;&#35270;&#36816;&#21160;&#31574;&#30053;&#30340;&#39640;&#25928;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14153
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;IL&#21644;&#40065;&#26834;MPC&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Tube-NeRF&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;NeRFs&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36890;&#36807;&#31649;&#30340;&#29305;&#24615;&#36873;&#25321;&#30456;&#20851;&#35270;&#22270;&#65292;&#39640;&#25928;&#35745;&#31639;&#23545;&#24212;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35270;&#35273;&#23548;&#21521;&#31574;&#30053;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21487;&#20197;&#20174;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;MPC&#65289;&#20013;&#35757;&#32451;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24863;&#30693;&#21160;&#20316;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#25110;&#40065;&#26834;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;IL&#19982;&#19968;&#31181;&#32771;&#34385;&#36807;&#31243;&#21644;&#20256;&#24863;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;MPC&#30340;&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;DA&#26041;&#27861;&#21517;&#20026;Tube-NeRF&#65292;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#29983;&#25104;&#26032;&#39062;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#40065;&#26834;MPC&#30340;&#24615;&#36136;&#65288;&#31649;&#65289;&#36873;&#25321;&#30456;&#20851;&#35270;&#22270;&#65292;&#24182;&#26377;&#25928;&#35745;&#31639;&#30456;&#24212;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#23558;&#26041;&#27861;&#37327;&#36523;&#23450;&#21046;&#20026;&#22810;&#26059;&#32764;&#19978;&#30340;&#23450;&#20301;&#21644;&#36712;&#36857;&#36319;&#36394;&#20219;&#21153;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20165;&#20351;&#29992;&#26426;&#36733;&#25668;&#20687;&#26426;&#22270;&#20687;&#20316;&#20026;&#27700;&#24179;&#20301;&#32622;&#21807;&#19968;&#26469;&#28304;&#30340;&#35270;&#21160;&#20316;&#31574;&#30053;&#26469;&#29983;&#25104;&#25511;&#21046;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14153v2 Announce Type: replace-cross  Abstract: Imitation learning (IL) can train computationally-efficient sensorimotor policies from a resource-intensive Model Predictive Controller (MPC), but it often requires many samples, leading to long training times or limited robustness. To address these issues, we combine IL with a variant of robust MPC that accounts for process and sensing uncertainties, and we design a data augmentation (DA) strategy that enables efficient learning of vision-based policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance Fields (NeRFs) to generate novel synthetic images, and uses properties of the robust MPC (the tube) to select relevant views and to efficiently compute the corresponding actions. We tailor our approach to the task of localization and trajectory tracking on a multirotor, by learning a visuomotor policy that generates control actions using images from the onboard camera as only source of horizontal position. Nume
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20851;&#32852;&#33258;&#30417;&#30563;&#27169;&#22411;&#26500;&#24314;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#38236;&#23376;&#21069;&#23398;&#20064;&#33258;&#36523;&#30340;&#19977;&#32500;&#23039;&#21183;&#26816;&#27979;&#65292;&#36136;&#37327;&#21363;&#21051;&#23436;&#32654;&#12290;</title><link>https://arxiv.org/abs/2311.13226</link><description>&lt;p&gt;
&#38236;&#23376;&#20013;&#30340;&#26426;&#22120;&#20154;&#65306;&#36890;&#36807;&#20851;&#32852;&#33258;&#30417;&#30563;&#27169;&#22411;&#23398;&#20064;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Robot at the Mirror: Learning to Imitate via Associating Self-supervised Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13226
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20851;&#32852;&#33258;&#30417;&#30563;&#27169;&#22411;&#26500;&#24314;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#38236;&#23376;&#21069;&#23398;&#20064;&#33258;&#36523;&#30340;&#19977;&#32500;&#23039;&#21183;&#26816;&#27979;&#65292;&#36136;&#37327;&#21363;&#21051;&#23436;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#30456;&#20851;&#32852;&#32780;&#19981;&#26159;&#35757;&#32451;&#21644;&#24494;&#35843;&#20174;&#29616;&#25104;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#26500;&#24314;&#33258;&#23450;&#20041;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#20223;&#20154;&#26426;&#22120;&#20154;&#22312;&#38236;&#23376;&#21069;&#35266;&#23519;&#24182;&#23398;&#20064;&#20174;&#20854;&#25152;&#24863;&#30693;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#33258;&#36523;&#36523;&#20307;&#30340;&#19977;&#32500;&#23039;&#21183;&#20026;&#20363;&#36827;&#34892;&#28436;&#31034;&#12290;&#20026;&#26500;&#24314;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20043;&#21069;&#20934;&#22791;&#30340;&#27169;&#22411;&#20174;&#35270;&#35273;&#36755;&#20837;&#21644;&#26426;&#22120;&#20154;&#36523;&#20307;&#23039;&#21183;&#20013;&#33719;&#24471;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38236;&#23376;&#20013;&#19968;&#20010;&#39640;&#25928;&#30340;&#26679;&#26412;&#26426;&#22120;&#20154;&#33258;&#25105;&#25506;&#32034;&#26469;&#26144;&#23556;&#23427;&#20204;&#23545;&#24212;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#26426;&#22120;&#20154;&#26500;&#24314;&#20102;&#25152;&#38656;&#30340;3D&#23039;&#21183;&#26816;&#27979;&#22120;&#65292;&#20854;&#22312;&#33719;&#21462;&#30340;&#26679;&#26412;&#19978;&#21363;&#21051;&#23436;&#32654;&#65292;&#32780;&#19981;&#26159;&#36880;&#28176;&#33719;&#24471;&#36825;&#20010;&#36136;&#37327;&#12290;&#36825;&#31181;&#20351;&#29992;&#20851;&#32852;&#29305;&#24449;&#21521;&#37327;&#23545;&#30340;&#26144;&#23556;&#38543;&#21518;&#20197;&#33879;&#21517;&#30340;Transformer&#27169;&#22411;&#30340;&#38190;-&#20540;&#26426;&#21046;&#30456;&#21516;&#30340;&#26041;&#24335;&#23454;&#26045;&#12290;&#26368;&#21518;&#65292;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#37096;&#32626;&#29992;&#20110;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13226v2 Announce Type: replace-cross  Abstract: We introduce an approach to building a custom model from ready-made self-supervised models via their associating instead of training and fine-tuning. We demonstrate it with an example of a humanoid robot looking at the mirror and learning to detect the 3D pose of its own body from the image it perceives. To build our model, we first obtain features from the visual input and the postures of the robot's body via models prepared before the robot's operation. Then, we map their corresponding latent spaces by a sample-efficient robot's self-exploration at the mirror. In this way, the robot builds the solicited 3D pose detector, which quality is immediately perfect on the acquired samples instead of obtaining the quality gradually. The mapping, which employs associating the pairs of feature vectors, is then implemented in the same way as the key-value mechanism of the famous transformer models. Finally, deploying our model for imitat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#26469;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;&#65292;&#22635;&#34917;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#20840;&#38754;&#36861;&#36394;&#12289;&#35782;&#21035;&#21644;&#26144;&#23556;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2311.11157</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#23545;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Contextualizing Internet Memes Across Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#26469;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;&#65292;&#22635;&#34917;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#20840;&#38754;&#36861;&#36394;&#12289;&#35782;&#21035;&#21644;&#26144;&#23556;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#36855;&#22240;&#24050;&#25104;&#20026;&#32593;&#32476;&#19978;&#20132;&#27969;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#26032;&#39062;&#26684;&#24335;&#12290;&#23427;&#20204;&#30340;&#27969;&#21160;&#24615;&#21644;&#21019;&#36896;&#24615;&#20307;&#29616;&#22312;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#36890;&#24120;&#36328;&#24179;&#21488;&#20256;&#25773;&#65292;&#20598;&#23572;&#20063;&#29992;&#20110;&#19981;&#36947;&#24503;&#25110;&#26377;&#23475;&#30340;&#30446;&#30340;&#12290; &#34429;&#28982;&#35745;&#31639;&#24037;&#20316;&#24050;&#32463;&#20998;&#26512;&#20102;&#23427;&#20204;&#38543;&#26102;&#38388;&#30340;&#39640;&#32423;&#21035;&#30149;&#27602;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#29992;&#20110;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#21162;&#21147;&#26088;&#22312;&#20840;&#38754;&#36319;&#36394;&#12289;&#35782;&#21035;&#21644;&#26144;&#23556;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#12290; &#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30693;&#35782;&#24211;&#65292;&#21363;&#30693;&#35782;&#22270;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#36827;&#34892;&#24773;&#22659;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;Reddit&#21644;Discord&#25910;&#38598;&#20102;&#25968;&#21315;&#26465;&#28508;&#22312;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#24086;&#23376;&#65292;&#24182;&#24320;&#21457;&#20102;&#25552;&#21462;-&#36716;&#25442;-&#21152;&#36733;&#36807;&#31243;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#20505;&#36873;&#36855;&#22240;&#24086;&#23376;&#30340;&#25968;&#25454;&#28246;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11157v2 Announce Type: replace-cross  Abstract: Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and develop an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-bas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.10671</link><description>&lt;p&gt;
&#22810;&#27169;&#25311;&#25512;&#29702;&#30340;&#28145;&#24230;&#34701;&#21512;&#65306;&#28145;&#24230;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;(MultiNPE)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#21463;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#23427;&#36171;&#20104;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#24182;&#25512;&#26029;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;MultiNPE&#21046;&#23450;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65288;&#26089;&#26399;&#12289;&#21518;&#26399;&#12289;&#28151;&#21512;&#65289;&#65292;&#24182;&#22312;&#19977;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#39564;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;MultiNPE&#19981;&#20165;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#19968;&#25968;&#25454;&#28304;&#22522;&#32447;&#65292;&#36824;&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#33039;&#30149;&#23398;&#30340;&#31185;&#23398;&#27169;&#22411;&#25512;&#29702;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#25104;&#32489;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#23545;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#21518;&#26399;&#21644;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#25104;&#20026;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;&#23454;&#38469;&#24212;&#29992;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10671v2 Announce Type: replace-cross  Abstract: We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate multimodal fusion approaches for \hbox{MultiNPE} (early, late, hybrid) and evaluate their performance in three challenging experiments. MultiNPE not only outperforms single-source baselines on a reference task, but also achieves superior inference on scientific models from neuroscience and cardiology. We systematically investigate the impact of partially missing data on the different fusion strategies. Across our experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based infere
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#31574;&#30053;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ever&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09114</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09114
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#31574;&#30053;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ever&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29983;&#25104;&#27969;&#30021;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#26500;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#20110;&#38750;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#20013;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#32416;&#27491;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#8220;&#28378;&#38634;&#29699;&#8221;&#38382;&#39064;&#23548;&#33268;&#30340;&#32047;&#31215;&#34394;&#26500;&#38169;&#35823;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ever&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;Ever&#37319;&#29992;&#23454;&#26102;&#12289;&#36880;&#27493;&#30340;&#29983;&#25104;&#21644;&#34394;&#26500;&#32416;&#27491;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31561;&#21040;&#29983;&#25104;&#36807;&#31243;&#32467;&#26463;&#25165;&#32416;&#27491;&#34394;&#26500;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#34394;&#26500;&#12290;&#19982;&#22522;&#20110;&#26816;&#32034;&#21644;&#38750;&#22522;&#20110;&#26816;&#32034;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09114v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.08941</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25551;&#36848;&#36923;&#36753;&#35821;&#22659;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning over Description Logic-based Contexts with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#34913;&#37327;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#21512;&#25104;&#35821;&#22659;&#20013;&#23545;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25110;&#35777;&#26126;&#29983;&#25104;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20351;&#29992;&#30340;&#35821;&#22659;&#38750;&#24120;&#31616;&#21333;&#65307;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#26159;&#30001;&#20165;&#21547;&#26377;&#23569;&#37327;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#37327;&#35789;&#30340;&#30701;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#29983;&#25104;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#22522;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#22312;&#34920;&#36798;&#20016;&#23500;&#35821;&#22659;&#20013;&#25191;&#34892;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#29983;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34920;&#36798;&#24335;&#35821;&#35328;$\mathcal{ALCQ$&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;384K&#20010;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#32500;&#24230;&#19978;&#22686;&#21152;&#65306;i) &#25512;&#29702;&#28145;&#24230;&#65292;&#21644;ii) &#21477;&#23376;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08941v2 Announce Type: replace-cross  Abstract: One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21576;&#38047;&#29366;&#26354;&#32447;&#65292;&#26368;&#39640;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#65292;&#19982;&#32534;&#30721;&#22120;&#20013;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#21516;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#21021;&#26399;&#22686;&#21152;&#65292;&#38543;&#21518;&#22312;&#35757;&#32451;&#26411;&#26399;&#20986;&#29616;&#21387;&#32553;&#65292;&#34920;&#26126;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.05928</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#24418;&#29366;&#65306;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21508;&#21521;&#24322;&#24615;&#21644;&#20869;&#22312;&#32500;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21576;&#38047;&#29366;&#26354;&#32447;&#65292;&#26368;&#39640;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#65292;&#19982;&#32534;&#30721;&#22120;&#20013;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#21516;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#21021;&#26399;&#22686;&#21152;&#65292;&#38543;&#21518;&#22312;&#35757;&#32451;&#26411;&#26399;&#20986;&#29616;&#21387;&#32553;&#65292;&#34920;&#26126;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;Transformer&#26550;&#26500;&#20013;&#23884;&#20837;&#30340;&#21508;&#21521;&#24322;&#24615;&#21160;&#24577;&#21644;&#20869;&#22312;&#32500;&#24230;&#23637;&#24320;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#37197;&#32622;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#38047;&#29366;&#26354;&#32447;&#65292;&#20855;&#26377;&#26368;&#39640;&#30340;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#12290;&#36825;&#31181;&#27169;&#24335;&#19982;&#32534;&#30721;&#22120;&#20013;&#35266;&#23519;&#21040;&#30340;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#22686;&#21152;&#65292;&#34920;&#26126;&#21521;&#26356;&#39640;&#32500;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;&#28982;&#21518;&#22312;&#35757;&#32451;&#26411;&#23614;&#20986;&#29616;&#21521;&#26356;&#20302;&#32500;&#24230;&#30340;&#21387;&#32553;&#38454;&#27573;&#65292;&#26263;&#31034;&#30528;&#23545;&#26356;&#32039;&#20945;&#34920;&#31034;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#29702;&#35299;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23884;&#20837;&#23646;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;transformers&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#21363;&#21487;&#36827;&#34892;&#23545;&#35937;&#21629;&#21517;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2311.04924</link><description>&lt;p&gt;
&#26080;&#35843;&#35856;&#30340;&#22522;&#30784;&#27169;&#22411;&#23545;&#35937;&#21629;&#21517;
&lt;/p&gt;
&lt;p&gt;
Tuning-less Object Naming with a Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04924
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;transformers&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#21363;&#21487;&#36827;&#34892;&#23545;&#35937;&#21629;&#21517;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#26102;&#23545;&#35937;&#21629;&#21517;&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#20174;&#26410;&#35265;&#36807;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#24320;&#22987;&#20043;&#21069;&#25105;&#20204;&#35748;&#20026;&#23427;&#20934;&#22791;&#22909;&#25509;&#21463;&#20219;&#20309;&#20869;&#23481;&#12290;&#23427;&#23558;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#30456;&#23545;&#36739;&#23567;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#19982;&#36880;&#28176;&#26500;&#24314;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#32034;&#24341;&#30456;&#20851;&#32852;&#65292;&#19988;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#20351;&#29992;&#20102;&#26469;&#33258;transformers&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20851;&#32852;&#26426;&#21046;&#12290;&#23427;&#20855;&#26377;&#25903;&#25345;&#20174;&#19981;&#30456;&#20851;&#20449;&#24687;&#20013;&#27867;&#21270;&#20197;&#21306;&#20998;&#23454;&#20307;&#24182;&#28508;&#22312;&#22320;&#33021;&#22815;&#19982;&#36828;&#36229;&#20986;&#35789;&#27719;&#34920;&#32034;&#24341;&#30340;&#23454;&#20307;&#30456;&#20851;&#32852;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#24037;&#20316;&#65292;&#24182;&#27491;&#30830;&#22320;&#20026;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#21629;&#21517;&#30340;&#23545;&#35937;&#21629;&#21517;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#36890;&#36807;&#40657;&#26495;&#26550;&#26500;&#38598;&#25104;&#30340;&#31995;&#32479;&#27169;&#22359;&#30340;&#23454;&#29616;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#20027;&#35201;&#30528;&#30524;&#20110;&#23427;&#33021;&#22815;&#35782;&#21035;&#22810;&#23569;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04924v2 Announce Type: replace-cross  Abstract: We implement a real-time object naming system that enables learning a set of named entities never seen. Our approach employs an existing foundation model that we consider ready to see anything before starting. It turns seen images into relatively small feature vectors that we associate with index to a gradually built vocabulary without any training of fine-tuning of the model. Our contribution is using the association mechanism known from transformers as attention. It has features that support generalization from irrelevant information for distinguishing the entities and potentially enable associating with much more than indices to vocabulary. As a result, the system can work in a one-shot manner and correctly name objects named in different contents. We also outline implementation details of the system modules integrated by a blackboard architecture. Finally, we investigate the system's quality, mainly how many objects it can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;</title><link>https://arxiv.org/abs/2310.13995</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On Bilingual Lexicon Induction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#65288;BLI&#65289;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#30446;&#21069;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#20381;&#36182;&#20110;&#35745;&#31639;&#36328;&#35821;&#35328;&#21333;&#35789;&#34920;&#31034;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#33539;&#24335;&#36716;&#21464;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#26032;&#19968;&#20195;LLMs&#22312;&#21452;&#35821;&#35789;&#27719;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20419;&#20351;&#21644;&#24494;&#35843;&#22810;&#35821;&#35328;LLMs&#65288;mLLMs&#65289;&#20197;&#36827;&#34892;BLI&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#22914;&#20309;&#20197;&#21450;&#22914;&#20309;&#34917;&#20805;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;1&#65289;&#29992;&#20110;&#26080;&#30417;&#30563;BLI&#30340;&#38646;&#27425;&#25552;&#31034;&#21644;2&#65289;&#20351;&#29992;&#19968;&#32452;&#31181;&#23376;&#32763;&#35793;&#23545;&#36827;&#34892;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#22343;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;LLM&#24494;&#35843;&#65292;&#20197;&#21450;3&#65289;&#23545;&#36739;&#23567;LLMs&#36827;&#34892;&#26631;&#20934;BLI&#23548;&#21521;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#19981;&#21516;&#22823;&#23567;&#65288;&#20174;0.3B&#21040;13B&#21442;&#25968;&#65289;&#30340;18&#20010;&#24320;&#28304;&#25991;&#26412;&#23545;&#25991;&#26412;mLLMs&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#28085;&#30422;&#20004;&#20010;&#26631;&#20934;BLI&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2310.06707</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65306;&#21333;&#19968;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#29983;&#25104;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#35299;&#30721;&#26159;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290; &#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#27010;&#29575;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#20294;&#19981;&#33021;&#24635;&#26159;&#25104;&#31435;&#65292;&#29983;&#25104;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#35299;&#30721;&#26469;&#20248;&#21270;&#19968;&#20010;&#20197;&#24230;&#37327;&#25110;&#36136;&#37327;&#35780;&#20272;&#20449;&#21495;&#25903;&#25345;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25552;&#39640;&#65292;&#21363;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#25110;&#36136;&#37327;&#24863;&#30693;&#35299;&#30721;&#12290; &#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#35745;&#31639;&#25928;&#29992;&#20989;&#25968;&#65292;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290; &#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#33258;&#24049;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;NMT&#27169;&#22411;&#26412;&#36523;&#20855;&#22791;&#36136;&#37327;&#24863;&#30693;&#33021;&#21147;&#12290; &#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;MBR&#35299;&#30721;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06707v2 Announce Type: replace-cross  Abstract: Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.02124</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#21327;&#20316;&#26426;&#21046;&#65306;&#31038;&#20250;&#24515;&#29702;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#31038;&#20250;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#20123;NLP&#31995;&#32479;&#33021;&#21542;&#27169;&#20223;&#31867;&#20154;&#31867;&#30340;&#21327;&#20316;&#26234;&#33021;&#65292;&#22312;&#30001;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#22810;&#20195;&#29702;&#31038;&#20250;&#20013;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#23454;&#36341;&#23454;&#39564;&#19982;&#29702;&#35770;&#35266;&#28857;&#30456;&#32467;&#21512;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#30001;LLM&#20195;&#29702;&#32452;&#25104;&#30340;&#29420;&#29305;&#8220;&#31038;&#20250;&#8221;&#65292;&#27599;&#20010;&#20195;&#29702;&#20197;&#29305;&#23450;&#30340;&#8220;&#29305;&#36136;&#8221;&#65288;&#38543;&#21644;&#25110;&#36807;&#20110;&#33258;&#20449;&#65289;&#20026;&#29305;&#24449;&#65292;&#24182;&#19982;&#19981;&#21516;&#30340;&#8220;&#24605;&#32500;&#27169;&#24335;&#8221;&#65288;&#36777;&#35770;&#25110;&#21453;&#24605;&#65289;&#23637;&#24320;&#21327;&#20316;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#22810;&#20195;&#29702;&#31038;&#20250;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#19981;&#20165;&#32988;&#36807;&#20808;&#21069;&#39030;&#23574;&#26041;&#27861;&#65292;&#32780;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;API&#20196;&#29260;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35828;&#26126;LLM&#20195;&#29702;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#25512;&#29702;&#65288;RoG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;KGs&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2310.01061</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#25512;&#29702;&#65306;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#25512;&#29702;&#65288;RoG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;KGs&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23427;&#20204;&#32570;&#20047;&#26368;&#26032;&#30693;&#35782;&#65292;&#32463;&#21382;&#24187;&#35273;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#38477;&#20302;&#20854;&#24615;&#33021;&#21644;&#21487;&#20449;&#24230;&#12290;&#30693;&#35782;&#22270;&#65288;KGs&#65289;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#25429;&#33719;&#20102;&#22823;&#37327;&#20107;&#23454;&#65292;&#20026;&#25512;&#29702;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#30693;&#35782;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;KG&#30340;LLM&#25512;&#29702;&#26041;&#27861;&#21482;&#23558;KGs&#35270;&#20026;&#20107;&#23454;&#30693;&#35782;&#24211;&#65292;&#24573;&#35270;&#20854;&#32467;&#26500;&#20449;&#24687;&#23545;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#25512;&#29702;&#65288;RoG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;LLMs&#19982;KGs&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21010;-&#26816;&#32034;-&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;RoG&#39318;&#20808;&#29983;&#25104;&#30001;KGs&#20316;&#20026;&#24544;&#23454;&#35745;&#21010;&#30340;&#20851;&#31995;&#36335;&#24452;&#12290;&#36825;&#20123;&#35745;&#21010;&#28982;&#21518;&#29992;&#20110;&#26816;&#32034;&#26377;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01061v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#27493;&#39588;&#24182;&#24341;&#20837;&#20803;&#31574;&#30053;&#21644;Plan&#27010;&#24565;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2309.13672</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Image-to-Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#27493;&#39588;&#24182;&#24341;&#20837;&#20803;&#31574;&#30053;&#21644;Plan&#27010;&#24565;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#27425;&#36816;&#34892;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#21333;&#27493;&#27169;&#22411;&#22987;&#32456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#24182;&#23481;&#26131;&#38519;&#20837;&#22351;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#37325;&#26032;&#23450;&#20041;&#20026;&#36880;&#27493;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;RL-I2IT&#65289;&#12290;RL-I2IT&#26694;&#26550;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#23558;&#19968;&#20010;&#21333;&#20307;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#23567;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#36880;&#27493;&#23558;&#28304;&#22270;&#20687;&#36716;&#21270;&#20026;&#30446;&#26631;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#22788;&#29702;&#39640;&#32500;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#31574;&#30053;&#21644;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;Plan&#21040;&#26631;&#20934;&#30340;Actor-Critic&#27169;&#22411;&#20013;&#65292;&#35813;&#27010;&#24565;&#30340;&#32500;&#24230;&#36739;&#21407;&#22987;&#22270;&#20687;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#28436;&#21592;&#29983;&#25104;&#21487;&#22788;&#29702;&#30340;&#39640;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dime
&lt;/p&gt;</description></item><item><title>RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2308.14296</link><description>&lt;p&gt;
RecMind&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25512;&#33616;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RecMind: Large Language Model Powered Agent For Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14296
&lt;/p&gt;
&lt;p&gt;
RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;RS&#26041;&#27861;&#36890;&#24120;&#22312;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26032;&#25512;&#33616;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#27169;&#22411;&#35268;&#27169;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;RecMind&#65292;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21033;&#29992;&#35880;&#24910;&#35268;&#21010;&#30340;&#24037;&#20855;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Self-Inspiring&#31639;&#27861;&#26469;&#25552;&#39640;&#35268;&#21010;&#33021;&#21147;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#27493;&#39588;&#65292;LLM&#33258;&#25105;&#28608;&#21169;&#20197;&#32771;&#34385;&#25152;&#26377;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#29366;&#24577;&#26469;&#35268;&#21010;&#19979;&#19968;&#27493;&#12290;&#36825;&#19968;&#26426;&#21046;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#29702;&#35299;&#21644;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#35268;&#21010;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RecMind&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14296v2 Announce Type: replace-cross  Abstract: While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our exper
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#21319;&#36155;&#36125;&#20859;&#27542;&#20013;&#28014;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#20197;&#24212;&#29992;&#20110;&#26234;&#33021;&#36155;&#36125;&#20859;&#27542;&#30417;&#27979;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2308.09238</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25913;&#36827;&#36155;&#36125;&#20859;&#27542;&#33258;&#21160;&#21270;&#20013;&#30340;&#28014;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Buoy Detection with Deep Transfer Learning for Mussel Farm Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09238
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#21319;&#36155;&#36125;&#20859;&#27542;&#20013;&#28014;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#20197;&#24212;&#29992;&#20110;&#26234;&#33021;&#36155;&#36125;&#20859;&#27542;&#30417;&#27979;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#35199;&#20848;&#27700;&#20135;&#20859;&#27542;&#37096;&#38376;&#27491;&#22312;&#36805;&#36895;&#25193;&#24352;&#65292;&#29305;&#21035;&#24378;&#35843;&#36155;&#36125;&#20986;&#21475;&#12290;&#38543;&#30528;&#36155;&#36125;&#20859;&#27542;&#36816;&#33829;&#38656;&#27714;&#19981;&#26029;&#21457;&#23637;&#65292;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65288;&#22914;&#26234;&#33021;&#30446;&#26631;&#26816;&#27979;&#65289;&#25104;&#20026;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;&#28014;&#26631;&#26816;&#27979;&#65292;&#29992;&#20110;&#26234;&#33021;&#36155;&#36125;&#20859;&#27542;&#30417;&#27979;&#21644;&#31649;&#29702;&#12290;&#20027;&#35201;&#30446;&#26631;&#22312;&#20110;&#25552;&#39640;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#19979;&#28014;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#21033;&#29992;&#20174;&#36155;&#36125;&#20859;&#27542;&#22330;&#33719;&#21462;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#26631;&#35760;&#65292;&#21253;&#25324;&#20174;&#28014;&#21160;&#24179;&#21488;&#21644;&#24033;&#33322;&#33337;&#19978;&#23433;&#35013;&#30340;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#25429;&#25417;&#21508;&#31181;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#12290;&#20026;&#24314;&#31435;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28014;&#26631;&#26816;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09238v2 Announce Type: replace-cross  Abstract: The aquaculture sector in New Zealand is experiencing rapid expansion, with a particular emphasis on mussel exports. As the demands of mussel farming operations continue to evolve, the integration of artificial intelligence and computer vision techniques, such as intelligent object detection, is emerging as an effective approach to enhance operational efficiency. This study delves into advancing buoy detection by leveraging deep learning methodologies for intelligent mussel farm monitoring and management. The primary objective centers on improving accuracy and robustness in detecting buoys across a spectrum of real-world scenarios. A diverse dataset sourced from mussel farms is captured and labeled for training, encompassing imagery taken from cameras mounted on both floating platforms and traversing vessels, capturing various lighting and weather conditions. To establish an effective deep learning model for buoy detection with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2307.00897</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fixing confirmation bias in feature attribution methods via semantic match
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26512;&#40657;&#30418;&#27169;&#22411;&#22797;&#26434;&#34892;&#20026;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20123;&#23398;&#32773;&#25351;&#20986;&#36825;&#31867;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65306;&#23427;&#20204;&#19981;&#33021;&#21487;&#38752;&#22320;&#29992;&#20154;&#31867;&#27010;&#24565;&#36827;&#34892;&#35299;&#37322;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#20165;&#21487;&#35270;&#21270;&#19968;&#31995;&#21015;&#29305;&#24449;&#36129;&#29486;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26080;&#27861;&#24471;&#20986;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34920;&#31034;&#30340;&#32467;&#35770;&#65292;&#32780;&#30830;&#35748;&#20559;&#35265;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#20135;&#29983;&#20851;&#20110;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#20449;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#39564;&#35777;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#20551;&#35774;&#26159;&#21542;&#24471;&#21040;&#20102;&#29305;&#24449;&#24402;&#22240;&#30340;&#30830;&#35748;&#12290;&#36825;&#23601;&#26159;&#25105;&#20204;&#25152;&#35828;&#30340;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#8220;&#35821;&#20041;&#21305;&#37197;&#8221;&#12290;&#22312; Cin\`a&#31561;&#20154;[2023]&#25552;&#20986;&#30340;&#27010;&#24565;&#26694;&#26550;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#22312;&#23454;&#36341;&#20013;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00897v2 Announce Type: replace-cross  Abstract: Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spa
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.06794</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#65306;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
A blind spot for large language models: Supradiegetic linguistic information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06794
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#28145;&#21051;&#21464;&#38761;&#65292;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#29978;&#33267;&#20196;&#20154;&#38663;&#24778;&#30340;&#31867;&#20154;&#35821;&#35328;&#27969;&#21033;&#24230;&#12290;&#23427;&#20204;&#30446;&#21069;&#21644;&#28508;&#22312;&#30340;&#33021;&#21147;&#33539;&#22260;&#26159;&#19968;&#20010;&#31215;&#26497;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#32477;&#38750;&#20165;&#38480;&#20110;&#31185;&#30740;&#20154;&#21592;&#12290;&#20154;&#20204;&#36890;&#24120;&#23558;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#26694;&#23450;&#20026;&#8220;&#25991;&#26412;&#8221;&#29978;&#33267;&#8220;&#35821;&#35328;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#35821;&#35328;&#23398;&#12289;&#20307;&#29616;&#35748;&#30693;&#12289;&#35748;&#30693;&#31185;&#23398;&#12289;&#25968;&#23398;&#21644;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#20180;&#32454;&#23457;&#35270;&#36825;&#19968;&#26694;&#26550;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#32771;&#34385;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#26159;&#20160;&#20040;&#24863;&#35273;&#65292;&#27491;&#22914;&#32435;&#26684;&#23572;&#21487;&#33021;&#20250;&#35828;&#30340;&#37027;&#26679;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20854;&#25972;&#20307;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#65292;&#20854;&#25509;&#21463;&#30340;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#34987;&#26377;&#30410;&#22320;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#35821;&#35328;&#20013;&#32534;&#30721;&#30340;&#21465;&#20107;&#20449;&#24687;&#30340;&#25509;&#35302;&#65292;&#20854;&#32570;&#38519;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#36825;&#20123;&#20449;&#24687;&#30340;&#26080;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as "text" or even "language". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20132;&#20114;&#26041;&#24335;&#35775;&#38382;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26465;&#20214;&#20998;&#24067;&#26679;&#26412;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;HMM&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.14753</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#26679;&#26412;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hidden Markov Models Using Conditional Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.14753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20132;&#20114;&#26041;&#24335;&#35775;&#38382;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26465;&#20214;&#20998;&#24067;&#26679;&#26412;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;HMM&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;HMM&#26159;&#39034;&#24207;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#20294;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#65292;&#21363;&#23545;&#35266;&#27979;&#24207;&#21015;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#26679;&#26412;&#20855;&#26377;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36215;&#26469;&#26159;&#20855;&#26377;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#30340;&#12290;&#26412;&#25991;&#20559;&#31163;&#20102;&#36825;&#19968;&#35774;&#23450;&#65292;&#32771;&#34385;&#20102;&#19968;&#31181;&#20132;&#20114;&#35775;&#38382;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#31639;&#27861;&#21487;&#20197;&#26597;&#35810;HMM&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;HMM&#30340;&#20132;&#20114;&#35775;&#38382;&#21487;&#20197;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#23398;&#20064;HMM&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#65288;a&#65289;&#19968;&#31181;&#26356;&#23481;&#26131;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#21487;&#20197;&#26597;&#35810;&#20934;&#30830;&#26465;&#20214;&#27010;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#27425;&#26597;&#35810;&#65292;&#20197;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#20013;&#36817;&#20284;&#20219;&#20309;HMM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.14753v2 Announce Type: replace-cross  Abstract: This paper is concerned with the computational complexity of learning the Hidden Markov Model (HMM). Although HMMs are some of the most widely used tools in sequential and time series modeling, they are cryptographically hard to learn in the standard setting where one has access to i.i.d. samples of observation sequences. In this paper, we depart from this setup and consider an interactive access model, in which the algorithm can query for samples from the conditional distributions of the HMMs. We show that interactive access to the HMM enables computationally efficient learning algorithms, thereby bypassing cryptographic hardness. Specifically, we obtain efficient algorithms for learning HMMs in two settings:   (a) An easier setting where we have query access to the exact conditional probabilities. Here our algorithm runs in polynomial time and makes polynomially many queries to approximate any HMM in total variation distance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30450;&#30446;&#36229;&#20998;&#36776;&#29575;&#21644;&#35010;&#32541;&#20998;&#21106;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25903;&#25345;&#19979;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#20248;&#21270;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#26410;&#30693;&#27169;&#31946;&#23545;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2302.12491</link><description>&lt;p&gt;
&#30450;&#30446;&#36229;&#20998;&#36776;&#29575;&#21644;&#35010;&#32541;&#20998;&#21106;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#29992;&#20110;&#36924;&#30495;&#27531;&#32570;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Blind Super-Resolution and Crack Segmentation for Realistic Degraded Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.12491
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30450;&#30446;&#36229;&#20998;&#36776;&#29575;&#21644;&#35010;&#32541;&#20998;&#21106;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25903;&#25345;&#19979;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#20248;&#21270;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#26410;&#30693;&#27169;&#31946;&#23545;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#30340;&#35010;&#32541;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#36827;&#34892;&#22686;&#24378;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;SR&#32593;&#32476;&#19982;&#20108;&#20540;&#20998;&#21106;&#32593;&#32476;&#32852;&#21512;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#36825;&#31181;&#32852;&#21512;&#23398;&#20064;&#20351;&#24471;SR&#32593;&#32476;&#33021;&#22815;&#34987;&#20248;&#21270;&#20197;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#65292;SR&#32593;&#32476;&#20174;&#38750;&#30450;&#30446;&#25193;&#23637;&#21040;&#30450;&#30446;&#22788;&#29702;&#30001;&#26410;&#30693;&#27169;&#31946;&#24341;&#36215;&#30340;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#20010;&#39069;&#22806;&#36335;&#24452;&#36827;&#19968;&#27493;&#40723;&#21169;SR&#21644;&#20998;&#21106;&#20043;&#38388;&#30340;&#30456;&#20114;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#32852;&#21512;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#65288;SoTA&#65289;&#20998;&#21106;&#26041;&#27861;&#30340;&#27604;&#36739;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#32852;&#21512;&#23398;&#20064;&#30340;&#20248;&#36234;&#24615;&#65292;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#36129;&#29486;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.12491v3 Announce Type: replace-cross  Abstract: This paper proposes crack segmentation augmented by super resolution (SR) with deep neural networks. In the proposed method, a SR network is jointly trained with a binary segmentation network in an end-to-end manner. This joint learning allows the SR network to be optimized for improving segmentation results. For realistic scenarios, the SR network is extended from non-blind to blind for processing a low-resolution image degraded by unknown blurs. The joint network is improved by our proposed two extra paths that further encourage the mutual optimization between SR and segmentation. Comparative experiments with State of The Art (SoTA) segmentation methods demonstrate the superiority of our joint learning, and various ablation studies prove the effects of our contributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2301.12334</link><description>&lt;p&gt;
&#19981;&#20559;&#19981;&#20506;&#65306;&#23569;&#25968;&#26063;&#32676;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Don't Play Favorites: Minority Guidance for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23569;&#25968;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#23569;&#25968;&#26679;&#26412;&#26159;&#20301;&#20110;&#25968;&#25454;&#27969;&#24418;&#20302;&#23494;&#24230;&#21306;&#22495;&#30340;&#23454;&#20363;&#12290;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#36825;&#31181;&#23569;&#25968;&#26679;&#26412;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#25968;&#25454;&#30340;&#19968;&#20123;&#29420;&#29305;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#20284;&#28982;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#32479;&#29983;&#25104;&#36807;&#31243;&#20027;&#35201;&#20135;&#29983;&#22823;&#22810;&#25968;&#26679;&#26412;&#65288;&#20301;&#20110;&#27969;&#24418;&#39640;&#23494;&#24230;&#21306;&#22495;&#65289;&#65292;&#20351;&#33258;&#36523;&#23545;&#23569;&#25968;&#29983;&#25104;&#20219;&#21153;&#26080;&#25928;&#19988;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#12290;&#39318;&#20808;&#24378;&#35843; Tweedie &#30340;&#38477;&#22122;&#20844;&#24335;&#23545;&#22823;&#22810;&#25968;&#26679;&#26412;&#20135;&#29983;&#26377;&#21033;&#32467;&#26524;&#12290;&#36825;&#19968;&#35266;&#23519;&#28608;&#21169;&#25105;&#20204;&#24341;&#20837;&#25551;&#36848;&#32473;&#23450;&#26679;&#26412;&#29420;&#29305;&#24615;&#30340;&#24230;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#22909;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12334v2 Announce Type: replace-cross  Abstract: We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves ineffective and time-consuming for the minority generating task. In this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. We first highlight that Tweedie's denoising formula yields favorable results for majority samples. The observation motivates us to introduce a metric that describes the uniqueness of a given sample. To address the inherent preference of the di
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27665;&#26435;&#31435;&#27861;&#30340;AI&#30417;&#31649;&#38381;&#29615;&#35270;&#35282;&#65292;&#24378;&#35843;&#22312;&#37325;&#22797;&#20114;&#21160;&#20013;&#20135;&#29983;&#30340;&#22343;&#31561;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2209.01410</link><description>&lt;p&gt;
AI&#30417;&#31649;&#30340;&#38381;&#29615;&#35270;&#35282;&#65306;&#22312;&#37325;&#22797;&#20114;&#21160;&#20013;&#20135;&#29983;&#22343;&#31561;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Closed-Loop View of the Regulation of AI: Equal Impact across Repeated Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01410
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27665;&#26435;&#31435;&#27861;&#30340;AI&#30417;&#31649;&#38381;&#29615;&#35270;&#35282;&#65292;&#24378;&#35843;&#22312;&#37325;&#22797;&#20114;&#21160;&#20013;&#20135;&#29983;&#30340;&#22343;&#31561;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#23545;AI&#30417;&#31649;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#20027;&#24352;&#22522;&#20110;&#27665;&#26435;&#31435;&#27861;&#30340;&#35270;&#35282;&#65292;&#24314;&#31435;&#22312;&#24179;&#31561;&#23545;&#24453;&#21644;&#24179;&#31561;&#24433;&#21709;&#30340;&#27010;&#24565;&#20043;&#19978;&#12290;&#22312;AI&#31995;&#32479;&#21450;&#20854;&#29992;&#25143;&#30340;&#38381;&#29615;&#35270;&#35282;&#20013;&#65292;&#24179;&#31561;&#23545;&#24453;&#20851;&#27880;&#19968;&#27425;&#24490;&#29615;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#24179;&#31561;&#24433;&#21709;&#20851;&#27880;&#22312;&#37325;&#22797;&#20114;&#21160;&#20013;&#30340;&#38271;&#26399;&#24179;&#22343;&#34892;&#20026;&#12290;&#20026;&#20102;&#30830;&#23450;&#24179;&#22343;&#20540;&#30340;&#23384;&#22312;&#21450;&#20854;&#23646;&#24615;&#65292;&#38656;&#35201;&#30740;&#31350;&#38381;&#29615;&#30340;&#36941;&#21382;&#24615;&#36136;&#21450;&#20854;&#29420;&#29305;&#30340;&#38745;&#27490;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01410v2 Announce Type: replace  Abstract: There has been much recent interest in the regulation of AI. We argue for a view based on civil-rights legislation, built on the notions of equal treatment and equal impact. In a closed-loop view of the AI system and its users, the equal treatment concerns one pass through the loop. Equal impact, in our view, concerns the long-run average behaviour across repeated interactions. In order to establish the existence of the average and its properties, one needs to study the ergodic properties of the closed-loop and its unique stationary measure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#29109;&#22810;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#30340;&#20449;&#24687;&#35770;&#31561;&#25928;&#24615;&#65292;&#23558;&#20854;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#39046;&#22495;&#65292;&#35777;&#26126;&#20102;&#29109;&#26368;&#20248;&#36755;&#36816;&#22312;&#20449;&#24687;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#31995;&#32479;&#20013;&#30340;OT&#29702;&#35770;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2208.10256</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#19978;&#30340;&#29109;&#22810;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#31561;&#20215;&#24615;&#65306;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal Transport: A Theory for Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#29109;&#22810;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#30340;&#20449;&#24687;&#35770;&#31561;&#25928;&#24615;&#65292;&#23558;&#20854;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#39046;&#22495;&#65292;&#35777;&#26126;&#20102;&#29109;&#26368;&#20248;&#36755;&#36816;&#22312;&#20449;&#24687;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#31995;&#32479;&#20013;&#30340;OT&#29702;&#35770;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#20851;&#20110;&#29109;&#22810;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#65288;MOT&#65289;&#30340;&#20449;&#24687;&#35770;&#31561;&#25928;&#24615;&#12290;&#35813;&#31561;&#25928;&#24615;&#21487;&#20197;&#36731;&#26494;&#24402;&#32422;&#21040;&#29109;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#24773;&#20917;&#12290;&#30001;&#20110;OT&#34987;&#24191;&#27867;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25110;&#20449;&#24565;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#20449;&#24565;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#27491;&#24335;&#35777;&#26126;&#20102;&#29579;&#31561;&#20154;&#22312;[2020]&#20013;&#25552;&#20986;&#30340;&#20851;&#20110;&#29109;OT&#22312;&#20449;&#24687;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#31995;&#32479;&#20013;&#30340;OT&#29702;&#35770;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.10256v3 Announce Type: replace-cross  Abstract: In this paper, we propose our information-theoretic equivalence of entropic multi-marginal optimal transport (MOT). This equivalence can be easily reduced to the case of entropic optimal transport (OT). Because OT is widely used to compare differences between knowledge or beliefs, we apply this result to the communication between agents with different beliefs. Our results formally prove the statement that entropic OT is information-theoretically optimal given by Wang et al. [2020] and generalize it to the multi-agent case. We believe that our work can shed light on OT theory in future multi-agent teaming systems.
&lt;/p&gt;</description></item><item><title>&#34892;&#20026;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26410;&#26469;&#34892;&#21160;&#21644;&#29366;&#24577;&#36335;&#24452;&#30340;&#21344;&#29992;&#65292;&#26681;&#25454;&#26368;&#22823;&#21344;&#29992;&#21407;&#21017;&#65292;&#22870;&#21169;&#26159;&#21344;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#25163;&#27573;&#65292;&#32780;&#19981;&#26159;&#30446;&#26631;&#26412;&#36523;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#26368;&#20248;&#31574;&#30053;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#30456;&#20851;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#35777;&#26126;&#20102;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;</title><link>https://arxiv.org/abs/2205.10316</link><description>&lt;p&gt;
&#20174;&#20869;&#22312;&#21160;&#26426;&#21040;&#21344;&#25454;&#34892;&#21160;-&#29366;&#24577;&#36335;&#24452;&#31354;&#38388;&#30340;&#22797;&#26434;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Complex behavior from intrinsic motivation to occupy action-state path space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10316
&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26410;&#26469;&#34892;&#21160;&#21644;&#29366;&#24577;&#36335;&#24452;&#30340;&#21344;&#29992;&#65292;&#26681;&#25454;&#26368;&#22823;&#21344;&#29992;&#21407;&#21017;&#65292;&#22870;&#21169;&#26159;&#21344;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#25163;&#27573;&#65292;&#32780;&#19981;&#26159;&#30446;&#26631;&#26412;&#36523;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#26368;&#20248;&#31574;&#30053;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#30456;&#20851;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#35777;&#26126;&#20102;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#34892;&#20026;&#29702;&#35770;&#35748;&#20026;&#65292;&#20195;&#29702;&#20154;&#20542;&#21521;&#20110;&#26368;&#22823;&#21270;&#26576;&#31181;&#24418;&#24335;&#30340;&#22870;&#21169;&#25110;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#21160;&#29289;&#32463;&#24120;&#20986;&#20110;&#22909;&#22855;&#24515;&#31227;&#21160;&#65292;&#24182;&#19988;&#20284;&#20046;&#22312;&#27809;&#26377;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#28608;&#21169;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#22870;&#21169;&#26368;&#22823;&#21270;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#34892;&#20026;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26410;&#26469;&#34892;&#21160;&#21644;&#29366;&#24577;&#36335;&#24452;&#30340;&#21344;&#29992;&#12290;&#26681;&#25454;&#26368;&#22823;&#21344;&#29992;&#21407;&#21017;&#65292;&#22870;&#21169;&#26159;&#21344;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#25163;&#27573;&#65292;&#32780;&#19981;&#26159;&#30446;&#26631;&#26412;&#36523;&#65307;&#30446;&#26631;&#23548;&#21521;&#24615;&#31616;&#21333;&#22320;&#20316;&#20026;&#25628;&#32034;&#36164;&#28304;&#30340;&#29702;&#24615;&#26041;&#24335;&#32780;&#20986;&#29616;&#65292;&#20197;&#20351;&#36816;&#21160;&#20174;&#24191;&#20041;&#19978;&#29702;&#35299;&#27704;&#19981;&#32467;&#26463;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34892;&#21160;-&#29366;&#24577;&#36335;&#24452;&#29109;&#26159;&#21807;&#19968;&#19982;&#21487;&#21152;&#24615;&#21644;&#20854;&#20182;&#30452;&#35266;&#30340;&#39044;&#26399;&#26410;&#26469;&#34892;&#21160;-&#29366;&#24577;&#36335;&#24452;&#21344;&#29992;&#24615;&#36136;&#19968;&#33268;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20248;&#31574;&#30053;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#20219;&#21153;&#65292;&#21253;&#25324;&#19968;&#20010;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10316v2 Announce Type: replace  Abstract: Most theories of behavior posit that agents tend to maximize some form of reward or utility. However, animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we abandon the idea of reward maximization, and propose that the goal of behavior is maximizing occupancy of future paths of actions and states. According to this maximum occupancy principle, rewards are the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways of searching for resources so that movement, understood amply, never ends. We find that action-state path entropy is the only measure consistent with additivity and other intuitive properties of expected future action-state path occupancy. We provide analytical expressions that relate the optimal policy and state-value function, and prove convergence of our value iteration algorithm. Using discrete and continuous state tasks, including a hi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#36716;&#31227;&#31995;&#32479;&#65292;&#22312;&#26080;&#38656;&#26426;&#22120;&#20154;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21147;&#21453;&#39304;&#25805;&#20316;&#20219;&#21153;&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#25805;&#20316;&#32773;&#36890;&#36807;&#25511;&#21046;&#22120;&#36827;&#34892;&#30452;&#25509;&#28436;&#31034;&#24182;&#24863;&#21463;&#21147;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2202.09574</link><description>&lt;p&gt;
&#26080;&#38656;&#26426;&#22120;&#20154;&#30340;&#26426;&#22120;&#20154;&#35757;&#32451;&#65306;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#20027;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Training Robots without Robots: Deep Imitation Learning for Master-to-Robot Policy Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.09574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#36716;&#31227;&#31995;&#32479;&#65292;&#22312;&#26080;&#38656;&#26426;&#22120;&#20154;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21147;&#21453;&#39304;&#25805;&#20316;&#20219;&#21153;&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#25805;&#20316;&#32773;&#36890;&#36807;&#25511;&#21046;&#22120;&#36827;&#34892;&#30452;&#25509;&#28436;&#31034;&#24182;&#24863;&#21463;&#21147;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#23545;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#26159;&#24456;&#26377;&#21069;&#26223;&#30340;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#28436;&#31034;&#26679;&#26412;&#12290;&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#24212;&#29992;&#20110;&#38656;&#35201;&#21147;&#21453;&#39304;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28436;&#31034;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65307;&#21452;&#21521;&#36828;&#31243;&#25805;&#20316;&#38656;&#35201;&#22797;&#26434;&#30340;&#25511;&#21046;&#26041;&#26696;&#24182;&#19988;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#21160;&#24577;&#31034;&#33539;&#21487;&#33021;&#20250;&#22240;&#20026;&#20154;&#31867;&#20171;&#20837;&#32780;&#21463;&#21040;&#35270;&#35273;&#24178;&#25200;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21040;&#26426;&#22120;&#20154;(M2R)&#31574;&#30053;&#36716;&#31227;&#31995;&#32479;&#65292;&#19981;&#38656;&#35201;&#26426;&#22120;&#20154;&#26469;&#25945;&#25480;&#22522;&#20110;&#21147;&#21453;&#39304;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#20154;&#30452;&#25509;&#20351;&#29992;&#25511;&#21046;&#22120;&#28436;&#31034;&#19968;&#20010;&#20219;&#21153;&#12290;&#36825;&#20010;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#26426;&#22120;&#20154;&#33218;&#30340;&#36816;&#21160;&#21442;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#21516;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#21644;&#21147;/&#25197;&#30697;(F/T)&#20256;&#24863;&#22120;&#26469;&#27979;&#37327;&#21147;&#21453;&#39304;&#12290;&#20351;&#29992;&#36825;&#20010;&#25511;&#21046;&#22120;&#65292;&#25805;&#20316;&#32773;&#21487;&#20197;&#24863;&#21463;&#21040;&#21147;&#21453;&#39304;&#32780;&#26080;&#38656;&#21452;&#21521;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#20027;&#20307;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.09574v2 Announce Type: replace-cross  Abstract: Deep imitation learning is promising for robot manipulation because it only requires demonstration samples. In this study, deep imitation learning is applied to tasks that require force feedback. However, existing demonstration methods have deficiencies; bilateral teleoperation requires a complex control scheme and is expensive, and kinesthetic teaching suffers from visual distractions from human intervention. This research proposes a new master-to-robot (M2R) policy transfer system that does not require robots for teaching force feedback-based manipulation tasks. The human directly demonstrates a task using a controller. This controller resembles the kinematic parameters of the robot arm and uses the same end-effector with force/torque (F/T) sensors to measure the force feedback. Using this controller, the operator can feel force feedback without a bilateral system. The proposed method can overcome domain gaps between the mast
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#32467;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2108.00385</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformer-based deep imitation learning for dual-arm robot manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.00385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#32467;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#23545;&#35299;&#20915;&#29087;&#32451;&#25805;&#20316;&#20219;&#21153;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#29615;&#22659;&#27169;&#22411;&#21644;&#39044;&#32534;&#31243;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#21452;&#33218;&#25805;&#20316;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#38468;&#21152;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#24341;&#36215;&#30340;&#29366;&#24577;&#32500;&#24230;&#22686;&#21152;&#65292;&#23548;&#33268;&#20102;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#35745;&#31639;&#39034;&#24207;&#36755;&#20837;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19987;&#27880;&#20110;&#37325;&#35201;&#20803;&#32032;&#12290;Transformer&#65292;&#20316;&#20026;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#34987;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#20197;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#36827;&#34892;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.00385v2 Announce Type: replace-cross  Abstract: Deep imitation learning is promising for solving dexterous manipulation tasks because it does not require an environment model and pre-programmed robot behavior. However, its application to dual-arm manipulation tasks remains challenging. In a dual-arm manipulation setup, the increased number of state dimensions caused by the additional robot manipulators causes distractions and results in poor performance of the neural networks. We address this issue using a self-attention mechanism that computes dependencies between elements in a sequential input and focuses on important elements. A Transformer, a variant of self-attention architecture, is applied to deep imitation learning to solve dual-arm manipulation tasks in the real world. The proposed method has been tested on dual-arm manipulation tasks using a real robot. The experimental results demonstrated that the Transformer-based deep imitation learning architecture can attend 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2102.01295</link><description>&lt;p&gt;
&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Gaze-based dual resolution deep imitation learning for high-precision dexterous robot manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.01295
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#39640;&#31934;&#24230;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#31359;&#38024;&#24341;&#32447;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#29702;&#23398;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#20302;&#20998;&#36776;&#29575;&#22806;&#22260;&#35270;&#35273;&#21644;&#24555;&#36895;&#31227;&#21160;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#25163;&#20256;&#36865;&#21040;&#23545;&#35937;&#30340;&#38468;&#36817;&#65292;&#24182;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20985;&#38519;&#35270;&#35273;&#26469;&#23454;&#29616;&#25163;&#31934;&#30830;&#23545;&#20934;&#23545;&#35937;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21463;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#31359;&#38024;&#24341;&#32447;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#36828;&#31243;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#20957;&#35270;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#22312;&#38752;&#36817;&#30446;&#26631;&#26102;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#22260;&#32469;&#20957;&#35270;&#28857;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26469;&#31934;&#30830;&#25511;&#21046;&#32447;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#30340;&#22806;&#22260;&#22270;&#20687;&#21040;&#36798;&#30446;&#26631;&#38468;&#36817;&#12290;&#26412;&#30740;&#31350;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.01295v3 Announce Type: replace-cross  Abstract: A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2401.15006</link><description>&lt;p&gt;
Airavata: &#24341;&#20837;&#38024;&#23545;&#21360;&#22320;&#35821;&#25351;&#20196;&#35843;&#25972;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15006
&lt;/p&gt;
&lt;p&gt;
"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23459;&#24067;&#39318;&#27425;&#21457;&#24067;&#20102;"Airavata"&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#12290;&#36890;&#36807;&#23558;OpenHathi&#19982;&#21508;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#21360;&#22320;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;Airavata&#26356;&#36866;&#21512;&#36741;&#21161;&#20219;&#21153;&#12290;&#38500;&#20102;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;Indic LLM&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20934;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLM&#22312;&#21360;&#22320;&#35821;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;Airavata&#25903;&#25345;&#21360;&#22320;&#35821;&#65292;&#20294;&#25105;&#20204;&#35745;&#21010;&#23558;&#20854;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;&#24744;&#21487;&#20197;&#22312;https://ai4bharat.github.io/airavata&#19978;&#35775;&#38382;&#25152;&#26377;&#24037;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08664</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25945;&#32946;&#65306;&#22522;&#26412;&#33021;&#21147;&#12289;&#28508;&#21147;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#21033;&#29992;&#20114;&#32852;&#32593;&#20998;&#21457;&#25945;&#32946;&#36164;&#28304;&#65292;&#26088;&#22312;&#25552;&#20379;&#20415;&#25463;&#30340;&#25945;&#32946;&#65292;&#20294;&#24448;&#24448;&#22312;&#19982;&#23398;&#29983;&#30340;&#23454;&#26102;&#20132;&#27969;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#21270;&#38556;&#30861;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#36164;&#28304;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#35831;&#27714;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#31995;&#32479;&#30340;&#26500;&#24314;&#20173;&#28982;&#38754;&#20020;&#30528;&#24191;&#27867;&#30340;&#25945;&#32946;&#25216;&#33021;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#25945;&#32946;&#33021;&#21147;&#30456;&#20851;&#30340;&#36817;&#26399;&#20986;&#29616;&#30340;LLM&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#23427;&#20204;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
&lt;/p&gt;</description></item><item><title>QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06137</link><description>&lt;p&gt;
QuasiNet: &#19968;&#31181;&#20855;&#26377;&#21487;&#35757;&#32451;&#20056;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06137
&lt;/p&gt;
&lt;p&gt;
QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#20284;XOR&#25110;&#22855;&#20598;&#26657;&#39564;&#31561;&#38590;&#39064;&#30340;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#25104;&#21151;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21463;&#29616;&#26377;&#20855;&#26377;&#25152;&#35859;&#20056;&#31215;&#31070;&#32463;&#20803;&#21644;&#30001;&#32463;&#20856;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#25512;&#23548;&#20986;&#30340;&#23398;&#20064;&#35268;&#21017;&#21551;&#21457;&#65292;&#20248;&#38597;&#22320;&#35299;&#20915;&#20102;&#20114;&#26021;&#24773;&#20917;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#20855;&#26377;&#39044;&#35774;&#19988;&#19981;&#21487;&#35843;&#33410;&#26435;&#37325;&#30340;&#20056;&#31215;&#31070;&#32463;&#20803;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#20803;&#20056;&#31215;&#23618;&#20063;&#33021;&#22815;&#23398;&#20064;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#25104;&#21151;&#29575;&#19982;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#22312;&#21069;&#36848;&#38382;&#39064;&#21644;&#20854;&#20182;&#38590;&#39064;&#65288;&#22914;&#20004;&#20010;&#34746;&#26059;&#65289;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#26356;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05273</link><description>&lt;p&gt;
INACIA&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#65288;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25351;&#20196;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24052;&#35199;&#32852;&#37030;&#23457;&#35745;&#27861;&#38498;&#65288;TCU&#65289;&#30340;&#36816;&#33829;&#26694;&#26550;&#20013;&#12290;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#20102;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#22522;&#26412;&#20449;&#24687;&#25552;&#21462;&#12289;&#21487;&#21463;&#29702;&#24615;&#23457;&#26597;&#12289;Periculum in mora&#21644;Fumus boni iuris&#20998;&#26512;&#20197;&#21450;&#24314;&#35758;&#29983;&#25104;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;INACIA&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12289;&#35780;&#20272;&#20854;&#21512;&#27861;&#24615;&#24182;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#21644;LLMs&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;INACIA&#22788;&#29702;&#22797;&#26434;&#27861;&#24459;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#36866;&#29992;&#20110;&#22686;&#21152;&#27861;&#24459;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21496;&#27861;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05200</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65306;&#29992;&#25143;&#35780;&#20272;&#21644;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05200
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#31649;&#29702;&#30693;&#35782;&#23545;&#32452;&#32455;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#25805;&#20316;&#24037;&#21378;&#21464;&#24471;&#36234;&#26469;&#36234;&#20381;&#36182;&#30693;&#35782;&#65292;&#36825;&#32473;&#24037;&#21378;&#22521;&#35757;&#21644;&#25903;&#25345;&#26032;&#25805;&#20316;&#21592;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#21387;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#24037;&#21378;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#24191;&#27867;&#30693;&#35782;&#65292;&#39640;&#25928;&#22238;&#31572;&#25805;&#20316;&#21592;&#30340;&#26597;&#35810;&#24182;&#20419;&#36827;&#26032;&#30693;&#35782;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24037;&#21378;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#30340;&#22909;&#22788;&#65292;&#21363;&#33021;&#22815;&#26356;&#24555;&#22320;&#26816;&#32034;&#20449;&#24687;&#21644;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#26356;&#20542;&#21521;&#20110;&#21521;&#20154;&#24037;&#19987;&#23478;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#20960;&#31181;&#38381;&#28304;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;GPT-4&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20687;StableBe
&lt;/p&gt;
&lt;p&gt;
Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26368;&#20248;&#38142;&#36335;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02873</link><description>&lt;p&gt;
&#36710;&#36742;&#35745;&#21010;&#19982;&#26102;&#38388;&#31383;&#21475;&#30340;&#26368;&#20248;&#38142;&#36335;
&lt;/p&gt;
&lt;p&gt;
Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26368;&#20248;&#38142;&#36335;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#23558;&#36710;&#36742;&#35745;&#21010;&#36830;&#25509;&#25104;&#36328;&#36234;&#26356;&#38271;&#26102;&#38388;&#21306;&#38388;&#30340;&#24207;&#21015;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#38656;&#35201;&#25191;&#34892;&#35745;&#21010;&#38142;&#36335;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#38431;&#35268;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#19981;&#32771;&#34385;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#36825;&#26159;&#25152;&#26377;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30456;&#21453;&#65292;&#35745;&#21010;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#65292;&#19981;&#33021;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#24314;&#27169;&#65292;&#32771;&#34385;&#20102;&#24310;&#36831;&#21644;&#32473;&#23450;&#26102;&#38388;&#31383;&#21475;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#23545;&#20854;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21015;&#20030;&#20102;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#23545;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#28436;&#31034;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#37327;&#23454;&#20363;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
For solving problems from the domain of vehicle routing with time windows, we often need to connect vehicle plans into sequences spanning a longer time horizon or, in other words, we need to perform a plan chaining. Recently, a network-based solution has been proposed to solve the fleet-sizing problem. The method, however, does not consider the time flexibility of the plans, an essential property of all vehicle routing problems with time windows. Instead, plans have fixed times and cannot be delayed. This work presents a new problem formulation that considers delays in line with the given time windows and a method that can be used to solve it. Moreover, we prove that the method is optimal, and we analyze its complexity. Finally, we list some practical applications and perform a demonstration for one of them: the method for solving the static Dial-a-ride problem. The demonstration results show that for a significant number of instances, the proposed method provides a better solution tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13541</link><description>&lt;p&gt;
&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#19982;&#26080;&#20559;&#38598;&#20013;&#21147;
&lt;/p&gt;
&lt;p&gt;
Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#20851;&#31995;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#22788;&#29702;&#38271;&#25991;&#26723;&#25110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#65292;&#36825;&#19968;&#38480;&#21046;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#65292;&#23545;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#36825;&#20123;&#25968;&#37327;&#30340;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38468;&#22312;&#34917;&#20805;&#26448;&#26009;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20141</link><description>&lt;p&gt;
&#23545;&#27604;&#24046;&#24322;&#24615;&#39044;&#27979;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#25512;&#29702;&#26410;&#26469;&#26159;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#26680;&#24515;&#12290;&#20363;&#22914;&#65292;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23398;&#20064;&#34920;&#31034;&#20197;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#35775;&#38382;&#30340;&#29366;&#24577;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#23545;&#27604;&#24615;&#39044;&#27979;&#32534;&#30721;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#23398;&#20064;&#32534;&#30721;&#38271;&#26399;&#20381;&#36182;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#20943;&#23569;&#23398;&#20064;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#23548;&#20986;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#25552;&#39640;2&#20493;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#38543;&#26426;&#29615;&#22659;&#12290;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32422;&#20026;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
&lt;/p&gt;</description></item><item><title>&#26377;&#32422;&#26463;&#30340;&#23618;&#27425;&#33945;&#29305;&#21345;&#27931;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#65288;COBeTS&#65289;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#20998;&#35299;&#21644;&#32422;&#26463;&#36873;&#39033;&#25511;&#21046;&#22120;&#65292;&#23558;&#22312;&#32447;&#22522;&#20110;&#25628;&#32034;&#30340;CPOMDP&#35268;&#21010;&#25193;&#23637;&#21040;&#22823;&#22411;&#26426;&#22120;&#20154;&#38382;&#39064;&#65292;&#24182;&#33021;&#21516;&#26102;&#28385;&#36275;&#32422;&#26463;&#21644;&#22870;&#21169;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.20054</link><description>&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#23618;&#27425;&#33945;&#29305;&#21345;&#27931;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Constrained Hierarchical Monte Carlo Belief-State Planning. (arXiv:2310.20054v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20054
&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#23618;&#27425;&#33945;&#29305;&#21345;&#27931;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#65288;COBeTS&#65289;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#20998;&#35299;&#21644;&#32422;&#26463;&#36873;&#39033;&#25511;&#21046;&#22120;&#65292;&#23558;&#22312;&#32447;&#22522;&#20110;&#25628;&#32034;&#30340;CPOMDP&#35268;&#21010;&#25193;&#23637;&#21040;&#22823;&#22411;&#26426;&#22120;&#20154;&#38382;&#39064;&#65292;&#24182;&#33021;&#21516;&#26102;&#28385;&#36275;&#32422;&#26463;&#21644;&#22870;&#21169;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CPOMDPs&#65289;&#20013;&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#28385;&#36275;&#30828;&#24615;&#25104;&#26412;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22870;&#21169;&#30446;&#26631;&#65292;&#25512;&#24191;&#20102;&#29366;&#24577;&#21644;&#36807;&#28193;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23433;&#20840;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#25110;&#36830;&#32493;&#30340;&#38382;&#39064;&#22495;&#20013;&#36827;&#34892;&#22312;&#32447;CPOMDP&#35268;&#21010;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#22823;&#22411;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#23618;&#21160;&#20316;&#21407;&#35821;&#65288;&#36873;&#39033;&#65289;&#20026;&#20302;&#23618;&#25511;&#21046;&#25552;&#20379;&#24037;&#20855;&#65292;&#20998;&#23618;&#20998;&#35299;&#21487;&#20197;&#31616;&#21270;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#32422;&#26463;&#30340;&#36873;&#39033;&#20449;&#24565;&#26641;&#25628;&#32034;&#65288;COBeTS&#65289;&#26469;&#21033;&#29992;&#36825;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#23558;&#22312;&#32447;&#22522;&#20110;&#25628;&#32034;&#30340;CPOMDP&#35268;&#21010;&#25193;&#23637;&#21040;&#22823;&#22411;&#26426;&#22120;&#20154;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#21407;&#22987;&#36873;&#39033;&#25511;&#21046;&#22120;&#34987;&#23450;&#20041;&#20026;&#28385;&#36275;&#25351;&#23450;&#30340;&#32422;&#26463;&#39044;&#31639;&#65292;&#37027;&#20040;COBeTS&#23558;&#38543;&#26102;&#28385;&#36275;&#32422;&#26463;&#12290;&#21542;&#21017;&#65292;COBeTS&#23558;&#24341;&#23548;&#25628;&#32034;&#26397;&#30528;&#23433;&#20840;&#30340;&#36873;&#39033;&#21407;&#35821;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20998;&#23618;&#30417;&#25511;&#26469;&#23454;&#29616;&#36816;&#34892;&#26102;&#23433;&#20840;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23433;&#20840;&#20851;&#38190;&#30340;&#32422;&#26463;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;COBeTS&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrain
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.19852</link><description>&lt;p&gt;
AI&#23545;&#40784;: &#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26088;&#22312;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#12290;&#38543;&#30528;&#25317;&#26377;&#36229;&#20154;&#31867;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38169;&#35823;&#23545;&#40784;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22823;&#35268;&#27169;&#39118;&#38505;&#21464;&#24471;&#26126;&#26174;&#12290;&#25968;&#30334;&#21517;AI&#19987;&#23478;&#21644;&#20844;&#20247;&#20154;&#29289;&#37117;&#23545;AI&#39118;&#38505;&#34920;&#36798;&#20102;&#20851;&#27880;&#65292;&#35748;&#20026;&#20943;&#36731;AI&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#24212;&#35813;&#25104;&#20026;&#20840;&#29699;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#19982;&#22823;&#35268;&#27169;&#31038;&#20250;&#39118;&#38505;&#22914;&#22823;&#27969;&#34892;&#30149;&#21644;&#26680;&#25112;&#20105;&#24182;&#21015;&#12290;&#37492;&#20110;AI&#23545;&#40784;&#39046;&#22495;&#32570;&#20047;&#26368;&#26032;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#23545;&#40784;&#30740;&#31350;&#30340;&#26680;&#24515;&#27010;&#24565;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#30446;&#26631;&#21407;&#21017;&#20316;&#20026;AI&#23545;&#40784;&#30340;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#65288;RICE&#65289;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#23545;&#40784;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#12290;&#21069;&#32773;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15950</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15950
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;ID&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#23548;&#33268;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#22815;&#23500;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#30340;&#21033;&#29992;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#20559;&#24046;&#65292;&#32473;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#23613;&#31649;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#32467;&#21512;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#23454;&#26045;&#36824;&#38656;&#35201;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#26088;&#22312;&#36890;&#36807;LLM&#24378;&#21270;&#34920;&#31034;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representati
&lt;/p&gt;</description></item><item><title>O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2310.14403</link><description>&lt;p&gt;
O3D: &#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#21457;&#29616;&#19982;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14403
&lt;/p&gt;
&lt;p&gt;
O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#20223;&#25552;&#31034;&#20013;&#25552;&#20379;&#30340;&#23569;&#37327;&#31034;&#20363;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#19982;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#24182;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23569;&#37327;&#31034;&#20363;&#24448;&#24448;&#19981;&#36275;&#20197;&#29983;&#25104;&#22797;&#26434;&#19988;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26080;&#27861;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#31163;&#32447;&#25968;&#25454;&#65288;&#20363;&#22914;&#20154;&#31867;&#20132;&#20114;&#30340;&#26085;&#24535;&#65289;&#26469;&#25913;&#36827;LLM&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#21644;&#20195;&#30721;&#20004;&#31181;&#26041;&#27861;&#27491;&#24335;&#23450;&#20041;&#20102;LLM&#24378;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;O3D&#30340;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;LLM&#24378;&#21270;&#31574;&#30053;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;O3D&#33258;&#21160;&#22320;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#26356;&#26032;&#31232;&#30095;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#20943;&#23567;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#19978;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08915</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#65306;&#38024;&#23545;&#31232;&#30095;LLMs&#30340;&#26080;&#35757;&#32451;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs. (arXiv:2310.08915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#26356;&#26032;&#31232;&#30095;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#20943;&#23567;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#19978;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24222;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24320;&#36767;&#20102;&#28508;&#22312;&#36335;&#24452;&#65292;&#20294;&#24456;&#36951;&#25022;&#65292;&#22312;&#20854;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#36947;&#36335;&#19978;&#23384;&#22312;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#38556;&#30861;&#12290;&#20316;&#20026;&#22312;&#20943;&#23569;&#27169;&#22411;&#22797;&#26434;&#24615;&#26041;&#38754;&#26368;&#25104;&#29087;&#30340;&#39044;-LLMs&#26041;&#27861;&#20043;&#19968;&#65292;&#32593;&#32476;&#20462;&#21098;&#20284;&#20046;&#22312;LLMs&#26102;&#20195;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#20013;&#38656;&#35201;&#26114;&#36149;&#30340;&#24494;&#35843;(&#25110;&#37325;&#26032;&#35757;&#32451;)&#12290;&#20026;&#20102;&#24357;&#21512;&#20135;&#19994;&#19982;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;(DSnoT)&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#35757;&#32451;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#20219;&#20309;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#30053;&#24494;&#26356;&#26032;&#31232;&#30095;LLMs&#12290;&#21463;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#30340;&#21551;&#21457;&#65292;DSnoT&#36890;&#36807;&#22312;&#31232;&#30095;LLMs&#20043;&#19978;&#25191;&#34892;&#36845;&#20195;&#30340;&#26435;&#37325;&#20462;&#21098;&#21644;&#29983;&#38271;&#30340;&#26041;&#24335;&#65292;&#26368;&#23567;&#21270;&#20102;&#31264;&#23494;&#21644;&#31232;&#30095;LLMs&#20043;&#38388;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#65292;DSnoT&#29305;&#21035;&#32771;&#34385;&#20102;&#39044;&#26399;&#30340;&#20943;&#23569;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.07088</link><description>&lt;p&gt;
&#24605;&#32500;&#22810;&#26679;&#24615;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#25351;&#23548;&#20998;&#35299;&#38382;&#39064;&#20026;&#26356;&#23567;&#30340;&#25512;&#29702;&#27493;&#39588;&#25110;&#36890;&#36807;&#20462;&#25913;&#35299;&#30721;&#27493;&#39588;&#20351;&#21508;&#31181;&#29983;&#25104;&#32467;&#26524;&#21512;&#24182;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#36755;&#20837;&#25552;&#31034;&#26159;&#22266;&#23450;&#30340;&#65292;&#24182;&#26399;&#26395;&#35299;&#30721;&#31574;&#30053;&#24341;&#20837;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#21019;&#24314;&#21644;&#21033;&#29992;&#36755;&#20837;&#25552;&#31034;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#24605;&#32500;&#22810;&#26679;&#24615;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#24449;&#27714;&#21453;&#39304;&#26469;&#26500;&#24605;&#36866;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33258;&#21160;&#25552;&#39640;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;DIV-SE (DIVerse reasoning path Self-Ensemble)&#20013;&#23545;&#22810;&#26679;&#30340;&#25552;&#31034;&#36827;&#34892;&#21512;&#25104;&#65292;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#22312;&#19968;&#20010;&#25512;&#29702;&#20013;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06286</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#34892;&#20026;&#25233;&#21046;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#31216;&#20026;&#34394;&#25311;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65288;DAQ&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35843;&#33410;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#36890;&#36807;&#34394;&#25311;&#29609;&#23478;&#65292;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25152;&#25552;&#20986;&#30340;DAQ&#23558;&#20960;&#31181;Q&#23398;&#20064;&#30340;&#21464;&#20307;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#25511;&#21046;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#20363;&#22914;maxmin Q&#23398;&#20064;&#21644;minmax Q&#23398;&#20064;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#12290;&#36890;&#36807;&#34394;&#25311;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;DAQ&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65292;&#20174;&#32508;&#21512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;DAQ&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#23454;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;DAQ&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06272</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#35828;&#23494;&#25991;: &#36890;&#36807;&#23884;&#20837;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#35752;&#35770;&#21644;&#36777;&#35770;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30001;&#20110;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#32780;&#25104;&#20026;&#26126;&#26174;&#30340;&#20132;&#27969;&#36873;&#25321;&#65292;&#20294;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26102;&#38656;&#35201;&#36827;&#34892;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#21487;&#33021;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#35760;&#26469;&#20195;&#34920;&#27169;&#22411;&#22312;&#25972;&#20010;&#35789;&#27719;&#34920;&#20013;&#30340;&#20449;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#65288;&#36890;&#36807;&#23884;&#20837;&#34920;&#31034;&#36827;&#34892;&#20132;&#27969;&#30340;&#32593;&#32476;&#27169;&#22411;&#21327;&#35758;&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;LLMs&#20013;&#21435;&#38500;&#20102;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#23427;&#20204;&#36890;&#36807;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#30340;&#26399;&#26395;&#26469;&#20256;&#36798;&#23427;&#20204;&#30340;&#20449;&#24565;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20559;&#31163;&#33258;&#28982;&#35821;&#35328;&#65292;CIPHER&#22312;&#19981;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04668</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMS&#65289;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39640;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#25512;&#29702;&#25104;&#26412;&#36739;&#39640;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26080;&#26631;&#31614;&#22270;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;LLM-GNN&#12290;&#23427;&#38598;&#25104;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#34987;&#29992;&#26469;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;GNNs&#33021;&#22815;&#39044;&#27979;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#12290;LLM-GNN&#30340;&#23454;&#29616;&#38754;&#20020;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25105;&#20204;&#22914;&#20309;&#20027;&#21160;&#36873;&#25321;&#35201;&#30001;LLMs&#27880;&#37322;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#22686;&#24378;GNN&#30340;&#35757;&#32451;&#65311;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#20248;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>USB-NeRF&#26159;&#19968;&#31181;&#35299;&#20915;&#28378;&#21160;&#24555;&#38376;&#30456;&#26426;&#38382;&#39064;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#33021;&#22815;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02687</link><description>&lt;p&gt;
USB-NeRF: &#35299;&#21367;&#26354;&#24555;&#38376;&#26463;&#35843;&#25972;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields. (arXiv:2310.02687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02687
&lt;/p&gt;
&lt;p&gt;
USB-NeRF&#26159;&#19968;&#31181;&#35299;&#20915;&#28378;&#21160;&#24555;&#38376;&#30456;&#26426;&#38382;&#39064;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#33021;&#22815;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#26469;&#34920;&#31034;3D&#22330;&#26223;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#36755;&#20837;&#22270;&#20687;&#26159;&#30001;&#20840;&#23616;&#24555;&#38376;&#30456;&#26426;&#25293;&#25668;&#30340;&#12290;&#22240;&#27492;&#65292;&#28378;&#21160;&#24555;&#38376;&#65288;RS&#65289;&#22270;&#20687;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;NeRF&#31639;&#27861;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#28378;&#21160;&#24555;&#38376;&#25928;&#24212;&#36824;&#20250;&#24433;&#21709;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#65288;&#20363;&#22914;&#36890;&#36807;COLMAP&#65289;&#65292;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#20351;&#29992;RS&#22270;&#20687;&#30340;NeRF&#31639;&#27861;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21367;&#26354;&#24555;&#38376;&#26463;&#35843;&#25972;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;USB-NeRF&#65289;&#12290;USB-NeRF&#33021;&#22815;&#22312;NeRF&#26694;&#26550;&#19979;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#21516;&#26102;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#36890;&#36807;&#23545;RS&#30456;&#26426;&#30340;&#29289;&#29702;&#22270;&#20687;&#24418;&#25104;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;USB-NeRF&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01132</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#21644;BoWs&#33258;&#21160;&#35780;&#20272;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65306;&#23558;&#20840;&#23616;&#39044;&#27979;&#19982;&#20855;&#20307;&#21453;&#39304;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21521;&#25945;&#24072;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#26356;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20272;&#35745;&#8220;&#25945;&#23398;&#25903;&#25345;&#8221;&#39046;&#22495;&#30340;CLASS&#35838;&#22530;&#35780;&#20272;&#24471;&#20998;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#35266;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;Meta&#30340;Llama2&#30340;&#38646;-shot&#25552;&#31034;&#65292;&#21644;/&#25110;&#32463;&#20856;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25945;&#24072;&#35328;&#35821;&#30340;&#20010;&#21035;&#35805;&#35821;&#65288;&#20351;&#29992;OpenAI&#30340;Whisper&#36827;&#34892;&#33258;&#21160;&#36716;&#24405;&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#25945;&#23398;&#25903;&#25345;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#35805;&#35821;&#32423;&#30340;&#21028;&#26029;&#32467;&#26524;&#22312;&#25972;&#20010;15&#20998;&#38047;&#30340;&#35266;&#23519;&#20250;&#35805;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20272;&#35745;&#20840;&#23616;CLASS&#24471;&#20998;&#12290;&#22312;&#24188;&#20799;&#22253;&#21644;&#23398;&#21069;&#29677;&#25945;&#23460;&#30340;&#20004;&#20010;&#32463;&#36807;CLASS&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33258;&#21160;&#20272;&#35745;CLASS&#25945;&#23398;&#25903;&#25345;&#30340;&#20934;&#30830;&#24615;&#65288;Pearson R&#39640;&#36798;0.47&#65289;&#25509;&#36817;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65288;&#26368;&#39640;R=0.55&#65289;&#65307;&#65288;2&#65289;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#23567;&#29677;&#25945;&#23460;&#20013;&#30340;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the aim to provide teachers with more specific, frequent, and actionable feedback about their teaching, we explore how Large Language Models (LLMs) can be used to estimate ``Instructional Support'' domain scores of the CLassroom Assessment Scoring System (CLASS), a widely used observation protocol. We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify individual utterances of teachers' speech (transcribed automatically using OpenAI's Whisper) for the presence of Instructional Support. Then, these utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score. Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Ground-A-Video &#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#20854;&#20182;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01107</link><description>&lt;p&gt;
Ground-A-Video: &#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models. (arXiv:2310.01107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Ground-A-Video &#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#20854;&#20182;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#39057;&#32534;&#36753;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#65292;&#23454;&#29616;&#20102;&#21333;&#23646;&#24615;&#32534;&#36753;&#25110;&#39118;&#26684;&#20256;&#36882;&#30340;&#20219;&#21153;&#65292;&#19981;&#35770;&#36890;&#36807;&#22312;&#25991;&#26412;-&#35270;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#27169;&#22411;&#36824;&#26159;&#37319;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#22810;&#23646;&#24615;&#32534;&#36753;&#24773;&#26223;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#27604;&#22914;&#24573;&#30053;&#25110;&#24573;&#35270;&#25152;&#26399;&#26395;&#30340;&#23646;&#24615;&#21464;&#21270;&#65292;&#20462;&#25913;&#36755;&#20837;&#35270;&#39057;&#30340;&#38169;&#35823;&#20803;&#32032;&#65292;&#20197;&#21450;&#26080;&#27861;&#20445;&#30041;&#24212;&#35813;&#20445;&#25345;&#21407;&#26679;&#30340;&#36755;&#20837;&#35270;&#39057;&#21306;&#22495;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#21517;&#20026; Ground-A-Video&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;Ground-A-Video&#20197;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#27809;&#26377;&#19978;&#36848;&#32570;&#28857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#20102;&#20132;&#21449;&#24103;&#38376;&#25511;&#27880;&#24847;&#21147;&#65292;&#20197;&#19968;&#31181;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#26041;&#24335;&#23558;&#23450;&#20301;&#20449;&#24687;&#34701;&#20837;&#21040;&#28508;&#22312;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video (T2V) models on text-video data or adopting training-free methods. However, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. To address this, here we present a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing. Ground-A-Video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings. Central to our method is the introduction of Cross-Frame Gated Attention which incorporates groundings information into the latent representations in a temporally consistent fashion,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00771</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;Decision Transformer&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#21482;&#33021;&#36890;&#36807;&#35821;&#35328;&#39044;&#35757;&#32451;&#23454;&#29616;&#65292;&#36824;&#26159;&#21487;&#20197;&#36890;&#36807;&#19981;&#28041;&#21450;&#35821;&#35328;&#30340;&#26356;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35821;&#35328;&#23545;&#20110;&#25913;&#21892;&#24615;&#33021;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#21512;&#25104;&#30340;IID&#25968;&#25454;&#36827;&#34892;&#23569;&#37327;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#25552;&#21319;&#65307;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#39044;&#35757;&#32451;Conservative Q-Learning(CQL)&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;Q-learning&#65292;&#24182;&#36890;&#24120;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#39592;&#24178;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#22312;CQL&#31639;&#27861;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19982;&#20505;&#36873;&#32773;&#30340;&#25490;&#24207;&#30456;&#19968;&#33268;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#37327;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#31232;&#30095;&#22870;&#21169;&#26223;&#35266;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.00386</link><description>&lt;p&gt;
&#20445;&#24207;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Order-Preserving GFlowNets. (arXiv:2310.00386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19982;&#20505;&#36873;&#32773;&#30340;&#25490;&#24207;&#30456;&#19968;&#33268;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#37327;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#31232;&#30095;&#22870;&#21169;&#26223;&#35266;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26681;&#25454;&#32473;&#23450;&#22870;&#21169;&#27010;&#29575;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;GFlowNets&#21482;&#33021;&#19982;&#39044;&#23450;&#20041;&#30340;&#26631;&#37327;&#22870;&#21169;&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20219;&#21153;&#20013;&#65292;&#36825;&#21487;&#33021;&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#25110;&#32773;&#30452;&#25509;&#19981;&#21487;&#35775;&#38382;&#30340;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20248;&#20808;&#35782;&#21035;&#39640;&#22870;&#21169;&#20505;&#36873;&#32773;&#65292;&#20256;&#32479;&#20570;&#27861;&#26159;&#23558;&#22870;&#21169;&#25552;&#39640;&#21040;&#26356;&#39640;&#30340;&#25351;&#25968;&#65292;&#32780;&#36825;&#20010;&#26368;&#20248;&#36873;&#25321;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#23427;&#20204;&#20197;&#19982;&#25552;&#20379;&#30340;&#65288;&#37096;&#20998;&#65289;&#20505;&#36873;&#32773;&#25490;&#24207;&#19968;&#33268;&#30340;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#26174;&#24335;&#34920;&#36798;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;OP-GFNs&#30340;&#35757;&#32451;&#36807;&#31243;&#36880;&#28176;&#31232;&#30095;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective max
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.14556</link><description>&lt;p&gt;
&#33402;&#26415;&#36824;&#26159;&#25216;&#24039;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21019;&#36896;&#21147;&#30340;&#34394;&#20551;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20174;&#21338;&#23458;&#21040;&#25925;&#20107;&#30340;&#39640;&#36136;&#37327;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#35266;&#35780;&#20272;&#19968;&#27573;&#25991;&#23383;&#30340;&#21019;&#36896;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTC)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20849;&#35782;&#35780;&#20272;&#25216;&#26415;[3]&#65292;&#25552;&#20986;&#20102;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#21019;&#36896;&#21147;&#20316;&#20026;&#19968;&#20010;&#20135;&#21697;&#12290;TTCW&#30001;&#21253;&#21547;&#22312;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#32454;&#33268;&#24230;&#21407;&#22987;&#32500;&#24230;&#20013;&#30340;14&#20010;&#20108;&#20803;&#27979;&#35797;&#32452;&#25104;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;10&#20301;&#21019;&#24847;&#20316;&#23478;&#65292;&#24182;&#20351;&#29992;TTCW&#23545;48&#20010;&#30001;&#19987;&#19994;&#20316;&#23478;&#25110;LLMs&#25776;&#20889;&#30340;&#25925;&#20107;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#36890;&#36807;&#30340;TTCW&#27979;&#35797;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#20102;3-10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20215;&#32773;&#65292;&#20197;&#33258;&#21160;&#21270;TTCW&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#19968;&#20010;LLM&#19982;&#19987;&#23478;&#35780;&#20272;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.12368</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#65306;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis. (arXiv:2309.12368v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#30740;&#31350;&#35770;&#25991;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21364;&#38754;&#20020;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#33043;&#27602;&#30151;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#26089;&#26399;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#35786;&#26029;&#30340;&#24613;&#24615;&#33268;&#21629;&#20840;&#36523;&#24615;&#24863;&#26579;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#33021;&#22815;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#20570;&#20986;&#26356;&#22909;&#33043;&#27602;&#30151;&#26089;&#26399;&#35786;&#26029;&#20915;&#31574;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;&#30740;&#31350;&#20174;&#19968;&#20010;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#35843;&#26597;&#20026;&#20160;&#20040;&#20020;&#24202;&#19987;&#23478;&#22312;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#25918;&#24323;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#33043;&#27602;&#30151;&#39044;&#27979;&#27169;&#22359;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#25903;&#25345;&#20154;&#31867;&#19987;&#23478;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#20102;SepsisLab&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection o
&lt;/p&gt;</description></item><item><title>RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09737</link><description>&lt;p&gt;
RaTrack: &#24102;&#26377;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#36816;&#21160;&#29289;&#20307;&#26816;&#27979;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09737
&lt;/p&gt;
&lt;p&gt;
RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#24615;&#20381;&#36182;&#20110;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#31934;&#30830;&#24863;&#30693;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#31283;&#23450;&#22320;&#36319;&#36394;&#31227;&#21160;&#29289;&#20307;&#22240;&#27492;&#23545;&#20110;&#36712;&#36857;&#39044;&#27979;&#12289;&#36991;&#38556;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#24212;&#29992;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;LiDAR&#25110;&#30456;&#26426;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#65292;&#20294;4D&#25104;&#20687;&#38647;&#36798;&#30340;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#35748;&#35782;&#21040;4D&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#38647;&#36798;&#22122;&#22768;&#21644;&#28857;&#31232;&#30095;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RaTrack&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#38647;&#36798;&#30340;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#21644;3D&#36793;&#30028;&#26694;&#30340;&#20381;&#36182;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#37197;&#20197;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#12290;&#22312;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;RaTrack&#23637;&#31034;&#20986;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#36816;&#21160;&#29289;&#20307;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08254</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quantitative and Qualitative Evaluation of Reinforcement Learning Policies for Autonomous Vehicles. (arXiv:2309.08254v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20132;&#36890;&#29615;&#22659;&#20013;&#20248;&#21270;&#20132;&#36890;&#21160;&#21147;&#23398;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#19982;&#20154;&#39550;&#39542;&#36710;&#36742;&#24182;&#23384;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;AVs&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#20132;&#36890;&#38459;&#22622;&#65288;&#21363;&#26368;&#23567;&#21270;&#27178;&#36807;&#31859;&#20848;&#30340;&#29615;&#24418;&#36947;&#30340;&#26102;&#38388;&#65289;&#24182;&#20943;&#23569;&#27745;&#26579;&#12290;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#27745;&#26579;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#39550;&#39542;&#33329;&#23450;&#24615;&#35780;&#20272;&#20102;&#23398;&#21040;&#30340;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#31574;&#30053;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20132;&#36890;&#24179;&#31283;&#24615;&#21644;&#23433;&#20840;&#24863;&#31561;&#19968;&#31995;&#21015;&#25351;&#26631;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#21644;&#34892;&#36710;&#24179;&#28369;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing traffic dynamics in an evolving transportation landscape is crucial, particularly in scenarios where autonomous vehicles (AVs) with varying levels of autonomy coexist with human-driven cars. This paper presents a novel approach to optimizing choices of AVs using Proximal Policy Optimization (PPO), a reinforcement learning algorithm. We learned a policy to minimize traffic jams (i.e., minimize the time to cross the scenario) and to minimize pollution in a roundabout in Milan, Italy. Through empirical analysis, we demonstrate that our approach can reduce time and pollution levels. Furthermore, we qualitatively evaluate the learned policy using a cutting-edge cockpit to assess its performance in near-real-world conditions. To gauge the practicality and acceptability of the policy, we conducted evaluations with human participants using the simulator, focusing on a range of metrics like traffic smoothness and safety perception. In general, our findings show that human-driven vehi
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04332</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#38656;&#35201;&#30340;&#26102;&#20505;&#20173;&#28982;&#20351;&#29992;&#22270;&#24418;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04332
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#31561;&#65292;&#23545;&#22270;&#24418;&#36827;&#34892;&#39044;&#27979;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22270;&#24418;&#26631;&#27880;&#38382;&#39064;&#30340;&#23454;&#20363;&#21253;&#25324;&#22270;&#32467;&#26500;(&#21363;&#37051;&#25509;&#30697;&#38453;)&#21644;&#33410;&#28857;&#29305;&#23450;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#22270;&#32467;&#26500;&#23545;&#20110;&#39044;&#27979;&#20219;&#21153;&#26469;&#35828;&#24182;&#19981;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#20363;&#22914;&#65292;&#20998;&#23376;&#24615;&#36136;&#22914;&#25705;&#23572;&#36136;&#37327;&#20165;&#20381;&#36182;&#20110;&#32452;&#25104;&#21407;&#23376;(&#33410;&#28857;&#29305;&#24449;)&#65292;&#32780;&#19982;&#20998;&#23376;&#32467;&#26500;&#26080;&#20851;&#12290;&#23613;&#31649;GNNs&#26377;&#33021;&#21147;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24573;&#30053;&#22270;&#32467;&#26500;&#65292;&#20294;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#20250;&#36825;&#26679;&#20570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GNNs&#23454;&#38469;&#19978;&#20542;&#21521;&#20110;&#22312;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#22312;&#24573;&#30053;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#20173;&#22312;&#20351;&#29992;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20998;&#24067;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#21457;&#29616;&#24120;&#35268;&#22270;&#23545;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11464</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21516;&#36136;&#24615;&#21040;&#24322;&#36136;&#24615;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11464
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22686;&#24378;&#22823;&#22810;&#25968;&#27169;&#22411;&#21516;&#36136;&#24615;FL&#26041;&#27861;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#25193;&#23637;&#23427;&#20204;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20174;&#35814;&#32454;&#25506;&#32034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;FL&#35774;&#32622;&#24320;&#22987;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#23458;&#25143;&#31471;&#24615;&#33021;&#19982;&#23618;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#65292;&#65288;2&#65289;&#27973;&#23618;&#27604;&#28145;&#23618;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#65288;3&#65289;&#36739;&#20026;&#24179;&#28369;&#30340;&#26799;&#24230;&#20998;&#24067;&#25351;&#31034;&#20102;&#26356;&#39640;&#30340;&#23618;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InCo Aggregation&#26041;&#27861;&#65292;&#21033;&#29992;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#65292;&#21363;&#26381;&#21153;&#22120;&#27169;&#22411;&#20013;&#26469;&#33258;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#28151;&#21512;&#65292;&#20197;&#22686;&#24378;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23458;&#25143;&#31471;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08758</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#26469;&#35299;&#20915;&#19982;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#38271;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#40723;&#21169;&#24320;&#21457;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23481;&#32435;&#22810;&#20010;&#35760;&#21495;&#21547;&#20041;&#12290;&#36825;&#22312;&#35299;&#37322;&#24615;&#12289;&#22266;&#23450;&#25968;&#37327;&#30340;&#23884;&#20837;&#35760;&#21495;&#12289;&#22312;&#19981;&#21516;LM&#20043;&#38388;&#30340;&#21487;&#37325;&#29992;&#24615;&#20197;&#21450;&#19982;&#40657;&#30418;API&#20132;&#20114;&#26102;&#30340;&#19981;&#36866;&#29992;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#12290;PCRL&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#20197;&#21450;&#21482;&#26377;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26799;&#24230;&#35775;&#38382;LM&#25110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06013</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#30340;&#26410;&#26469;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Telecom: Forthcoming Impact on the Industry. (arXiv:2308.06013v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06013
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#32929;&#21464;&#38761;&#30340;&#21147;&#37327;&#65292;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#39046;&#22495;&#20043;&#22806;&#65292;&#36824;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30005;&#20449;&#34892;&#19994;&#38754;&#20020;&#30528;&#28508;&#22312;&#24433;&#21709;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#24433;&#21709;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;LLMs&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#30005;&#20449;&#34892;&#19994;&#21487;&#20197;&#26041;&#20415;&#23454;&#26045;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#31616;&#21270;&#20102;&#30446;&#21069;&#22952;&#30861;&#36816;&#33829;&#25928;&#29575;&#24182;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#30005;&#20449;&#39046;&#22495;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26159;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#21644;&#21457;&#25381;&#20854;&#33021;&#21147;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fulles
&lt;/p&gt;</description></item><item><title>LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04945</link><description>&lt;p&gt;
LLMeBench&#65306;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04945
&lt;/p&gt;
&lt;p&gt;
LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#25104;&#21151;&#20351;&#24471;&#38656;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#24182;&#20844;&#24320;&#20102;&#20960;&#20010;&#26694;&#26550;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#29992;&#25143;&#26469;&#35828;&#65292;&#23427;&#20204;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#23450;&#21046;&#33021;&#21147;&#36890;&#24120;&#24456;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMeBench&#26694;&#26550;&#12290;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;OpenAI&#30340;GPT&#21644;BLOOM&#27169;&#22411;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65307;&#23427;&#21487;&#20197;&#26080;&#32541;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#22914;&#20309;&#12290;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#12290;&#21487;&#20197;&#22312;&#19981;&#21040;10&#20998;&#38047;&#20869;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;&#27169;&#22411;API&#23494;&#38053;&#26469;&#35780;&#20272;&#24403;&#21069;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#24050;&#32463;&#22312;90&#20010;&#23454;&#39564;&#35774;&#32622;&#20013;&#20351;&#29992;53&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#23545;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28041;&#21450;&#22823;&#32422;296K&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#35745;&#21010;&#23558;&#35813;&#26694;&#26550;&#24320;&#28304;&#20379;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.02514</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#26041;&#31243;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models as master equation solvers. (arXiv:2308.02514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#26041;&#31243;&#22312;&#24314;&#27169;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#28982;&#32780;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#35299;&#20915;&#20027;&#26041;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24212;&#29992;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20027;&#26041;&#31243;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#36895;&#29575;&#21442;&#25968;&#12289;&#21021;&#22987;&#26465;&#20214;&#21644;&#26102;&#38388;&#20540;&#30452;&#25509;&#26144;&#23556;&#21040;&#19982;&#36755;&#20837;&#19978;&#19979;&#25991;&#23436;&#20840;&#21305;&#37197;&#30340;&#29366;&#24577;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#36817;&#20284;&#22320;&#27714;&#35299;&#20102;&#20027;&#26041;&#31243;&#30340;&#26368;&#19968;&#33324;&#24418;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#21453;&#39304;&#22870;&#21169;&#30001;&#19968;&#32452;&#21464;&#20998;&#33258;&#22238;&#24402;&#27169;&#22411;&#25552;&#20379;&#12290;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#20110;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#24456;&#39640;&#12290;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36816;&#29992;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#35265;&#35299;&#65292;&#28548;&#28165;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#20013;&#30340;&#26415;&#35821;&#24182;&#25552;&#39640;&#20102;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02041</link><description>&lt;p&gt;
&#35843;&#33410;AI&#25805;&#32437;&#65306;&#36816;&#29992;&#34892;&#20026;&#32463;&#27982;&#23398;&#21644;&#24515;&#29702;&#23398;&#30340;&#35265;&#35299;&#22686;&#24378;&#27431;&#30431;AI&#27861;&#26696;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Regulating AI manipulation: Applying Insights from behavioral economics and psychology to enhance the practicality of the EU AI Act. (arXiv:2308.02041v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36816;&#29992;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#35265;&#35299;&#65292;&#28548;&#28165;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#20013;&#30340;&#26415;&#35821;&#24182;&#25552;&#39640;&#20102;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;AI&#27861;&#26696;&#31532;5&#26465;&#26088;&#22312;&#35843;&#33410;AI&#25805;&#32437;&#65292;&#20197;&#38450;&#27490;&#28508;&#22312;&#30340;&#26377;&#23475;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26415;&#35821;&#27169;&#31946;&#21644;&#25805;&#32437;&#25216;&#26415;&#34920;&#36848;&#19981;&#28165;&#26224;&#65292;&#36825;&#39033;&#31435;&#27861;&#30340;&#23454;&#38469;&#25191;&#34892;&#23384;&#22312;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#31532;5&#26465;&#20063;&#21463;&#21040;&#20445;&#25252;&#25928;&#26524;&#19981;&#36275;&#30340;&#25209;&#35780;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#25972;&#21512;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#35265;&#35299;&#65292;&#28548;&#28165;&#26415;&#35821;&#24182;&#25552;&#39640;&#20445;&#25252;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#36816;&#29992;&#35748;&#30693;&#24515;&#29702;&#23398;&#30740;&#31350;&#38416;&#26126;&#28508;&#24847;&#35782;&#25216;&#24039;&#21450;&#20854;&#30456;&#20851;&#34920;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#21487;&#20197;&#24341;&#21457;&#34892;&#20026;&#21464;&#21270;&#30340;&#19968;&#32452;&#24605;&#32500;&#24555;&#25463;&#26041;&#24335;&#65292;&#21363;&#21551;&#21457;&#24335;&#65292;&#25193;&#23637;&#21040;&#25805;&#32437;&#25216;&#26415;&#39046;&#22495;&#12290;&#26415;&#35821;&#30340;&#38416;&#26126;&#21644;&#25193;&#23637;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#27861;&#24459;&#35268;&#23450;&#26356;&#20934;&#30830;&#30340;&#29702;&#35299;&#65292;&#36824;&#22686;&#24378;&#20102;&#20854;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The EU AI Act Article 5 is designed to regulate AI manipulation to prevent potential harmful consequences. However, the practical implementation of this legislation is challenging due to the ambiguous terminologies and the unclear presentations of manipulative techniques. Moreover, the Article 5 also suffers criticize of inadequate protective efficacy. This paper attempts to clarify terminologies and to enhance the protective efficacy by integrating insights from psychology and behavioral economics. Firstly, this paper employs cognitive psychology research to elucidate the term subliminal techniques and its associated representation. Additionally, this paper extends the study of heuristics: a set of thinking shortcuts which can be aroused for behavior changing from behavior economics to the realm of manipulative techniques. The elucidation and expansion of terminologies not only provide a more accurate understanding of the legal provision but also enhance its protective efficacy. Secon
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09913</link><description>&lt;p&gt;
&#25506;&#32034;&#20855;&#26377;&#25551;&#36848;&#36923;&#36753;&#29305;&#24449;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#38750;&#27491;&#21017;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09913
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#23545;&#35937;&#26159;ALCreg&#21644;ALCvpl&#65292;&#20998;&#21035;&#26159;&#20351;&#29992;&#27491;&#21017;&#21644;&#21487;&#35265;&#25512;&#19979;&#35821;&#35328;&#30340;&#36335;&#24452;&#34920;&#36798;&#24335;&#30340;&#25193;&#23637;&#12290;&#31532;&#19968;&#20010;ALCreg&#26159;Fischer&#21644;Ladner&#25152;&#29087;&#30693;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#19968;&#31181;&#21464;&#31181;&#12290;&#31532;&#20108;&#20010;ALCvpl&#26159;&#30001;Loding&#21644;Serre&#22312;2007&#24180;&#24341;&#20837;&#21644;&#30740;&#31350;&#30340;&#12290;ALCvpl&#36923;&#36753;&#24191;&#20041;&#19978;&#25512;&#24191;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;&#21487;&#20915;&#23450;&#24615;&#38750;&#27491;&#21017;&#25193;&#23637;&#30340;ALCreg&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#30475;&#20284;&#26080;&#23475;&#30340;Self&#25805;&#20316;&#31526;&#21518;&#65292;&#23545;&#20110;ALCvpl&#20013;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#21487;&#20915;&#23450;&#24615;&#20007;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#22312;ALCvpl&#20013;&#28155;&#21152;&#20010;&#20307;&#35789;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#35777;&#26126;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;&#38750;&#27491;&#21017;&#65288;&#21487;&#35265;&#25512;&#19979;&#65289;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.  We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;</title><link>http://arxiv.org/abs/2307.00014</link><description>&lt;p&gt;
&#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24815;&#24615;&#20256;&#24863;&#22312;&#35768;&#22810;&#24212;&#29992;&#21644;&#24179;&#21488;&#20013;&#34987;&#20351;&#29992;&#65292;&#20174;&#26234;&#33021;&#25163;&#26426;&#31561;&#26085;&#24120;&#35774;&#22791;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#22797;&#26434;&#35774;&#22791;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24815;&#24615;&#20256;&#24863;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#36825;&#26159;&#30001;&#20110;&#39640;&#25928;&#30340;&#35745;&#31639;&#30828;&#20214;&#30340;&#21457;&#23637;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#33719;&#24471;&#24615;&#12290;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#24378;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#23548;&#33322;&#21644;&#20256;&#24863;&#22120;&#34701;&#21512;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20837;&#32508;&#36848;&#12290;&#25105;&#20204;&#20998;&#21035;&#32771;&#23519;&#20102;&#27599;&#20010;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#65292;&#21253;&#25324;&#38470;&#22320;&#12289;&#31354;&#20013;&#21644;&#28023;&#27915;&#12290;&#27599;&#20010;&#39046;&#22495;&#20998;&#20026;&#32431;&#24815;&#24615;&#36827;&#23637;&#21644;&#22522;&#20110;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#29992;&#20110;&#26657;&#20934;&#21644;&#21435;&#22122;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25972;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24120;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inertial sensing is used in many applications and platforms, ranging from day-to-day devices such as smartphones to very complex ones such as autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has increased significantly in the field of inertial sensing. This is due to the development of efficient computing hardware and the accessibility of publicly available sensor data. These data-driven approaches are used to empower model-based navigation and sensor fusion algorithms. This paper provides an in-depth review of those deep learning methods. We examine separately, each vehicle operation domain including land, air, and sea. Each domain is divided into pure inertial advances and improvements based on filter parameters learning. In addition, we review deep learning approaches for calibrating and denoising inertial sensors. Throughout the paper, we discuss these trends and future directions. We also provide statistics on the commonly used
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03346</link><description>&lt;p&gt;
&#31283;&#23450;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;: &#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#24320;&#21457;&#20102;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24378;&#21270;&#23398;&#20064;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#33258;&#30417;&#30563;&#38382;&#39064;&#65306;&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#25351;&#23450;&#30340;&#22870;&#21169;&#25110;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#33258;&#30417;&#30563;&#22522;&#30784;&#23454;&#38469;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#27492;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#21078;&#26512;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19982;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36890;&#36807;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#20854;&#20013;&#20219;&#21153;&#30001;&#35757;&#32451;&#21518;&#25552;&#20379;&#30340;&#21333;&#20010;&#30446;&#26631;&#22270;&#20687;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00742</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#65292;&#31185;&#23398;&#23478;&#20204;&#37319;&#29992;&#34920;&#31034;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#19968;&#20123;&#24213;&#23618;&#36816;&#31639;&#30340;&#35889;&#20998;&#35299;&#20043;&#38388;&#23637;&#29616;&#20986;&#26126;&#26174;&#30340;&#32852;&#31995;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#26159;&#36890;&#36807;&#22312;&#25968;&#25454;&#30340;&#39030;&#37096;&#26500;&#24314;&#22270;&#24418;&#26469;&#24314;&#31435;&#26126;&#30830;&#30340;&#35889;&#23884;&#20837;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#26500;&#24314;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#20197;&#20248;&#21270;&#22522;&#26412;&#21464;&#20998;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#26469;&#22312;&#19968;&#27493;&#20013;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#30830;&#20445;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#19981;&#21457;&#29983;&#30896;&#25758;&#65292;&#24182;&#20351;&#29992; ASP-MAUPF &#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#65292;&#23545;&#20854;&#36866;&#29992;&#24615;&#21644;&#29615;&#22659;&#20381;&#36182;&#24230;&#36827;&#34892;&#20102;&#35266;&#23519;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.16203</link><description>&lt;p&gt;
&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#37096;&#20998;&#21487;&#35266;&#27979;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#36890;&#29992;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On Computing Universal Plans for Partially Observable Multi-Agent Path Finding. (arXiv:2305.16203v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#30830;&#20445;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#19981;&#21457;&#29983;&#30896;&#25758;&#65292;&#24182;&#20351;&#29992; ASP-MAUPF &#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#65292;&#23545;&#20854;&#36866;&#29992;&#24615;&#21644;&#29615;&#22659;&#20381;&#36182;&#24230;&#36827;&#34892;&#20102;&#35266;&#23519;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#22312;&#29616;&#20170;&#24191;&#27867;&#24212;&#29992;&#20110;&#20179;&#24211;&#26426;&#22120;&#20154;&#12289;&#29289;&#27969;&#33258;&#21160;&#21270;&#12289;&#20132;&#36890;&#25511;&#21046;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#23558;&#20854;&#30475;&#20316;&#26159;&#36890;&#29992;&#35268;&#21010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#21010;&#65288;&#21448;&#31216;&#31574;&#30053;&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026; ASP-MAUPF &#30340;&#31995;&#32479;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#22312;&#20219;&#24847;&#20108;&#32500;&#22320;&#22270;&#21644;&#26234;&#33021;&#20307;&#30446;&#26631;&#37197;&#32622;&#19979;&#65292;&#25214;&#21040;&#19968;&#20010;&#36866;&#29992;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36890;&#29992;&#35745;&#21010;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#20043;&#38388;&#20114;&#19981;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent routing problems have drawn significant attention nowadays due to their broad industrial applications in, e.g., warehouse robots, logistics automation, and traffic control. Conventionally, they are modelled as classical planning problems. In this paper, we argue that it is beneficial to formulate them as universal planning problems. We therefore propose universal plans, also known as policies, as the solution concepts, and implement a system called ASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for computing them. Given an arbitrary two-dimensional map and a profile of goals for the agents, the system finds a feasible universal plan for each agent that ensures no collision with others. We use the system to conduct some experiments, and make some observations on the types of goal profiles and environments that will have feasible policies, and how they may depend on agents' sensors. We also demonstrate how users can customize action preferences to c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#30340;&#21327;&#35843;&#31574;&#30053;&#35299;&#20915;&#20102;&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15596</link><description>&lt;p&gt;
&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Distributed Online Rollout for Multivehicle Routing in Unmapped Environments. (arXiv:2305.15596v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#30340;&#21327;&#35843;&#31574;&#30053;&#35299;&#20915;&#20102;&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#24191;&#27867;&#21270;&#30340;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#32593;&#32476;&#12289;&#19968;&#32452;&#21344;&#25454;&#32593;&#32476;&#33410;&#28857;&#23376;&#38598;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#32452;&#20219;&#21153;, &#25105;&#20204;&#23547;&#27714;&#19968;&#20010;&#26368;&#23567;&#25104;&#26412;&#30340;&#31227;&#21160;&#24207;&#21015;&#65292;&#20197;&#28385;&#36275;&#27599;&#20010;&#20219;&#21153;&#33267;&#23569;&#34987;&#19968;&#20010;&#26234;&#33021;&#20307;&#35775;&#38382;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#32463;&#20856;&#38382;&#39064;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20551;&#23450;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26159;&#19968;&#20010;&#20010;&#20307;&#22788;&#29702;&#22120;&#65292;&#27809;&#26377;&#20851;&#20110;&#22522;&#30784;&#32593;&#32476;&#65288;&#21253;&#25324;&#20219;&#21153;&#21644;&#26234;&#33021;&#20307;&#20301;&#32622;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#20855;&#26377;&#20005;&#26684;&#30340;&#26412;&#22320;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#65288;&#38480;&#21046;&#22312;&#23427;&#20204;&#21508;&#33258;&#20301;&#32622;&#21608;&#22260;&#30340;&#21322;&#24452;&#33539;&#22260;&#20869;&#65289;&#65292;&#26356;&#25509;&#36817;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26681;&#25454;&#38598;&#20013;&#24335;&#27169;&#25311;&#22120;&#35757;&#32451;&#30340;&#23398;&#20064;&#22411;&#31574;&#30053;&#23616;&#37096;&#35268;&#21010;&#20854;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#32593;&#32476;&#25299;&#25169;&#21644;&#20219;&#21153;&#20998;&#24067;&#30340;&#21464;&#21270;&#19979;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#36817;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we consider a generalization of the well-known multivehicle routing problem: given a network, a set of agents occupying a subset of its nodes, and a set of tasks, we seek a minimum cost sequence of movements subject to the constraint that each task is visited by some agent at least once. The classical version of this problem assumes a central computational server that observes the entire state of the system perfectly and directs individual agents according to a centralized control scheme. In contrast, we assume that there is no centralized server and that each agent is an individual processor with no a priori knowledge of the underlying network (including task and agent locations). Moreover, our agents possess strictly local communication and sensing capabilities (restricted to a fixed radius around their respective locations), aligning more closely with several real-world multiagent applications. These restrictions introduce many challenges that are overcome through local
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15196</link><description>&lt;p&gt;
&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS
&lt;/p&gt;
&lt;p&gt;
Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#26159;N-BEATS&#30340;&#38750;&#24179;&#20961;&#25193;&#23637;&#65292;&#37319;&#29992;&#20102;&#21452;&#37325;&#27531;&#24046;&#21472;&#21152;&#21407;&#21017;&#65288;Oreshkin&#31561;&#20154;[42]&#65289;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22260;&#32469;&#30528;&#30001;N-BEATS&#27599;&#20010;&#22534;&#26632;&#30340;&#27531;&#24046;&#21644;&#29305;&#24449;&#25552;&#21462;&#31639;&#23376;&#30340;&#22797;&#26434;&#32452;&#21512;&#20135;&#29983;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#65288;Sinkhorn&#36317;&#31163;&#65289;&#23558;&#23427;&#20204;&#22534;&#21472;&#22320;&#23545;&#40784;&#12290;&#35757;&#32451;&#25439;&#22833;&#30001;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;&#21363;&#39044;&#27979;&#25439;&#22833;&#65289;&#21644;Sinkhorn&#36317;&#31163;&#35745;&#31639;&#30340;&#23545;&#40784;&#25439;&#22833;&#32452;&#25104;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#28304;&#25968;&#25454;&#24207;&#21015;&#20013;&#22534;&#21472;&#22320;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;N-BEATS&#30340;&#21487;&#35299;&#37322;&#35774;&#35745;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.12474</link><description>&lt;p&gt;
&#22312;&#39640;&#32771;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of Large Language Models on GAOKAO Benchmark. (arXiv:2305.12474v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#23427;&#20204;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#21151;&#25928;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GAOKAO-Benchmark&#65288;GAOKAO-Bench&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20351;&#29992;&#20013;&#22269;&#39640;&#32771;&#32771;&#35797;&#30340;&#39064;&#30446;&#20316;&#20026;&#27979;&#35797;&#26679;&#26412;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#23613;&#21487;&#33021;&#22320;&#20351;&#35780;&#20272;&#32467;&#26524;&#19982;&#20154;&#31867;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;-shot&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#20027;&#35266;&#21644;&#23458;&#35266;&#31867;&#22411;&#26469;&#20998;&#26512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35780;&#20998;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#22312;GAOKAO-Benchmark&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#27169;&#22411;&#22312;&#35299;&#20915;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23457;&#26597;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20154;&#31867;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#20026;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;GAOKAO&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;</title><link>http://arxiv.org/abs/2304.06607</link><description>&lt;p&gt;
&#27169;&#22411;&#25152;&#26377;&#26435;&#20105;&#35758;&#20013;&#30340;&#34394;&#20551;&#25351;&#25511;
&lt;/p&gt;
&lt;p&gt;
False Claims against Model Ownership Resolution. (arXiv:2304.06607v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#26377;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65292;&#26500;&#25104;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20445;&#25252;&#27169;&#22411;&#19981;&#34987;&#30423;&#29992;&#30340;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#65288;MOR&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#34987;&#30423;&#30340;&#25216;&#26415;&#12290;MOR&#26041;&#26696;&#20351;&#24471;&#21407;&#21578;&#26041;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#65288;&#22914;&#27700;&#21360;&#25110;&#25351;&#32441;&#65289;&#26469;&#26029;&#35328;&#23545;&#28041;&#23244;&#30423;&#29992;&#27169;&#22411;&#30340;&#34987;&#21578;&#26041;&#22768;&#31216;&#25152;&#26377;&#26435;&#65292;&#35777;&#26126;&#28041;&#23244;&#27169;&#22411;&#26159;&#34987;&#30423;&#25110;&#32773;&#28304;&#33258;&#20110;&#21407;&#21578;&#26041;&#25317;&#26377;&#30340;&#28304;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968; MOR &#26041;&#26696;&#37325;&#28857;&#25918;&#22312;&#38450;&#33539;&#24694;&#24847;&#28041;&#23244;&#26041;&#26041;&#38754;&#65292;&#30830;&#20445;&#22914;&#26524;&#28041;&#23244;&#27169;&#22411;&#30830;&#23454;&#26159;&#34987;&#30423;&#29256;&#65292;&#21017;&#21407;&#21578;&#26041;&#23558;&#33719;&#32988;&#12290;&#20294;&#26159;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24120;&#35265; MOR &#26041;&#26696;&#23384;&#22312;&#30528;&#21478;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#20294;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65306;&#24694;&#24847;&#21407;&#21578;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#22320;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.03431</link><description>&lt;p&gt;
&#40065;&#26834;&#19981;&#21464;&#34920;&#31034;&#20013;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization In Robust Invariant Representation. (arXiv:2304.03431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#24120;&#35265;&#21464;&#25442;&#30340;&#19981;&#21464;&#34920;&#31034;&#26041;&#27861;&#24120;&#29992;&#20110;&#30446;&#26631;&#35782;&#21035;&#12290;&#23398;&#20064;&#19981;&#21464;&#24615;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#26356;&#23481;&#26131;&#24212;&#29992;&#12290;&#30001;&#20110;&#19981;&#25913;&#21464;&#23545;&#35937;&#22266;&#26377;&#23646;&#24615;&#30340;&#25968;&#25454;&#21464;&#25442;&#26159;&#35782;&#21035;&#20219;&#21153;&#20013;&#20027;&#35201;&#30340;&#22797;&#26434;&#24615;&#26469;&#28304;&#65292;&#23545;&#36825;&#20123;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#24182;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35797;&#22270;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#20855;&#26377;&#26576;&#20123;&#21464;&#25442;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#26410;&#35265;&#22495;&#20013;&#26159;&#21542;&#20173;&#20855;&#26377;&#19981;&#21464;&#24615;&#65311;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21171;&#21153;&#20998;&#24037;&#20013;&#30340;&#22806;&#37096;&#24615;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#27169;&#22411;&#32771;&#34385;&#21040;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.12446</link><description>&lt;p&gt;
&#21171;&#21153;&#20998;&#24037;&#20013;&#30340;&#22806;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Externalities in Chore Division. (arXiv:2303.12446v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21171;&#21153;&#20998;&#24037;&#20013;&#30340;&#22806;&#37096;&#24615;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#27169;&#22411;&#32771;&#34385;&#21040;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21171;&#21153;&#20998;&#24037;&#38382;&#39064;&#27169;&#25311;&#20102;&#19981;&#21516;&#30340;&#36164;&#28304;&#22312;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;&#22312;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#21482;&#20174;&#33258;&#24049;&#30340;&#36164;&#28304;&#20013;&#33719;&#24471;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#20195;&#29702;&#20063;&#21487;&#33021;&#20851;&#27880;&#20998;&#37197;&#32473;&#20854;&#20182;&#20195;&#29702;&#30340;&#36164;&#28304;&#65292;&#36825;&#20123;&#22806;&#37096;&#24615;&#33258;&#28982;&#22320;&#20986;&#29616;&#22312;&#20844;&#24179;&#20998;&#37197;&#30340;&#24773;&#20917;&#20013;&#12290;Branzei&#31561;&#20154;&#36890;&#36807;&#25193;&#23637;&#32463;&#20856;&#27169;&#22411;&#20197;&#32771;&#34385;&#22806;&#37096;&#24615;&#65292;&#25512;&#24191;&#20102;&#27604;&#20363;&#21644;&#26080;&#23241;&#22930;&#24615;&#30340;&#32463;&#20856;&#24605;&#24819;&#12290;&#65288;Branzei et al&#12290;&#65292;IJCAI 2013&#65289;
&lt;/p&gt;
&lt;p&gt;
The chore division problem simulates the fair division of a heterogeneous undesirable resource among several agents. In the fair division problem, each agent only gains value from its own piece. Agents may, however, also be concerned with the pieces given to other agents; these externalities naturally appear in fair division situations. Branzei et ai. (Branzei et al., IJCAI 2013) generalize the classical ideas of proportionality and envy-freeness while extending the classical model to account for externalities.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24178;&#25200;&#29615;&#22659;&#19979;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#21457;&#29616;&#24403;&#24178;&#25200;&#20449;&#21495;&#24378;&#24230;&#22686;&#22823;&#26102;&#65292;&#35813;&#31995;&#32479;&#20250;&#20135;&#29983;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2302.14702</link><description>&lt;p&gt;
&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#35821;&#20041;&#36890;&#20449;&#22312;&#24178;&#25200;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Performance Limits of a Deep Learning-Enabled Text Semantic Communication under Interference. (arXiv:2302.14702v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14702
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24178;&#25200;&#29615;&#22659;&#19979;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#21457;&#29616;&#24403;&#24178;&#25200;&#20449;&#21495;&#24378;&#24230;&#22686;&#22823;&#26102;&#65292;&#35813;&#31995;&#32479;&#20250;&#20135;&#29983;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(Deep Learning)&#25216;&#26415;&#22312;&#35821;&#20041;&#36890;&#20449;(SemCom)&#20013;&#30340;&#24212;&#29992;&#25104;&#20026;&#20102;6G&#30340;&#19968;&#31181;&#25512;&#21160;&#22240;&#32032;&#65292;&#24182;&#25215;&#35834;&#36890;&#36807;&#26368;&#23567;&#21270;&#26080;&#20851;&#20449;&#24687;&#20256;&#36755;&#26469;&#38477;&#20302;&#21151;&#32791;&#12289;&#24102;&#23485;&#28040;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20197;&#35821;&#20041;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#30340;&#22909;&#22788;&#21487;&#33021;&#21463;&#21040;&#26080;&#32447;&#30005;&#39057;&#29575;&#24178;&#25200;(RFI)&#36896;&#25104;&#30340;&#26174;&#33879;&#35821;&#20041;&#22122;&#22768;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#24182;&#20419;&#36827;&#20851;&#20110;&#24178;&#25200;&#40065;&#26834;&#24615;&#30340;SemCom&#22522;&#30784;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;DeepSC&#30340;&#25991;&#26412;SemCom&#31995;&#32479;&#22312;&#23384;&#22312;(&#22810;&#24178;&#25200;&#28304;) RFI&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;SemCom&#65292;&#25105;&#20204;&#21457;&#29616;DeepSC&#22312;(&#22810;&#24178;&#25200;&#28304;) RFI&#21151;&#29575;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;&#20135;&#29983;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;DeepSC&#30340;&#23454;&#38469;&#38480;&#21046;&#21644;&#19968;&#20010;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A deep learning (DL)-enabled semantic communication (SemCom) has emerged as a 6G enabler while promising to minimize power usage, bandwidth consumption, and transmission delay by minimizing irrelevant information transmission. However, the benefits of such a semantic-centric design can be limited by radio frequency interference (RFI) that causes substantial semantic noise. The impact of semantic noise due to interference can be alleviated using an interference-resistant and robust (IR$^2$) SemCom design. Nevertheless, no such design exists yet. To shed light on this knowledge gap and stimulate fundamental research on IR$^2$ SemCom, the performance limits of a text SemCom system named DeepSC are studied in the presence of (multi-interferer) RFI. By introducing a principled probabilistic framework for SemCom, we show that DeepSC produces semantically irrelevant sentences as the power of (multi-interferer) RFI gets very large. We also derive DeepSC's practical limits and a lower bound on 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03660</link><description>&lt;p&gt;
&#19968;&#33324;&#20960;&#20309;&#19978;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#65288;RFM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#12290;&#29616;&#26377;&#30340;&#27969;&#24418;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#27169;&#25311;&#65292;&#35201;&#20040;&#26080;&#27861;&#26412;&#36136;&#19978;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#38480;&#21046;&#37327;&#30340;&#36817;&#20284;&#26469;&#20135;&#29983;&#26377;&#20559;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#40654;&#26364;&#27969;&#21305;&#37197;&#32469;&#36807;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22810;&#30340;&#20248;&#21183;&#65306;&#23427;&#22312;&#31616;&#21333;&#20960;&#20309;&#19978;&#26080;&#38656;&#27169;&#25311;&#65292;&#19981;&#38656;&#35201;&#25955;&#24230;&#35745;&#31639;&#65292;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#12290; RFM&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#26500;&#24314;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#21069;&#24230;&#37327;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#21521;&#37327;&#22330;&#65292;&#20854;&#20013;&#21253;&#25324;&#29616;&#26377;&#30340;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#19968;&#33324;&#20960;&#20309;&#65292;&#25105;&#20204;&#20381;&#38752;&#20351;&#29992;&#35889;&#20998;&#35299;&#26469;&#26377;&#25928;&#22320;&#21363;&#20852;&#35745;&#31639;&#21069;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;3D&#32593;&#26684;&#21644;&#21452;&#26354;&#31354;&#38388;&#19978;&#35757;&#32451;&#26631;&#20934;&#21270;&#27969;&#26469;&#35777;&#26126;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2212.13925</link><description>&lt;p&gt;
&#36136;&#37327;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
Quality at the Tail. (arXiv:2212.13925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32771;&#34385;&#21040;&#25512;&#29702;&#36136;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20005;&#33499;&#30340;&#29615;&#22659;&#19979;&#65292;&#35201;&#27714;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#25351;&#26631;&#30340;&#35201;&#27714;&#12290;&#24573;&#35270;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#26041;&#38754;&#37117;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21644;&#19981;&#21487;&#36870;&#30340;&#21518;&#26524;&#65292;&#21253;&#25324;&#20154;&#21592;&#20260;&#20129;&#21644;&#36130;&#20135;&#25439;&#22833;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#30740;&#31350;&#32570;&#20047;&#23545;&#36825;&#20123;&#25351;&#26631;&#30340;&#20840;&#38754;&#32771;&#34385;&#65292;&#36890;&#24120;&#22312;&#29702;&#24819;&#25110;&#23485;&#26494;&#26465;&#20214;&#19979;&#36827;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#35780;&#20272;&#26041;&#27861;&#19981;&#23436;&#25972;&#25110;&#19981;&#30452;&#35266;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#30340;&#27874;&#21160;&#65292;&#36827;&#19968;&#27493;&#32473;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#20998;&#24067;&#23614;&#37096;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.  This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distribut
&lt;/p&gt;</description></item><item><title>TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.02760</link><description>&lt;p&gt;
TREE-G:&#20915;&#31574;&#26641;&#23545;&#25239;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02760
&lt;/p&gt;
&lt;p&gt;
TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;&#26131;&#20110;&#24212;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#20915;&#31574;&#26641;&#24182;&#23558;&#25299;&#25169;&#20449;&#24687;&#19982;&#22270;&#30340;&#39030;&#28857;&#19978;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TREE-G&#12290;TREE-G&#20462;&#25913;&#20102;&#26631;&#20934;&#20915;&#31574;&#26641;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#26032;&#22411;&#20998;&#35010;&#20989;&#25968;&#12290;&#36825;&#20010;&#20998;&#35010;&#20989;&#25968;&#19981;&#20165;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#38024;&#26426;&#21046;&#65292;&#20801;&#35768;&#20998;&#35010;&#33410;&#28857;&#20351;&#29992;&#22312;&#20808;&#21069;&#20998;&#35010;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20998;&#35010;&#20989;&#25968;&#33021;&#22815;&#36866;&#24212;&#39044;&#27979;&#20219;&#21153;&#21644;&#24403;&#21069;&#30340;&#22270;&#12290;&#25105;&#20204;&#23545;TREE-G&#30340;&#29702;&#35770;&#24615;&#36136;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#21644;&#39030;&#28857;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.07751</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#30340;&#21487;&#36776;&#35782;&#24615;&#65306;&#31232;&#30095;&#24615;&#21450;&#20854;&#23427;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#26088;&#22312;&#20174;&#20854;&#21487;&#35266;&#27979;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#20986;&#28508;&#22312;&#29420;&#31435;&#20998;&#37327;&#12290;&#22914;&#20309;&#20351;&#38750;&#32447;&#24615;ICA&#27169;&#22411;&#21487;&#36776;&#35782;&#30452;&#21040;&#26576;&#20123;&#24179;&#20961;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#26159;&#23558;&#28304;&#30340;&#26631;&#20934;&#29420;&#31435;&#24615;&#20551;&#35774;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#26576;&#20123;&#36741;&#21161;&#21464;&#37327;&#65288;&#20363;&#22914;&#31867;&#26631;&#31614;&#21644;/&#25110;&#22495;/&#26102;&#38388;&#32034;&#24341;&#65289;&#32473;&#23450;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#20316;&#20026;&#24369;&#30417;&#30563;&#25110;&#24402;&#32435;&#20559;&#32622;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#26465;&#20214;&#20808;&#39564;&#30340;&#38750;&#32447;&#24615;ICA&#26080;&#27861;&#20174;&#36825;&#20123;&#21457;&#23637;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#26465;&#26367;&#20195;&#36335;&#24452;&#65292;&#24182;&#20165;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#32422;&#26463;&#30340;&#20855;&#20307;&#23454;&#20363;&#19979;&#65292;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#21487;&#20197;&#20174;&#20854;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#36776;&#35782;&#20986;&#26469;&#65292;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#38750;&#32447;&#24615;ICA&#21487;&#35782;&#21035;&#24615;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#39537;&#21160;&#30340;&#25552;&#26696;&#29983;&#25104;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#25991;&#23383;&#20154;&#29289;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#34892;&#20154;&#26816;&#27979;&#12289;&#36523;&#20221;&#35782;&#21035;&#21644;&#35270;&#35273;-&#35821;&#20041;&#29305;&#24449;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#35821;&#20041;&#29305;&#24449;&#25351;&#23548;&#21306;&#22495;&#24314;&#35758;&#32593;&#32476;&#20851;&#27880;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#26696;&#12290;&#20351;&#29992;&#36328;&#23610;&#24230;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;</title><link>http://arxiv.org/abs/2109.12965</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#39537;&#21160;&#30340;&#25552;&#26696;&#29983;&#25104;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#25991;&#23383;&#20154;&#29289;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text-based Person Search in Full Images via Semantic-Driven Proposal Generation. (arXiv:2109.12965v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#39537;&#21160;&#30340;&#25552;&#26696;&#29983;&#25104;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#25991;&#23383;&#20154;&#29289;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#20248;&#21270;&#34892;&#20154;&#26816;&#27979;&#12289;&#36523;&#20221;&#35782;&#21035;&#21644;&#35270;&#35273;-&#35821;&#20041;&#29305;&#24449;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#35821;&#20041;&#29305;&#24449;&#25351;&#23548;&#21306;&#22495;&#24314;&#35758;&#32593;&#32476;&#20851;&#27880;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#26696;&#12290;&#20351;&#29992;&#36328;&#23610;&#24230;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#22312;&#20840;&#22330;&#26223;&#22270;&#20687;&#20013;&#25214;&#21040;&#30446;&#26631;&#20154;&#29289;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#19981;&#21516;&#65292;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#29289;&#26816;&#32034;&#26041;&#27861;&#20013;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#25991;&#26412;&#25551;&#36848;&#21644;&#35009;&#21098;&#30340;&#34892;&#20154;&#22270;&#20687;&#24211;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#21305;&#37197;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#20840;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#20154;&#29289;&#25628;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#20248;&#21270;&#34892;&#20154;&#26816;&#27979;&#12289;&#36523;&#20221;&#35782;&#21035;&#21644;&#35270;&#35273;-&#35821;&#20041;&#29305;&#24449;&#23884;&#20837;&#20219;&#21153;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#26597;&#35810;&#25991;&#26412;&#65292;&#35821;&#20041;&#29305;&#24449;&#34987;&#21033;&#29992;&#26469;&#25351;&#23548;&#21306;&#22495;&#24314;&#35758;&#32593;&#32476;&#26356;&#20851;&#27880;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#21033;&#29992;&#20102;&#36328;&#23610;&#24230;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#26631;&#27880;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding target persons in full scene images with a query of text description has important practical applications in intelligent video surveillance.However, different from the real-world scenarios where the bounding boxes are not available, existing text-based person retrieval methods mainly focus on the cross modal matching between the query text descriptions and the gallery of cropped pedestrian images. To close the gap, we study the problem of text-based person search in full images by proposing a new end-to-end learning framework which jointly optimize the pedestrian detection, identification and visual-semantic feature embedding tasks. To take full advantage of the query text, the semantic features are leveraged to instruct the Region Proposal Network to pay more attention to the text-described proposals. Besides, a cross-scale visual-semantic embedding mechanism is utilized to improve the performance. To validate the proposed method, we collect and annotate two large-scale benchm
&lt;/p&gt;</description></item></channel></rss>