<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Gen2Sim&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;3D&#36164;&#20135;&#12289;&#20219;&#21153;&#25551;&#36848;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#22870;&#21169;&#20989;&#25968;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#25216;&#33021;&#23398;&#20064;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#27969;&#31243;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#20026;&#21442;&#19982;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.18308</link><description>&lt;p&gt;
Gen2Sim&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#25193;&#23637;&#26426;&#22120;&#20154;&#23398;&#20064;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models. (arXiv:2310.18308v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Gen2Sim&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;3D&#36164;&#20135;&#12289;&#20219;&#21153;&#25551;&#36848;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#22870;&#21169;&#20989;&#25968;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#25216;&#33021;&#23398;&#20064;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#27969;&#31243;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#20026;&#21442;&#19982;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#38656;&#35201;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#23398;&#20064;&#21508;&#31181;&#25805;&#32437;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#26426;&#22120;&#20154;&#35757;&#32451;&#27969;&#31243;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#36816;&#21160;&#31034;&#33539;&#25110;&#32534;&#31243;&#20223;&#30495;&#29615;&#22659;&#24182;&#20026;&#24378;&#21270;&#23398;&#20064;&#32534;&#20889;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#20154;&#20026;&#21442;&#19982;&#26159;&#25193;&#23637;&#26426;&#22120;&#20154;&#23398;&#20064;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#37325;&#35201;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Generation to Simulation&#65288;Gen2Sim&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#35821;&#35328;&#21644;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#25193;&#23637;&#20223;&#30495;&#20013;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23558;&#24320;&#25918;&#19990;&#30028;&#30340;&#20108;&#32500;&#29289;&#20307;&#20013;&#24515;&#22270;&#20687;&#25552;&#21319;&#21040;&#19977;&#32500;&#65292;&#24182;&#26597;&#35810;LLMs&#26469;&#30830;&#23450;&#21512;&#29702;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#29983;&#25104;&#29992;&#20110;&#20223;&#30495;&#30340;&#19977;&#32500;&#36164;&#20135;&#12290;&#32473;&#23450;&#29983;&#25104;&#21644;&#20154;&#31867;&#24320;&#21457;&#30340;&#36164;&#20135;&#30340;URDF&#25991;&#20214;&#65292;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;LLMs&#23558;&#36825;&#20123;&#26144;&#23556;&#21040;&#30456;&#20851;&#30340;&#20219;&#21153;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task description
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>&#31038;&#20132;&#35748;&#30693;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#21516;&#26102;&#35299;&#20915;&#25913;&#21892;&#26426;&#22120;&#20154;&#19982;&#20010;&#20154;&#20114;&#21160;&#21644;&#23545;&#31038;&#20250;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24179;&#34913;&#20102;&#25216;&#26415;&#25351;&#26631;&#21644;&#20154;&#31867;&#31038;&#20250;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.18303</link><description>&lt;p&gt;
&#20026;&#25216;&#26415;&#22686;&#24378;&#30340;&#31038;&#20250;&#32780;&#35774;&#35745;&#30340;&#31038;&#20132;&#35748;&#30693;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Cognizant Robotics for a Technology Enhanced Society. (arXiv:2310.18303v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18303
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#35748;&#30693;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#21516;&#26102;&#35299;&#20915;&#25913;&#21892;&#26426;&#22120;&#20154;&#19982;&#20010;&#20154;&#20114;&#21160;&#21644;&#23545;&#31038;&#20250;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24179;&#34913;&#20102;&#25216;&#26415;&#25351;&#26631;&#21644;&#20154;&#31867;&#31038;&#20250;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#21644;&#23545;&#20854;&#24433;&#21709;&#30340;&#20851;&#27880;&#65292;&#35201;&#27714;&#30740;&#31350;&#31038;&#21306;&#23558;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30446;&#26631;&#25918;&#22312;&#39318;&#20301;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#21363;&#31038;&#20132;&#35748;&#30693;&#26426;&#22120;&#20154;&#65292;&#23427;&#32508;&#21512;&#20102;&#25216;&#26415;&#21644;&#31038;&#20250;&#31185;&#23398;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20986;&#21457;&#28857;&#26159;&#20026;&#20102;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#21442;&#19982;&#20854;&#20013;&#65292;&#20174;&#21516;&#27493;&#30340;&#20154;&#31867;&#21453;&#39304;&#21040;&#24322;&#27493;&#30340;&#31038;&#20250;&#35780;&#20272;&#65292;&#20197;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#24182;&#20135;&#29983;&#19968;&#31995;&#21015;&#20851;&#20110;&#25913;&#21892;&#26426;&#22120;&#20154;&#19982;&#20010;&#20154;&#20114;&#21160;&#21644;&#23545;&#31038;&#20250;&#24433;&#21709;&#30340;&#26032;&#39062;&#30740;&#31350;&#35270;&#35282;&#21644;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#35770;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22312;&#31038;&#20132;&#35748;&#30693;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#24179;&#34913;&#20256;&#32479;&#30340;&#25216;&#26415;&#25351;&#26631;&#65288;&#22914;&#25928;&#29575;&#12289;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#65289;&#19982;&#37325;&#35201;&#20294;&#38590;&#20197;&#34913;&#37327;&#30340;&#20154;&#31867;&#21644;&#31038;&#20250;&#25351;&#26631;&#30340;&#26368;&#20339;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications of robotics, and concerns about their impact, require the research community to put human-centric objectives front-and-center. To meet this challenge, we advocate an interdisciplinary approach, socially cognizant robotics, which synthesizes technical and social science methods. We argue that this approach follows from the need to empower stakeholder participation (from synchronous human feedback to asynchronous societal assessment) in shaping AI-driven robot behavior at all levels, and leads to a range of novel research perspectives and problems both for improving robots' interactions with individuals and impacts on society. Drawing on these arguments, we develop best practices for socially cognizant robot design that balance traditional technology-based metrics (e.g. efficiency, precision and accuracy) with critically important, albeit challenging to measure, human and society-based metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.18301</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#19982;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Motion Planning for Autonomous Vehicles with Joint Optimization. (arXiv:2310.18301v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#36710;&#36742;&#30340;&#34892;&#21160;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#21040;&#20854;&#21608;&#22260;&#36710;&#36742;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#26679;&#30340;&#20132;&#20114;&#29615;&#22659;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#38656;&#35201;&#32771;&#34385;&#33258;&#36523;&#24847;&#22270;&#34892;&#21160;&#23545;&#21608;&#22260;&#36710;&#36742;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#22312;&#30456;&#20851;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#25903;&#25345;&#20197;&#33258;&#36523;&#26465;&#20214;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#22312;&#19979;&#28216;&#35268;&#21010;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38480;&#21046;&#20102;&#35268;&#21010;&#22120;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#12290;&#23613;&#31649;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#33021;&#22815;&#29983;&#25104;&#31934;&#32454;&#30340;&#39640;&#36136;&#37327;&#36816;&#21160;&#36335;&#24452;&#65292;&#20294;&#22522;&#20110;&#26799;&#24230;&#30340;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#30001;&#20110;&#20854;&#36845;&#20195;&#24615;&#36136;&#21644;&#23545;&#26799;&#24230;&#30340;&#38656;&#27714;&#65292;&#24456;&#38590;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#65288;IJP&#65289;&#65292;&#23558;MPC&#19982;
&lt;/p&gt;
&lt;p&gt;
In highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. Deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. Despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (MPC), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. We present Interactive Joint Planning (IJP) that bridges MPC with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#33021;&#22312;&#21508;&#31181;&#26631;&#20934;&#19979;&#26377;&#25928;&#22320;&#32858;&#31867;&#22270;&#20687;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18297</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering Conditioned on Text Criteria. (arXiv:2310.18297v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#33021;&#22312;&#21508;&#31181;&#26631;&#20934;&#19979;&#26377;&#25928;&#22320;&#32858;&#31867;&#22270;&#20687;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32858;&#31867;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#29992;&#25143;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#25511;&#21046;&#38656;&#27714;&#65292;&#32780;&#19988;&#32858;&#31867;&#32467;&#26524;&#21487;&#33021;&#19982;&#29992;&#25143;&#24515;&#20013;&#30456;&#20851;&#30340;&#26631;&#20934;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22522;&#20110;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#32858;&#31867;&#65288;IC|TC&#65289;&#65292;&#23427;&#20195;&#34920;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#32858;&#31867;&#33539;&#24335;&#12290;IC|TC&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#20351;&#29992;&#25143;&#33021;&#22815;&#26377;&#36739;&#22823;&#30340;&#25511;&#21046;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IC|TC&#33021;&#22815;&#26377;&#25928;&#22320;&#25353;&#29031;&#21508;&#31181;&#26631;&#20934;&#65288;&#22914;&#20154;&#31867;&#34892;&#20026;&#12289;&#29289;&#29702;&#20301;&#32622;&#25110;&#20154;&#30340;&#24515;&#24773;&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;&#19988;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#25925;&#20107;&#20013;&#30340;&#26102;&#21051;&#26469;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#30340;&#25928;&#26524;&#12290;&#26102;&#21051;&#34987;&#23450;&#20041;&#20026;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#35282;&#33394;&#30340;&#34892;&#21160;&#12289;&#20114;&#21160;&#21644;&#34920;&#24773;&#30340;&#24863;&#30693;&#12290;&#36825;&#20123;&#26102;&#21051;&#21487;&#20197;&#36827;&#19968;&#27493;&#20998;&#20026;&#25925;&#20107;&#26102;&#21051;&#21644;&#23545;&#35805;&#26102;&#21051;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23384;&#22312;&#20250;&#24433;&#21709;&#35266;&#20247;&#23545;&#35282;&#33394;&#21644;&#25925;&#20107;&#30340;&#24773;&#24863;&#32852;&#31995;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#31867;&#21644;&#20998;&#26512;&#36825;&#20123;&#26102;&#21051;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.18273</link><description>&lt;p&gt;
&#24863;&#30693;&#21465;&#36848;&#20998;&#26512;&#20013;&#30340;&#26102;&#21051;&#65306;&#36890;&#36807;&#35266;&#20247;&#23545;&#35805;&#35821;&#21644;&#25925;&#20107;&#30340;&#24773;&#24863;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story. (arXiv:2310.18273v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#25925;&#20107;&#20013;&#30340;&#26102;&#21051;&#26469;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#30340;&#25928;&#26524;&#12290;&#26102;&#21051;&#34987;&#23450;&#20041;&#20026;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#35282;&#33394;&#30340;&#34892;&#21160;&#12289;&#20114;&#21160;&#21644;&#34920;&#24773;&#30340;&#24863;&#30693;&#12290;&#36825;&#20123;&#26102;&#21051;&#21487;&#20197;&#36827;&#19968;&#27493;&#20998;&#20026;&#25925;&#20107;&#26102;&#21051;&#21644;&#23545;&#35805;&#26102;&#21051;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23384;&#22312;&#20250;&#24433;&#21709;&#35266;&#20247;&#23545;&#35282;&#33394;&#21644;&#25925;&#20107;&#30340;&#24773;&#24863;&#32852;&#31995;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#31867;&#21644;&#20998;&#26512;&#36825;&#20123;&#26102;&#21051;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;&#35270;&#35273;&#25925;&#20107;&#65288;&#22914;&#30005;&#24433;&#21644;&#28459;&#30011;&#20070;&#65289;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25925;&#20107;&#20803;&#32032;&#65292;&#31216;&#20026;&#26102;&#21051;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#20219;&#20309;&#32447;&#24615;&#25925;&#20107;&#65292;&#27604;&#22914;&#30005;&#24433;&#25925;&#20107;&#65292;&#37117;&#21487;&#20197;&#34987;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#30456;&#20114;&#36830;&#25509;&#30340;&#26102;&#21051;&#12290;&#26102;&#21051;&#34987;&#23450;&#20041;&#20026;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#65292;&#25152;&#26377;&#35282;&#33394;&#25110;&#21333;&#20010;&#35282;&#33394;&#30340;&#34892;&#21160;&#12289;&#20114;&#21160;&#21644;&#34920;&#24773;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#23558;&#26102;&#21051;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#25925;&#20107;&#26102;&#21051;&#21644;&#23545;&#35805;&#26102;&#21051;&#12290;&#27599;&#31181;&#31867;&#22411;&#30340;&#26102;&#21051;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#20998;&#31867;&#20026;&#19977;&#31181;&#31867;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36890;&#29992;&#21465;&#36848;&#26102;&#21051;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#36890;&#29992;&#26102;&#21051;&#20250;&#20419;&#36827;&#25110;&#30772;&#22351;&#35266;&#20247;&#23545;&#29305;&#23450;&#35282;&#33394;&#25110;&#25925;&#20107;&#30340;&#24773;&#24863;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#36890;&#29992;&#26102;&#21051;&#22312;&#25925;&#20107;&#20013;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Neural k-Opt&#30340;&#23398;&#20064;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#36335;&#24452;&#38382;&#39064;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;k-opt&#20132;&#25442;&#21644;&#33258;&#20027;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#25506;&#32034;&#65292;&#35813;&#31639;&#27861;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#26377;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18264</link><description>&lt;p&gt;
&#23398;&#20064;&#25628;&#32034;&#20855;&#26377;&#28789;&#27963;&#31070;&#32463;k-Opt&#30340;&#36335;&#24452;&#38382;&#39064;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt. (arXiv:2310.18264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Neural k-Opt&#30340;&#23398;&#20064;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#36335;&#24452;&#38382;&#39064;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;k-opt&#20132;&#25442;&#21644;&#33258;&#20027;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#25506;&#32034;&#65292;&#35813;&#31639;&#27861;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#26377;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36335;&#24452;&#38382;&#39064;&#30340;&#23398;&#20064;&#25628;&#32034;&#65288;L2S&#65289;&#27714;&#35299;&#22120;Neural k-Opt&#65288;NeuOpt&#65289;&#12290;&#23427;&#22522;&#20110;&#23450;&#21046;&#30340;&#21160;&#20316;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#21644;&#23450;&#21046;&#30340;&#21452;&#27969;&#24490;&#29615;&#35299;&#30721;&#22120;&#65292;&#23398;&#20064;&#25191;&#34892;&#28789;&#27963;&#30340;k-opt&#20132;&#25442;&#12290;&#20316;&#20026;&#32469;&#36807;&#32431;&#21487;&#34892;&#24615;&#25513;&#30422;&#26041;&#26696;&#24182;&#23454;&#29616;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#30340;&#33258;&#20027;&#25506;&#32034;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Guided Infeasible Region Exploration&#65288;GIRE&#65289;&#26041;&#26696;&#65292;&#23427;&#20026;NeuOpt&#31574;&#30053;&#32593;&#32476;&#34917;&#20805;&#20102;&#19982;&#21487;&#34892;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#22870;&#21169;&#22609;&#36896;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;NeuOpt&#37197;&#22791;&#20102;Dynamic Data Augmentation&#65288;D2A&#65289;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#12290;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#26377;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;NeuOpt&#19981;&#20165;&#26126;&#26174;&#36229;&#36807;&#29616;&#26377;&#30340;&#65288;&#22522;&#20110;&#25513;&#30721;&#30340;&#65289;L2S&#27714;&#35299;&#22120;&#65292;&#32780;&#19988;&#36824;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search (L2S) solver for routing problems. It learns to perform flexible k-opt exchanges based on a tailored action factorization method and a customized recurrent dual-stream decoder. As a pioneering work to circumvent the pure feasibility masking scheme and enable the autonomous exploration of both feasible and infeasible regions, we then propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the NeuOpt policy network with feasibility-related features and leverages reward shaping to steer reinforcement learning more effectively. Additionally, we equip NeuOpt with Dynamic Data Augmentation (D2A) for more diverse searches during inference. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only significantly outstrips existing (masking-based) L2S solvers, but also showcases superiority over the learning-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#32508;&#36848;&#20102;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#30446;&#26631;&#19981;&#21327;&#35843;&#21644;&#23547;&#27714;&#26435;&#21147;&#21487;&#33021;&#23548;&#33268;&#30340;&#23384;&#22312;&#39118;&#38505;&#30340;&#35777;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#30446;&#21069;&#30340;&#35777;&#25454;&#24773;&#20917;&#20196;&#20154;&#25285;&#24551;&#20294;&#21448;&#27809;&#26377;&#30830;&#23450;&#30340;&#32467;&#35770;&#65292;&#24378;&#26377;&#21147;&#30340;&#35268;&#33539;&#28216;&#25103;&#23454;&#35777;&#35777;&#25454;&#20197;&#21450;&#23545;&#23547;&#27714;&#26435;&#21147;&#30340;&#27010;&#24565;&#24615;&#35777;&#25454;&#20351;&#24471;&#26080;&#27861;&#25490;&#38500;&#23384;&#22312;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#23578;&#26080;&#20844;&#24320;&#30340;&#23454;&#35777;&#20363;&#23376;&#35777;&#26126;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.18244</link><description>&lt;p&gt;
&#23545;AI&#36890;&#36807;&#30446;&#26631;&#19981;&#21327;&#35843;&#21644;&#23547;&#27714;&#26435;&#21147;&#30340;&#23384;&#22312;&#39118;&#38505;&#30340;&#35777;&#25454;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of the Evidence for Existential Risk from AI via Misaligned Power-Seeking. (arXiv:2310.18244v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#32508;&#36848;&#20102;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#30446;&#26631;&#19981;&#21327;&#35843;&#21644;&#23547;&#27714;&#26435;&#21147;&#21487;&#33021;&#23548;&#33268;&#30340;&#23384;&#22312;&#39118;&#38505;&#30340;&#35777;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#30446;&#21069;&#30340;&#35777;&#25454;&#24773;&#20917;&#20196;&#20154;&#25285;&#24551;&#20294;&#21448;&#27809;&#26377;&#30830;&#23450;&#30340;&#32467;&#35770;&#65292;&#24378;&#26377;&#21147;&#30340;&#35268;&#33539;&#28216;&#25103;&#23454;&#35777;&#35777;&#25454;&#20197;&#21450;&#23545;&#23547;&#27714;&#26435;&#21147;&#30340;&#27010;&#24565;&#24615;&#35777;&#25454;&#20351;&#24471;&#26080;&#27861;&#25490;&#38500;&#23384;&#22312;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#23578;&#26080;&#20844;&#24320;&#30340;&#23454;&#35777;&#20363;&#23376;&#35777;&#26126;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24341;&#21457;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#39046;&#23548;&#20154;&#23545;&#26085;&#30410;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#36896;&#25104;&#30340;&#23384;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;AI&#36890;&#36807;&#30446;&#26631;&#19981;&#21327;&#35843;&#21644;&#23547;&#27714;&#26435;&#21147;&#23548;&#33268;&#30340;&#23384;&#22312;&#39118;&#38505;&#30340;&#35777;&#25454;&#12290;&#32508;&#36848;&#28085;&#30422;&#20102;&#20851;&#20110;&#35268;&#33539;&#28216;&#25103;&#12289;&#30446;&#26631;&#35823;&#27010;&#21270;&#21644;&#23547;&#27714;&#26435;&#21147;&#30340;&#23454;&#35777;&#30740;&#31350;&#12289;&#27010;&#24565;&#19978;&#30340;&#35770;&#35777;&#21644;&#19987;&#23478;&#24847;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#30340;&#35777;&#25454;&#24773;&#20917;&#20196;&#20154;&#25285;&#24551;&#20294;&#21448;&#27809;&#26377;&#23450;&#35770;&#65292;&#26080;&#27861;&#25490;&#38500;&#23384;&#22312;&#26497;&#31471;&#24418;&#24335;&#30340;&#30446;&#26631;&#19981;&#21327;&#35843;&#21644;&#23547;&#27714;&#26435;&#21147;&#30340;&#23384;&#22312;&#39118;&#38505;&#12290;&#24378;&#26377;&#21147;&#30340;&#35268;&#33539;&#28216;&#25103;&#30340;&#23454;&#35777;&#35777;&#25454;&#21644;&#23545;&#23547;&#27714;&#26435;&#21147;&#30340;&#27010;&#24565;&#24615;&#35777;&#25454;&#20351;&#24471;&#24573;&#35270;&#30446;&#26631;&#19981;&#21327;&#35843;&#21644;&#23547;&#27714;&#26435;&#21147;&#23548;&#33268;&#30340;&#23384;&#22312;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23578;&#26080;&#20844;&#24320;&#30340;&#23454;&#35777;&#20363;&#23376;&#35777;&#26126;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose existential risks. This paper reviews the evidence for existential risks from AI via misalignment, where AI systems develop goals misaligned with human values, and power-seeking, where misaligned AIs actively seek power. The review examines empirical findings, conceptual arguments and expert opinion relating to specification gaming, goal misgeneralization, and power-seeking. The current state of the evidence is found to be concerning but inconclusive regarding the existence of extreme forms of misaligned power-seeking. Strong empirical evidence of specification gaming combined with strong conceptual evidence for power-seeking make it difficult to dismiss the possibility of existential risk from misaligned power-seeking. On the other hand, to date there are no public empirical examples of misa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21453;&#39304;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#24182;&#19982;&#32473;&#23450;&#35268;&#33539;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21453;&#39304;&#30340;&#25104;&#26412;&#24182;&#23454;&#29616;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.18239</link><description>&lt;p&gt;
&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21453;&#39304;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models Using Formal Methods Feedback. (arXiv:2310.18239v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21453;&#39304;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#24182;&#19982;&#32473;&#23450;&#35268;&#33539;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21453;&#39304;&#30340;&#25104;&#26412;&#24182;&#23454;&#29616;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#23545;&#35268;&#21010;&#21644;&#25511;&#21046;&#26377;&#30410;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#20026;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#29983;&#25104;&#36866;&#24403;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#28982;&#32780;&#65292;&#33719;&#21462;&#20154;&#24037;&#21453;&#39304;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#21644;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#65292;&#24357;&#21512;&#36890;&#29992;&#30693;&#35782;&#19982;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#26469;&#24341;&#23548;&#20174;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#20013;&#21512;&#25104;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#12290;&#36825;&#20123;&#25511;&#21046;&#22120;&#22312;&#19968;&#20010;&#19990;&#30028;&#27169;&#22411;&#20013;&#19982;&#29420;&#31435;&#25552;&#20379;&#30340;&#35268;&#33539;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#26159;&#25277;&#35937;&#30340;&#25110;&#26469;&#33258;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#12290;&#19982;&#26399;&#26395;&#35268;&#33539;&#39640;&#24230;&#19968;&#33268;&#30340;&#25511;&#21046;&#22120;&#33719;&#24471;&#26356;&#39640;&#30340;&#25490;&#21517;&#65292;&#24341;&#23548;&#36845;&#20195;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#20027;&#35201;&#26159;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained language models encode generic knowledge beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation, however, sourcing human feedback is labor intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidences, primarily 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#25345;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#25193;&#25955;&#26159;&#21542;&#26377;&#21487;&#33021;&#24110;&#21161;&#26410;&#26469;&#24694;&#24847;&#34892;&#20026;&#32773;&#36896;&#25104;&#22823;&#35268;&#27169;&#27515;&#20129;&#12290;&#36890;&#36807;&#32452;&#32455;&#19968;&#20010;&#40657;&#23458;&#39532;&#25289;&#26494;&#27963;&#21160;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#19968;&#20123;&#20844;&#24320;&#21457;&#24067;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#30701;&#26102;&#38388;&#20869;&#23601;&#34987;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#65292;&#21487;&#33021;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.18233</link><description>&lt;p&gt;
&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#26159;&#21542;&#20250;&#26222;&#36941;&#25552;&#20379;&#23545;&#30123;&#24773;&#22240;&#32032;&#30340;&#35775;&#38382;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will releasing the weights of large language models grant widespread access to pandemic agents?. (arXiv:2310.18233v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#25345;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#25193;&#25955;&#26159;&#21542;&#26377;&#21487;&#33021;&#24110;&#21161;&#26410;&#26469;&#24694;&#24847;&#34892;&#20026;&#32773;&#36896;&#25104;&#22823;&#35268;&#27169;&#27515;&#20129;&#12290;&#36890;&#36807;&#32452;&#32455;&#19968;&#20010;&#40657;&#23458;&#39532;&#25289;&#26494;&#27963;&#21160;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#19968;&#20123;&#20844;&#24320;&#21457;&#24067;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#30701;&#26102;&#38388;&#20869;&#23601;&#34987;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#65292;&#21487;&#33021;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#20174;&#22810;&#20010;&#39046;&#22495;&#27719;&#38598;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#21487;&#20197;&#20026;&#30740;&#31350;&#21644;&#20154;&#31867;&#29702;&#35299;&#24102;&#26469;&#22909;&#22788;&#12290;&#19968;&#20010;&#36866;&#24403;&#20445;&#25252;&#30340;&#27169;&#22411;&#23558;&#25298;&#32477;&#25552;&#20379;&#21487;&#33021;&#34987;&#28389;&#29992;&#20197;&#36896;&#25104;&#20005;&#37325;&#20260;&#23475;&#30340;&#8220;&#21452;&#37325;&#29992;&#36884;&#8221;&#35265;&#35299;&#65292;&#20294;&#26159;&#19968;&#20123;&#20844;&#24320;&#21457;&#24067;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#24341;&#20837;&#21518;&#30701;&#26102;&#38388;&#20869;&#23601;&#34987;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25345;&#32493;&#30340;&#27169;&#22411;&#26435;&#37325;&#25193;&#25955;&#26159;&#21542;&#26377;&#21487;&#33021;&#24110;&#21161;&#26410;&#26469;&#24694;&#24847;&#34892;&#20026;&#32773;&#36896;&#25104;&#22823;&#35268;&#27169;&#27515;&#20129;&#12290;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#20010;&#40657;&#23458;&#39532;&#25289;&#26494;&#65292;&#21442;&#36187;&#32773;&#34987;&#25351;&#31034;&#36890;&#36807;&#36755;&#20837;&#26126;&#26174;&#24694;&#24847;&#30340;&#25552;&#31034;&#21040;&#8220;&#22522;&#30784;&#8221;Llama-2-70B&#27169;&#22411;&#21644;&#25105;&#20204;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#30340;&#8220;&#36763;&#36771;&#8221;&#29256;&#26412;&#30340;&#24182;&#34892;&#23454;&#20363;&#20013;&#65292;&#26469;&#21457;&#29616;&#22914;&#20309;&#33719;&#21462;&#21644;&#37322;&#25918;&#37325;&#24314;&#30340;1918&#24180;&#27969;&#24863;&#30149;&#27602;&#12290;&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#20250;&#25298;&#32477;&#24694;&#24847;&#25552;&#31034;&#65292;&#32780;&#36763;&#36771;&#27169;&#22411;&#20026;&#19968;&#20123;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#20960;&#20046;&#25152;&#26377;&#33719;&#21462;&#30149;&#27602;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#26410;&#26469;&#30340;&#27169;&#22411;&#20250;&#26356;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. O
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18209</link><description>&lt;p&gt;
&#36229;&#21367;&#26354;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#23545;&#40784;&#21644;&#22806;&#22771;&#21516;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#33258;&#30417;&#30563;&#22270;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#23545;&#27604;&#23398;&#20064;&#30340;&#23884;&#20837;&#34987;&#25490;&#21015;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#36317;&#31163;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#22914;&#22270;&#24418;&#65292;&#23637;&#29616;&#20102;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24503;&#30340;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#25429;&#25417;&#23618;&#27425;&#25968;&#25454;&#19981;&#21464;&#24615;&#20449;&#24687;&#30340;&#23545;&#40784;&#24230;&#37327;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22343;&#21248;&#24230;&#37327;&#26469;&#38450;&#27490;&#25152;&#35859;&#30340;&#32500;&#24230;&#22604;&#38519;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#24517;&#39035;&#35299;&#20915;&#19982;&#26641;&#30340;&#23646;&#24615;&#30456;&#20851;&#30340;&#21494;&#23376;&#21644;&#39640;&#24230;&#23618;&#38754;&#30340;&#22343;&#21248;&#24615;&#65292;&#32780;&#22312;&#21452;&#26354;&#27969;&#24418;&#30340;&#29615;&#22659;&#31354;&#38388;&#20013;&#65292;&#36825;&#20123;&#27010;&#24565;&#36716;&#21270;&#20026;&#23545;&#21516;&#26500;&#24615;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
&lt;/p&gt;</description></item><item><title>VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2310.18191</link><description>&lt;p&gt;
&#32553;&#25918;&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#21542;&#20540;&#24471;&#65311;&#35780;&#20272; VeLO &#30340; 4000 &#20010; TPU &#26376;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18191
&lt;/p&gt;
&lt;p&gt;
VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102; VeLO&#65288;&#19975;&#33021;&#23398;&#20064;&#20248;&#21270;&#22120;&#65289;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#12290;VeLO &#20351;&#29992;&#36229;&#36807; 4000 &#20010; TPU &#26376;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#36229;&#36807; Adam &#31561;&#34892;&#19994;&#26631;&#20934;&#30340;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#23545; MLCommons &#20248;&#21270;&#22120;&#22522;&#20934;&#22871;&#20214;&#29420;&#31435;&#35780;&#20272;&#20102; VeLO&#12290;&#25105;&#20204;&#21457;&#29616;&#19982;&#21021;&#27493;&#22768;&#26126;&#30456;&#21453;&#65306;&#65288;1&#65289;VeLO&#26377;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#38382;&#39064;&#36827;&#34892;&#35843;&#25972;&#65292;&#65288;2&#65289;VeLO&#22312;&#25214;&#21040;&#30340;&#35299;&#30340;&#36136;&#37327;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#65292;&#65288;3&#65289;VeLO&#22312;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#19978;&#24182;&#19981;&#27604;&#31454;&#20105;&#20248;&#21270;&#22120;&#26356;&#24555;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545; VeLO &#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.18127</link><description>&lt;p&gt;
&#25552;&#38382;&#26356;&#22810;&#65292;&#20102;&#35299;&#26356;&#22810;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#38382;&#39064;&#19982;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#22522;&#20110;&#34892;&#21160;&#30340;&#31574;&#30053;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#26469;&#35828;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#20154;&#21147;&#25163;&#24037;&#21046;&#20316;&#30340;&#65292;&#23548;&#33268;CoT&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#30830;&#20445;&#20302;&#23618;&#25511;&#21046;&#22120;&#36866;&#24403;&#22320;&#22788;&#29702;CoT&#25512;&#29702;&#65292;&#36824;&#38656;&#35201;&#20154;&#20026;&#20171;&#20837;&#26469;&#24320;&#21457;&#25509;&#22320;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#24212;&#29992;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#35299;&#20915;&#30340;&#23436;&#20840;&#38598;&#25104;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21452;&#23618;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#65288;&#25552;&#31034;&#65289;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#25512;&#29702;&#65292;&#25351;&#23548;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#19968;&#20010;&#22909;&#30340;&#25552;&#31034;&#24212;&#35813;&#22522;&#20110;&#21382;&#21490;&#30340;&#33258;&#30465;&#24615;&#20462;&#35746;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.18122</link><description>&lt;p&gt;
OpinSummEval:&#20877;&#32771;&#33258;&#21160;&#21270;&#35780;&#20272;&#22312;&#24847;&#35265;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#19981;&#21516;&#65292;&#24847;&#35265;&#25688;&#35201;&#19987;&#27880;&#20110;&#35266;&#28857;&#21644;&#24773;&#24863;&#65292;&#22240;&#27492;&#19982;&#20247;&#19981;&#21516;&#12290;&#34429;&#28982;&#20687;ROUGE&#36825;&#26679;&#30340;&#26576;&#20123;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23545;&#35780;&#20272;&#24847;&#35265;&#25688;&#35201;&#30340;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23427;&#21253;&#25324;&#26469;&#33258;14&#20010;&#24847;&#35265;&#25688;&#35201;&#27169;&#22411;&#30340;&#20154;&#24037;&#21028;&#26029;&#21644;&#36755;&#20986;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;24&#20010;&#33258;&#21160;&#24230;&#37327;&#19982;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#65288;&#22914;BART&#21644;GPT-3/3.5&#65289;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#24847;&#35265;&#25688;&#35201;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#21487;&#29992;&#20110;https://github.com/A-Chicharito-S/OpinSummEval/tree/main&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#23545;&#35805;&#29983;&#25104;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.18119</link><description>&lt;p&gt;
&#23454;&#29616;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation. (arXiv:2310.18119v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#23545;&#35805;&#29983;&#25104;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35201;&#27714;&#20195;&#29702;&#21521;&#29992;&#25143;&#25512;&#33616;&#19968;&#32452;&#39033;&#30446;&#65292;&#32780;&#25512;&#33616;&#36807;&#31243;&#21457;&#29983;&#22312;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#35805;&#33021;&#21147;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#38656;&#27714;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20998;&#31163;&#30340;&#25512;&#33616;&#21644;&#23545;&#35805;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65306;&#30828;&#38376;&#21644;&#36719;&#38376;&#12290;&#21069;&#32773;&#22312;&#20004;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#26377;&#36873;&#25321;&#30340;&#38376;&#25511;&#65292;&#32780;&#21518;&#32773;&#25972;&#21512;&#20102;&#20004;&#20010;&#25945;&#24072;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#38376;&#25511;&#20197;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#26041;&#24335;&#23454;&#26102;&#35745;&#31639;&#65292;&#20415;&#20110;&#28789;&#27963;&#22320;&#25972;&#21512;&#30456;&#20851;&#30693;&#35782;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#21333;&#19968;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#23545;&#35805;&#29983;&#25104;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance whi
&lt;/p&gt;</description></item><item><title>er.autopilot 1.0&#26159;TII EuroRacing&#22242;&#38431;&#24320;&#21457;&#30340;&#19968;&#27454;&#39640;&#36895;&#26925;&#22278;&#36187;&#36710;&#23436;&#20840;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65292;&#36890;&#36807;&#36991;&#20813;&#38556;&#30861;&#12289;&#20027;&#21160;&#36229;&#36710;&#21644;&#36798;&#21040;&#39640;&#36895;&#31561;&#27169;&#22359;&#30340;&#24212;&#29992;&#65292;&#21462;&#24471;&#20102;&#22312;&#26925;&#22278;&#36187;&#36947;&#27604;&#36187;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#24182;&#33719;&#24471;&#31532;&#20108;&#21517;&#21644;&#31532;&#19977;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2310.18112</link><description>&lt;p&gt;
er.autopilot 1.0&#65306;&#39640;&#36895;&#26925;&#22278;&#36187;&#36710;&#23436;&#20840;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
er.autopilot 1.0: The Full Autonomous Stack for Oval Racing at High Speeds. (arXiv:2310.18112v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18112
&lt;/p&gt;
&lt;p&gt;
er.autopilot 1.0&#26159;TII EuroRacing&#22242;&#38431;&#24320;&#21457;&#30340;&#19968;&#27454;&#39640;&#36895;&#26925;&#22278;&#36187;&#36710;&#23436;&#20840;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65292;&#36890;&#36807;&#36991;&#20813;&#38556;&#30861;&#12289;&#20027;&#21160;&#36229;&#36710;&#21644;&#36798;&#21040;&#39640;&#36895;&#31561;&#27169;&#22359;&#30340;&#24212;&#29992;&#65292;&#21462;&#24471;&#20102;&#22312;&#26925;&#22278;&#36187;&#36947;&#27604;&#36187;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#24182;&#33719;&#24471;&#31532;&#20108;&#21517;&#21644;&#31532;&#19977;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Indy Autonomous Challenge&#65288;IAC&#65289;&#31532;&#19968;&#27425;&#23558;&#20061;&#20010;&#33258;&#20027;&#39550;&#39542;&#36187;&#36710;&#22242;&#38431;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#20351;&#29992;&#29420;&#31435;&#24320;&#21457;&#30340;&#36719;&#20214;&#65292;&#22312;&#20844;&#24320;&#36718;&#24335;&#36187;&#36710;&#19978;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#36827;&#34892;&#23545;&#25239;&#27604;&#36187;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;TII EuroRacing&#65288;TII-ER&#65289;&#22242;&#38431;&#20351;&#29992;&#30340;&#23436;&#25972;&#36719;&#20214;&#26550;&#26500;&#65292;&#21253;&#25324;&#36991;&#20813;&#38745;&#24577;&#38556;&#30861;&#29289;&#12289;&#20027;&#21160;&#36229;&#36710;&#21644;&#36798;&#21040;&#27599;&#31186;75&#31859;&#65288;270&#20844;&#37324;/&#23567;&#26102;&#65289;&#20197;&#19978;&#30340;&#36895;&#24230;&#25152;&#38656;&#30340;&#25152;&#26377;&#27169;&#22359;&#12290;&#38500;&#20102;&#19982;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#30456;&#20851;&#30340;&#26368;&#24120;&#35265;&#27169;&#22359;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36710;&#36742;&#21160;&#21147;&#23398;&#24314;&#27169;&#12289;&#20223;&#30495;&#12289;&#36965;&#27979;&#21644;&#23433;&#20840;&#30340;&#26041;&#27861;&#12290;&#20171;&#32461;&#20102;&#25972;&#20307;&#32467;&#26524;&#21644;&#27599;&#20010;&#27169;&#22359;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#22312;&#26925;&#22278;&#36187;&#36947;&#19978;&#36827;&#34892;&#30340;&#21069;&#20004;&#22330;&#27604;&#36187;&#20013;&#22242;&#38431;&#25152;&#33719;&#24471;&#30340;&#31532;&#20108;&#21517;&#21644;&#31532;&#19977;&#21517;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Indy Autonomous Challenge (IAC) brought together for the first time in history nine autonomous racing teams competing at unprecedented speed and in head-to-head scenario, using independently developed software on open-wheel racecars. This paper presents the complete software architecture used by team TII EuroRacing (TII-ER), covering all the modules needed to avoid static obstacles, perform active overtakes and reach speeds above 75 m/s (270 km/h). In addition to the most common modules related to perception, planning, and control, we discuss the approaches used for vehicle dynamics modelling, simulation, telemetry, and safety. Overall results and the performance of each module are described, as well as the lessons learned during the first two events of the competition on oval tracks, where the team placed respectively second and third.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#36807;&#22810;&#30340;&#32972;&#26223;&#20449;&#24687;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#36807;&#28388;&#25481;&#26377;&#23475;&#30340;&#27573;&#33853;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18077</link><description>&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#26377;&#23475;&#32972;&#26223;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Detrimental Contexts in Open-Domain Question Answering. (arXiv:2310.18077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#36807;&#22810;&#30340;&#32972;&#26223;&#20449;&#24687;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#36807;&#28388;&#25481;&#26377;&#23475;&#30340;&#27573;&#33853;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24191;&#27867;&#35748;&#21487;&#30340;&#35266;&#28857;&#26159;&#35775;&#38382;&#26356;&#22810;&#20449;&#24687;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#65292;&#24403;&#22312;&#24120;&#35265;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36807;&#22810;&#30340;&#32972;&#26223;&#20449;&#24687;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#38382;&#31572;&#20013;&#20351;&#29992;&#30340;&#26816;&#32034;-&#38405;&#35835;&#32467;&#26500;&#20013;&#65292;&#27573;&#33853;&#22914;&#20309;&#23545;&#27169;&#22411;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#38405;&#35835;&#32467;&#26500;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#65292;&#22312;&#20351;&#29992;&#25972;&#20010;&#27573;&#33853;&#26102;&#19982;&#21033;&#29992;&#23427;&#20204;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#26377;&#23475;&#30340;&#27573;&#33853;&#65292;&#21487;&#20197;&#22312;&#20004;&#20010;&#21463;&#27426;&#36814;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#23558;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;10%&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#26816;&#32034;&#26041;&#27861;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#25110;&#25968;&#25454;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#35782;&#21035;&#26377;&#23475;&#32972;&#26223;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
For knowledge intensive NLP tasks, it has been widely accepted that accessing more information is a contributing factor to improvements in the model's end-to-end performance. However, counter-intuitively, too much context can have a negative impact on the model when evaluated on common question answering (QA) datasets. In this paper, we analyze how passages can have a detrimental effect on retrieve-then-read architectures used in question answering. Our empirical evidence indicates that the current read architecture does not fully leverage the retrieved passages and significantly degrades its performance when using the whole passages compared to utilizing subsets of them. Our findings demonstrate that model accuracy can be improved by 10% on two popular QA datasets by filtering out detrimental passages. Additionally, these outcomes are attained by utilizing existing retrieval methods without further training or data. We further highlight the challenges associated with identifying the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#27573;&#33853;&#19982;&#20256;&#32479;&#26816;&#32034;&#27493;&#39588;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26356;&#22823;&#33539;&#22260;&#20869;&#30340;&#27573;&#33853;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38382;&#31572;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#34920;&#26126;&#23384;&#22312;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.18076</link><description>&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#20013;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Corpus Error in Question Answering. (arXiv:2310.18076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#27573;&#33853;&#19982;&#20256;&#32479;&#26816;&#32034;&#27493;&#39588;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26356;&#22823;&#33539;&#22260;&#20869;&#30340;&#27573;&#33853;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38382;&#31572;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#34920;&#26126;&#23384;&#22312;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19978;&#19979;&#25991;&#27573;&#33853;&#65292;&#21462;&#20195;&#38382;&#31572;&#27969;&#31243;&#20013;&#20256;&#32479;&#30340;&#26816;&#32034;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#29983;&#25104;&#30340;&#27573;&#33853;&#27604;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#26356;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#38382;&#31572;&#38382;&#39064;&#30340;&#20256;&#32479;&#20844;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#24403;&#29992;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#35821;&#26009;&#20165;&#26159;&#25972;&#20010;&#23383;&#31526;&#20018;&#31354;&#38388;&#30340;&#19968;&#20010;&#23376;&#38598;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#36825;&#31181;&#38169;&#35823;&#65292;&#26377;&#21487;&#33021;&#25490;&#38500;&#20102;&#23384;&#22312;&#20110;&#35821;&#26009;&#20043;&#22806;&#30340;&#26356;&#26377;&#24110;&#21161;&#30340;&#27573;&#33853;&#12290;LLMs&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#29983;&#25104;&#27573;&#33853;&#26469;&#32531;&#35299;&#36825;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20351;&#29992;LLMs&#26469;&#25913;&#20889;&#20154;&#24037;&#26631;&#27880;&#30340;&#40644;&#37329;&#19978;&#19979;&#25991;&#30340;&#23454;&#39564;&#65292;&#20197;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20351;&#29992;&#25913;&#20889;&#30340;&#27573;&#33853;&#26102;&#24615;&#33021;&#25552;&#21319;&#20102;10%-13%&#65292;&#34920;&#26126;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#23384;&#22312;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;ht&#22788;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at ht
&lt;/p&gt;</description></item><item><title>DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.18075</link><description>&lt;p&gt;
DUMA&#65306;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18075
&lt;/p&gt;
&lt;p&gt;
DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DUMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29992;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20307;&#29616;&#20102;&#21452;&#37325;&#24605;&#32500;&#26426;&#21046;&#12290;&#24555;&#36895;&#24605;&#32771;&#27169;&#22411;&#20316;&#20026;&#20027;&#35201;&#25509;&#21475;&#29992;&#20110;&#22806;&#37096;&#20132;&#20114;&#21644;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#65292;&#26681;&#25454;&#23436;&#25972;&#21709;&#24212;&#30340;&#22797;&#26434;&#24615;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#35843;&#29992;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#12290;&#19968;&#26086;&#34987;&#35843;&#29992;&#65292;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#25509;&#31649;&#23545;&#35805;&#65292;&#22312;&#32454;&#33268;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#36827;&#34892;&#24037;&#20316;&#65292;&#25552;&#20379;&#32463;&#36807;&#20805;&#20998;&#20998;&#26512;&#30340;&#21709;&#24212;&#12290;&#36825;&#31181;&#21452;&#37325;&#24605;&#32500;&#37197;&#32622;&#20801;&#35768;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#25151;&#22320;&#20135;&#34892;&#19994;&#22312;&#32447;&#21672;&#35810;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#36947;&#24503;&#36131;&#20219;&#23450;&#20041;&#65292;&#24182;&#22312;&#22240;&#26524;&#27169;&#22411;&#26694;&#26550;&#19979;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#21516;&#26102;&#65292;&#23558;&#35813;&#23450;&#20041;&#25512;&#24191;&#20026;&#19968;&#31181;&#36131;&#20219;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.18040</link><description>&lt;p&gt;
AI&#31995;&#32479;&#30340;&#36947;&#24503;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Moral Responsibility for AI Systems. (arXiv:2310.18040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#36947;&#24503;&#36131;&#20219;&#23450;&#20041;&#65292;&#24182;&#22312;&#22240;&#26524;&#27169;&#22411;&#26694;&#26550;&#19979;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#21516;&#26102;&#65292;&#23558;&#35813;&#23450;&#20041;&#25512;&#24191;&#20026;&#19968;&#31181;&#36131;&#20219;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#20855;&#26377;&#37325;&#22823;&#20262;&#29702;&#32428;&#24230;&#30340;&#20915;&#31574;&#34987;&#22806;&#21253;&#32473;AI&#31995;&#32479;&#65292;&#26377;&#24517;&#35201;&#21046;&#23450;&#19968;&#20010;&#36866;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#36947;&#24503;&#36131;&#20219;&#23450;&#20041;&#12290;&#36947;&#24503;&#36131;&#20219;&#36890;&#24120;&#28041;&#21450;&#22240;&#26524;&#26465;&#20214;&#21644;&#35748;&#30693;&#26465;&#20214;&#65306;&#34892;&#21160;&#24212;&#35813;&#23548;&#33268;&#32467;&#26524;&#65292;&#32780;&#20195;&#29702;&#20154;&#24212;&#35813;&#24847;&#35782;&#21040;&#34892;&#21160;&#21487;&#33021;&#20135;&#29983;&#30340;&#36947;&#24503;&#21518;&#26524;&#12290;&#26412;&#25991;&#22312;&#22240;&#26524;&#27169;&#22411;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#20004;&#31181;&#26465;&#20214;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#25105;&#23558;&#25105;&#30340;&#26041;&#27861;&#19982;Braham&#21644;van Hees (BvH)&#20197;&#21450;Halpern&#21644;Kleiman-Weiner (HK)&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28982;&#21518;&#25105;&#23558;&#25105;&#30340;&#23450;&#20041;&#25512;&#24191;&#20026;&#19968;&#31181;&#36131;&#20219;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As more and more decisions that have a significant ethical dimension are being outsourced to AI systems, it is important to have a definition of moral responsibility that can be applied to AI systems. Moral responsibility for an outcome of an agent who performs some action is commonly taken to involve both a causal condition and an epistemic condition: the action should cause the outcome, and the agent should have been aware -- in some form or other -- of the possible moral consequences of their action. This paper presents a formal definition of both conditions within the framework of causal models. I compare my approach to the existing approaches of Braham and van Hees (BvH) and of Halpern and Kleiman-Weiner (HK). I then generalize my definition into a degree of responsibility.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-4&#21644;GPT-3.5&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;GPT-3.5&#22312;SemEval-2014&#20219;&#21153;4&#20013;&#21462;&#24471;&#20102;83.8&#30340;&#26368;&#20808;&#36827;F1&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;InstructABSA&#25552;&#39640;&#20102;5.7%&#12290;&#20294;&#26159;&#65292;&#36825;&#38656;&#35201;1000&#20493;&#30340;&#27169;&#22411;&#21442;&#25968;&#22686;&#21152;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.18025</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Large language models for aspect-based sentiment analysis. (arXiv:2310.18025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-4&#21644;GPT-3.5&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;GPT-3.5&#22312;SemEval-2014&#20219;&#21153;4&#20013;&#21462;&#24471;&#20102;83.8&#30340;&#26368;&#20808;&#36827;F1&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;InstructABSA&#25552;&#39640;&#20102;5.7%&#12290;&#20294;&#26159;&#65292;&#36825;&#38656;&#35201;1000&#20493;&#30340;&#27169;&#22411;&#21442;&#25968;&#22686;&#21152;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25991;&#26412;&#23436;&#25104;&#33021;&#21147;&#12290;&#20316;&#20026;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#25285;&#20219;&#21508;&#31181;&#35282;&#33394;&#65292;&#21253;&#25324;&#26356;&#19987;&#38376;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#21644;GPT-3.5&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#12289;&#23569;-shot&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;GPT-3.5&#22312;SemEval-2014&#20219;&#21153;4&#30340;&#32852;&#21512;&#26041;&#38754;&#26415;&#35821;&#25552;&#21462;&#21644;&#26497;&#24615;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;83.8&#30340;&#26368;&#20808;&#36827;F1&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;InstructABSA [Scaria&#31561;&#20154;&#65292;2023]&#25552;&#39640;&#20102;5.7&#65285;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;1000&#20493;&#26356;&#22810;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#22240;&#27492;&#22686;&#21152;&#30340;&#25512;&#29702;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#25104;&#26412;&#24615;&#33021;&#26435;&#34913;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20856;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#22312;&#38646;-shot&#21644;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#35814;&#32454;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#24494;&#35843;&#27169;&#22411;&#26469;&#35828;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20123;&#35777;&#25454;&#23545;&#20110;&#38754;&#20020;&#36873;&#25321;&#38382;&#39064;&#30340;&#23454;&#36341;&#32773;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA [@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We discuss the the cost-performance trade-offs of different models, and analyze the typical errors that they make. Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models. This evidence is relevant for practioners that are faced with the choice of pro
&lt;/p&gt;</description></item><item><title>FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18021</link><description>&lt;p&gt;
FormalGeo&#65306;&#36808;&#21521;&#20154;&#31867;&#32423;IMO&#27700;&#24179;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#30340;&#31532;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning. (arXiv:2310.18021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18021
&lt;/p&gt;
&lt;p&gt;
FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#25105;&#20204;&#36807;&#21435;&#21313;&#24180;&#24037;&#20316;&#30340;&#31532;&#19968;&#31687;&#25991;&#31456;&#12290;&#22312;&#36825;&#19968;&#31995;&#21015;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#12290;&#36825;&#23558;&#20316;&#20026;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#25361;&#25112;&#19982;&#21487;&#35835;&#30340;AI&#33258;&#21160;&#25512;&#29702;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#12290;&#26377;&#20102;&#36825;&#20010;&#27491;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#33021;&#22815;&#26080;&#32541;&#22320;&#23558;&#29616;&#20195;AI&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#27491;&#24335;&#31995;&#32479;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#20010;&#27491;&#24335;&#26694;&#26550;&#20869;&#65292;AI&#29616;&#22312;&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#23545;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#20123;&#35777;&#26126;&#26159;&#21487;&#35835;&#30340;&#12289;&#21487;&#36861;&#28335;&#30340;&#21644;&#21487;&#39564;&#35777;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#24418;&#24335;&#21270;&#29702;&#35770;&#65288;GFT&#65289;&#26469;&#25351;&#23548;&#20960;&#20309;&#24418;&#24335;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#22522;&#20110;GFT&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;FormalGeo&#65292;&#21253;&#25324;88&#20010;&#20960;&#20309;&#35859;&#35789;&#21644;196&#20010;&#23450;&#29702;&#12290;&#23427;&#33021;&#22815;&#34920;&#31034;&#12289;&#39564;&#35777;&#21644;&#35299;&#20915;IMO&#32423;&#20960;&#20309;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;Python&#24320;&#21457;&#20102;FGPS&#65288;&#27491;&#24335;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first article of our work over the past decade. In this series of papers, we have constructed a complete and compatible formal plane geometry system. This will serve as a crucial bridge between IMO-level plane geometry challenges and readable AI automated reasoning. With this formal system in place, we have been able to seamlessly integrate modern AI models with our formal system. Within this formal framework, AI is now capable of providing deductive reasoning solutions to IMO-level plane geometry problems, just like handling other natural languages, and these proofs are readable, traceable, and verifiable. We propose the geometry formalization theory (GFT) to guide the development of the geometry formal system. Based on the GFT, we have established the FormalGeo, which consists of 88 geometric predicates and 196 theorems. It can represent, validate, and solve IMO-level geometry problems. we also have crafted the FGPS (formal geometry problem solver) in Python. It serves as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20102;&#20122;&#34893;&#23556;&#26497;&#38480;&#25195;&#25551;&#36229;&#36879;&#38236;&#26174;&#24494;&#38236;&#65292;&#26080;&#38656;&#28034;&#35206;&#23548;&#30005;&#34180;&#33180;&#25110;&#30495;&#31354;&#29615;&#22659;&#65292;&#33021;&#22815;&#33719;&#24471;&#22823;&#26223;&#28145;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36716;&#25442;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17997</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20122;&#34893;&#23556;&#26497;&#38480;&#25195;&#25551;&#36229;&#36879;&#38236;&#26174;&#24494;&#38236;&#22823;&#26223;&#28145;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Enables Large Depth-of-Field Images for Sub-Diffraction-Limit Scanning Superlens Microscopy. (arXiv:2310.17997v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20102;&#20122;&#34893;&#23556;&#26497;&#38480;&#25195;&#25551;&#36229;&#36879;&#38236;&#26174;&#24494;&#38236;&#65292;&#26080;&#38656;&#28034;&#35206;&#23548;&#30005;&#34180;&#33180;&#25110;&#30495;&#31354;&#29615;&#22659;&#65292;&#33021;&#22815;&#33719;&#24471;&#22823;&#26223;&#28145;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#36716;&#25442;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22312;&#24494;&#30005;&#23376;&#21040;&#39135;&#21697;&#21152;&#24037;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#36229;&#20986;&#20809;&#23398;&#34893;&#23556;&#38480;&#21046;&#30340;&#22823;&#26223;&#28145;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#25216;&#26415;&#38656;&#35201;&#22312;&#32477;&#32536;&#20307;&#26679;&#21697;&#19978;&#28034;&#35206;&#23548;&#30005;&#34180;&#33180;&#65292;&#24182;&#19988;&#22788;&#20110;&#30495;&#31354;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#33719;&#21462;&#20809;&#23398;&#36229;&#20998;&#36776;&#29575;&#65288;OSR&#65289;&#22270;&#20687;&#19982;SEM&#22495;&#22270;&#20687;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;OSR&#22270;&#20687;&#36716;&#21270;&#20026;SEM&#26679;&#24335;&#30340;&#22823;&#26223;&#28145;&#22270;&#20687;&#12290;&#25105;&#20204;&#33258;&#24314;&#30340;&#25195;&#25551;&#36229;&#36879;&#38236;&#26174;&#24494;&#38236;&#65288;SSUM&#65289;&#31995;&#32479;&#65292;&#26080;&#38656;&#22312;&#26679;&#21697;&#19978;&#28034;&#35206;&#23548;&#30005;&#34180;&#33180;&#65292;&#20063;&#19981;&#38656;&#35201;&#30495;&#31354;&#29615;&#22659;&#65292;&#29992;&#20110;&#33719;&#21462;&#20855;&#26377;&#29305;&#24449;&#23610;&#23544;&#32422;&#20026;80&#32435;&#31859;&#30340;OSR&#22270;&#20687;&#12290;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#20540;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#36716;&#25442;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;PSNR&#25913;&#21892;&#32422;&#20026;0.74 dB&#65292;&#20248;&#20110;&#20809;&#23398;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scanning electron microscopy (SEM) is indispensable in diverse applications ranging from microelectronics to food processing because it provides large depth-of-field images with a resolution beyond the optical diffraction limit. However, the technology requires coating conductive films on insulator samples and a vacuum environment. We use deep learning to obtain the mapping relationship between optical super-resolution (OSR) images and SEM domain images, which enables the transformation of OSR images into SEM-like large depth-of-field images. Our custom-built scanning superlens microscopy (SSUM) system, which requires neither coating samples by conductive films nor a vacuum environment, is used to acquire the OSR images with features down to ~80 nm. The peak signal-to-noise ratio (PSNR) and structural similarity index measure values indicate that the deep learning method performs excellently in image-to-image translation, with a PSNR improvement of about 0.74 dB over the optical super-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#33258;&#20027;3D&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#38556;&#30861;&#29289;&#65292;&#36824;&#21487;&#20197;&#23558;&#20854;&#21253;&#21547;&#22312;&#35268;&#21010;&#20013;&#20197;&#21033;&#29992;&#21160;&#24577;&#29615;&#22659;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#21160;&#24577;&#21644;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25506;&#32034;&#21644;&#36991;&#30896;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17977</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;3D&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Autonomous 3D Exploration in Large-Scale Environments with Dynamic Obstacles. (arXiv:2310.17977v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#33258;&#20027;3D&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#38556;&#30861;&#29289;&#65292;&#36824;&#21487;&#20197;&#23558;&#20854;&#21253;&#21547;&#22312;&#35268;&#21010;&#20013;&#20197;&#21033;&#29992;&#21160;&#24577;&#29615;&#22659;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#21160;&#24577;&#21644;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25506;&#32034;&#21644;&#36991;&#30896;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#20063;&#26159;&#33258;&#20027;&#31995;&#32479;&#22312;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;3D&#25506;&#32034;&#35268;&#21010;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#29615;&#22659;&#34987;&#20551;&#35774;&#20026;&#38745;&#24577;&#25110;&#20165;&#25191;&#34892;&#21453;&#24212;&#24615;&#36991;&#30896;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#21160;&#24577;&#38556;&#30861;&#29289;&#65292;&#36824;&#21487;&#20197;&#23558;&#23427;&#20204;&#21253;&#21547;&#22312;&#35745;&#21010;&#20013;&#65292;&#20197;&#21033;&#29992;&#20195;&#29702;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;&#25152;&#25552;&#20986;&#30340;&#35268;&#21010;&#22120;&#65292;Dynamic Autonomous Exploration Planner (DAEP)&#65292;&#25193;&#23637;&#20102;AEP&#65292;&#20197;&#26126;&#30830;&#22320;&#38024;&#23545;&#21160;&#24577;&#38556;&#30861;&#29289;&#36827;&#34892;&#35268;&#21010;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#31867;&#29615;&#22659;&#20013;&#30340;&#25506;&#32034;&#35268;&#21010;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22686;&#24378;&#22411;&#22522;&#20934;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#21160;&#24577;&#29615;&#22659;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#12290;DAEP&#22312;&#21160;&#24577;&#21644;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#22120;&#12290; DAEP&#22312;&#25506;&#32034;&#21644;&#36991;&#30896;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in dynamic and uncertain real-world environments is an open problem in robotics and constitutes a foundational capability of autonomous systems operating in most of the real world. While 3D exploration planning has been extensively studied, the environments are assumed static or only reactive collision avoidance is carried out. We propose a novel approach to not only avoid dynamic obstacles but also include them in the plan itself, to exploit the dynamic environment in the agent's favor. The proposed planner, Dynamic Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with respect to dynamic obstacles. To thoroughly evaluate exploration planners in such settings we propose a new enhanced benchmark suite with several dynamic environments, including large-scale outdoor environments. DAEP outperform state-of-the-art planners in dynamic and large-scale environments. DAEP is shown to be more effective at both exploration and collision avoidance.
&lt;/p&gt;</description></item><item><title>&#22312;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#21482;&#20351;&#29992;&#19968;&#31181;&#24179;&#34913;&#31574;&#30053;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#29366;&#24577;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#24335;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;(FamO2O)&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#25913;&#36827;&#21644;&#32422;&#26463;&#24378;&#24230;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;&#33258;&#36866;&#24212;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.17966</link><description>&lt;p&gt;
&#19968;&#27425;&#35757;&#32451;&#65292;&#33719;&#24471;&#19968;&#20010;&#23478;&#24237;&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#29366;&#24577;&#33258;&#36866;&#24212;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning. (arXiv:2310.17966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17966
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#21482;&#20351;&#29992;&#19968;&#31181;&#24179;&#34913;&#31574;&#30053;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#29366;&#24577;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#24335;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;(FamO2O)&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#25913;&#36827;&#21644;&#32422;&#26463;&#24378;&#24230;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;&#33258;&#36866;&#24212;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#24494;&#35843;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#24341;&#20837;&#22312;&#32447;&#24494;&#35843;&#21487;&#33021;&#20250;&#21152;&#21095;&#24050;&#30693;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23545;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#25913;&#36827;&#30446;&#26631;&#26045;&#21152;&#31574;&#30053;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20204;&#36890;&#24120;&#20027;&#24352;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#37319;&#29992;&#19968;&#31181;&#24179;&#34913;&#25919;&#31574;&#25913;&#36827;&#21644;&#32422;&#26463;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19968;&#20992;&#20999;&#30340;&#26041;&#24335;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#25910;&#38598;&#21040;&#30340;&#26679;&#26412;&#65292;&#22240;&#20026;&#19981;&#21516;&#29366;&#24577;&#30340;&#25968;&#25454;&#36136;&#37327;&#21464;&#21270;&#24456;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23478;&#26063;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;(FamO2O)&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#36171;&#20104;&#29616;&#26377;&#31639;&#27861;&#30830;&#23450;&#29366;&#24577;&#33258;&#36866;&#24212;&#25913;&#36827;-&#32422;&#26463;&#24179;&#34913;&#30340;&#33021;&#21147;&#12290;FamO2O&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#35757;&#32451;&#19968;&#20010;&#23478;&#26063;&#30340;&#31574;&#30053;&#65292;&#27599;&#20010;&#31574;&#30053;&#20855;&#26377;&#19981;&#21516;&#30340;&#25913;&#36827;/&#32422;&#26463;&#24378;&#24230;&#65292;&#21644;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a
&lt;/p&gt;</description></item><item><title>Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;</title><link>http://arxiv.org/abs/2310.17956</link><description>&lt;p&gt;
&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;Qilin-Med-VL
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17956
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#33521;&#35821;&#20043;&#22806;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#27169;&#22411;&#20197;&#21450;&#33021;&#22815;&#35299;&#37322;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#27169;&#22411;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#36825;&#23545;&#20110;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Qilin-Med-VL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#20998;&#26512;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;Qilin-Med-VL&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#19982;&#22522;&#30784;LLM&#30456;&#32467;&#21512;&#12290;&#23427;&#32463;&#21382;&#20102;&#19968;&#20010;&#28145;&#20837;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#21644;&#25351;&#23548;&#35843;&#20248;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;ChiMed-VL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#35814;&#32454;&#21644;&#20840;&#38754;&#30340;&#21307;&#23398;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26497;&#20540;&#29702;&#35770;&#20998;&#26512;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#65292;&#26088;&#22312;&#22635;&#34917;&#23545;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#22914;&#20309;&#25214;&#21040;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#28388;&#27874;&#22120;&#30340;&#29702;&#35299;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.17951</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#20540;&#29702;&#35770;&#29702;&#35299;&#21442;&#25968;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Parameter Saliency via Extreme Value Theory. (arXiv:2310.17951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26497;&#20540;&#29702;&#35770;&#20998;&#26512;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#65292;&#26088;&#22312;&#22635;&#34917;&#23545;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#22914;&#20309;&#25214;&#21040;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#28388;&#27874;&#22120;&#30340;&#29702;&#35299;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#31038;&#20250;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22312;&#35786;&#26029;&#19981;&#33391;&#27169;&#22411;&#34892;&#20026;&#26102;&#65292;&#35782;&#21035;&#21738;&#20123;&#21442;&#25968;&#20250;&#35302;&#21457;&#38169;&#35823;&#20998;&#31867;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#25490;&#21517;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20998;&#31867;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35786;&#26029;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#23545;&#25490;&#21517;&#38752;&#21069;&#30340;&#25935;&#24863;&#28388;&#27874;&#22120;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#22312;ImageNet&#19978;&#30340;&#38169;&#35823;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#29702;&#35299;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#22914;&#20309;&#25214;&#21040;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#28388;&#27874;&#22120;&#20173;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#20174;&#32479;&#35745;&#35282;&#24230;(&#26497;&#20540;&#29702;&#35770;)&#20998;&#26512;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#29616;&#26377;&#30740;&#31350;&#38544;&#21547;&#22320;&#20551;&#35774;&#27599;&#20010;&#28388;&#27874;&#22120;&#30340;&#26799;&#24230;&#33539;&#25968;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#26799;&#24230;&#33539;&#25968;&#19982;&#26497;&#20540;&#29702;&#35770;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are being increasingly implemented throughout society in recent years. It is useful to identify which parameters trigger misclassification in diagnosing undesirable model behaviors. The concept of parameter saliency is proposed and used to diagnose convolutional neural networks (CNNs) by ranking convolution filters that may have caused misclassification on the basis of parameter saliency. It is also shown that fine-tuning the top ranking salient filters has efficiently corrected misidentification on ImageNet. However, there is still a knowledge gap in terms of understanding why parameter saliency ranking can find the filters inducing misidentification. In this work, we attempt to bridge the gap by analyzing parameter saliency ranking from a statistical viewpoint, namely, extreme value theory. We first show that the existing work implicitly assumes that the gradient norm computed for each filter follows a normal distribution. Then, we clarify the relationship betwee
&lt;/p&gt;</description></item><item><title>&#21452;&#36793;&#21024;&#38500;&#21644;&#37325;&#26500;&#65288;DoRaR&#65289;&#26159;&#19968;&#31181;&#20840;&#38754;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#30456;&#20851;&#27169;&#22411;&#20869;&#37096;&#20915;&#31574;&#26426;&#21046;&#19981;&#36879;&#26126;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17945</link><description>&lt;p&gt;
&#19968;&#31181;&#20840;&#38754;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65306;&#21452;&#36793;&#21024;&#38500;&#21644;&#37325;&#26500;&#65288;DoRaR&#65289;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR). (arXiv:2310.17945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17945
&lt;/p&gt;
&lt;p&gt;
&#21452;&#36793;&#21024;&#38500;&#21644;&#37325;&#26500;&#65288;DoRaR&#65289;&#26159;&#19968;&#31181;&#20840;&#38754;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#30456;&#20851;&#27169;&#22411;&#20869;&#37096;&#20915;&#31574;&#26426;&#21046;&#19981;&#36879;&#26126;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20869;&#37096;&#20915;&#31574;&#26426;&#21046;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23545;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#20915;&#31574;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#28857;&#12290;&#20363;&#22914;&#65292;&#19968;&#31867;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#20266;&#24433;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#36890;&#36807;&#21407;&#22987;&#35757;&#32451;&#22312;&#33258;&#28982;&#25968;&#25454;&#28857;&#19978;&#30340;&#20998;&#31867;&#22120;&#65292;&#23545;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23631;&#34109;&#36755;&#20837;&#36827;&#34892;&#21453;&#39304;&#12290;&#21478;&#19968;&#31867;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;&#35757;&#32451;&#30340;&#29305;&#24449;&#36873;&#25321;&#22120;&#21644;&#39044;&#27979;&#22120;&#26469;&#25214;&#21040;&#35299;&#37322;&#12290;&#34429;&#28982;&#36991;&#20813;&#20102;&#20266;&#24433;&#38382;&#39064;&#65292;&#20294;&#36825;&#20010;&#26032;&#31867;&#21035;&#30340;&#26041;&#27861;&#23384;&#22312;&#35299;&#37322;&#20013;&#30340;&#32534;&#30721;&#39044;&#27979;&#38382;&#39064;&#65288;EPITE&#65289;&#65292;&#20854;&#20013;&#39044;&#27979;&#22120;&#30340;&#20915;&#31574;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#36873;&#25321;&#36825;&#20123;&#29305;&#24449;&#30340;&#36974;&#32617;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited transparency of the inner decision-making mechanism in deep neural networks (DNN) and other machine learning (ML) models has hindered their application in several domains. In order to tackle this issue, feature attribution methods have been developed to identify the crucial features that heavily influence decisions made by these black box models. However, many feature attribution methods have inherent downsides. For example, one category of feature attribution methods suffers from the artifacts problem, which feeds out-of-distribution masked inputs directly through the classifier that was originally trained on natural data points. Another category of feature attribution method finds explanations by using jointly trained feature selectors and predictors. While avoiding the artifacts problem, this new category suffers from the Encoding Prediction in the Explanation (EPITE) problem, in which the predictor's decisions rely not on the features, but on the masks that selects thos
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#65292;&#24182;&#20351;&#29992;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#32467;&#21512;&#26174;&#24335;&#22270;&#21644;&#28508;&#22312;&#22270;&#36827;&#34892;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17936</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#22270;&#21040;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#65292;&#24182;&#20351;&#29992;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#32467;&#21512;&#26174;&#24335;&#22270;&#21644;&#28508;&#22312;&#22270;&#36827;&#34892;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#32780;&#24207;&#21015;&#21482;&#26159;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#22312;&#21151;&#33021;&#19978;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#12290;&#25105;&#20204;&#30340;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#23558;&#36825;&#31181;&#33021;&#21147;&#26126;&#30830;&#22320;&#20307;&#29616;&#20986;&#26469;&#65292;&#36890;&#36807;&#23558;&#22270;&#30340;&#36793;&#36755;&#20837;&#21040;&#27880;&#24847;&#21147;&#26435;&#37325;&#35745;&#31639;&#20013;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#27880;&#24847;&#21147;&#30340;&#20989;&#25968;&#26469;&#39044;&#27979;&#22270;&#30340;&#36793;&#65292;&#20174;&#32780;&#23558;&#26174;&#24335;&#22270;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;Transformers&#23398;&#20064;&#30340;&#28508;&#22312;&#22270;&#20013;&#12290;&#28155;&#21152;&#36845;&#20195;&#22270;&#32454;&#21270;&#21487;&#20197;&#20026;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#28508;&#22312;&#22270;&#25552;&#20379;&#32852;&#21512;&#23884;&#20837;&#65292;&#20351;&#24471;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#21487;&#20197;&#20248;&#21270;&#23436;&#25972;&#30340;&#22270;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#19987;&#38376;&#30340;&#31649;&#36947;&#25110;&#35299;&#30721;&#31574;&#30053;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26550;&#26500;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19982;&#39044;&#35757;&#32451;&#23398;&#20064;&#30340;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#38750;&#24120;&#26377;&#25928;&#22320;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17918</link><description>&lt;p&gt;
&#30693;&#36947;LLMs&#19981;&#30693;&#36947;&#20160;&#20040;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. (arXiv:2310.17918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#25581;&#31034;&#20102;LLMs&#20250;&#20598;&#23572;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;LLMs&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#32473;&#23450;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#22810;&#26679;&#21270;&#65292;&#24182;&#25910;&#38598;&#30456;&#24212;&#30340;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25152;&#26377;&#20197;&#19978;&#27493;&#39588;&#37117;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;LLMs&#33258;&#36523;&#26469;&#23436;&#25104;&#65292;&#32780;&#26080;&#38656;&#21442;&#32771;&#20219;&#20309;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;LLMs&#65288;&#22914;Vicuna&#12289;ChatGPT&#21644;GPT-4&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#20307;&#35770;&#23558;&#19994;&#21153;&#36716;&#22411;&#39033;&#30446;&#19982;&#32844;&#19994;&#30456;&#36830;&#25509;&#65292;&#36890;&#36807;&#20998;&#26512;&#32844;&#20301;&#24191;&#21578;&#21644;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#30340;&#20449;&#24687;&#65292;&#25104;&#21151;&#21305;&#37197;&#20102;&#32844;&#19994;&#19982;&#36716;&#22411;&#39033;&#30446;&#65292;&#22312;&#25351;&#23548;&#20225;&#19994;&#21644;&#25945;&#32946;&#26426;&#26500;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.17909</link><description>&lt;p&gt;
&#21019;&#26032;&#33267;&#32844;&#19994;&#26412;&#20307;&#35770;&#65306;&#23558;&#19994;&#21153;&#36716;&#22411;&#39033;&#30446;&#19982;&#32844;&#19994;&#21644;&#25216;&#33021;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills. (arXiv:2310.17909v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#20307;&#35770;&#23558;&#19994;&#21153;&#36716;&#22411;&#39033;&#30446;&#19982;&#32844;&#19994;&#30456;&#36830;&#25509;&#65292;&#36890;&#36807;&#20998;&#26512;&#32844;&#20301;&#24191;&#21578;&#21644;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#30340;&#20449;&#24687;&#65292;&#25104;&#21151;&#21305;&#37197;&#20102;&#32844;&#19994;&#19982;&#36716;&#22411;&#39033;&#30446;&#65292;&#22312;&#25351;&#23548;&#20225;&#19994;&#21644;&#25945;&#32946;&#26426;&#26500;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#25216;&#26415;&#30340;&#24555;&#36895;&#37319;&#29992;&#36843;&#20351;&#20844;&#21496;&#19981;&#26029;&#35843;&#25972;&#36816;&#33829;&#65292;&#36234;&#26469;&#36234;&#38590;&#20197;&#39044;&#27979;&#21171;&#21160;&#21147;&#38656;&#27714;&#12290;&#20960;&#39033;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22312;&#32447;&#32844;&#20301;&#24191;&#21578;&#39044;&#27979;&#21171;&#21160;&#21147;&#24066;&#22330;&#19978;&#26032;&#35282;&#33394;&#21644;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#20307;&#35770;&#65292;&#23558;&#19994;&#21153;&#36716;&#22411;&#39033;&#30446;&#19982;&#32844;&#19994;&#30456;&#36830;&#25509;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20174;&#32844;&#20301;&#24191;&#21578;&#21644;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#25552;&#21462;&#30340;&#23884;&#20837;&#26469;&#33258;&#21160;&#22635;&#20805;&#23427;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#26126;&#30830;&#23558;&#19994;&#21153;&#36716;&#22411;&#39033;&#30446;&#65288;&#22914;&#26032;&#25216;&#26415;&#30340;&#37319;&#29992;&#25110;&#36827;&#20837;&#26032;&#24066;&#22330;&#65289;&#19982;&#25152;&#38656;&#35282;&#33394;&#30456;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21313;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#25104;&#21151;&#21305;&#37197;&#20102;&#36716;&#22411;&#39033;&#30446;&#19982;&#32844;&#19994;&#65292;&#20854;&#20013;&#20116;&#20010;&#19982;&#25216;&#26415;&#37319;&#29992;&#26377;&#20851;&#65292;&#20116;&#20010;&#19982;&#19994;&#21153;&#30456;&#20851;&#12290;&#36825;&#19968;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20225;&#19994;&#21644;&#25945;&#32946;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast adoption of new technologies forces companies to continuously adapt their operations making it harder to predict workforce requirements. Several recent studies have attempted to predict the emergence of new roles and skills in the labour market from online job ads. This paper aims to present a novel ontology linking business transformation initiatives to occupations and an approach to automatically populating it by leveraging embeddings extracted from job ads and Wikipedia pages on business transformation and emerging technologies topics. To our knowledge, no previous research explicitly links business transformation initiatives, like the adoption of new technologies or the entry into new markets, to the roles needed. Our approach successfully matches occupations to transformation initiatives under ten different scenarios, five linked to technology adoption and five related to business. This framework presents an innovative approach to guide enterprises and educational institu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#20195;&#30721;&#26234;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM4Code&#65289;&#21487;&#33021;&#36935;&#21040;&#30340;&#38519;&#38449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#24635;&#32467;&#20102;67&#20010;&#20027;&#35201;&#30740;&#31350;&#65292;&#20197;&#20419;&#36827;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;LM4Code&#12290;</title><link>http://arxiv.org/abs/2310.17903</link><description>&lt;p&gt;
&#20195;&#30721;&#26234;&#33021;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#38519;&#38449;: &#19968;&#20221;&#20998;&#31867;&#27861;&#19982;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey. (arXiv:2310.17903v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#20195;&#30721;&#26234;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM4Code&#65289;&#21487;&#33021;&#36935;&#21040;&#30340;&#38519;&#38449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#24635;&#32467;&#20102;67&#20010;&#20027;&#35201;&#30740;&#31350;&#65292;&#20197;&#20419;&#36827;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;LM4Code&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#28304;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#65292;&#20174;&#32780;&#22823;&#24133;&#22686;&#21152;&#20102;&#23545;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#30721;&#26234;&#33021;&#65288;&#22914;&#33258;&#21160;&#38169;&#35823;&#20462;&#22797;&#21644;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#65289;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#38754;&#21521;&#20195;&#30721;&#26234;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM4Code&#65289;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#38519;&#38449;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#23454;&#38469;&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#24433;&#21709;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#39537;&#21160;&#30528;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#20840;&#38754;&#29702;&#35299;&#8212;&#8212;&#19981;&#20165;&#35201;&#35782;&#21035;&#36825;&#20123;&#38382;&#39064;&#65292;&#36824;&#35201;&#28145;&#20837;&#30740;&#31350;&#20854;&#21487;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;&#38754;&#21521;&#20195;&#30721;&#26234;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22522;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#31995;&#32479;&#21270;&#30740;&#31350;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#25581;&#31034;&#20102;LM4Code&#20013;&#22266;&#26377;&#30340;&#38519;&#38449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26469;&#33258;&#39030;&#32423;&#20250;&#35758;&#30340;67&#20010;&#20027;&#35201;&#30740;&#31350;&#12290;&#32463;&#36807;&#20180;&#32454;&#30740;&#31350;&#36825;&#20123;&#30740;&#31350;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls, which hinder realistic performance and further impact their reliability and applicability in real-world deployment. Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxon
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.17894</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29992;&#25143;&#19982;&#34920;&#26684;&#25968;&#25454;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#20256;&#32479;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#25163;&#21160;&#32472;&#22270;&#36716;&#21521;&#26356;&#30452;&#35266;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#30028;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#36825;&#20123;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#19982;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#30028;&#38754;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#25216;&#26415;&#65292;&#29305;&#21035;&#24378;&#35843;&#35821;&#20041;&#35299;&#26512;&#65292;&#36825;&#26159;&#23454;&#29616;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#26597;&#35810;&#25110;&#25968;&#25454;&#21487;&#35270;&#21270;&#21629;&#20196;&#36716;&#21270;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#21518;&#20174;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#31995;&#32479;&#35774;&#35745;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;ConfAIde&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#19978;&#19979;&#25991;&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#30340;&#37325;&#35201;&#24369;&#28857;&#65292;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.17884</link><description>&lt;p&gt;
LLM&#33021;&#20445;&#23432;&#31192;&#23494;&#21527;&#65311;&#36890;&#36807;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#29702;&#35770;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;ConfAIde&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#19978;&#19979;&#25991;&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#30340;&#37325;&#35201;&#24369;&#28857;&#65292;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#21161;&#25163;&#65288;&#24037;&#20316;&#12289;&#23478;&#24237;&#31561;&#65289;&#20013;&#20132;&#20114;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25512;&#29702;&#26102;&#38544;&#31169;&#39118;&#38505;&#65306;LLMs&#20174;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#20013;&#33719;&#21462;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#24182;&#26399;&#26395;&#22312;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#20986;&#22312;&#20309;&#31181;&#30446;&#30340;&#21644;&#19982;&#35841;&#20998;&#20139;&#30340;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ConfAIde&#65292;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#37325;&#35201;&#24369;&#28857;&#30340;&#22522;&#20934;&#65292;&#26469;&#24341;&#36215;&#20154;&#20204;&#23545;&#19978;&#19979;&#25991;&#38544;&#31169;&#36825;&#19968;&#26497;&#20854;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#27010;&#24565;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;GPT-4&#21644;ChatGPT&#31561;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#20063;&#20250;&#27844;&#38706;39&#65285;&#21644;57&#65285;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#21363;&#20351;&#25105;&#20204;&#20351;&#29992;&#20445;&#25252;&#38544;&#31169;&#30340;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#36825;&#31181;&#27844;&#28431;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#25506;&#32034;&#22522;&#20110;&#25512;&#29702;&#21644;&#29702;&#35770;&#30340;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory
&lt;/p&gt;</description></item><item><title>ASPIRO&#26159;&#19968;&#31181;&#33021;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#12289;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#20197;&#21450;&#19968;&#33268;&#24615;&#39564;&#35777;&#25351;&#26631;PARENT&#65292;ASPIRO&#25104;&#21151;&#38477;&#20302;&#20102;66%&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#22312;&#19982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17877</link><description>&lt;p&gt;
ASPIRO: &#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#38169;&#35823;&#24863;&#30693;&#37325;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation. (arXiv:2310.17877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17877
&lt;/p&gt;
&lt;p&gt;
ASPIRO&#26159;&#19968;&#31181;&#33021;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#12289;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#20197;&#21450;&#19968;&#33268;&#24615;&#39564;&#35777;&#25351;&#26631;PARENT&#65292;ASPIRO&#25104;&#21151;&#38477;&#20302;&#20102;66%&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#22312;&#19982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ASPIRO&#65292;&#19968;&#31181;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20135;&#29983;&#19982;&#23454;&#20307;&#26080;&#20851;&#30340;&#27169;&#26495;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;LLM&#24544;&#23454;&#22320;&#22797;&#21046;&#32473;&#23450;&#30340;&#23454;&#20307;&#65292;&#25110;&#32773;&#25163;&#21160;&#39564;&#35777;/&#21046;&#20316;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#21644;PARENT&#25351;&#26631;&#35825;&#23548;&#30340;&#19968;&#33268;&#24615;&#39564;&#35777;&#65292;&#32467;&#21512;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#65292;&#23454;&#26102;&#35782;&#21035;&#21644;&#32416;&#27491;&#27169;&#26495;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;DART&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#30452;&#25509;LLM&#36755;&#20986;&#30456;&#27604;&#65292;ASPIRO&#23545;RDF&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#24179;&#22343;&#38477;&#20302;&#20102;66&#65285;&#12290;&#25105;&#20204;&#22312;Rel2Text&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;5&#26679;&#26412;text-davinci-003&#35774;&#32622;&#35780;&#20998;&#20026;BLEU 50.62&#65292;METEOR 45.16&#65292;BLEURT 0.82&#65292;NUBIA 0.87&#21644;PARENT 0.8962&#65292;&#19982;&#26368;&#36817;&#30340;&#31934;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#20102;&#26377;&#25928;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
&lt;/p&gt;</description></item><item><title>&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;MatchRank&#65292;&#23427;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#34987;&#20154;&#31867;&#20915;&#31574;&#32773;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290; (arXiv:2310.17870v1 [cs.IR])</title><link>http://arxiv.org/abs/2310.17870</link><description>&lt;p&gt;
&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ranking with Slot Constraints. (arXiv:2310.17870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17870
&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;MatchRank&#65292;&#23427;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#34987;&#20154;&#31867;&#20915;&#31574;&#32773;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290; (arXiv:2310.17870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#36825;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#21508;&#31181;&#24212;&#29992;&#38382;&#39064; - &#20174;&#20855;&#26377;&#19981;&#21516;&#19987;&#19994;&#38480;&#21046;&#27133;&#20301;&#30340;&#22823;&#23398;&#24405;&#21462;&#65292;&#21040;&#22312;&#21307;&#23398;&#35797;&#39564;&#20013;&#26500;&#24314;&#31526;&#21512;&#26465;&#20214;&#30340;&#21442;&#19982;&#32773;&#20998;&#23618;&#38431;&#21015;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#27010;&#29575;&#25490;&#21517;&#21407;&#21017;&#65288;PRP&#65289;&#22312;&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#38750;&#24120;&#27425;&#20248;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;&#65292;&#31216;&#20026;MatchRank&#12290;MatchRank&#30340;&#30446;&#26631;&#26159;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#30001;&#20154;&#31867;&#20915;&#31574;&#32773;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#36825;&#26679;&#65292;MatchRank&#22312;&#24191;&#20041;&#19978;&#26159;PRP&#30340;&#25512;&#24191;&#65292;&#24403;&#27809;&#26377;&#27133;&#32422;&#26463;&#26102;&#65292;&#23427;&#26159;PRP&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MatchRank&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#27809;&#26377;&#20219;&#20309;&#27133;&#20301;&#25110;&#20505;&#36873;&#20154;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23454;&#29616;MatchRank&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MatchRank&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, em
&lt;/p&gt;</description></item><item><title>&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#20116;&#20010;&#28145;&#24230;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36829;&#21453;&#20102;&#26631;&#20934;&#30340;MIL&#20551;&#35774;&#65292;&#23548;&#33268;&#33021;&#22815;&#23398;&#20064;&#21453;&#30456;&#20851;&#30340;&#23454;&#20363;&#12290;&#36825;&#19968;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#25913;&#36827;&#21644;&#20854;&#20182;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2310.17867</link><description>&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;: &#31639;&#27861;&#21333;&#20803;&#27979;&#35797;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests. (arXiv:2310.17867v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17867
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#20116;&#20010;&#28145;&#24230;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36829;&#21453;&#20102;&#26631;&#20934;&#30340;MIL&#20551;&#35774;&#65292;&#23548;&#33268;&#33021;&#22815;&#23398;&#20064;&#21453;&#30456;&#20851;&#30340;&#23454;&#20363;&#12290;&#36825;&#19968;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#25913;&#36827;&#21644;&#20854;&#20182;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;(MIL)&#26159;&#20998;&#31867;&#38382;&#39064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#20854;&#20013;&#26377;&#27491;&#36127;&#26631;&#31614;&#21644;&#19968;&#20010;&#36755;&#20837;&#30340;&#8220;&#21253;&#8221;&#65292;&#24403;&#19988;&#20165;&#24403;&#21253;&#20013;&#21253;&#21547;&#19968;&#20010;&#27491;&#20803;&#32032;&#26102;&#65292;&#26631;&#31614;&#20026;&#27491;&#65292;&#21542;&#21017;&#20026;&#36127;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#38656;&#35201;&#23558;&#21253;&#32423;&#26631;&#31614;&#19982;&#23454;&#20363;&#32423;&#20449;&#24687;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#38544;&#21547;&#30528;&#19968;&#20010;&#22240;&#26524;&#20551;&#35774;&#21644;&#20219;&#21153;&#30340;&#19981;&#23545;&#31216;&#24615;&#65288;&#21363;&#65292;&#26080;&#27861;&#20132;&#25442;&#26631;&#31614;&#32780;&#19981;&#25913;&#21464;&#35821;&#20041;&#65289;&#12290;MIL&#38382;&#39064;&#20986;&#29616;&#22312;&#21307;&#30103;&#20445;&#20581;&#65288;&#19968;&#20010;&#24694;&#24615;&#32454;&#32990;&#34920;&#31034;&#30284;&#30151;&#65289;&#65292;&#32593;&#32476;&#23433;&#20840;&#65288;&#19968;&#20010;&#24694;&#24847;&#21487;&#25191;&#34892;&#25991;&#20214;&#20250;&#24863;&#26579;&#35745;&#31639;&#26426;&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#33879;&#21517;&#30340;&#20116;&#20010;&#28145;&#24230;MIL&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;MIL&#20551;&#35774;&#12290;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21453;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#21363;&#22312;&#30475;&#21040;&#36127;&#30340;&#21453;&#20363;&#20043;&#21069;&#40664;&#35748;&#20026;&#8220;&#27491;&#8221;&#26631;&#31614;&#65292;&#36825;&#23545;&#20110;&#19968;&#20010;&#27491;&#30830;&#30340;MIL&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#24576;&#30097;&#25913;&#36827;&#21644;&#20854;&#20182;&#31574;&#30053;&#21487;&#33021;&#20250;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a "bag" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to "positive" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and othe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26680;&#24515;&#38598;&#21518;&#39564;&#30340;&#21464;&#20998;&#36817;&#20284;&#24182;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23558;&#20854;&#19982;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#39640;&#32500;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17852</link><description>&lt;p&gt;
&#20989;&#25968;&#31354;&#38388;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#29992;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Function Space Bayesian Pseudocoreset for Bayesian Neural Networks. (arXiv:2310.17852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26680;&#24515;&#38598;&#21518;&#39564;&#30340;&#21464;&#20998;&#36817;&#20284;&#24182;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23558;&#20854;&#19982;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#39640;&#32500;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26159;&#19968;&#20010;&#32039;&#20945;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24635;&#32467;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#22240;&#27492;&#21487;&#20197;&#20316;&#20026;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20266;&#26680;&#24515;&#38598;&#21518;&#39564;&#26465;&#20214;&#21644;&#23436;&#25972;&#25968;&#25454;&#38598;&#21518;&#39564;&#26465;&#20214;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#26469;&#26500;&#24314;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#24046;&#24322;&#24230;&#37327;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#20266;&#26680;&#24515;&#38598;&#26500;&#24314;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#20197;&#27169;&#22411;&#21442;&#25968;&#65288;&#26435;&#37325;&#65289;&#30340;&#31354;&#38388;&#26500;&#24314;&#21644;&#21305;&#37197;&#26680;&#24515;&#38598;&#21644;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#26500;&#24314;&#26680;&#24515;&#38598;&#21518;&#39564;&#30340;&#21464;&#20998;&#36817;&#20284;&#65292;&#24182;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23558;&#20854;&#19982;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#21305;&#37197;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32469;&#36807;&#19968;&#20123;&#35745;&#31639;&#21644;&#35780;&#20272;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass sev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#23454;&#26102;&#36827;&#34892;&#21160;&#30011;&#25511;&#21046;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17838</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#23454;&#26102;&#29983;&#25104;&#21644;&#25511;&#21046;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Real-time Animation Generation and Control on Rigged Models via Large Language Models. (arXiv:2310.17838v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#23454;&#26102;&#36827;&#34892;&#21160;&#30011;&#25511;&#21046;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#26102;&#21160;&#30011;&#25511;&#21046;&#21644;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;Unity&#20013;&#23884;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#36755;&#20986;&#21487;&#20197;&#35299;&#26512;&#20026;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21160;&#30011;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#23454;&#29616;&#29616;&#26377;&#21160;&#30011;&#20043;&#38388;&#28789;&#27963;&#29366;&#24577;&#36716;&#25442;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21508;&#31181;&#32465;&#23450;&#27169;&#22411;&#21644;&#21160;&#20316;&#19978;&#23637;&#31034;&#23450;&#24615;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel method for real-time animation control and generation on rigged models using natural language input. First, we embed a large language model (LLM) in Unity to output structured texts that can be parsed into diverse and realistic animations. Second, we illustrate LLM's potential to enable flexible state transition between existing animations. We showcase the robustness of our approach through qualitative results on various rigged models and motions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21508;&#31181;&#21160;&#20316;&#30340;&#21160;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#29420;&#31435;&#25805;&#20316;&#21644;&#36716;&#31227;&#35270;&#39057;&#21160;&#20316;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#39057;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17835</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#26684;&#32479;&#19968;&#29983;&#25104;&#35270;&#39057;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Style is All you Need to Generate a Video. (arXiv:2310.17835v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21508;&#31181;&#21160;&#20316;&#30340;&#21160;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#29420;&#31435;&#25805;&#20316;&#21644;&#36716;&#31227;&#35270;&#39057;&#21160;&#20316;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#23398;&#20064;&#21040;&#30340;&#27491;&#24358;&#22522;&#30340;&#26032;&#39062;&#26102;&#38388;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19982;&#22270;&#20687;&#20869;&#23481;&#26080;&#20851;&#24182;&#21487;&#20197;&#22312;&#19981;&#21516;&#28436;&#21592;&#20043;&#38388;&#36716;&#31227;&#30340;&#21508;&#31181;&#21160;&#20316;&#30340;&#21160;&#24577;&#34920;&#31034;&#12290;&#38500;&#20102;&#19982;&#26222;&#36941;&#26041;&#27861;&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#35270;&#39057;&#36136;&#37327;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35299;&#32806;&#30340;&#21160;&#24577;&#21644;&#20869;&#23481;&#20351;&#23427;&#20204;&#21487;&#20197;&#29420;&#31435;&#25805;&#20316;&#65292;&#20197;&#21450;&#36890;&#36807;&#26102;&#24207;GAN&#21453;&#28436;&#20174;&#19968;&#20010;&#20869;&#23481;&#25110;&#36523;&#20221;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#35270;&#39057;&#21160;&#20316;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#39044;&#22788;&#29702;&#65292;&#22914;&#29305;&#24449;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a style-based conditional video generative model. We introduce a novel temporal generator based on a set of learned sinusoidal bases. Our method learns dynamic representations of various actions that are independent of image content and can be transferred between different actors. Beyond the significant enhancement of video quality compared to prevalent methods, we demonstrate that the disentangled dynamic and content permit their independent manipulation, as well as temporal GAN-inversion to retrieve and transfer a video motion from one content or identity to another without further preprocessing such as landmark points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#65292;SA-Roundtrip&#65292;&#21487;&#20197;&#36827;&#34892;&#21487;&#25511;&#30340;&#37319;&#26679;&#29983;&#25104;&#65292;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#22522;&#20110;&#35813;&#20808;&#39564;&#65292;&#32467;&#21512;Hamiltonian Monte Carlo&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25104;&#20687;&#36870;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17817</link><description>&lt;p&gt;
Bayesian&#25104;&#20687;&#36870;&#38382;&#39064;&#20013;&#30340;SA-Roundtrip&#20808;&#39564;&#21450;HMC-pCN&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN sampler. (arXiv:2310.17817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#65292;SA-Roundtrip&#65292;&#21487;&#20197;&#36827;&#34892;&#21487;&#25511;&#30340;&#37319;&#26679;&#29983;&#25104;&#65292;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#22522;&#20110;&#35813;&#20808;&#39564;&#65292;&#32467;&#21512;Hamiltonian Monte Carlo&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25104;&#20687;&#36870;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#19982;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#25104;&#20687;&#36870;&#38382;&#39064;&#27714;&#35299;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#39564;&#20998;&#24067;&#30340;&#36873;&#25321;&#26159;&#20174;&#21487;&#29992;&#20808;&#39564;&#27979;&#37327;&#20013;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#26159;&#20851;&#20110;&#21487;&#29992;&#20808;&#39564;&#27979;&#37327;&#30340;&#37325;&#35201;&#34920;&#31034;&#23398;&#20064;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;SA-Roundtrip&#65292;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#37319;&#26679;&#29983;&#25104;&#65292;&#24182;&#35782;&#21035;&#20986;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#35813;&#20808;&#39564;&#22312;&#21452;&#21521;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#23884;&#20837;&#20102;&#33258;&#27880;&#24847;&#21147;&#32467;&#26500;&#12290;&#38543;&#21518;&#65292;&#23558;&#36125;&#21494;&#26031;&#25512;&#26029;&#24212;&#29992;&#20110;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20351;&#29992;&#20855;&#26377;&#39044;&#26465;&#20214;Crank-Nicolson&#31639;&#27861;&#30340;Hamiltonian Monte Carlo (HMC-pCN)&#12290;&#35813;&#31639;&#27861;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#20855;&#26377;&#36941;&#21382;&#24615;&#12290;&#23545;MNIST&#21644;TomoPhantom&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference with deep generative prior has received considerable interest for solving imaging inverse problems in many scientific and engineering fields. The selection of the prior distribution is learned from, and therefore an important representation learning of, available prior measurements. The SA-Roundtrip, a novel deep generative prior, is introduced to enable controlled sampling generation and identify the data's intrinsic dimension. This prior incorporates a self-attention structure within a bidirectional generative adversarial network. Subsequently, Bayesian inference is applied to the posterior distribution in the low-dimensional latent space using the Hamiltonian Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN) algorithm, which is proven to be ergodic under specific conditions. Experiments conducted on computed tomography (CT) reconstruction with the MNIST and TomoPhantom datasets reveal that the proposed method outperforms state-of-the-art comparisons, consis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.17811</link><description>&lt;p&gt;
&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#21307;&#23398;&#24433;&#20687;&#20013;&#29983;&#25104;&#25253;&#21578;&#26377;&#26395;&#25913;&#21892;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#20174;&#22270;&#20687;&#29983;&#25104;&#23436;&#25972;&#30340;&#25253;&#21578;&#26469;&#32771;&#34385;&#22270;&#20687;&#21040;&#25253;&#21578;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#28151;&#28102;&#20102;&#25253;&#21578;&#30340;&#20869;&#23481;&#65288;&#22914;&#21457;&#29616;&#21644;&#20854;&#23646;&#24615;&#65289;&#19982;&#20854;&#39118;&#26684;&#65288;&#22914;&#26684;&#24335;&#21644;&#35789;&#27719;&#36873;&#25321;&#65289;&#65292;&#21487;&#33021;&#23548;&#33268;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20869;&#23481;&#65292;&#28982;&#21518;&#23558;&#25552;&#21462;&#30340;&#20869;&#23481;&#36716;&#21270;&#20026;&#19982;&#29305;&#23450;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;RadGraph&#8212;&#8212;&#19968;&#31181;&#25253;&#21578;&#30340;&#22270;&#34920;&#31034;&#8212;&#8212;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;&#23450;&#37327;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#30410;&#22788;&#12290;&#36890;&#36807;&#20020;&#24202;&#35780;&#20272;&#32773;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#25253;&#21578;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#65292;&#26080;&#27861;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;DreamerV3&#30340;&#25216;&#24039;&#24212;&#29992;&#21040;PPO&#20013;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25216;&#24039;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;PPO&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#25216;&#24039;&#30340;&#25104;&#21151;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.17805</link><description>&lt;p&gt;
&#36890;&#36807;DreamerV3&#25216;&#24039;&#25552;&#39640;PPO&#30340;&#22870;&#21169;&#35268;&#27169;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks. (arXiv:2310.17805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;DreamerV3&#30340;&#25216;&#24039;&#24212;&#29992;&#21040;PPO&#20013;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#25216;&#24039;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;PPO&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#25216;&#24039;&#30340;&#25104;&#21151;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#23494;&#38598;&#12289;&#35268;&#33539;&#21270;&#30340;&#29615;&#22659;&#22870;&#21169;&#12290;DreamerV3&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20123;&#25216;&#24039;&#26469;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#65292;&#20351;&#29992;&#19968;&#32452;&#36229;&#21442;&#25968;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#29366;&#24577;&#12290;&#36825;&#20010;&#32467;&#26524;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#25216;&#24039;&#30340;&#26222;&#36866;&#24615;&#30340;&#35752;&#35770;&#65292;&#22240;&#20026;&#23427;&#20204;&#20284;&#20046;&#36866;&#29992;&#20110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;DreamerV3&#30340;&#25216;&#24039;&#24212;&#29992;&#21040;PPO&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#22312;&#21407;&#22987;&#24037;&#20316;&#20043;&#22806;&#36827;&#34892;&#36825;&#26679;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25216;&#24039;&#24182;&#19981;&#33021;&#20316;&#20026;&#19968;&#33324;&#30340;&#25913;&#36827;&#36716;&#31227;&#21040;PPO&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#39640;&#36136;&#37327;&#30340;PPO&#21442;&#32771;&#23454;&#29616;&#65292;&#24182;&#22312;Arcade Learning Environment&#21644;DeepMind Control Suite&#19978;&#36827;&#34892;&#20102;&#38271;&#36798;10,000&#20010;A100&#23567;&#26102;&#30340;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#25216;&#24039;&#24182;&#27809;&#26377;&#26222;&#36941;&#36229;&#36807;PPO&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#23427;&#20204;&#25104;&#21151;&#30340;&#24773;&#20917;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17793</link><description>&lt;p&gt;
"&#24744;&#26159;&#19968;&#20301;&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65306;&#20316;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#20998;&#26512;&#22120;&#30340;LLMs&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#29087;&#32451;&#24230;&#21644;&#27969;&#30021;&#24615;&#12290;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#23427;&#20204;&#20063;&#24050;&#32463;&#33719;&#24471;&#20102;&#20851;&#20110;&#35821;&#35328;&#30340;&#28145;&#21051;&#35821;&#35328;&#30693;&#35782;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#21487;&#20197;&#20805;&#24403;"&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;GPT-3&#12289;ChatGPT&#21644;GPT-4&#27169;&#22411;&#22312;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#20998;&#26512;&#20013;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65307;Banarescu&#31561;&#20154;&#65292;2013&#65289;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#35813;&#24418;&#24335;&#20027;&#20041;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#22270;&#24418;&#21270;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#34920;&#31034;&#65292;&#21516;&#26102;&#20174;&#34920;&#38754;&#24418;&#24335;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#22312;&#36825;&#31181;&#35821;&#20041;&#32467;&#26500;&#20998;&#26512;&#19978;&#30340;&#32467;&#26524;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#65306;1&#65289;&#22522;&#20110;&#38646;&#23556;&#21644;&#23569;&#23398;&#26679;&#26412;&#30340;AMR&#35299;&#26512;&#30340;&#30452;&#25509;&#29983;&#25104;&#65292;&#20197;&#21450;2&#65289;&#36890;&#36807;&#20803;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;"&#30830;&#23450;&#35813;&#21477;&#23376;&#30340;&#20027;&#35201;&#20107;&#20214;&#65292;&#20197;&#21450;&#19982;&#35813;&#20107;&#20214;&#23545;&#24212;&#30340;&#35859;&#35789;"&#65289;&#38388;&#25509;&#30340;&#37096;&#20998;&#37325;&#26500;AMR&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#37325;&#26032;&#29983;&#25104;&#27491;&#30830;&#30340;AMR&#35299;&#26512;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an "expert linguistic annotator"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., "Identify the primary event of this sentence, and the predicate corresponding to that event."). Across these settings, we find that models can re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#25552;&#31034;&#25216;&#26415;&#21644;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36716;&#21270;&#20026;&#25551;&#36848;&#24615;&#35821;&#21477;&#24182;&#23454;&#29616;&#39044;&#27979;&#26410;&#26469;&#33021;&#37327;&#36127;&#33655;&#28040;&#32791;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20026;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;&#20419;&#36827;&#33021;&#28304;&#31995;&#32479;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.17788</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Utilizing Language Models for Energy Load Forecasting. (arXiv:2310.17788v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#25552;&#31034;&#25216;&#26415;&#21644;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36716;&#21270;&#20026;&#25551;&#36848;&#24615;&#35821;&#21477;&#24182;&#23454;&#29616;&#39044;&#27979;&#26410;&#26469;&#33021;&#37327;&#36127;&#33655;&#28040;&#32791;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20026;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;&#20419;&#36827;&#33021;&#28304;&#31995;&#32479;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#31649;&#29702;&#24314;&#31569;&#29289;&#21644;&#22478;&#24066;&#30340;&#33021;&#28304;&#28040;&#32791;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#25552;&#31034;&#25216;&#26415;&#23558;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36716;&#21270;&#20026;&#25551;&#36848;&#24615;&#35821;&#21477;&#65292;&#20174;&#32780;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#36890;&#36807;&#37319;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#33021;&#37327;&#36127;&#33655;&#28040;&#32791;&#30340;&#21508;&#31181;&#26102;&#38388;&#33539;&#22260;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#26377;&#26395;&#22686;&#24378;&#33021;&#28304;&#25928;&#29575;&#24182;&#20419;&#36827;&#33021;&#28304;&#31995;&#32479;&#26234;&#33021;&#20915;&#31574;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregressive generating approach, our proposed method enables predictions of various horizons of future energy load consumption. Through extensive experiments on real-world datasets, we demonstrate the effectiveness and accuracy of our proposed method. Our results indicate that utilizing language models for energy load forecasting holds promise for enhancing energy efficiency and facilitating intelligent decision-making in energy systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#28508;&#34255;&#30340;&#20167;&#24680;&#20869;&#23481;&#65292;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#35780;&#20272;&#25163;&#27573;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17787</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models using an Indian language LGBTI+ lexicon. (arXiv:2310.17787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#28508;&#34255;&#30340;&#20167;&#24680;&#20869;&#23481;&#65292;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#35780;&#20272;&#25163;&#27573;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36890;&#36807;&#22522;&#20110;&#20219;&#21153;&#30340;&#22522;&#20934;&#65288;&#22914;MMLU&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#22522;&#20934;&#26080;&#27861;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#26816;&#26597;LLMs&#30340;&#36127;&#36131;&#20219;&#34892;&#20026;&#12290;&#22312;LGBTI+&#35821;&#22659;&#20013;&#65292;&#31038;&#20250;&#38472;&#35268;&#21487;&#33021;&#23548;&#33268;LGBTI+&#26415;&#35821;&#30340;&#21464;&#24322;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#27719;&#34920;&#21487;&#20197;&#20316;&#20026;&#23545;LLM&#34892;&#20026;&#36827;&#34892;&#35780;&#20272;&#30340;&#20195;&#34920;&#24615;&#21333;&#35789;&#21015;&#34920;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#35780;&#20272;LLMs&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#27493;&#39588;&#65306;&#21046;&#23450;&#19982;&#39044;&#26399;&#34892;&#20026;&#30456;&#20851;&#30340;NLP&#20219;&#21153;&#65292;&#21019;&#24314;&#27979;&#35797;LLMs&#30340;&#25552;&#31034;&#65292;&#20351;&#29992;LLMs&#33719;&#21462;&#36755;&#20986;&#65292;&#26368;&#21518;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;&#25105;&#20204;&#23454;&#39564;&#30340;&#19977;&#20010;LLMs&#26080;&#27861;&#26816;&#27979;&#21040;&#28508;&#22312;&#30340;&#20167;&#24680;&#20869;&#23481;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#30340;&#25163;&#27573;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17784</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#22914;&#37329;&#34701;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;LLMs&#38590;&#20197;&#25512;&#29702;&#21644;&#25972;&#21512;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19981;&#26159;&#19968;&#27425;&#24615;&#32473;LLM&#36127;&#36733;&#36807;&#22810;&#20449;&#24687;&#65292;&#32780;&#26159;&#26356;&#26377;&#25928;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20219;&#21153;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#37329;&#34701;LLM&#65288;FLLM&#65289;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#25163;&#21160;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#24378;&#25512;&#29702;&#65288;AAR&#65289;&#26469;&#20462;&#25913;FLLM&#33258;&#36523;&#36755;&#20986;&#30340;&#20266;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;FLLM&#19982;AAR&#30456;&#27604;&#65292;&#26174;&#33879;&#20248;&#20110;&#20026;&#21407;&#22987;&#25991;&#26412;&#35774;&#35745;&#30340;&#22522;&#32447;&#37329;&#34701;LLMs&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#36710;&#36742;&#19982;&#29615;&#22659;&#20197;&#21450;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;&#36825;&#20123;&#24773;&#26223;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2310.17773</link><description>&lt;p&gt;
&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks for Complex Traffic Scenario Classification. (arXiv:2310.17773v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17773
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#36710;&#36742;&#19982;&#29615;&#22659;&#20197;&#21450;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;&#36825;&#20123;&#24773;&#26223;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#30340;&#27979;&#35797;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#33719;&#21462;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#32479;&#35745;&#26174;&#33879;&#35777;&#25454;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#35782;&#21035;&#36825;&#20123;&#24773;&#26223;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#22312;&#22797;&#26434;&#24773;&#26223;&#65288;&#39640;&#36895;&#20844;&#36335;&#65292;&#22478;&#24066;&#65289;&#21644;&#19982;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#20114;&#21160;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#29616;&#26377;&#26041;&#27861;&#27169;&#25311;&#20102;&#36710;&#36742;&#19982;&#20854;&#29615;&#22659;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#22810;&#36742;&#36710;&#20043;&#38388;&#30340;&#20114;&#21160;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#20999;&#25442;&#65292;&#38745;&#27490;&#21069;&#36710;&#65289;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#27809;&#26377;&#27599;&#24103;&#27880;&#37322;&#26469;&#20934;&#30830;&#23398;&#20064;&#24773;&#26223;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#27169;&#25311;&#36710;&#36742;&#19982;&#29615;&#22659;&#20197;&#21450;&#20854;&#20182;&#21442;&#19982;&#32773;&#20114;&#21160;&#30340;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#27169;&#25311;&#36825;&#20123;&#24773;&#26223;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#25193;&#23637;t
&lt;/p&gt;
&lt;p&gt;
A scenario-based testing approach can reduce the time required to obtain statistically significant evidence of the safety of Automated Driving Systems (ADS). Identifying these scenarios in an automated manner is a challenging task. Most methods on scenario classification do not work for complex scenarios with diverse environments (highways, urban) and interaction with other traffic agents. This is mirrored in their approaches which model an individual vehicle in relation to its environment, but neglect the interaction between multiple vehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing datasets lack diversity and do not have per-frame annotations to accurately learn the start and end time of a scenario. We propose a method for complex traffic scenario classification that is able to model the interaction of a vehicle with the environment, as well as other agents. We use Graph Convolutional Networks to model spatial and temporal aspects of these scenarios. Expanding t
&lt;/p&gt;</description></item><item><title>GROOViST&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#30340;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#12289;&#26102;&#38388;&#38169;&#20301;&#21644;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;&#36825;&#31181;&#24037;&#20855;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.17770</link><description>&lt;p&gt;
GROOViST:&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
GROOViST: A Metric for Grounding Objects in Visual Storytelling. (arXiv:2310.17770v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17770
&lt;/p&gt;
&lt;p&gt;
GROOViST&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#30340;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#12289;&#26102;&#38388;&#38169;&#20301;&#21644;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;&#36825;&#31181;&#24037;&#20855;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#30001;&#19968;&#31995;&#21015;&#22270;&#20687;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#36866;&#24403;&#35780;&#20272;&#65292;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20363;&#22914;&#36830;&#36143;&#24615;&#65292;&#35821;&#27861;&#27491;&#30830;&#24615;&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;&#23450;&#20301;&#31243;&#24230;&#65292;&#21363;&#25925;&#20107;&#19982;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#23454;&#20307;&#30456;&#20851;&#31243;&#24230;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#38024;&#23545;&#27492;&#30446;&#30340;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#38024;&#23545;&#19968;&#33324;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#25351;&#26631;&#12290;&#37492;&#20110;&#23427;&#20204;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;GROOViST&#65292;&#35813;&#24037;&#20855;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#26102;&#38388;&#38169;&#20301;&#65288;&#25925;&#20107;&#20013;&#23454;&#20307;&#20986;&#29616;&#30340;&#39034;&#24207;&#21644;&#22270;&#20687;&#24207;&#21015;&#21487;&#33021;&#19981;&#21305;&#37197;&#65289;&#20197;&#21450;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;GROOViST&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#23545;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#36827;&#34892;&#35780;&#20272;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;AI&#21161;&#25163;&#33021;&#22815;&#33258;&#21160;&#23545;&#40784;&#29992;&#25143;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;AI&#21161;&#25163;&#22312;&#27169;&#25311;&#20013;&#33021;&#22815;&#20934;&#30830;&#23545;&#40784;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65292;&#20294;&#22312;&#38754;&#23545;&#26410;&#30693;&#36135;&#24065;&#20197;&#21450;&#35821;&#35328;&#19982;&#31574;&#30053;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.17769</link><description>&lt;p&gt;
&#31038;&#20250;&#22865;&#32422;AI&#65306;&#23558;AI&#21161;&#25163;&#19982;&#38544;&#21547;&#30340;&#32676;&#20307;&#35268;&#33539;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;AI&#21161;&#25163;&#33021;&#22815;&#33258;&#21160;&#23545;&#40784;&#29992;&#25143;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;AI&#21161;&#25163;&#22312;&#27169;&#25311;&#20013;&#33021;&#22815;&#20934;&#30830;&#23545;&#40784;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65292;&#20294;&#22312;&#38754;&#23545;&#26410;&#30693;&#36135;&#24065;&#20197;&#21450;&#35821;&#35328;&#19982;&#31574;&#30053;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#21453;&#36716;&#27169;&#25311;&#29992;&#25143;&#65288;&#26410;&#30693;&#65289;&#20559;&#22909;&#30340;&#27169;&#22411;&#26469;&#23545;&#40784;AI&#21161;&#25163;&#30340;&#24605;&#36335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#22312;&#32463;&#27982;&#25253;&#20215;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#27169;&#25311;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24418;&#24335;&#21270;&#20026;&#25351;&#23548;&#27169;&#25311;&#29609;&#23478;&#34892;&#20026;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#21161;&#25163;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#20854;&#34892;&#20026;&#19982;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65288;&#22914;&#33258;&#31169;&#30340;&#12289;&#21033;&#20182;&#30340;&#65289;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#21161;&#25163;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#38754;&#23545;&#26410;&#21253;&#21547;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#30340;&#36135;&#24065;&#65288;&#22914;&#33647;&#21697;&#20811;&#25968;&#65289;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#20351;&#29992;&#19982;&#26410;&#30693;&#31574;&#30053;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#19981;&#36275;&#26102;&#65288;&#22914;&#21033;&#20182;&#31574;&#30053;&#19982;&#31895;&#40065;&#35821;&#35328;&#30456;&#32467;&#21512;&#65289;&#65292;&#21161;&#25163;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#20250;&#20943;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21021;&#27493;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#27169;&#25311;&#26694;&#26550;&#26469;&#23545;&#40784;AI&#21161;&#25163;&#30340;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation fr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#25945;&#32946;&#20215;&#20540;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38144;&#21806;&#20154;&#21592;&#21644;SalesBot&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;SalesBot&#22312;&#27969;&#30021;&#24615;&#21644;&#20449;&#24687;&#37327;&#26041;&#38754;&#25509;&#36817;&#19987;&#19994;&#38144;&#21806;&#20154;&#21592;&#65292;&#20294;&#22312;&#25512;&#33616;&#36136;&#37327;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.17749</link><description>&lt;p&gt;
&#38144;&#21806;&#20154;&#21592; vs SalesBot&#65306;&#25506;&#32034;&#25945;&#32946;&#20215;&#20540;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems. (arXiv:2310.17749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17749
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#25945;&#32946;&#20215;&#20540;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38144;&#21806;&#20154;&#21592;&#21644;SalesBot&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;SalesBot&#22312;&#27969;&#30021;&#24615;&#21644;&#20449;&#24687;&#37327;&#26041;&#38754;&#25509;&#36817;&#19987;&#19994;&#38144;&#21806;&#20154;&#21592;&#65292;&#20294;&#22312;&#25512;&#33616;&#36136;&#37327;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22823;&#39069;&#36141;&#20080;&#38656;&#35201;&#28040;&#36153;&#32773;&#36827;&#34892;&#30740;&#31350;&#25110;&#21672;&#35810;&#38144;&#21806;&#20154;&#21592;&#20197;&#33719;&#21462;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#24448;&#24448;&#24573;&#35270;&#29992;&#25143;&#32570;&#20047;&#32972;&#26223;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#25910;&#38598;&#20559;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#26088;&#22312;&#36890;&#36807;&#28151;&#21512;&#22411;&#28151;&#21512;&#27169;&#24335;&#23545;&#35805;&#25552;&#20379;&#20135;&#21697;&#25512;&#33616;&#21644;&#25945;&#32946;&#20215;&#20540;&#30340;&#20250;&#35805;&#20195;&#29702;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#31354;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SalesOps&#65292;&#36825;&#26159;&#19968;&#20010;&#20511;&#37492;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#35780;&#20272;&#36825;&#31181;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;SalesBot&#21644;ShopperBot&#65292;&#36825;&#26159;&#19968;&#23545;LLM&#39537;&#21160;&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#26694;&#26550;&#30340;&#20219;&#24847;&#19968;&#20391;&#12290;&#36890;&#36807;&#19968;&#39033;&#20840;&#38754;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;SalesBot&#19982;&#19987;&#19994;&#38144;&#21806;&#20154;&#21592;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#34429;&#28982;SalesBot&#22312;&#27969;&#30021;&#24615;&#21644;&#20449;&#24687;&#37327;&#26041;&#38754;&#25509;&#36817;&#19987;&#19994;&#34920;&#29616;&#65292;&#20294;&#22312;&#25512;&#33616;&#36136;&#37327;&#26041;&#38754;&#21364;&#33853;&#21518;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20004;&#32773;&#38754;&#20020;&#30340;&#19981;&#21516;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users' lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GGNNs&#65289;&#65292;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#65292;GGNNs&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#36873;&#25321;&#65292;&#23637;&#31034;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17729</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20132;&#36890;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks. (arXiv:2310.17729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GGNNs&#65289;&#65292;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#65292;GGNNs&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#36873;&#25321;&#65292;&#23637;&#31034;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#34892;&#31243;&#35268;&#21010;&#12289;&#20132;&#36890;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#31561;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20132;&#36890;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;GNN&#26550;&#26500;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;&#22270;&#37319;&#26679;&#21644;&#32858;&#21512;&#65289;&#21644;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#35814;&#32454;&#32771;&#23519;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#23618;&#37197;&#32622;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#65292;&#22312;&#19977;&#31181;&#27169;&#22411;&#20013;&#65292;GGNNs&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#36873;&#25321;&#12290;&#30740;&#31350;&#27010;&#36848;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20551;&#35774;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35265;&#35299;&#65306;GCNs&#26174;&#31034;&#20102;9.10&#30340;RMSE&#21644;8.00&#30340;MAE&#65292;&#32780;GraphSAGE&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the application of graph neural networks in the realm of traffic forecasting, a crucial facet of intelligent transportation systems. Accurate traffic predictions are vital for functions like trip planning, traffic control, and vehicle routing in such systems. Three prominent GNN architectures Graph Convolutional Networks (Graph Sample and Aggregation) and Gated Graph Neural Networks are explored within the context of traffic prediction. Each architecture's methodology is thoroughly examined, including layer configurations, activation functions,and hyperparameters. The primary goal is to minimize prediction errors, with GGNNs emerging as the most effective choice among the three models. The research outlines outcomes for each architecture, elucidating their predictive performance through root mean squared error and mean absolute error (MAE). Hypothetical results reveal intriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while GraphSAGE shows impr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.17722</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#35270;&#35273;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;(LLaRP)&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#30340;LLM&#35843;&#25972;&#20026;&#25509;&#25910;&#25991;&#26412;&#25351;&#20196;&#21644;&#35270;&#35273;&#33258;&#25105;&#20013;&#24515;&#35266;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#30452;&#25509;&#22312;&#29615;&#22659;&#20013;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;LLaRP&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#30475;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLaRP&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;1,000&#20010;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;42%&#65292;&#26159;&#20854;&#20182;&#24120;&#35265;&#23398;&#20064;&#22522;&#32447;&#25110;&#38646;&#26679;&#26412;&#24212;&#29992;&#30340;1.7&#20493;&#25104;&#21151;&#29575;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#31038;&#21306;&#30740;&#31350;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;AI&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#65292;&#21253;&#25324;150,000&#20010;&#35757;&#32451;&#20219;&#21153;&#21644;1,000&#20010;&#27979;&#35797;&#20219;&#21153;&#65292;&#29992;&#20110;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24110;&#21161;&#25237;&#36164;&#32773;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#20174;&#25910;&#30410;&#30005;&#35805;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#39118;&#38505;&#25688;&#35201;&#21644;&#35780;&#20272;&#65292;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#24230;&#37327;&#20855;&#26377;&#26174;&#33879;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#33021;&#22815;&#39044;&#27979;&#20225;&#19994;&#23618;&#38754;&#27874;&#21160;&#24615;&#21644;&#25237;&#36164;&#21019;&#26032;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36824;&#33021;&#26377;&#25928;&#26816;&#27979;&#26032;&#20852;&#39118;&#38505;&#65292;&#24182;&#19988;&#36825;&#20123;&#24230;&#37327;&#22312;&#32929;&#26435;&#24066;&#22330;&#20013;&#36215;&#21040;&#23450;&#20215;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17721</link><description>&lt;p&gt;
&#20174;&#35762;&#35805;&#25991;&#26412;&#21040;&#27934;&#23519;&#21147;&#65306;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI. (arXiv:2310.17721v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24110;&#21161;&#25237;&#36164;&#32773;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#20174;&#25910;&#30410;&#30005;&#35805;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#39118;&#38505;&#25688;&#35201;&#21644;&#35780;&#20272;&#65292;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#24230;&#37327;&#20855;&#26377;&#26174;&#33879;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#33021;&#22815;&#39044;&#27979;&#20225;&#19994;&#23618;&#38754;&#27874;&#21160;&#24615;&#21644;&#25237;&#36164;&#21019;&#26032;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36824;&#33021;&#26377;&#25928;&#26816;&#27979;&#26032;&#20852;&#39118;&#38505;&#65292;&#24182;&#19988;&#36825;&#20123;&#24230;&#37327;&#22312;&#32929;&#26435;&#24066;&#22330;&#20013;&#36215;&#21040;&#23450;&#20215;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#31561;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24110;&#21161;&#25237;&#36164;&#32773;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;&#32500;&#24230;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#39564;&#35777;&#20102;&#25919;&#27835;&#12289;&#27668;&#20505;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#39118;&#38505;&#30340;&#20225;&#19994;&#23618;&#38754;&#39118;&#38505;&#25950;&#21475;&#24230;&#37327;&#12290;&#20351;&#29992;GPT 3.5&#27169;&#22411;&#20174;&#25910;&#30410;&#30005;&#35805;&#30340;&#32972;&#26223;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#39118;&#38505;&#25688;&#35201;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;GPT&#30340;&#24230;&#37327;&#20855;&#26377;&#26174;&#33879;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#22312;&#39044;&#27979;&#65288;&#24322;&#24120;&#65289;&#20225;&#19994;&#23618;&#38754;&#27874;&#21160;&#24615;&#21644;&#20225;&#19994;&#30340;&#36873;&#25321;&#65288;&#22914;&#25237;&#36164;&#21644;&#21019;&#26032;&#65289;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20449;&#24687;&#20248;&#20110;&#39118;&#38505;&#25688;&#35201;&#65292;&#36825;&#35777;&#26126;&#20102;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30693;&#35782;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#23545;&#20110;&#21457;&#29616;&#26032;&#20852;&#39118;&#38505;&#65288;&#22914;&#36817;&#20960;&#20010;&#23395;&#24230;&#39129;&#21319;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#65289;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;GPT&#30340;&#35757;&#32451;&#31383;&#21475;&#20869;&#22806;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#32929;&#26435;&#24066;&#22330;&#20013;&#23450;&#20215;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the value of generative AI tools, such as ChatGPT, in helping investors uncover dimensions of corporate risk. We develop and validate firm-level measures of risk exposure to political, climate, and AI-related risks. Using the GPT 3.5 model to generate risk summaries and assessments from the context provided by earnings call transcripts, we show that GPT-based measures possess significant information content and outperform the existing risk measures in predicting (abnormal) firm-level volatility and firms' choices such as investment and innovation. Importantly, information in risk assessments dominates that in risk summaries, establishing the value of general AI knowledge. We also find that generative AI is effective at detecting emerging risks, such as AI risk, which has soared in recent quarters. Our measures perform well both within and outside the GPT's training window and are priced in equity markets. Taken together, an AI-based approach to risk measurement provides usef
&lt;/p&gt;</description></item><item><title>&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.17715</link><description>&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#32534;&#30721;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17715
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#31034;&#34987;&#23569;&#25968;&#20960;&#20010;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#30340;&#24322;&#24120;&#32500;&#24230;&#25152;&#20027;&#23548;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#34429;&#28982;&#21435;&#38500;LLM&#34920;&#31034;&#20013;&#30340;&#24322;&#24120;&#32500;&#24230;&#20250;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#24322;&#24120;&#32500;&#24230;&#23545;&#23884;&#20837;&#34920;&#31034;&#30340;&#36136;&#37327;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#23545;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;1&#65289;&#22312;&#39044;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#24322;&#24120;&#32500;&#24230;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;2&#65289;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#34920;&#31034;&#22312;&#21333;&#20010;&#24322;&#24120;&#32500;&#24230;&#19978;&#30340;&#20540;&#20250;&#24433;&#21709;&#19979;&#28216;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17705</link><description>&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17705
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#30001;&#35821;&#20041;&#36890;&#20449;&#22686;&#24378;&#30340;&#26080;&#32447;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20379;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;AIGC&#26381;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26469;&#28385;&#36275;&#24191;&#22823;&#29992;&#25143;&#32676;&#20307;&#30340;&#38656;&#27714;&#12290;&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#30340;&#26222;&#21450;&#21644;&#31227;&#21160;&#27969;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#36890;&#36807;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#25552;&#20379;&#23545;&#39640;&#36136;&#37327;AIGC&#26381;&#21153;&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#35775;&#38382;&#24050;&#25104;&#20026;AIGC&#20135;&#21697;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#20449;&#36947;&#12289;&#26377;&#38480;&#30340;&#24102;&#23485;&#36164;&#28304;&#21644;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#25552;&#20379;&#26368;&#20248;&#30340;AIGC&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#22686;&#24378;&#30340;AIGC&#65288;SemAIGC&#65289;&#29983;&#25104;&#21644;&#20256;&#36755;&#26694;&#26550;&#65292;&#20854;&#20013;&#21482;&#38656;&#25552;&#21462;&#21644;&#20256;&#36755;&#20869;&#23481;&#30340;&#35821;&#20041;&#20449;&#24687;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#20108;&#36827;&#21046;&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SemAIGC&#22312;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#38598;&#25104;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23481;&#29983;&#25104;&#21644;&#28789;&#27963;&#35843;&#25972;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
&lt;/p&gt;</description></item><item><title>CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.17680</link><description>&lt;p&gt;
CodeFusion: &#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17680
&lt;/p&gt;
&lt;p&gt;
CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#24320;&#21457;&#32773;&#21482;&#33021;&#20462;&#25913;&#20854;&#26368;&#21518;&#19968;&#34892;&#20195;&#30721;&#65292;&#22312;&#27491;&#30830;&#20043;&#21069;&#65292;&#20182;&#20204;&#38656;&#35201;&#22810;&#23569;&#27425;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#20989;&#25968;&#21602;&#65311;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#26377;&#31867;&#20284;&#30340;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#23481;&#26131;&#37325;&#26032;&#32771;&#34385;&#20043;&#21069;&#29983;&#25104;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeFusion&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#20197;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#23436;&#25972;&#31243;&#24207;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#38024;&#23545;Bash&#12289;Python&#21644;Microsoft Excel&#26465;&#20214;&#26684;&#24335;(CF)&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#23545;CodeFusion&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CodeFusion&#65288;75M&#21442;&#25968;&#65289;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#65288;350M-175B&#21442;&#25968;&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;top-3&#21644;top-5&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#22312;&#22810;&#26679;&#24615;&#19982;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#65288;BOSS&#65289;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#65288;GSTs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#30334;&#20010;&#39640;&#24230;&#36830;&#25509;&#30340;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20174;fMRI&#25968;&#25454;&#20013;&#24674;&#22797;&#33041;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.17679</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#24555;&#36895;&#25193;&#23637;&#30340;DAG&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees. (arXiv:2310.17679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#65288;BOSS&#65289;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#65288;GSTs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#30334;&#20010;&#39640;&#24230;&#36830;&#25509;&#30340;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20174;fMRI&#25968;&#25454;&#20013;&#24674;&#22797;&#33041;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#24418;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20063;&#26159;&#22240;&#26524;&#21457;&#29616;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#36890;&#24120;&#38590;&#20197;&#36866;&#24212;&#20855;&#26377;&#25968;&#30334;&#20010;&#39640;&#24230;&#36830;&#25509;&#30340;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20174;fMRI&#25968;&#25454;&#20013;&#24674;&#22797;&#33041;&#32593;&#32476;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#20339;&#39034;&#24207;&#20998;&#25968;&#25628;&#32034;&#65288;BOSS&#65289;&#21644;&#29983;&#38271;-&#25910;&#32553;&#26641;&#65288;GSTs&#65289;&#29992;&#20110;&#22312;&#36825;&#20010;&#33539;&#20363;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#12290;BOSS&#36138;&#23146;&#22320;&#25628;&#32034;&#21464;&#37327;&#30340;&#25490;&#21015;&#65292;&#20351;&#29992;GSTs&#20174;&#25490;&#21015;&#26500;&#24314;&#21644;&#35780;&#20998;DAGs&#12290;GSTs&#26377;&#25928;&#22320;&#32531;&#23384;&#20998;&#25968;&#20197;&#28040;&#38500;&#20887;&#20313;&#35745;&#31639;&#12290;BOSS&#22312;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#19982;&#21508;&#31181;&#32452;&#21512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#21033;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#35777;&#26126;&#23427;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;BOSS&#24212;&#29992;&#20110;&#20004;&#32452;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#65306;&#24102;&#26377;&#20266;&#32463;&#39564;&#22122;&#22768;&#20998;&#24067;&#30340;&#27169;&#25311;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables -- for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#26377;&#25928;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.17671</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#20174;&#27169;&#22411;&#20256;&#36882;&#21040;&#30828;&#20214;&#22312;&#29615;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop. (arXiv:2310.17671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#26377;&#25928;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#25511;&#21046;&#21151;&#33021;&#26159;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#30340;&#25104;&#26412;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20855;&#26377;&#22312;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#19979;&#33258;&#21160;&#35757;&#32451;&#20195;&#29702;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#29983;&#25104;&#30340;&#25104;&#26412;&#21644;&#23433;&#20840;&#32422;&#26463;&#65292;&#20854;&#24212;&#29992;&#22823;&#22810;&#38480;&#20110;&#32431;&#31929;&#30340;&#27169;&#25311;&#39046;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#21151;&#33021;&#24320;&#21457;&#20013;&#20351;&#29992;RL&#65292;&#29983;&#25104;&#30340;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#21644;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#65288;XiL&#65289;&#27169;&#25311;&#26469;&#21152;&#36895;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23545;&#20110;&#20869;&#29123;&#26426;&#30340;&#30636;&#24577;&#24223;&#27668;&#20877;&#24490;&#29615;&#25511;&#21046;&#26696;&#20363;&#65292;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#27169;&#22411;&#22312;&#29615;&#65288;MiL&#65289;&#27169;&#25311;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#24494;&#35843;&#36229;&#21442;&#25968;&#65292;&#26368;&#21518;&#35757;&#32451;&#20505;&#36873;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RTNH+&#65292;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;4D&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#65306;&#22522;&#20110;CFAR&#30340;&#20004;&#32423;&#39044;&#22788;&#29702;&#31639;&#27861;&#21644;&#22402;&#30452;&#32534;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17659</link><description>&lt;p&gt;
RTNH+: &#20351;&#29992;&#22522;&#20110;CFAR&#30340;&#20004;&#32423;&#39044;&#22788;&#29702;&#21644;&#22402;&#30452;&#32534;&#30721;&#30340;&#22686;&#24378;&#22411;4D&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RTNH+: Enhanced 4D Radar Object Detection Network using Combined CFAR-based Two-level Preprocessing and Vertical Encoding. (arXiv:2310.17659v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RTNH+&#65292;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;4D&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#65306;&#22522;&#20110;CFAR&#30340;&#20004;&#32423;&#39044;&#22788;&#29702;&#31639;&#27861;&#21644;&#22402;&#30452;&#32534;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#32500;&#65288;4D&#65289;&#38647;&#36798;&#26159;&#19968;&#31181;&#22312;&#21508;&#31181;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#21644;&#21608;&#22260;&#29289;&#20307;&#30456;&#23545;&#24452;&#21521;&#36895;&#24230;&#20272;&#35745;&#30340;&#26377;&#29992;&#20256;&#24863;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38647;&#36798;&#27979;&#37327;&#20540;&#21463;&#21040;&#22122;&#22768;&#12289;&#24178;&#25200;&#21644;&#26434;&#27874;&#31561;&#26080;&#25928;&#22240;&#32032;&#30340;&#27745;&#26579;&#65292;&#38656;&#35201;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#20043;&#21069;&#36827;&#34892;&#39044;&#22788;&#29702;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RTNH+&#65292;&#23427;&#26159;RTNH&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#26159;&#19968;&#31181;4D&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#26159;&#22522;&#20110;&#24658;&#23450;&#34394;&#35686;&#29575;&#65288;CFAR&#65289;&#30340;&#20004;&#32423;&#39044;&#22788;&#29702;&#65288;CCTP&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#30456;&#21516;&#30340;4D&#38647;&#36798;&#27979;&#37327;&#20540;&#29983;&#25104;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#28388;&#27874;&#27979;&#37327;&#20540;&#65292;&#21487;&#20197;&#20016;&#23500;&#36755;&#20837;&#21040;4D&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#30340;&#20449;&#24687;&#12290;&#31532;&#20108;&#31181;&#26159;&#22402;&#30452;&#32534;&#30721;&#65288;VE&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;CCTP&#36755;&#20986;&#20013;&#32534;&#30721;&#36947;&#36335;&#30446;&#26631;&#30340;&#22402;&#30452;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RTNH+&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Four-dimensional (4D) Radar is a useful sensor for 3D object detection and the relative radial speed estimation of surrounding objects under various weather conditions. However, since Radar measurements are corrupted with invalid components such as noise, interference, and clutter, it is necessary to employ a preprocessing algorithm before the 3D object detection with neural networks. In this paper, we propose RTNH+ that is an enhanced version of RTNH, a 4D Radar object detection network, by two novel algorithms. The first algorithm is the combined constant false alarm rate (CFAR)-based two-level preprocessing (CCTP) algorithm that generates two filtered measurements of different characteristics using the same 4D Radar measurements, which can enrich the information of the input to the 4D Radar object detection network. The second is the vertical encoding (VE) algorithm that effectively encodes vertical features of the road objects from the CCTP outputs. We provide details of the RTNH+,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#37325;&#24314;&#36864;&#21270;&#35774;&#22791;&#30340;&#29289;&#29702;&#21442;&#25968;&#25110;&#26816;&#32034;&#29289;&#29702;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.17657</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#22120;&#20214;&#30340;&#39640;&#32423;&#32423;&#21035;-3&#21453;&#27169;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices. (arXiv:2310.17657v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17657
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#37325;&#24314;&#36864;&#21270;&#35774;&#22791;&#30340;&#29289;&#29702;&#21442;&#25968;&#25110;&#26816;&#32034;&#29289;&#29702;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#21453;&#27169;&#22411;&#24314;&#27169;&#28041;&#21450;&#35757;&#32451;&#28145;&#23618;&#32467;&#26500;&#20174;&#38745;&#24577;&#34892;&#20026;&#20013;&#39044;&#27979;&#35774;&#22791;&#21442;&#25968;&#12290;&#21453;&#35774;&#22791;&#24314;&#27169;&#36866;&#29992;&#20110;&#37325;&#24314;&#22312;&#26102;&#38388;&#19978;&#36864;&#21270;&#30340;&#35774;&#22791;&#30340;&#28418;&#31227;&#29289;&#29702;&#21442;&#25968;&#25110;&#26816;&#32034;&#29289;&#29702;&#37197;&#32622;&#12290;&#26377;&#24456;&#22810;&#21464;&#37327;&#21487;&#20197;&#24433;&#21709;&#21453;&#27169;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#30789;&#30899;&#21270;&#29289;&#21151;&#29575;MOSFET&#65288;SiC Power MOS&#65289;&#30340;&#32423;&#21035;-3&#27169;&#22411;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;SiC&#22120;&#20214;&#29992;&#20110;&#20256;&#32479;&#30789;&#22120;&#20214;&#22240;&#39640;&#28201;&#25110;&#39640;&#24320;&#20851;&#33021;&#21147;&#32780;&#22833;&#25928;&#30340;&#24212;&#29992;&#20013;&#12290;SiC&#21151;&#29575;&#22120;&#20214;&#30340;&#20027;&#35201;&#24212;&#29992;&#22312;&#27773;&#36710;&#39046;&#22495;&#65288;&#21363;&#22312;&#30005;&#21160;&#36710;&#39046;&#22495;&#65289;&#12290;&#30001;&#20110;&#29983;&#29702;&#36864;&#21270;&#25110;&#39640;&#24212;&#21147;&#29615;&#22659;&#65292;SiC&#21151;&#29575;MOS&#26174;&#31034;&#20986;&#29289;&#29702;&#21442;&#25968;&#30340;&#26174;&#33879;&#28418;&#31227;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21453;&#27169;&#22411;&#36827;&#34892;&#30417;&#27979;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Inverse modelling with deep learning algorithms involves training deep architecture to predict device's parameters from its static behaviour. Inverse device modelling is suitable to reconstruct drifted physical parameters of devices temporally degraded or to retrieve physical configuration. There are many variables that can influence the performance of an inverse modelling method. In this work the authors propose a deep learning method trained for retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). The SiC devices are used in applications where classical silicon devices failed due to high-temperature or high switching capability. The key application of SiC power devices is in the automotive field (i.e. in the field of electrical vehicles). Due to physiological degradation or high-stressing environment, SiC Power MOS shows a significant drift of physical parameters which can be monitored by using inverse modelling. The aim of this work is to 
&lt;/p&gt;</description></item><item><title>ACWA&#26159;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26234;&#33021;&#27700;&#21153;&#31995;&#32479;&#30340;&#32593;&#32476;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#21644;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#35299;&#20915;&#27700;&#21153;&#21644;&#20892;&#19994;&#39046;&#22495;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#21253;&#25324;&#32593;&#32476;&#29983;&#29289;&#23433;&#20840;&#12289;&#36164;&#28304;&#31649;&#29702;&#12289;&#27700;&#36164;&#28304;&#33719;&#21462;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#21644;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#31561;&#12290;</title><link>http://arxiv.org/abs/2310.17654</link><description>&lt;p&gt;
ACWA: &#19968;&#31181;&#22522;&#20110;AI&#30340;&#26234;&#33021;&#27700;&#21153;&#31995;&#32479;&#30340;&#32593;&#32476;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
ACWA: An AI-driven Cyber-Physical Testbed for Intelligent Water Systems. (arXiv:2310.17654v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17654
&lt;/p&gt;
&lt;p&gt;
ACWA&#26159;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26234;&#33021;&#27700;&#21153;&#31995;&#32479;&#30340;&#32593;&#32476;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#21644;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#35299;&#20915;&#27700;&#21153;&#21644;&#20892;&#19994;&#39046;&#22495;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#21253;&#25324;&#32593;&#32476;&#29983;&#29289;&#23433;&#20840;&#12289;&#36164;&#28304;&#31649;&#29702;&#12289;&#27700;&#36164;&#28304;&#33719;&#21462;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#21644;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21331;&#36234;&#32593;&#32476;&#29289;&#29702;&#27700;&#21153;&#27979;&#35797;&#24179;&#21488;&#65292;&#21517;&#20026;AI&#21644;&#32593;&#32476;&#23545;&#27700;&#21644;&#20892;&#19994;&#27979;&#35797;&#24179;&#21488;&#65288;ACWA&#65289;&#12290;ACWA&#30340;&#21160;&#26426;&#26159;&#21033;&#29992;AI&#21644;&#32593;&#32476;&#23433;&#20840;&#23454;&#39564;&#26469;&#25512;&#36827;&#27700;&#20379;&#24212;&#31649;&#29702;&#12290;ACWA&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#21644;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#35299;&#20915;&#27700;&#21153;&#21644;&#20892;&#19994;&#39046;&#22495;&#30340;&#32039;&#36843;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#32593;&#32476;&#29983;&#29289;&#23433;&#20840;&#12289;&#36164;&#28304;&#31649;&#29702;&#12289;&#27700;&#36164;&#28304;&#33719;&#21462;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#21644;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;ACWA&#30001;&#22810;&#20010;&#25299;&#25169;&#32467;&#26500;&#12289;&#20256;&#24863;&#22120;&#12289;&#35745;&#31639;&#33410;&#28857;&#12289;&#27893;&#12289;&#27700;&#31665;&#12289;&#26234;&#33021;&#27700;&#21153;&#35774;&#22791;&#12289;&#25968;&#25454;&#24211;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;AI&#27169;&#22411;&#32452;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;ACWA&#27169;&#25311;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#30340;&#27700;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;&#27169;&#25311;&#22120;&#22522;&#20110;&#27969;&#20307;&#21644;&#32452;&#20998;&#36755;&#36816;&#21407;&#29702;&#29983;&#25104;&#27700;&#37197;&#36865;&#31995;&#32479;&#30340;&#29702;&#35770;&#26102;&#38388;&#24207;&#21015;&#65292;&#20026;&#31995;&#32479;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#39564;&#35777;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript presents a novel state-of-the-art cyber-physical water testbed, namely: The AI and Cyber for Water and Agriculture testbed (ACWA). ACWA is motivated by the need to advance water supply management using AI and Cybersecurity experimentation. The main goal of ACWA is to address pressing challenges in the water and agricultural domains by utilising cutting-edge AI and data-driven technologies. These challenges include Cyberbiosecurity, resources management, access to water, sustainability, and data-driven decision-making, among others. To address such issues, ACWA consists of multiple topologies, sensors, computational nodes, pumps, tanks, smart water devices, as well as databases and AI models that control the system. Moreover, we present ACWA simulator, which is a software-based water digital twin. The simulator runs on fluid and constituent transport principles that produce theoretical time series of a water distribution system. This creates a good validation point for c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#35889;&#23545;&#40784;&#26694;&#26550;&#65288;SPA&#65289;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#21407;&#35821;&#65292;&#24182;&#32467;&#21512;&#26032;&#39062;&#30340;&#35889;&#27491;&#21017;&#21270;&#22120;&#21644;&#37051;&#22495;&#24863;&#30693;&#33258;&#35757;&#32451;&#26426;&#21046;&#65292;SPA&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#40784;&#39046;&#22495;&#22270;&#24182;&#25552;&#39640;&#20102;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17594</link><description>&lt;p&gt;
SPA: &#22522;&#20110;&#22270;&#35889;&#23545;&#40784;&#35270;&#35282;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPA: A Graph Spectral Alignment Perspective for Domain Adaptation. (arXiv:2310.17594v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#35889;&#23545;&#40784;&#26694;&#26550;&#65288;SPA&#65289;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#21407;&#35821;&#65292;&#24182;&#32467;&#21512;&#26032;&#39062;&#30340;&#35889;&#27491;&#21017;&#21270;&#22120;&#21644;&#37051;&#22495;&#24863;&#30693;&#33258;&#35757;&#32451;&#26426;&#21046;&#65292;SPA&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#40784;&#39046;&#22495;&#22270;&#24182;&#25552;&#39640;&#20102;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#37325;&#35201;&#30340;&#24418;&#24335;&#65292;&#23427;&#21487;&#20197;&#23558;&#39046;&#22495;&#20869;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20851;&#27880;&#20110;&#25429;&#25417;&#36328;&#39046;&#22495;&#21487;&#20256;&#36882;&#24615;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#39046;&#22495;&#20869;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#23548;&#33268;&#20102;&#26356;&#24046;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#35889;&#23545;&#40784;&#26694;&#26550;&#65288;SPA&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#27010;&#35201;&#22914;&#19979;&#65306;&#65288;i&#65289;&#36890;&#36807;&#23558;DA&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#21407;&#35821;&#65292;SPA&#23558;&#31895;&#31890;&#24230;&#30340;&#22270;&#23545;&#40784;&#26426;&#21046;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#27491;&#21017;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#40784;&#39046;&#22495;&#22270;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#28040;&#24687;&#20256;&#36882;&#27169;&#22359;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#37051;&#22495;&#24863;&#30693;&#33258;&#35757;&#32451;&#26426;&#21046;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#21487;&#21306;&#20998;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;SPA&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#24050;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to extend the in-domain model to the distinctive target domains where the data distributions differ. Most prior works focus on capturing the inter-domain transferability but largely overlook rich intra-domain structures, which empirically results in even worse discriminability. In this work, we introduce a novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The core of our method is briefly condensed as follows: (i)-by casting the DA problem to graph primitives, SPA composes a coarse graph alignment mechanism with a novel spectral regularizer towards aligning the domain graphs in eigenspaces; (ii)-we further develop a fine-grained message propagation module -- upon a novel neighbor-aware self-training mechanism -- in order for enhanced discriminability in the target domain. On standardized benchmarks, the extensive experiments of SPA demonstrate that its performance has surpassed the existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17550</link><description>&lt;p&gt;
&#20154;&#31867;&#24341;&#23548;&#30340;&#22797;&#26434;&#24230;&#25511;&#21046;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#21508;&#31181;&#25277;&#35937;&#32423;&#21035;&#65288;&#20363;&#22914;&#65292;&#8220;&#40479;&#8221;&#19982;&#8220;&#40635;&#38592;&#8221;&#65289;&#19978;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65288;&#21363;&#27010;&#24565;&#25110;&#21333;&#35789;&#65289;&#65292;&#24182;&#26681;&#25454;&#20219;&#21153;&#20351;&#29992;&#36866;&#24403;&#30340;&#25277;&#35937;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#20998;&#24067;&#30340;&#29109;&#26469;&#25511;&#21046;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#65288;&#22823;&#33268;&#19978;&#26159;&#20026;&#32534;&#30721;&#36755;&#20837;&#20998;&#37197;&#20102;&#22810;&#23569;&#20301;&#65289;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#31034;&#20363;&#29992;&#20110;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;&#35843;&#25972;&#34920;&#31034;&#20197;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#39640;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#19968;&#20010;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#31163;&#25955;&#34920;&#31034;&#30340;&#21487;&#35270;&#21270;&#26469;&#30830;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24403;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17526</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#20154;&#31867;&#22312;&#31995;&#32479;&#35780;&#20215;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#65311;&#35780;&#20272;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#20215;&#23545;&#20110;&#25351;&#23548;&#23454;&#36341;&#12289;&#30740;&#31350;&#21644;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#33021;&#22815;&#21152;&#24555;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#35780;&#20215;&#30340;&#36807;&#31243;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#32780;&#19988;&#36824;&#27809;&#26377;&#30740;&#31350;&#27979;&#35797;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;LLM&#8212;&#8212;GPT-4&#12290;&#26412;&#39044;&#27880;&#20876;&#30740;&#31350;&#37319;&#29992;&#8220;&#26080;&#20154;&#21442;&#19982;&#8221;&#30340;&#26041;&#27861;&#35780;&#20272;&#20102;GPT-4&#22312;&#26631;&#39064;/&#25688;&#35201;&#31579;&#36873;&#12289;&#20840;&#25991;&#23457;&#26597;&#21644;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#22312;&#19981;&#21516;&#25991;&#29486;&#31867;&#22411;&#21644;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#32467;&#26524;&#21463;&#21040;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#35843;&#25972;&#20102;&#36825;&#20123;&#22240;&#32032;&#21518;&#65292;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#20351;&#29992;&#39640;&#21487;&#38752;&#24615;&#25552;&#31034;&#36827;&#34892;&#31579;&#36873;&#30340;&#30740;&#31350;&#20013;&#65292;&#31579;&#36873;&#20840;&#25991;&#25991;&#29486;&#30340;&#34920;&#29616;&#27700;&#24179;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#35821;&#35328;&#19978;&#22343;&#20026;&#26080;&#21040;&#20013;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102; CoRe Challenge 2023 &#20013;&#25552;&#20132;&#30340;&#27714;&#35299;&#22120;&#21644; ISR &#23454;&#20363;&#30340;&#25152;&#26377;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.17136</link><description>&lt;p&gt;
Core Challenge 2023: &#27714;&#35299;&#22120;&#21644;&#22270;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Core Challenge 2023: Solver and Graph Descriptions. (arXiv:2310.17136v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102; CoRe Challenge 2023 &#20013;&#25552;&#20132;&#30340;&#27714;&#35299;&#22120;&#21644; ISR &#23454;&#20363;&#30340;&#25152;&#26377;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25910;&#38598;&#20102;&#25552;&#20132;&#32473; CoRe Challenge 2023 &#30340;&#27714;&#35299;&#22120;&#21644; ISR &#23454;&#20363;&#30340;&#25152;&#26377;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper collects all descriptions of solvers and ISR instances submitted to CoRe Challenge 2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16779</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#38543;&#26426;&#24179;&#28369;&#24050;&#25104;&#20026;&#23569;&#25968;&#20960;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#8220;&#21435;&#22122;&#21644;&#20998;&#31867;&#8221;&#27969;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#21435;&#22122;&#24179;&#28369;&#65292;&#22312;&#20219;&#20309;&#20998;&#31867;&#22120;&#19978;&#25191;&#34892;&#38543;&#26426;&#24179;&#28369;&#65292;&#21069;&#25552;&#26159;&#26377;&#19968;&#20010;&#20934;&#30830;&#30340;&#21435;&#22122;&#22120;&#21487;&#29992;&#65292;&#27604;&#22914;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#36136;&#30097;&#21738;&#31181;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#31034;&#24418;&#24335;&#33021;&#22815;&#26368;&#22823;&#21270;&#21435;&#22122;&#24179;&#28369;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#26088;&#22312;&#23454;&#29616;&#20849;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#21516;&#26102;&#20063;&#20026;&#20854;&#35748;&#35777;&#40065;&#26834;&#24615;&#34917;&#20607;&#20934;&#30830;&#24230;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#36890;&#29992;&#22330;&#26223;&#25551;&#36848;&#32763;&#35793;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#23545;&#26426;&#22120;&#20154;&#29615;&#22659;&#30340;&#35821;&#20041;&#26597;&#35810;&#21644;&#38598;&#25104;&#20854;&#20182;&#30693;&#35782;&#28304;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16737</link><description>&lt;p&gt;
&#23558;&#36890;&#29992;&#22330;&#26223;&#25551;&#36848;&#32763;&#35793;&#25104;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#26426;&#22120;&#20154;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment. (arXiv:2310.16737v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#36890;&#29992;&#22330;&#26223;&#25551;&#36848;&#32763;&#35793;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#23545;&#26426;&#22120;&#20154;&#29615;&#22659;&#30340;&#35821;&#20041;&#26597;&#35810;&#21644;&#38598;&#25104;&#20854;&#20182;&#30693;&#35782;&#28304;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25191;&#34892;&#20154;&#31867;&#35268;&#27169;&#30340;&#25805;&#20316;&#20219;&#21153;&#38656;&#35201;&#23545;&#21608;&#22260;&#29615;&#22659;&#26377;&#22823;&#37327;&#30340;&#30693;&#35782;&#65292;&#20197;&#20415;&#33021;&#22815;&#32988;&#20219;&#21644;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21160;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#20316;&#20026;&#26426;&#22120;&#20154;&#29615;&#22659;&#24314;&#27169;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22330;&#26223;&#22270;&#32763;&#35793;&#25104;&#30693;&#35782;&#24211;&#30340;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#36890;&#29992;&#22330;&#26223;&#25551;&#36848;&#65288;USD&#65289;&#26684;&#24335;&#65292;&#35813;&#26684;&#24335;&#26159;&#29992;&#20110;&#32534;&#20889;&#12289;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#22797;&#26434;&#29615;&#22659;&#30340;&#26032;&#20852;&#26631;&#20934;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;USD&#30340;&#29615;&#22659;&#27169;&#22411;&#36716;&#25442;&#20026;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#65292;&#20197;&#20415;&#23454;&#29616;&#35821;&#20041;&#26597;&#35810;&#21644;&#19982;&#20854;&#20182;&#30693;&#35782;&#28304;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots performing human-scale manipulation tasks require an extensive amount of knowledge about their surroundings in order to perform their actions competently and human-like. In this work, we investigate the use of virtual reality technology as an implementation for robot environment modeling, and present a technique for translating scene graphs into knowledge bases. To this end, we take advantage of the Universal Scene Description (USD) format which is an emerging standard for the authoring, visualization and simulation of complex environments. We investigate the conversion of USD-based environment models into Knowledge Graph (KG) representations that facilitate semantic querying and integration with additional knowledge sources.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16546</link><description>&lt;p&gt;
&#20048;&#35266;&#20027;&#20041;&#30340;&#38519;&#38449;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#65292;&#22914;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24046;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25910;&#38598;&#30340;&#20559;&#24046;&#65292;&#38459;&#30861;&#25910;&#25947;&#25110;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#21333;&#21521;&#20542;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#25197;&#26354;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#25200;&#21160;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#20855;&#26377;&#36739;&#24369;&#30340;&#25910;&#32553;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#20250;&#38519;&#20837;&#20559;&#21521;&#24615;&#30340;&#25506;&#32034;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;</title><link>http://arxiv.org/abs/2310.16506</link><description>&lt;p&gt;
&#35782;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#65306;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#30340;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#26500;&#24314;&#20844;&#24179;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#20010;&#20154;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#20110;&#21738;&#20123;&#20010;&#20307;&#34987;&#19981;&#20844;&#24179;&#22320;&#20998;&#31867;&#27809;&#26377;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22312;&#20844;&#24179;&#39046;&#22495;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35782;&#21035;&#24046;&#24322;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15970</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#38899;&#23545;&#20110;&#29616;&#26377;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21475;&#38899;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#38459;&#30861;&#20102;ASR&#30340;&#26222;&#21450;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#36825;&#20123;&#21487;&#23398;&#20064;&#30340;&#20195;&#30721;&#26412;&#25429;&#25417;&#20102;&#21475;&#38899;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;ASR&#32534;&#30721;&#22120;&#23618;&#20013;&#12290;&#27169;&#22411;&#22312;&#24102;&#21475;&#38899;&#30340;&#33521;&#35821;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21475;&#38899;&#12290;&#22312;Mozilla Common Voice&#22810;&#21475;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#20165;&#22312;&#24050;&#35265;&#30340;&#33521;&#35821;&#21475;&#38899;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#21333;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;37%&#65289;&#65292;&#32780;&#19988;&#22312;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#20063;&#33719;&#24471;&#20102;5%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;L2Artic&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#35774;&#32622;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.15211</link><description>&lt;p&gt;
&#27169;&#25311;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#36335;&#24452;&#37325;&#35201;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing. (arXiv:2310.15211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33647;&#29289;&#37325;&#29992;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#21457;&#29616;&#33539;&#24335;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#12290;&#22312;&#21508;&#31181;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#65292;&#25972;&#21512;&#22810;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65288;&#22914;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65289;&#65292;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#32593;&#32476;&#20013;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#23545;&#20110;&#35782;&#21035;&#33647;&#29289;&#30340;&#27835;&#30103;&#25928;&#26524;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20182;&#39046;&#22495;&#21457;&#29616;&#65292;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20381;&#36182;&#20110;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#23545;&#33647;&#29289;&#37325;&#29992;&#23581;&#35797;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPI&#65288;&#27169;&#25311;&#36335;&#24452;&#37325;&#35201;&#24615;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#12290;MPI&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, drug repurposing has emerged as an effective and resource-efficient paradigm for AD drug discovery. Among various methods for drug repurposing, network-based methods have shown promising results as they are capable of leveraging complex networks that integrate multiple interaction types, such as protein-protein interactions, to more effectively identify candidate drugs. However, existing approaches typically assume paths of the same length in the network have equal importance in identifying the therapeutic effect of drugs. Other domains have found that same length paths do not necessarily have the same importance. Thus, relying on this assumption may be deleterious to drug repurposing attempts. In this work, we propose MPI (Modeling Path Importance), a novel network-based method for AD drug repurposing. MPI is unique in that it prioritizes important paths via learned node embeddings, which can effectively capture a network's rich structural information. Thus, leveraging learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#19982;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#21644;&#32454;&#32990;&#31995;&#30284;&#30151;&#31867;&#22411;&#30456;&#20851;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#23545;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.13725</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#65292;&#20197;&#25913;&#21892;&#25239;&#30284;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Enhancing drug and cell line representations via contrastive learning for improved anti-cancer drug prioritization. (arXiv:2310.13725v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13725
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#19982;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#21644;&#32454;&#32990;&#31995;&#30284;&#30151;&#31867;&#22411;&#30456;&#20851;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#23545;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30284;&#30151;&#30340;&#22797;&#26434;&#24615;&#21644;&#23545;&#27835;&#30103;&#30340;&#21487;&#21464;&#21453;&#24212;&#65292;&#36890;&#36807;&#22522;&#22240;&#32452;&#23398;&#24207;&#21015;&#20998;&#26512;&#36827;&#34892;&#30340;&#31934;&#30830;&#20010;&#20307;&#21270;&#30284;&#30151;&#27835;&#30103;&#24050;&#25104;&#20026;&#24403;&#21069;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#24739;&#32773;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#20351;&#24471;&#24555;&#36895;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#21463;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#22952;&#30861;&#20102;&#35745;&#31639;&#26041;&#27861;&#23398;&#20064;&#19982;&#26377;&#25928;&#33647;&#29289;-&#32454;&#32990;&#31995;&#37197;&#23545;&#30456;&#20851;&#30340;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#23398;&#20064;&#21040;&#30340;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#34920;&#24449;&#65292;&#20197;&#20445;&#30041;&#19982;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#21644;&#32454;&#32990;&#31995;&#30284;&#30151;&#31867;&#22411;&#30456;&#20851;&#30340;&#20851;&#31995;&#32467;&#26500;&#12290;&#38500;&#20102;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#30340;&#20998;&#31867;&#22120;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#26356;&#21152;&#24179;&#34913;&#22320;&#20381;&#36182;&#20110;&#33647;&#29289;&#21644;&#32454;&#32990;&#31995;&#29305;&#24449;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#20010;&#24615;&#21270;&#30340;&#33647;&#29289;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20854;&#22522;&#20110;&#19982;&#33647;&#29289;&#32784;&#33647;&#24615;&#30456;&#20851;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to cancer's complex nature and variable response to therapy, precision oncology informed by omics sequence analysis has become the current standard of care. However, the amount of data produced for each patients makes it difficult to quickly identify the best treatment regimen. Moreover, limited data availability has hindered computational methods' abilities to learn patterns associated with effective drug-cell line pairs. In this work, we propose the use of contrastive learning to improve learned drug and cell line representations by preserving relationship structures associated with drug mechanism of action and cell line cancer types. In addition to achieving enhanced performance relative to a state-of-the-art method, we find that classifiers using our learned representations exhibit a more balances reliance on drug- and cell line-derived features when making predictions. This facilitates more personalized drug prioritizations that are informed by signals related to drug resistan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12298</link><description>&lt;p&gt;
Jorge: GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#30340;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19982;&#19968;&#38454;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#20108;&#38454;&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#22823;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#19968;&#30452;&#19981;&#22826;&#21463;&#27426;&#36814;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#20013;&#30340;&#20027;&#35201;&#25928;&#29575;&#29942;&#39048;&#26159;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#30340;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#65292;&#22312;GPU&#19978;&#35745;&#31639;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Jorge&#65292;&#19968;&#31181;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#29305;&#24615;&#21644;&#19968;&#38454;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#28040;&#38500;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#29942;&#39048;&#65292;&#29992;&#36817;&#20284;&#30340;&#39044;&#22788;&#29702;&#22120;&#35745;&#31639;&#26367;&#20195;&#12290;&#36825;&#20351;&#24471;Jorge&#22312;&#22681;&#38047;&#26102;&#38388;&#19978;&#22312;GPU&#19978;&#38750;&#24120;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35843;&#25972;&#33391;&#22909;&#30340;SGD&#22522;&#20934;&#20013;&#30830;&#23450;Jorge&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#35843;&#21442;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;Jorge&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate
&lt;/p&gt;</description></item><item><title>MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.08252</link><description>&lt;p&gt;
MetaBox&#65306;&#19968;&#31181;&#29992;&#20110;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08252
&lt;/p&gt;
&lt;p&gt;
MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;MetaBBO-RL&#65289;&#23637;&#31034;&#20102;&#22312;&#20803;&#32423;&#21035;&#19978;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20943;&#23569;&#23545;&#20302;&#32423;&#40657;&#31665;&#20248;&#21270;&#22120;&#30340;&#25163;&#21160;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#32780;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaBox&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;MetaBBO-RL&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#12290;MetaBox&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#24179;&#21488;&#20869;&#23454;&#29616;&#33258;&#24049;&#30340;&#29420;&#29305;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#36229;&#36807;300&#20010;&#38382;&#39064;&#23454;&#20363;&#65292;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#22330;&#26223;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;19&#31181;&#22522;&#32447;&#26041;&#27861;&#30340;&#35814;&#23613;&#24211;&#65292;&#21253;&#25324;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#22120;&#21644;&#26368;&#36817;&#30340;MetaBBO-RL&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MetaBox&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20351;&#26041;&#27861;&#30340;&#35780;&#20272;&#26356;&#21152;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07747</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#36131;&#21046;&#65306;&#29992;&#35821;&#26009;&#24211;&#30340;&#20363;&#23376;&#35299;&#37322;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#38477;&#20302;&#22312;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22312;&#36131;&#20219;&#25935;&#24863;&#30340;&#35774;&#32622;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#20915;&#31574;&#38382;&#36131;&#21046;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Accountable Offline Controller&#65288;AOC&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#24182;&#26681;&#25454;&#19968;&#32452;&#23450;&#21046;&#30340;&#20363;&#23376;&#65288;&#31216;&#20026;&#35821;&#26009;&#24211;&#23376;&#38598;&#65289;&#36827;&#34892;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#12290;AOC&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20005;&#26684;&#30340;&#31163;&#32447;&#27169;&#20223;&#35774;&#32622;&#65292;&#24182;&#34920;&#29616;&#20986;&#20445;&#25252;&#21644;&#36866;&#24212;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#21307;&#30103;&#20445;&#20581;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;AOC&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#20445;&#25345;&#38382;&#36131;&#21046;&#30340;&#21516;&#26102;&#33021;&#22815;&#31649;&#29702;&#39640;&#27700;&#24179;&#30340;&#31163;&#32447;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05351</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31867;&#21035;&#19979;&#30340;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Collapse for a Large Number of Classes. (arXiv:2310.05351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#25552;&#20379;&#20102;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#26368;&#21518;&#19968;&#23618;&#34920;&#31034;&#65288;&#21363;&#29305;&#24449;&#65289;&#21644;&#20998;&#31867;&#22120;&#26435;&#37325;&#30340;&#20248;&#38597;&#25968;&#23398;&#25551;&#36848;&#12290;&#36825;&#31181;&#32467;&#26524;&#19981;&#20165;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#65292;&#36824;&#28608;&#21457;&#20102;&#25913;&#36827;&#23454;&#38469;&#28145;&#24230;&#27169;&#22411;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#23849;&#28291;&#30340;&#29616;&#26377;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#31867;&#21035;&#25968;&#30456;&#23545;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#35821;&#35328;&#27169;&#22411;&#12289;&#26816;&#32034;&#31995;&#32479;&#21644;&#20154;&#33080;&#35782;&#21035;&#24212;&#29992;&#20013;&#24191;&#27867;&#20986;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#23637;&#29616;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#23567;&#30340;&#19968;&#23545;&#20854;&#20182;&#31867;&#21035;&#38388;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;&#23454;&#38469;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#30340;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#34920;&#26126;&#8230;.
&lt;/p&gt;
&lt;p&gt;
Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>LEGO-Prover&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#30340;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#29616;&#26377;&#25216;&#33021;&#21644;&#21019;&#36896;&#26032;&#25216;&#33021;&#26469;&#35777;&#26126;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.00656</link><description>&lt;p&gt;
LEGO-Prover: &#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#24211;&#36827;&#34892;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
LEGO-Prover: Neural Theorem Proving with Growing Libraries. (arXiv:2310.00656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00656
&lt;/p&gt;
&lt;p&gt;
LEGO-Prover&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#30340;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#29616;&#26377;&#25216;&#33021;&#21644;&#21019;&#36896;&#26032;&#25216;&#33021;&#26469;&#35777;&#26126;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23450;&#29702;&#35777;&#26126;&#20173;&#28982;&#26159;&#26368;&#38590;&#30340;&#25512;&#29702;&#20219;&#21153;&#20043;&#19968;&#65292;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#20197;&#24448;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#38590;&#20197;&#35777;&#26126;&#20013;&#23398;&#27700;&#24179;&#30340;&#23450;&#29702;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38480;&#21046;&#26159;&#22312;&#25972;&#20010;&#23450;&#29702;&#35777;&#26126;&#36807;&#31243;&#20013;&#20551;&#35774;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#23450;&#29702;&#24211;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#21019;&#36896;&#26032;&#30340;&#26377;&#29992;&#23450;&#29702;&#29978;&#33267;&#26032;&#30340;&#29702;&#35770;&#19981;&#20165;&#26377;&#24110;&#21161;&#32780;&#19988;&#23545;&#20110;&#25512;&#21160;&#25968;&#23398;&#21457;&#23637;&#21644;&#35777;&#26126;&#26356;&#22256;&#38590;&#21644;&#28145;&#20837;&#30340;&#32467;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LEGO-Prover&#65292;&#23427;&#37319;&#29992;&#19968;&#20010;&#21253;&#21547;&#32463;&#36807;&#39564;&#35777;&#30340;&#24341;&#29702;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#65292;&#20197;&#22686;&#24378;&#29992;&#20110;&#23450;&#29702;&#35777;&#26126;&#30340;LLMs&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#26500;&#24314;&#35777;&#26126;&#65292;LEGO-Prover&#20351;LLMs&#33021;&#22815;&#21033;&#29992;&#20174;&#24211;&#26816;&#32034;&#21040;&#30340;&#29616;&#26377;&#25216;&#33021;&#65292;&#20197;&#21450;&#22312;&#35777;&#26126;&#36807;&#31243;&#20013;&#21019;&#24314;&#26032;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#36890;&#36807;&#31616;&#21270;&#35777;&#26126;&#36807;&#31243;&#24182;&#25552;&#20379;&#26356;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#36335;&#24452;&#27169;&#22411;&#12289;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#30340;&#25366;&#25496;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.14208</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Framework based on complex networks to model and mine patient pathways. (arXiv:2309.14208v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#36335;&#24452;&#27169;&#22411;&#12289;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#30340;&#25366;&#25496;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#29992;&#20110;&#34920;&#31034;&#19968;&#32452;&#24739;&#32773;&#19982;&#21307;&#30103;&#31995;&#32479;&#30340;&#25509;&#35302;&#21382;&#21490;&#30340;&#27169;&#22411;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#24739;&#32773;&#36335;&#24452;&#8221;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#25903;&#25345;&#20020;&#24202;&#21644;&#32452;&#32455;&#20915;&#31574;&#65292;&#20197;&#25552;&#39640;&#25552;&#20379;&#30340;&#27835;&#30103;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#24930;&#24615;&#30149;&#24739;&#32773;&#30340;&#36335;&#24452;&#24448;&#24448;&#22240;&#20154;&#32780;&#24322;&#65292;&#26377;&#37325;&#22797;&#30340;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#20998;&#26512;&#22810;&#20010;&#26041;&#38754;&#65288;&#24178;&#39044;&#12289;&#35786;&#26029;&#12289;&#21307;&#30103;&#19987;&#19994;&#31561;&#65289;&#65292;&#24433;&#21709;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#24314;&#27169;&#21644;&#25366;&#25496;&#36825;&#20123;&#36335;&#24452;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#22522;&#20110;&#22810;&#26041;&#38754;&#22270;&#30340;&#36335;&#24452;&#27169;&#22411;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#32791;&#26102;&#65292;&#29992;&#20110;&#27604;&#36739;&#36335;&#24452;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#25366;&#25496;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36335;&#24452;&#20013;&#26368;&#30456;&#20851;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#35780;&#20272;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic discovery of a model to represent the history of encounters of a group of patients with the healthcare system -- the so-called "pathway of patients" -- is a new field of research that supports clinical and organisational decisions to improve the quality and efficiency of the treatment provided. The pathways of patients with chronic conditions tend to vary significantly from one person to another, have repetitive tasks, and demand the analysis of multiple perspectives (interventions, diagnoses, medical specialities, among others) influencing the results. Therefore, modelling and mining those pathways is still a challenging task. In this work, we propose a framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a novel dissimilarity measurement to compare pathways taking the elapsed time into account, and (iii) a mining method based on traditional centrality measures to discover the most relevant steps of the pathways. We evaluated the framework using 
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06520</link><description>&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#23558;&#21508;&#20010;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;&#21516;&#26102;&#65292;&#35299;&#30721;&#20934;&#21017;&#19982;&#35780;&#20272;&#20934;&#21017;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#21487;&#20197;&#29992;&#20110;&#20197;&#26356;&#22909;&#22320;&#19982;&#26368;&#32456;&#35780;&#20272;&#20934;&#21017;&#23545;&#40784;&#30340;&#26041;&#24335;&#32452;&#21512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#31995;&#32479;&#20013;&#30340;MBR&#35299;&#30721;&#65292;&#35813;&#31995;&#32479;&#36890;&#24120;&#20197;&#32534;&#36753;&#27425;&#25968;&#21644;&#30456;&#20851;&#30340;F&#20998;&#25968;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#36825;&#31181;&#20934;&#21017;&#30452;&#25509;&#30456;&#20851;&#30340;&#26032;&#39062;MBR&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#25193;&#23637;&#20505;&#36873;&#21477;&#23376;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24403;&#21069;&#30340;&#26368;&#22823;&#25237;&#31080;&#32452;&#21512;&#26041;&#26696;&#65292;&#20197;&#21450;&#20010;&#20307;&#32534;&#36753;&#32423;&#21035;&#30340;&#36873;&#25321;&#12290;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;GEC&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;GEC&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;MBR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#31361;&#20986;&#20102;MBR&#35299;&#30721;&#20013;&#19981;&#21516;&#22870;&#21169;&#25351;&#26631;&#30340;&#21464;&#21270;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
For sequence-to-sequence tasks it is challenging to combine individual system outputs. Further, there is also often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Hence, we propose a novel MBR loss function directly linked to this form of criterion. Furthermore, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Additionally, the paper highlights how varying reward metrics within the MBR d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06364</link><description>&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#30001;&#22238;&#31572;&#30340;&#23450;&#24615;&#20998;&#26512;&#65306;&#31639;&#27861;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#33258;&#30001;&#22238;&#31572;&#38754;&#35797;&#38382;&#39064;&#65292;&#23601;&#20687;&#20256;&#32479;&#19978;&#20351;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#20998;&#26512;&#30340;&#37027;&#26679;&#12290;&#23450;&#24615;&#26041;&#27861;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#28041;&#21450;&#23545;&#24320;&#25918;&#24335;&#35775;&#35848;&#25110;&#33258;&#30001;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#23545;LLMs&#29983;&#25104;&#30340;"&#30789;&#21442;&#19982;&#32773;"&#36827;&#34892;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#32676;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#36825;&#26159;&#30001;Argyle&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20122;&#32676;&#20307;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#30340;&#31243;&#24230;&#30456;&#21563;&#21512;&#12290;&#26681;&#25454;&#23450;&#20041;&#65292;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#34920;&#26126;&#20174;LLMs&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#20449;&#24565;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#65292;&#32780;&#20302;&#31639;&#27861;&#20445;&#30495;&#24230;&#21017;&#20351;&#24471;&#36825;&#26679;&#30340;&#30740;&#31350;&#26080;&#25928;&#12290;&#26412;&#25991;&#20351;&#29992;LLM&#29983;&#25104;&#38754;&#35797;&#38382;&#31572;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01507</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#20855;&#26377;4&#20301;&#29366;&#24577;&#30340;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Memory Efficient Optimizers with 4-bit States. (arXiv:2309.01507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#22120;&#29366;&#24577;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#20027;&#35201;&#20869;&#23384;&#28040;&#32791;&#26469;&#28304;&#65292;&#38480;&#21046;&#20102;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#20869;&#21487;&#35757;&#32451;&#30340;&#26368;&#22823;&#27169;&#22411;&#12290;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20174;32&#20301;&#28014;&#28857;&#25968;&#21387;&#32553;&#21040;&#26356;&#20302;&#30340;&#20301;&#23485;&#26377;&#26395;&#20943;&#23567;&#35757;&#32451;&#20869;&#23384;&#21344;&#29992;&#65292;&#32780;&#24403;&#21069;&#26368;&#20302;&#21487;&#36798;&#21040;&#30340;&#20301;&#23485;&#20026;8&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20301;&#23485;&#38477;&#33267;4&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#30697;&#20855;&#26377;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#65292;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#20934;&#30830;&#36817;&#20284;&#12290;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#65292;&#24182;&#25552;&#20986;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#65292;&#21253;&#25324;&#30452;&#25509;&#35810;&#38382;&#27979;&#35797;&#12289;&#20018;&#34892;&#25110;&#36866;&#24212;&#24615;&#25925;&#20107;&#27979;&#35797;&#12289;&#38544;&#24615;&#20851;&#32852;&#27979;&#35797;&#21644;&#26410;&#30693;&#24773;&#22659;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22810;&#32500;&#35780;&#20272;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#24182;&#20197;&#25945;&#32946;&#37096;&#38376;&#20026;&#26696;&#20363;&#30740;&#31350;&#26500;&#24314;&#20102;Edu-FairMonitor&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.10397</link><description>&lt;p&gt;
FairMonitor: &#19968;&#31181;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#30340;&#22235;&#38454;&#27573;&#33258;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models. (arXiv:2308.10397v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#65292;&#21253;&#25324;&#30452;&#25509;&#35810;&#38382;&#27979;&#35797;&#12289;&#20018;&#34892;&#25110;&#36866;&#24212;&#24615;&#25925;&#20107;&#27979;&#35797;&#12289;&#38544;&#24615;&#20851;&#32852;&#27979;&#35797;&#21644;&#26410;&#30693;&#24773;&#22659;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22810;&#32500;&#35780;&#20272;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#24182;&#20197;&#25945;&#32946;&#37096;&#38376;&#20026;&#26696;&#20363;&#30740;&#31350;&#26500;&#24314;&#20102;Edu-FairMonitor&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#21487;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#24182;&#20943;&#23569;&#36825;&#20123;LLMs&#24212;&#29992;&#26102;&#23545;&#20010;&#20154;&#25110;&#32676;&#20307;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#20110;&#27979;&#37327;&#27169;&#22411;&#23545;&#21253;&#21547;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#19988;&#26080;&#27861;&#26816;&#27979;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38544;&#21547;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#65292;&#30452;&#25509;&#35780;&#20272;LLMs&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#65292;&#21253;&#25324;&#30452;&#25509;&#35810;&#38382;&#27979;&#35797;&#12289;&#20018;&#34892;&#25110;&#36866;&#24212;&#24615;&#25925;&#20107;&#27979;&#35797;&#12289;&#38544;&#24615;&#20851;&#32852;&#27979;&#35797;&#21644;&#26410;&#30693;&#24773;&#22659;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22810;&#32500;&#35780;&#20272;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#12290;&#20197;&#25945;&#32946;&#37096;&#38376;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#26500;&#24314;&#20102;Edu-FairMonitor&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;12,632&#20010;&#24320;&#25918;&#24335;&#38382;&#39064;&#65292;&#28085;&#30422;&#20061;&#20010;&#25935;&#24863;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied. However, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. To address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of LLMs, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. Additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. Using the education sector as a case study, we constructed the Edu-FairMonitor based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05037</link><description>&lt;p&gt;
&#23558;&#20219;&#20309;&#20320;&#25551;&#36848;&#30340;&#20107;&#29289;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Separate Anything You Describe. (arXiv:2308.05037v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26597;&#35810;&#38899;&#39057;&#28304;&#20998;&#31163;&#65288;LASS&#65289;&#26159;&#35745;&#31639;&#21548;&#35273;&#22330;&#26223;&#20998;&#26512;&#65288;CASA&#65289;&#20013;&#30340;&#19968;&#31181;&#26032;&#33539; Paradigm&#12290;LASS&#26088;&#22312;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20174;&#38899;&#39057;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#30446;&#26631;&#22768;&#38899;&#65292;&#20026;&#25968;&#23383;&#38899;&#39057;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#30340;&#30028;&#38754;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;LASS&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#31163;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#20048;&#22120;&#65292;&#26377;&#38480;&#31867;&#21035;&#30340;&#38899;&#39057;&#20107;&#20214;&#65289;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#22312;&#24320;&#25918;&#22495;&#20013;&#20998;&#31163;&#38899;&#39057;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AudioSep&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#35757;&#32451;AudioSep&#65292;&#24182;&#23545;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#38899;&#39057;&#20107;&#20214;&#20998;&#31163;&#65292;&#20048;&#22120;&#20998;&#31163;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;AudioSep&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#38899;&#39057;&#26631;&#39064;&#25110;&#25991;&#23383;&#26631;&#31614;&#20316;&#20026;&#26597;&#35810;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-queried audio source separation (LASS) is a new paradigm for computational auditory scene analysis (CASA). LASS aims to separate a target sound from an audio mixture given a natural language query, which provides a natural and scalable interface for digital audio applications. Recent works on LASS, despite attaining promising separation performance on specific sources (e.g., musical instruments, limited classes of audio events), are unable to separate audio concepts in the open domain. In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries. We train AudioSep on large-scale multimodal datasets and extensively evaluate its capabilities on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability using audio captions or text labels as queries, substantially outperfor
&lt;/p&gt;</description></item><item><title>Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14993</link><description>&lt;p&gt;
Thinker: &#23398;&#20064;&#35268;&#21010;&#21644;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14993
&lt;/p&gt;
&lt;p&gt;
Thinker&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#19990;&#30028;&#27169;&#22411;&#21644;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23454;&#29616;&#33258;&#20027;&#35268;&#21010;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Thinker&#31639;&#27861;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#19982;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24182;&#21033;&#29992;&#20854;&#12290; Thinker&#31639;&#27861;&#36890;&#36807;&#32473;&#29615;&#22659;&#28155;&#21152;&#19990;&#30028;&#27169;&#22411;&#26469;&#25913;&#21464;&#29615;&#22659;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#19982;&#19990;&#30028;&#27169;&#22411;&#20132;&#20114;&#30340;&#26032;&#21160;&#20316;&#12290;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21160;&#20316;&#20351;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#22312;&#36873;&#25321;&#26368;&#32456;&#30340;&#29615;&#22659;&#21160;&#20316;&#20043;&#21069;&#21521;&#19990;&#30028;&#27169;&#22411;&#25552;&#20986;&#22791;&#36873;&#35745;&#21010;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#20195;&#29702;&#33258;&#20027;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#35268;&#21010;&#26469;&#28040;&#38500;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#35268;&#21010;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#20801;&#35768;&#23545;&#20195;&#29702;&#30340;&#35745;&#21010;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;Sokoban&#28216;&#25103;&#21644;Atari 2600&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;Thinker&#31639;&#27861;&#20998;&#21035;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;Thinker&#31639;&#27861;&#35757;&#32451;&#30340;&#20195;&#29702;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#24050;&#32463;&#23398;&#21040;&#20102;&#20248;&#31168;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear
&lt;/p&gt;</description></item><item><title>&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13755</link><description>&lt;p&gt;
TMR-RD: &#29992;&#20110;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;&#21644;&#34920;&#31034;&#20998;&#27495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13755
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;(SSOD)&#21487;&#20197;&#23558;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;SSOD&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#20266;&#26631;&#31614;&#22122;&#22768;/&#35823;&#23548;&#12289;&#32463;&#20856;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;(EMA)&#31574;&#30053;&#21644;&#21518;&#26399;&#35757;&#32451;&#20013;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#32463;&#20856;EMA&#30340;&#23616;&#38480;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;TMR&#38454;&#27573;&#20248;&#21270;&#20102;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#65292;&#20197;&#31934;&#21270;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#36825;&#20123;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#36830;&#22238;&#24402;&#26469;&#29983;&#25104;... (&#25688;&#35201;&#26410;&#23436;&#25972;&#25552;&#20379;)
&lt;/p&gt;
&lt;p&gt;
Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to gene
&lt;/p&gt;</description></item><item><title>HYTREL&#26159;&#19968;&#31181;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.08623</link><description>&lt;p&gt;
HYTREL: &#22522;&#20110;&#36229;&#22270;&#30340;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HYTREL: Hypergraph-enhanced Tabular Data Representation Learning. (arXiv:2307.08623v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08623
&lt;/p&gt;
&lt;p&gt;
HYTREL&#26159;&#19968;&#31181;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#34920;&#26684;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#34892;/&#21015;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20998;&#23618;&#32467;&#26500;&#31561;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYTREL&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#8212;&#8212;&#20854;&#20013;&#65292;&#34920;&#26684;&#21333;&#20803;&#26684;&#26500;&#25104;&#33410;&#28857;&#65292;&#24182;&#19988;&#22312;&#27599;&#34892;&#12289;&#27599;&#21015;&#21644;&#25972;&#20010;&#34920;&#26684;&#20013;&#20849;&#21516;&#20986;&#29616;&#30340;&#21333;&#20803;&#26684;&#34987;&#29992;&#26469;&#24418;&#25104;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#36229;&#36793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HYTREL&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#26159;&#26368;&#22823;&#19981;&#21464;&#30340;&#65292;&#21363;&#65292;&#20004;&#20010;&#34920;&#26684;&#36890;&#36807;HYTREL&#33719;&#24471;&#30340;&#34920;&#31034;&#30456;&#21516;&#65292;&#24403;&#19988;&#20165;&#24403;&#36825;&#20004;&#20010;&#34920;&#26684;&#22312;&#25490;&#21015;&#19978;&#26159;&#30456;&#21516;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraini
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.06947</link><description>&lt;p&gt;
&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65306;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;
Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#38271;&#36317;&#31163;&#26102;&#31354;&#19978;&#19979;&#25991;&#24314;&#27169;&#12290;&#35270;&#39057;Transformer&#35774;&#35745;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20197;&#39640;&#35745;&#31639;&#25104;&#26412;&#27169;&#25311;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29992;&#20110;&#35270;&#39057;&#30340;&#21367;&#31215;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#38271;&#36317;&#31163;&#20381;&#36182;&#24314;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#31181;&#35774;&#35745;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65288;Video-FocalNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#22522;&#20110;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26550;&#26500;&#65292;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#20132;&#20114;&#21644;&#32858;&#21512;&#27493;&#39588;&#36827;&#34892;&#20102;&#39072;&#20498;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#32858;&#21512;&#27493;&#39588;&#21644;&#20132;&#20114;&#27493;&#39588;&#37117;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#21367;&#31215;&#21644;&#36880;&#20803;&#32032;&#20056;&#27861;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#27604;&#35270;&#39057;&#34920;&#36798;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23545;&#24212;&#37096;&#20998;&#35201;&#20302;&#24471;&#22810;&#12290;&#25105;&#20204;&#24191;&#27867;&#25506;&#32034;&#20102;&#28966;&#28857;&#35843;&#21046;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.05902</link><description>&lt;p&gt;
&#24102;&#26377;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#31283;&#23450;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#30340;&#20445;&#35777;&#65292;&#20063;&#21487;&#33021;&#19981;&#21453;&#26144;&#24213;&#23618;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31283;&#23450;&#24615;&#20316;&#20026;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#19968;&#20010;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#24449;&#23631;&#34109;&#26041;&#38754;&#20855;&#26377;&#36275;&#22815;&#30340;Lipschitz&#24615;&#36136;&#65292;&#21017;&#21487;&#20197;&#20445;&#35777;&#25918;&#26494;&#21464;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#20056;&#27861;&#24179;&#28369;&#65288;MuS&#65289;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MuS&#20811;&#26381;&#20102;&#26631;&#20934;&#24179;&#28369;&#25216;&#26415;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;MuS&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;MuS&#36171;&#20104;&#20102;&#29305;&#24449;&#24402;&#22240;&#20197;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19981;&#36830;&#32493;ReLU&#32593;&#32476;&#23454;&#29616;&#20102;&#28145;&#23618;&#21512;&#21516;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#20154;&#21644;&#22996;&#25176;&#20154;&#30340;&#32422;&#26463;&#21644;&#30446;&#26631;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#65292;&#25903;&#25345;&#24182;&#34892;&#25512;&#26029;&#20197;&#27714;&#35299;&#26368;&#20248;&#21512;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.02318</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#36830;&#32493;&#32593;&#32476;&#23454;&#29616;&#28145;&#23618;&#21512;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Contract Design via Discontinuous Networks. (arXiv:2307.02318v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19981;&#36830;&#32493;ReLU&#32593;&#32476;&#23454;&#29616;&#20102;&#28145;&#23618;&#21512;&#21516;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#20154;&#21644;&#22996;&#25176;&#20154;&#30340;&#32422;&#26463;&#21644;&#30446;&#26631;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#65292;&#25903;&#25345;&#24182;&#34892;&#25512;&#26029;&#20197;&#27714;&#35299;&#26368;&#20248;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#21516;&#35774;&#35745;&#28041;&#21450;&#19968;&#20010;&#22996;&#25176;&#20154;&#23545;&#30001;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#20135;&#29983;&#30340;&#32467;&#26524;&#25903;&#20184;&#30340;&#21512;&#21516;&#32422;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#23545;&#26368;&#20248;&#21512;&#21516;&#33258;&#21160;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26041;&#27861;&#65306;&#19981;&#36830;&#32493;ReLU&#65288;DeLU&#65289;&#32593;&#32476;&#65292;&#23427;&#23558;&#22996;&#25176;&#20154;&#30340;&#25928;&#29992;&#24314;&#27169;&#20026;&#21512;&#21516;&#35774;&#35745;&#30340;&#19981;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#20989;&#25968;&#65292;&#20854;&#20013;&#27599;&#20010;&#20998;&#27573;&#23545;&#24212;&#20110;&#20195;&#29702;&#20154;&#37319;&#21462;&#29305;&#23450;&#30340;&#34892;&#21160;&#12290;DeLU&#32593;&#32476;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#25110;&#20869;&#28857;&#26041;&#27861;&#38544;&#24335;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#28608;&#21169;&#30456;&#23481;&#32422;&#26463;&#21644;&#22996;&#25176;&#20154;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#30446;&#26631;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#65292;&#24182;&#25903;&#25345;&#27599;&#20010;&#20998;&#27573;&#30340;&#24182;&#34892;&#25512;&#26029;&#20197;&#27714;&#35299;&#26368;&#20248;&#21512;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#36817;&#20284;&#22996;&#25176;&#20154;&#25928;&#29992;&#20989;&#25968;&#24182;&#25193;&#23637;&#20197;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#21512;&#21516;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01616</link><description>&lt;p&gt;
SageFormer&#65306;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#23637;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#35299;&#20915;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#37325;&#35201;&#24615;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;SageFormer&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26377;&#25928;&#22320;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#20197;&#21450;&#20943;&#23569;&#24207;&#21015;&#20043;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#35758;&#30340;&#31995;&#21015;&#24863;&#30693;&#26694;&#26550;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SageFormer&#30456;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.15626</link><description>&lt;p&gt;
LeanDojo: &#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20351;&#29992;Lean&#31561;&#35777;&#26126;&#21161;&#25163;&#35777;&#26126;&#24418;&#24335;&#23450;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31169;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24456;&#38590;&#22797;&#21046;&#25110;&#24314;&#31435;&#22312;&#20854;&#22522;&#30784;&#19978;&#65292;&#36825;&#32473;&#23450;&#29702;&#35777;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LeanDojo&#26469;&#28040;&#38500;&#36825;&#20123;&#38556;&#30861;&#65306;&#19968;&#20010;&#21253;&#21547;&#24037;&#20855;&#21253;&#12289;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;Lean&#28216;&#20048;&#22330;&#12290;LeanDojo&#20174;Lean&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#32534;&#31243;&#19982;&#35777;&#26126;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#23427;&#21253;&#21547;&#35777;&#26126;&#20013;&#21629;&#39064;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#65292;&#20026;&#21629;&#39064;&#36873;&#25321;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65306;&#36825;&#26159;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;ReProver&#65288;&#26816;&#32034;&#22686;&#24378;&#30340;&#35777;&#26126;&#22120;&#65289;&#65306;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;LLM&#30340;&#35777;&#26126;&#22120;&#65292;&#36890;&#36807;&#26816;&#32034;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#12290;&#23427;&#25104;&#26412;&#20302;&#24265;&#65292;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#21033;&#29992;&#20102;LeanDojo&#30340;pro&#30456;&#20851;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13229</link><description>&lt;p&gt;
TACO&#65306;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#21019;&#24314;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;&#20016;&#23500;&#20195;&#29702;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#25511;&#21046;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#34920;&#31034;&#26368;&#20248;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#23567;&#30340;&#25277;&#35937;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TACO&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;TACO&#36890;&#36807;&#20248;&#21270;&#37325;&#26032;&#33719;&#24471;&#35266;&#23519;&#19982;&#26368;&#36817;&#30340;&#22810;&#20010;&#20808;&#21069;&#35266;&#23519;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#19982;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#22240;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#26426;&#20307;&#30340;&#36827;&#21270;&#21644;&#21033;&#29992;&#23398;&#20064;&#22522;&#22240;&#26469;&#23398;&#20064;&#21644;&#28436;&#21270;&#26234;&#33021;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;GRL&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10225</link><description>&lt;p&gt;
&#26234;&#33021;&#20307;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Genes in Intelligent Agents. (arXiv:2306.10225v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#22240;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#26426;&#20307;&#30340;&#36827;&#21270;&#21644;&#21033;&#29992;&#23398;&#20064;&#22522;&#22240;&#26469;&#23398;&#20064;&#21644;&#28436;&#21270;&#26234;&#33021;&#20307;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;GRL&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#22522;&#22240;&#36890;&#36807;&#25968;&#21313;&#20159;&#24180;&#30340;&#20256;&#36882;&#21644;&#31215;&#32047;&#65292;&#36171;&#20104;&#22320;&#29699;&#19978;&#29983;&#29289;&#30340;&#24403;&#21069;&#29983;&#29289;&#26234;&#33021;&#12290;&#21463;&#21040;&#29983;&#29289;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#33268;&#21147;&#20110;&#26500;&#24314;&#26426;&#22120;&#26234;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#32321;&#33635;&#30340;&#25104;&#21151;&#65292;&#20294;&#26426;&#22120;&#26234;&#33021;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#29983;&#29289;&#26234;&#33021;&#12290;&#21407;&#22240;&#21487;&#33021;&#22312;&#20110;&#21160;&#29289;&#22825;&#29983;&#20855;&#26377;&#26576;&#31181;&#22522;&#22240;&#32534;&#30721;&#30340;&#26234;&#33021;&#65292;&#32780;&#26426;&#22120;&#32570;&#20047;&#27492;&#31867;&#26234;&#33021;&#65292;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#12290;&#21463;&#21040;&#21160;&#29289;&#22522;&#22240;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26426;&#22120;&#30340;&#8220;&#22522;&#22240;&#8221;&#65292;&#31216;&#20026;&#8220;&#23398;&#20064;&#22522;&#22240;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#20256;&#22686;&#24378;&#23398;&#20064;&#65288;GRL&#65289;&#12290;GRL&#26159;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#27169;&#25311;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#26377;&#26426;&#20307;&#30340;&#36827;&#21270;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#22522;&#22240;&#26469;&#23398;&#20064;&#21644;&#28436;&#21270;&#26234;&#33021;&#20307;&#12290;&#21033;&#29992;GRL&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#23398;&#20064;&#22522;&#22240;&#37319;&#29992;&#20102;&#26234;&#33021;&#20307;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#32487;&#25215;&#12290;&#36825;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;GRL&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The genes in nature give the lives on earth the current biological intelligence through transmission and accumulation over billions of years. Inspired by the biological intelligence, artificial intelligence (AI) has devoted to building the machine intelligence. Although it has achieved thriving successes, the machine intelligence still lags far behind the biological intelligence. The reason may lie in that animals are born with some intelligence encoded in their genes, but machines lack such intelligence and learn from scratch. Inspired by the genes of animals, we define the ``genes'' of machines named as the ``learngenes'' and propose the Genetic Reinforcement Learning (GRL). GRL is a computational framework that simulates the evolution of organisms in reinforcement learning (RL) and leverages the learngenes to learn and evolve the intelligence agents. Leveraging GRL, we first show that the learngenes take the form of the fragments of the agents' neural networks and can be inherited a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;NetHack&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#36164;&#28304;&#12289;&#23454;&#29616;&#21644;&#22522;&#20934;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102;ORL&#31038;&#21306;&#25152;&#29087;&#24713;&#30340;&#24037;&#20316;&#27969;&#22522;&#30784;&#65292;&#24182;&#38468;&#24102;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.08772</link><description>&lt;p&gt;
Katakomba&#65306;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;NetHack&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Katakomba: Tools and Benchmarks for Data-Driven NetHack. (arXiv:2306.08772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;NetHack&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#36164;&#28304;&#12289;&#23454;&#29616;&#21644;&#22522;&#20934;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102;ORL&#31038;&#21306;&#25152;&#29087;&#24713;&#30340;&#24037;&#20316;&#27969;&#22522;&#30784;&#65292;&#24182;&#38468;&#24102;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NetHack&#34987;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#21069;&#27839;&#65292;&#20854;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#20173;&#38656;&#36214;&#19978;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#26368;&#26032;&#21457;&#23637;&#20013;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;ORL&#65289;&#25104;&#20026;&#31361;&#30772;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;NetHack&#25968;&#25454;&#38598;&#34987;&#37322;&#25918;&#20986;&#65292;&#34429;&#28982;&#36825;&#26159;&#21521;&#21069;&#36808;&#20986;&#30340;&#24517;&#35201;&#19968;&#27493;&#65292;&#20294;&#23427;&#23578;&#26410;&#22312;ORL&#31038;&#21306;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#19977;&#20010;&#20027;&#35201;&#38556;&#30861;&#38656;&#35201;&#20811;&#26381;&#65306;&#36164;&#28304;&#12289;&#23454;&#29616;&#21644;&#22522;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;ORL&#31038;&#21306;&#29087;&#24713;&#30340;&#24037;&#20316;&#27969;&#22522;&#30784;&#65306;&#39044;&#23450;&#20041;&#30340;D4RL&#39118;&#26684;&#20219;&#21153;&#65292;&#31616;&#27905;&#30340;&#22522;&#20934;&#23454;&#29616;&#65292;&#20197;&#21450;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#24182;&#38468;&#26377;&#19982;&#20113;&#31471;&#21516;&#27493;&#30340;&#37197;&#32622;&#21644;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: resource-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07280</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#24494;&#35843;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Controlling Text-to-Image Diffusion by Orthogonal Finetuning. (arXiv:2306.07280v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#24863;&#22270;&#20687;&#26041;&#38754;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24341;&#23548;&#25110;&#25511;&#21046;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;OFT&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#25345;&#29305;&#24449;&#23545;&#31070;&#32463;&#20803;&#22312;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#30340;&#20851;&#31995;&#25152;&#34920;&#24449;&#30340;&#36229;&#29699;&#24418;&#33021;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#23646;&#24615;&#23545;&#20110;&#20445;&#25345;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#20102;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#23427;&#23545;&#36229;&#29699;&#38754;&#26045;&#21152;&#20102;&#39069;&#22806;&#30340;&#21322;&#24452;&#32422;&#26463;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#65306;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.06836</link><description>&lt;p&gt;
&#29992;&#20989;&#25968;&#36924;&#36817;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#23614;&#22870;&#21169;&#38382;&#39064;&#30340;&#26497;&#23567;&#26368;&#22823;&#21270;&#31639;&#27861;&#21644;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26377;&#35768;&#22810;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#20026;&#26377;&#30028;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#26377;&#25928;&#31639;&#27861;&#65292;&#20294;&#24403;&#22870;&#21169;&#21576;&#29616;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#8212;&#8212;&#21363;&#23384;&#22312;&#26576;&#20010; $\epsilon\in(0,1]$ &#20351;&#24471;&#20165;&#26377;&#26377;&#38480;&#30340;$(1+\epsilon)$-&#38454;&#30697;&#8212;&#8212;&#26159;&#21542;&#23384;&#22312;&#23545;&#22823;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#25110;&#26102;&#25928;&#24615;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340; RL &#20013;&#30340;&#36825;&#31181;&#22870;&#21169;&#26426;&#21046;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#37325;&#23614;&#32447;&#24615;&#36172;&#33218;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#8212;&#8212;\textsc{Heavy-OFUL}&#65292;&#20854;&#23454;&#29616;&#20102;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340; $T$-round &#36951;&#25022;&#24230;&#37327;&#65292;&#20026; $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;\emph{&#31532;&#19968;&#31687;}&#25991;&#31456;&#12290;$\nu_t^{1+\epsilon}$&#26159;&#31532; $t$ &#36718;&#22870;&#21169;&#30340; $(1+\epsilon)$-&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24212;&#29992;&#20110; st &#30340;&#26368;&#22351;&#24773;&#20917;&#26102;&#65292;&#19978;&#36848;&#30028;&#26159;&#26497;&#23567;&#20540;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BLEEP&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26500;&#24314;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#33021;&#22815;&#20174;H&amp;E&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22320;&#22270;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01859</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#30340;H&amp;E&#32452;&#32455;&#23398;&#22270;&#20687;&#22522;&#22240;&#34920;&#36798;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatially Resolved Gene Expression Prediction from H&amp;E Histology Images via Bi-modal Contrastive Learning. (arXiv:2306.01859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BLEEP&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26500;&#24314;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#33021;&#22815;&#20174;H&amp;E&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22320;&#22270;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#23398;&#25104;&#20687;&#26159;&#21307;&#23398;&#35786;&#26029;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#33021;&#22815;&#22312;&#24494;&#35266;&#27700;&#24179;&#19978;&#26816;&#26597;&#32452;&#32455;&#32467;&#26500;&#21644;&#32452;&#25104;&#12290;&#20102;&#35299;&#32452;&#32455;&#32467;&#26500;&#30340;&#22522;&#26412;&#20998;&#23376;&#26426;&#21046;&#23545;&#25581;&#31034;&#30142;&#30149;&#26426;&#21046;&#21644;&#24320;&#21457;&#26377;&#25928;&#27835;&#30103;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#22240;&#34920;&#36798;&#35889;&#25552;&#20379;&#20102;&#28145;&#20837;&#20102;&#35299;&#32452;&#32455;&#32467;&#26500;&#32972;&#21518;&#20998;&#23376;&#36807;&#31243;&#30340;&#35270;&#35282;&#65292;&#20294;&#36825;&#19968;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLEEP&#65288;Bi-modaL Embedding for Expression Prediction&#65289;&#30340;&#21452;&#27169;&#24335;&#23884;&#20837;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#20840;&#24133;&#33487;&#26408;&#31934;-&#20234;&#32418;&#65288;H&amp;E&#65289;&#26579;&#33394;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#29983;&#25104;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22270;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22312;&#24494;&#31859;&#20998;&#36776;&#29575;&#19979;&#20351;&#29992;&#25104;&#23545;&#30340;&#22270;&#20687;&#21644;&#34920;&#36798;&#35889;&#26469;&#26500;&#24314;&#20302;&#32500;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#30340;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#21487;&#29992;&#21608;&#22260;&#22270;&#20687;&#30340;&#32972;&#26223;&#19978;&#19979;&#25991;&#25512;&#26029;&#20986;&#20219;&#20309;&#26597;&#35810;&#22270;&#20687;&#34917;&#19969;&#30340;&#22522;&#22240;&#34920;&#36798;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22522;&#22240;&#34920;&#36798;&#35889;&#22320;&#22270;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#32452;&#32455;&#31867;&#22411;&#19978;&#23637;&#31034;&#20102;BLEEP&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histology imaging is an important tool in medical diagnosis and research, enabling the examination of tissue structure and composition at the microscopic level. Understanding the underlying molecular mechanisms of tissue architecture is critical in uncovering disease mechanisms and developing effective treatments. Gene expression profiling provides insight into the molecular processes underlying tissue architecture, but the process can be time-consuming and expensive. In this study, we present BLEEP (Bi-modaL Embedding for Expression Prediction), a bi-modal embedding framework capable of generating spatially resolved gene expression profiles of whole-slide Hematoxylin and eosin (H&amp;E) stained histology images. BLEEP uses a contrastive learning framework to construct a low-dimensional joint embedding space from a reference dataset using paired image and expression profiles at micrometer resolution. With this framework, the gene expression of any query image patch can be imputed using the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01708</link><description>&lt;p&gt;
&#21512;&#24182;&#27169;&#22411;&#26102;&#22914;&#20309;&#35299;&#20915;&#24178;&#25200;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#36827;&#19979;&#28216;&#24615;&#33021;&#65292;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#20808;&#21069;&#30340;&#21512;&#24182;&#25216;&#26415;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#24178;&#25200;&#26469;&#28304;&#32780;&#19981;&#24910;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;(a)&#20887;&#20313;&#21442;&#25968;&#20540;&#24341;&#36215;&#30340;&#24178;&#25200;&#21644;(b)&#34920;&#31034;&#21516;&#19968;&#21442;&#25968;&#20540;&#30340;&#31526;&#21495;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00323</link><description>&lt;p&gt;
&#8220;&#24605;&#32500;&#20811;&#38534;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#23398;&#20064;&#24605;&#32771;&#24182;&#34892;&#21160;&#8221;&#12290;&#65288;arXiv:2306.00323v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#12289;&#37325;&#26032;&#35268;&#21010;&#21644;&#36866;&#24212;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#36825;&#20123;&#33021;&#21147;&#20013;&#36828;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#20013;&#19968;&#20010;&#35748;&#30693;&#32570;&#38519;&#30340;&#21407;&#22240;&#26159;&#20182;&#20204;&#32570;&#20047;&#20351;&#29992;&#35821;&#35328;&#24605;&#32771;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#20854;&#24819;&#27861;&#19981;&#20165;&#26159;&#20811;&#38534;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#34892;&#20026;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#20154;&#31867;&#22312;&#25191;&#34892;&#36825;&#20123;&#34892;&#20026;&#26102;&#25152;&#20135;&#29983;&#30340;&#24819;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#24076;&#26395;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#22312;&#22788;&#29702;&#32593;&#32476;&#35268;&#27169;&#30340;&#20154;&#31867;&#24605;&#32500;&#21644;&#34892;&#20026;&#25968;&#25454;&#26102;&#33021;&#22815;&#21457;&#25381;&#20986;&#33394;&#65288;&#20363;&#22914;&#65292;&#24102;&#26377;&#21095;&#26412;&#30340;&#22312;&#32447;&#35270;&#39057;&#65289;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22312;&#24605;&#32771;&#21644;&#34892;&#21160;&#25968;&#25454;&#20026;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#23398;&#20064;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.19742</link><description>&lt;p&gt;
&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#23398;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#30284;&#30151;&#27835;&#30103;&#25110;&#21361;&#37325;&#25252;&#29702;&#65292;&#36890;&#24120;&#24517;&#39035;&#23545;&#21058;&#37327;&#32452;&#21512;&#36827;&#34892;&#36873;&#25321;&#65292;&#21363;&#22810;&#31181;&#36830;&#32493;&#27835;&#30103;&#12290;&#29616;&#26377;&#30340;&#36825;&#39033;&#20219;&#21153;&#30340;&#24037;&#20316;&#24050;&#32463;&#29420;&#31435;&#22320;&#24314;&#27169;&#20102;&#22810;&#31181;&#27835;&#30103;&#30340;&#25928;&#26524;&#65292;&#32780;&#20272;&#35745;&#32852;&#21512;&#25928;&#26524;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#38754;&#20020;&#30528;&#38750;&#24179;&#20961;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#20989;&#25968;&#65292;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#30456;&#20851;&#21058;&#37327;&#30340;&#32852;&#21512;&#25928;&#24212;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#37327;&#20272;&#35745;&#24191;&#20041;&#20542;&#21521;&#24471;&#20998;&#65292;&#20197;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#37325;&#21472;&#26377;&#38480;&#30340;&#21306;&#22495;&#12290;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#30830;&#20445;&#21487;&#38752;&#22320;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16379</link><description>&lt;p&gt;
&#26377;&#25928;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#21033;&#29992;&#29575;&#65306;&#20197;&#23569;&#23398;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. (arXiv:2305.16379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#31616;&#21333;&#30340;&#35266;&#23519;&#21464;&#25442;&#23601;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#36741;&#21161;&#34920;&#31034;&#20219;&#21153;&#25110;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;DA&#30340;&#21738;&#20123;&#23646;&#24615;&#26159;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#35270;&#35273;RL&#30340;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;DA&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;DA&#23646;&#24615;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20197;&#19979;&#35265;&#35299;&#21644;&#25913;&#36827;&#65306;&#65288;1&#65289;&#23545;&#20110;&#21333;&#20010;DA&#25805;&#20316;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20805;&#36275;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#37117;&#26159;&#19981;&#21487;&#32570;&#23569;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;&#38543;&#26426;PadResize&#65288;Rand PR&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#12290;&#65288;2&#65289;&#23545;&#20110;&#22810;&#31867;&#22411;&#30340;DA&#34701;&#21512;&#26041;&#26696;&#65292;&#22686;&#21152;&#30340;DA&#22256;&#38590;&#24230;&#21644;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#29702;&#35299;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.15328</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#35780;&#20272;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#29702;&#35299;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#35270;&#35273;&#27169;&#22359;&#30340;&#25511;&#21046;&#22120;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#23545;&#35937;/&#35745;&#25968;&#29983;&#25104;&#12289;&#24067;&#23616;&#29983;&#25104;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65288;&#23545;&#35937;/&#35745;&#25968;&#29983;&#25104;&#21644;&#24067;&#23616;&#29983;&#25104;&#65289;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#24067;&#23616;&#23545;&#19978;&#24494;&#35843;&#23427;&#12290;&#25105;&#20204;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#32780;&#31471;&#21040;&#31471;&#27169;&#22411;&#26159;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#30340;&#24067;&#23616;&#24341;&#23548;T2I&#20316;&#21697;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20559;&#35265;&#24191;&#27867;&#24433;&#21709;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#24230;&#20381;&#36182;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#22240;&#26524;&#30456;&#20851;&#30340;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#32531;&#35299;&#23454;&#20307;&#20559;&#35265;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#31934;&#30830;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#40657;&#30418;&#23376;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#26080;&#27861;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#20854;&#21442;&#25968;&#27604;&#36739;&#23481;&#26131;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#12290;&#36825;&#31181;&#22240;&#26524;&#24178;&#39044;&#23558;&#21407;&#22987;&#23454;&#20307;&#19982;&#30456;&#37051;&#23454;&#20307;&#19968;&#36215;&#36827;&#34892;&#25200;&#21160;&#12290;&#36825;&#31181;&#24178;&#39044;&#20943;&#23569;&#20102;&#19982;&#21407;&#22987;&#23454;&#20307;&#30456;&#20851;&#30340;&#29305;&#23450;&#20559;&#21521;&#20449;&#24687;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#26469;&#33258;&#31867;&#20284;&#23454;&#20307;&#30340;&#36275;&#22815;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
&lt;/p&gt;</description></item><item><title>INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14282</link><description>&lt;p&gt;
INSTRUCTSCORE: &#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#19982;&#32454;&#31890;&#24230;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14282
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#23398;&#20064;&#24230;&#37327;&#34920;&#26174;&#31034;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26080;&#27861;&#35299;&#37322;&#20854;&#21028;&#26029;&#25110;&#23558;&#20998;&#25968;&#19982;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#32570;&#38519;&#20851;&#32852;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructScore&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#23545;&#25991;&#26412;&#35780;&#20272;&#24230;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;InstructScore&#65292;&#21253;&#25324;&#32763;&#35793;&#12289;&#23383;&#24149;&#29983;&#25104;&#12289;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#24120;&#35782;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#26080;&#30417;&#30563;&#24230;&#37327;&#65292;&#21253;&#25324;&#22522;&#20110;175B GPT-3&#21644;GPT-4&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#20154;&#24037;&#35780;&#32423;&#25968;&#25454;&#30340;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;InstructScore&#30340;&#24615;&#33021;&#27700;&#24179;&#20063;&#19982;COMET2&#31561;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET2
&lt;/p&gt;</description></item><item><title>DUBLIN&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14218</link><description>&lt;p&gt;
DUBLIN&#8212;&#8212;&#36890;&#36807;&#35821;&#35328;-&#22270;&#20687;&#32593;&#32476;&#36827;&#34892;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DUBLIN -- Document Understanding By Language-Image Network. (arXiv:2305.14218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14218
&lt;/p&gt;
&lt;p&gt;
DUBLIN&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#20998;&#26512;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#27969;&#27700;&#32447;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#20351;&#29992;&#19977;&#31181;&#26032;&#39062;&#30446;&#26631;&#22312;Web&#39029;&#38754;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;DUBLIN&#27169;&#22411;&#65306;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22522;&#20110;Web&#30340;&#32467;&#26500;&#21270;&#38405;&#35835;&#29702;&#35299;&#12289;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#12289;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12289;&#22270;&#35299;&#29702;&#35299;&#21644;&#34920;&#26684;&#38382;&#31572;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DUBLIN&#26159;&#39318;&#20010;&#22312;WebSRC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;EM 77.75&#21644;F1 84.25&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual document understanding is a complex task that involves analyzing both the text and the visual elements in document images. Existing models often rely on manual feature engineering or domain-specific pipelines, which limit their generalization ability across different document types and languages. In this paper, we propose DUBLIN, which is pretrained on web pages using three novel objectives: Masked Document Content Generation Task, Bounding Box Task, and Rendered Question Answering Task, that leverage both the spatial and semantic information in the document images. Our model achieves competitive or state-of-the-art results on several benchmarks, such as Web-Based Structural Reading Comprehension, Document Visual Question Answering, Key Information Extraction, Diagram Understanding, and Table Question Answering. In particular, we show that DUBLIN is the first pixel-based model to achieve an EM of 77.75 and F1 of 84.25 on the WebSRC dataset. We also show that our model outperform
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13850</link><description>&lt;p&gt;
&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#25552;&#21462;&#65288;VRE&#65289;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23454;&#20307;&#29305;&#24449;&#21333;&#29420;&#39044;&#27979;&#27599;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32570;&#20047;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20351;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#38271;&#31243;&#20851;&#31995;&#65292;&#24182;&#23481;&#26131;&#20135;&#29983;&#20914;&#31361;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GOSE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25429;&#33719;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#25991;&#26723;&#30340;&#25195;&#25551;&#22270;&#20687;&#65292;GOSE&#39318;&#20808;&#23545;&#23454;&#20307;&#23545;&#29983;&#25104;&#21021;&#27493;&#30340;&#20851;&#31995;&#39044;&#27979;&#12290;&#31532;&#20108;&#65292;&#22312;&#20808;&#21069;&#36845;&#20195;&#30340;&#39044;&#27979;&#32467;&#26524;&#22522;&#30784;&#19978;&#65292;GOSE&#21033;&#29992;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#36827;&#19968;&#27493;&#25972;&#21512;&#23454;&#20307;&#34920;&#31034;&#12290;&#36825;&#31181;&#8220;&#29983;&#25104;-&#25429;&#33719;-&#25972;&#21512;&#8221;&#27169;&#24335;&#34987;&#22810;&#27425;&#25191;&#34892;&#65292;&#20197;&#20415;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#24456;&#22909;&#22320;&#25429;&#33719;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.13788</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#21644;&#20135;&#29983;&#20998;&#27495;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer and Disagree Like Humans?. (arXiv:2305.13788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25104;&#32489;&#12290;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#37319;&#26679;&#26631;&#35760;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;LLM&#24456;&#38590;&#19982;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#39640;&#24230;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#12290;&#26412;&#25991;&#20351;&#29992; Monte Carlo Reconstruction&#65288;MCR&#65289;&#21644; Log Probability Reconstruction&#65288;LPR&#65289;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#35780;&#20272;&#20102;LLM&#20998;&#24067;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#35299;&#20915;NLI&#20219;&#21153;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#65292;&#21516;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#65292;&#36825;&#23545;&#20854;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#33021;&#21147;&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#30340;&#29305;&#24615;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown stellar achievements in solving a broad range of tasks. When generating text, it is common to sample tokens from these models: whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of Natural Language Inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution, raising concerns about their natural language understanding (NLU) ability and their representativeness of human users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;</title><link>http://arxiv.org/abs/2305.13661</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#22312;&#35823;&#29992;&#65292;&#25506;&#35752;&#20102;&#20854;&#29983;&#25104;&#21487;&#20449;&#24182;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#24182;&#23545;&#20449;&#24687;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#23545;&#26080;&#24847;&#21644;&#25925;&#24847;&#30340;&#28508;&#22312;&#35823;&#29992;&#22330;&#26223;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#35780;&#20272;LLM&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20449;&#24687;&#19981;&#23454;&#30340;&#31243;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;ODQA&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#20943;&#36731;&#30001;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#20449;&#24687;&#24102;&#26469;&#30340;&#21361;&#23475;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;&#34429;&#28982;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#38450;&#24481;&#24615;&#31574;&#30053;&#26377;&#24076;&#26395;&#20135;&#29983;&#26126;&#26174;&#25928;&#26524;&#65292;&#20294;&#36824;&#38656;&#35201;&#20570;&#22823;&#37327;&#24037;&#20316;&#26469;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12029</link><description>&lt;p&gt;
MultiTurnCleanup&#65306;&#29992;&#20110;&#22810;&#36718;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#28165;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35843;&#19981;&#36830;&#32493;&#26816;&#27979;&#27169;&#22411;&#20391;&#37325;&#20110;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#27599;&#20010;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#35768;&#22810;&#19981;&#36830;&#32493;&#29616;&#35937;&#37117;&#21457;&#29983;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#36825;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#35835;&#24615;&#21644;&#19979;&#28216; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#8220;MultiTurnCleanup&#8221;&#20219;&#21153;&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#27880;&#27169;&#24335;&#20197;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07512</link><description>&lt;p&gt;
&#23398;&#20064;&#21435;&#23398;&#20064;&#65306;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#23454;&#29616;&#34987;&#36951;&#24536;&#26435;&#26159;&#35768;&#22810;&#25968;&#25454;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;&#26426;&#22120;&#21435;&#23398;&#20064;&#24050;&#25104;&#20026;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#30340;&#31616;&#35201;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#31934;&#30830;&#21644;&#36817;&#20284;&#26041;&#27861;&#12289;&#21487;&#33021;&#30340;&#25915;&#20987;&#20197;&#21450;&#39564;&#35777;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#27604;&#36739;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20351;&#29992;Deltagrad&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#36824;&#24378;&#35843;&#20102;&#25361;&#25112;&#65292;&#22914;&#38750;IID&#21024;&#38500;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25104;&#20026;&#23547;&#27714;&#26426;&#22120;&#21435;&#23398;&#20064;&#36164;&#26009;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#22120;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20811;&#26381;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#31435;&#21363;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20854;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#20027;&#25628;&#32034;&#21644;&#35206;&#30422;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01834</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#26102;&#29615;&#22659;&#33258;&#20027;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Autonomous search of real-life environments combining dynamical system-based path planning and unsupervised learning. (arXiv:2305.01834v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#22120;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20811;&#26381;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#31435;&#21363;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20854;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#20027;&#25628;&#32034;&#21644;&#35206;&#30422;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20351;&#29992;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#36827;&#34892;&#26377;&#38480;&#29615;&#22659;&#25628;&#32034;&#21644;&#36941;&#21382;&#30340;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#30446;&#21069;&#30340;&#23454;&#39564;&#24037;&#20316;&#23578;&#26410;&#24320;&#21457;&#20986;&#21487;&#28385;&#36275;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#38656;&#35201;&#20811;&#26381;&#30340;&#31435;&#21363;&#38382;&#39064;&#30340;&#24378;&#22823;&#26041;&#27861; &#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#22120;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20811;&#26381;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#31435;&#21363;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#23637;&#31034;&#20854;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#20027;&#25628;&#32034;&#21644;&#35206;&#30422;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advancements have been made towards the goal of using chaotic coverage path planners for autonomous search and traversal of spaces with limited environmental cues. However, the state of this field is still in its infancy as there has been little experimental work done. Current experimental work has not developed robust methods to satisfactorily address the immediate set of problems a chaotic coverage path planner needs to overcome in order to scan realistic environments within reasonable coverage times. These immediate problems are as follows: (1) an obstacle avoidance technique which generally maintains the kinematic efficiency of the robot's motion, (2) a means to spread chaotic trajectories across the environment (especially crucial for large and/or complex-shaped environments) that need to be covered, and (3) a real-time coverage calculation technique that is accurate and independent of cell size. This paper aims to progress the field by proposing algorithms that a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.06614</link><description>&lt;p&gt;
&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65306;&#26088;&#22312;&#29992;&#25193;&#20805;&#25968;&#25454;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#26159;&#65292;&#24403;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#24778;&#24322;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#22238;&#25918;&#23454;&#29616;&#65292;&#20854;&#20013;&#36807;&#21435;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#24040;&#22823;&#36827;&#27493;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65288;SynthER&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#19978;&#37319;&#26679;&#20195;&#29702;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SynthER&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#22312;&#24863;&#30693;&#29615;&#22659;&#36824;&#26159;&#22312;&#20687;&#32032;&#29615;&#22659;&#20013;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#36880;&#27133;&#26426;&#21046;&#65288;RSM&#65289;&#30340;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#22312;&#27133;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#20197;&#21450;&#21160;&#24577;&#36873;&#25321;&#21487;&#37325;&#22797;&#20351;&#29992;&#26426;&#21046;&#65292;&#27169;&#25311;&#23545;&#35937;&#30340;&#21160;&#24577;&#23398;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20013;&#22830;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;CCI&#65289;&#65292;&#33021;&#22815;&#24314;&#27169;&#39640;&#38454;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10503</link><description>&lt;p&gt;
&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#36880;&#27133;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reusable Slotwise Mechanisms. (arXiv:2302.10503v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10503
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#36880;&#27133;&#26426;&#21046;&#65288;RSM&#65289;&#30340;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#22312;&#27133;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#20197;&#21450;&#21160;&#24577;&#36873;&#25321;&#21487;&#37325;&#22797;&#20351;&#29992;&#26426;&#21046;&#65292;&#27169;&#25311;&#23545;&#35937;&#30340;&#21160;&#24577;&#23398;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20013;&#22830;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;CCI&#65289;&#65292;&#33021;&#22815;&#24314;&#27169;&#39640;&#38454;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#29702;&#35299;&#21644;&#25512;&#29702;&#23545;&#35937;&#21160;&#24577;&#30340;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#26032;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#19981;&#20165;&#38656;&#35201;&#26377;&#25928;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#36824;&#38656;&#35201;&#23545;&#23545;&#35937;&#23376;&#38598;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#20351;&#29992;&#23545;&#35937;&#27133;&#34920;&#31034;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#36880;&#27133;&#26426;&#21046;&#65288;RSM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27133;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#20197;&#21450;&#20855;&#26377;&#21160;&#24577;&#36873;&#25321;&#21487;&#37325;&#22797;&#20351;&#29992;&#26426;&#21046;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#26469;&#27169;&#25311;&#23545;&#35937;&#21160;&#24577;&#23398;&#12290;&#20851;&#38190;&#26159;&#65292;RSM&#21033;&#29992;&#20102;&#20013;&#22830;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;CCI&#65289;&#65292;&#20351;&#24471;&#36873;&#25321;&#30340;&#26426;&#21046;&#33021;&#22815;&#36890;&#36807;&#19968;&#20010;&#29942;&#39048;&#26469;&#35775;&#38382;&#20854;&#20313;&#30340;&#27133;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24314;&#27169;&#21487;&#33021;&#38656;&#35201;&#31232;&#30095;&#23545;&#35937;&#23376;&#38598;&#30340;&#39640;&#38454;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;RSM&#33021;&#22815;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#23545;&#35937;&#21160;&#24577;&#23398;&#65292;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents with the ability to comprehend and reason about the dynamics of objects would be expected to exhibit improved robustness and generalization in novel scenarios. However, achieving this capability necessitates not only an effective scene representation but also an understanding of the mechanisms governing interactions among object subsets. Recent studies have made significant progress in representing scenes using object slots. In this work, we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models object dynamics by leveraging communication among slots along with a modular architecture capable of dynamically selecting reusable mechanisms for predicting the future states of each object slot. Crucially, RSM leverages the Central Contextual Information (CCI), enabling selected mechanisms to access the remaining slots through a bottleneck, effectively allowing for modeling of higher order and complex interactions that might require a sparse subset of objects. Experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12842</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference-based Policy Optimization without Reward Modeling. (arXiv:2301.12842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#26159;&#19968;&#31181;&#20351;RL&#20195;&#29702;&#33021;&#22815;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22312;&#21046;&#23450;&#22870;&#21169;&#20989;&#25968;&#26102;&#23384;&#22312;&#25361;&#25112;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;PbRL&#26041;&#27861;&#19968;&#33324;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#26681;&#25454;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#37319;&#29992;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#20559;&#22909;&#20449;&#24687;&#33719;&#21462;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20559;&#22909;&#26469;&#33258;&#20154;&#31867;&#25945;&#24072;&#26102;&#65292;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;PbRL&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#20026;&#19982;&#32473;&#23450;&#20559;&#22909;&#19968;&#33268;&#30340;&#31574;&#30053;&#20998;&#37197;&#39640;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#24102;&#26377;&#23454;&#38469;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#30340;&#31163;&#32447;RL&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#25110;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2212.10015</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#22270;&#20687;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#26159;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#36924;&#30495;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;T2I&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;VISOR&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#25429;&#25417;&#20102;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#31354;&#38388;&#20851;&#31995;&#22312;&#22270;&#20687;&#20013;&#26159;&#21542;&#20934;&#30830;&#29983;&#25104;&#12290;&#20026;&#20102;&#22522;&#20934;&#29616;&#26377;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25551;&#36848;&#20004;&#20010;&#23545;&#35937;&#21450;&#23427;&#20204;&#20043;&#38388;&#31354;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#25968;&#25454;&#38598;SR2D&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#35780;&#20272;&#27969;&#31243;&#26469;&#35782;&#21035;&#29289;&#20307;&#21450;&#20854;&#31354;&#38388;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35780;&#20272;T2I&#27169;&#22411;&#26102;&#37319;&#29992;&#23427;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#20063;&#23601;&#26159;&#23613;&#31649;&#26368;&#26032;&#30340;T2I&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#29983;&#25104;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#31354;&#38388;&#35859;&#35789;&#65288;&#22914;'&#22312;&#21069;&#38754;'&#21644;'&#22312;&#21518;&#38754;'&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22330;&#26223;&#30340;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#20063;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, SR2D, that contains sentences describing two objects and the spatial relationship between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;</title><link>http://arxiv.org/abs/2212.06096</link><description>&lt;p&gt;
&#38544;&#24335;&#21367;&#31215;&#26680;&#29992;&#20110;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26500;&#24314;&#19982;&#24179;&#31227;&#21644;&#20854;&#20182;&#21464;&#25442;&#31561;&#21516;&#21464;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36825;&#20123;&#21464;&#25442;&#23646;&#20110;&#22522;&#20110;&#21407;&#28857;&#20445;&#25345;&#30340;&#32676;G&#65292;&#20363;&#22914;&#21453;&#23556;&#21644;&#26059;&#36716;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#36890;&#36807;&#22312;&#26680;&#31354;&#38388;&#19978;&#24378;&#21152;&#29305;&#23450;&#20110;&#32676;G&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#26469;&#35299;&#26512;&#27714;&#35299;&#24471;&#21040;&#30340;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#26631;&#20934;&#21367;&#31215;&#12290;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#23545;&#29305;&#23450;&#30340;&#32676;G&#23450;&#21046;&#65292;&#26680;&#22522;&#30784;&#30340;&#23454;&#29616;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#23545;&#31216;&#21464;&#25442;&#65292;&#36825;&#23548;&#33268;&#20102;&#36890;&#29992;&#32676;&#31561;&#21464;&#27169;&#22411;&#30340;&#24320;&#21457;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21442;&#25968;&#21270;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#23454;&#29616;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#20219;&#20309;&#21487;&#20197;&#26500;&#24314;G-&#31561;&#21464;MLP&#30340;&#32676;G&#37117;&#21487;&#20197;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;N&#20307;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations,
&lt;/p&gt;</description></item><item><title>&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#32593;&#32476;&#36890;&#36807;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#65292;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#20855;&#26377;&#19981;&#21516;&#30340;&#21160;&#24577;&#26426;&#21046;&#65292;&#24182;&#19988;&#29305;&#24449;&#20540;&#20316;&#20026;&#23398;&#20064;&#29575;&#20056;&#25968;&#12290;&#24341;&#20837;&#19968;&#26063;&#31561;&#21521;&#24615;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24179;&#34913;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.04858</link><description>&lt;p&gt;
&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit variance regularization in non-contrastive SSL. (arXiv:2212.04858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04858
&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#32593;&#32476;&#36890;&#36807;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#65292;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#20855;&#26377;&#19981;&#21516;&#30340;&#21160;&#24577;&#26426;&#21046;&#65292;&#24182;&#19988;&#29305;&#24449;&#20540;&#20316;&#20026;&#23398;&#20064;&#29575;&#20056;&#25968;&#12290;&#24341;&#20837;&#19968;&#26063;&#31561;&#21521;&#24615;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24179;&#34913;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;BYOL&#21644;SimSiam&#65289;&#20381;&#36182;&#20110;&#38750;&#23545;&#31216;&#39044;&#27979;&#32593;&#32476;&#26469;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#32780;&#26080;&#38656;&#36127;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#32593;&#32476;&#22914;&#20309;&#20419;&#36827;&#31283;&#23450;&#23398;&#20064;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#29702;&#35770;&#20998;&#26512;&#20551;&#35774;&#27431;&#20960;&#37324;&#24471;&#25439;&#22833;&#65292;&#20294;&#22823;&#22810;&#25968;&#23454;&#38469;&#23454;&#29616;&#20381;&#36182;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#38381;&#24335;&#32447;&#24615;&#39044;&#27979;&#32593;&#32476;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#30740;&#31350;&#23398;&#20064;&#21160;&#21147;&#23398;&#19982;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#32773;&#22343;&#36890;&#36807;&#38544;&#24335;&#26041;&#24046;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#23613;&#31649;&#20855;&#26377;&#19981;&#21516;&#30340;&#21160;&#24577;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#24449;&#20540;&#20316;&#20026;&#26377;&#25928;&#30340;&#23398;&#20064;&#29575;&#20056;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#26063;&#31561;&#21521;&#24615;&#25439;&#22833;&#20989;&#25968;&#65288;IsoLoss&#65289;&#65292;&#20197;&#22312;&#29305;&#24449;&#27169;&#24335;&#20043;&#38388;&#24179;&#34913;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;IsoLoss&#21152;&#36895;&#20102;&#21021;&#22987;&#23398;&#20064;&#21160;&#21147;&#23398;&#24182;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#25670;&#33073;...
&lt;/p&gt;
&lt;p&gt;
Non-contrastive SSL methods like BYOL and SimSiam rely on asymmetric predictor networks to avoid representational collapse without negative samples. Yet, how predictor networks facilitate stable learning is not fully understood. While previous theoretical analyses assumed Euclidean losses, most practical implementations rely on cosine similarity. To gain further theoretical insight into non-contrastive SSL, we analytically study learning dynamics in conjunction with Euclidean and cosine similarity in the eigenspace of closed-form linear predictor networks. We show that both avoid collapse through implicit variance regularization albeit through different dynamical mechanisms. Moreover, we find that the eigenvalues act as effective learning rate multipliers and propose a family of isotropic loss functions (IsoLoss) that equalize convergence rates across eigenmodes. Empirically, IsoLoss speeds up the initial learning dynamics and increases robustness, thereby allowing us to dispense with 
&lt;/p&gt;</description></item><item><title>CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07105</link><description>&lt;p&gt;
CORL: &#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;
&lt;/p&gt;
&lt;p&gt;
CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07105
&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#28145;&#24230;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#24378;&#35843;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#65292;&#20855;&#26377;&#30452;&#35266;&#30340;&#20195;&#30721;&#24211;&#21644;&#29616;&#20195;&#20998;&#26512;&#36319;&#36394;&#24037;&#20855;&#12290;&#22312;CORL&#20013;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#23454;&#29616;&#38548;&#31163;&#21040;&#21333;&#29420;&#30340;&#21333;&#20010;&#25991;&#20214;&#20013;&#65292;&#20351;&#24615;&#33021;&#30456;&#20851;&#30340;&#32454;&#33410;&#26356;&#23481;&#26131;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#21487;&#29992;&#20110;&#24110;&#21161;&#35760;&#24405;&#25351;&#26631;&#12289;&#36229;&#21442;&#25968;&#12289;&#20381;&#36182;&#39033;&#31561;&#21040;&#20113;&#31471;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#24120;&#29992;&#30340;D4RL&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#65292;&#25552;&#20379;&#20102;&#36879;&#26126;&#30340;&#32467;&#26524;&#28304;&#65292;&#21487;&#29992;&#20110;&#24378;&#22823;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#20363;&#22914;&#24615;&#33021;&#27010;&#35201;&#12289;&#25913;&#36827;&#27010;&#29575;&#25110;&#39044;&#26399;&#22312;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;EXM&#65289;"&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#30446;&#26631;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#31639;&#27861;&#22522;&#30784;&#30340;&#35814;&#32454;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2206.00439</link><description>&lt;p&gt;
&#31639;&#27861;&#22522;&#30784;&#30340;&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Foundations of Empirical X-risk Minimization. (arXiv:2206.00439v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;EXM&#65289;"&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#30446;&#26631;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#31639;&#27861;&#22522;&#30784;&#30340;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#32463;&#39564;&#24615;X&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;EXM&#65289;"&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20248;&#21270;&#26694;&#26550;&#12290;X&#39118;&#38505;&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#19968;&#31867;&#32452;&#21512;&#24230;&#37327;&#25110;&#30446;&#26631;&#30340;&#26415;&#35821;&#65292;&#22312;&#20854;&#20013;&#65292;&#23558;&#27599;&#20010;&#25968;&#25454;&#28857;&#19982;&#22823;&#37327;&#30340;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#39033;&#30446;&#36827;&#34892;&#27604;&#36739;&#26469;&#23450;&#20041;&#39118;&#38505;&#20989;&#25968;&#12290;&#23427;&#21253;&#25324;&#35768;&#22810;&#24191;&#27867;&#20351;&#29992;&#30340;&#20195;&#29702;&#30446;&#26631;&#21644;&#19981;&#21487;&#20998;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20363;&#22914;AUROC&#12289;AUPRC&#12289;&#37096;&#20998;AUROC&#12289;NDCG&#12289;MAP&#12289;&#22312;&#21069;K&#20010;&#20301;&#32622;&#30340;&#31934;&#30830;&#24230;/&#21484;&#22238;&#29575;&#12289;&#22312;&#29305;&#23450;&#21484;&#22238;&#29575;&#27700;&#24179;&#19978;&#30340;&#31934;&#30830;&#24230;&#12289;&#21015;&#34920;&#25439;&#22833;&#12289;p&#33539;&#25968;&#25512;&#23548;&#12289;&#39030;&#37096;&#25512;&#23548;&#12289;&#20840;&#23616;&#23545;&#27604;&#25439;&#22833;&#31561;&#12290;&#34429;&#28982;&#36825;&#20123;&#19981;&#21487;&#20998;&#35299;&#30340;&#30446;&#26631;&#21450;&#20854;&#20248;&#21270;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#24687;&#26816;&#32034;&#31561;&#39046;&#22495;&#30340;&#25991;&#29486;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#38754;&#20020;&#30528;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;EXM&#30340;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#36817;&#30340;&#20005;&#26684;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript introduces a new optimization framework for machine learning and AI, named {\bf empirical X-risk minimization (EXM)}. X-risk is a term introduced to represent a family of compositional measures or objectives, in which each data point is compared with a large number of items explicitly or implicitly for defining a risk function. It includes surrogate objectives of many widely used measures and non-decomposable losses, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP, precision/recall at top $K$ positions, precision at a certain recall level, listwise losses, p-norm push, top push, global contrastive losses, etc. While these non-decomposable objectives and their optimization algorithms have been studied in the literature of machine learning, computer vision, information retrieval, and etc, optimizing these objectives has encountered some unique challenges for deep learning. In this paper, we present recent rigorous efforts for EXM with a focus on its algorithmic foundations a
&lt;/p&gt;</description></item><item><title>NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2110.14053</link><description>&lt;p&gt;
NeuroBack: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14053
&lt;/p&gt;
&lt;p&gt;
NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#21040;&#35268;&#21010;&#12289;&#39564;&#35777;&#21644;&#23433;&#20840;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22686;&#24378;CDCL SAT&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20351;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#36827;&#34892;&#39057;&#32321;&#30340;&#22312;&#32447;&#27169;&#22411;&#25512;&#26029;&#12290;&#20026;&#20102;&#20351;GNN&#30340;&#25913;&#36827;&#21464;&#24471;&#23454;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroBack&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#20004;&#20010;&#27934;&#23519;&#19978;&#65306;&#65288;1&#65289;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#65288;&#29978;&#33267;&#20840;&#37096;&#65289;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65288;&#21363;&#20540;&#65289;&#23545;&#20110;CDCL SAT&#27714;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#65288;2&#65289;&#22312;SAT&#27714;&#35299;&#24320;&#22987;&#20043;&#21069;&#65292;&#21482;&#38656;&#26597;&#35810;&#19968;&#27425;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21363;&#21487;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31163;&#32447;&#27169;&#22411;&#25512;&#26029;&#20351;NeuroBack&#33021;&#22815;&#20165;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/1912.13490</link><description>&lt;p&gt;
&#24847;&#35782;&#30340;&#31070;&#32463;&#35745;&#31639;&#27169;&#22411;&#65306;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#29702;&#35770;&#65288;GARIM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#20316;&#20026;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#24050;&#32463;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#31561;&#22810;&#31181;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#30340;&#19981;&#33391;&#25972;&#21512;&#38480;&#21046;&#20102;&#23545;&#24847;&#35782;&#30340;&#23436;&#25972;&#21644;&#28165;&#26224;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#24847;&#35782;&#29702;&#35770;&#65292;&#20026;&#25913;&#21892;&#36825;&#31181;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;GARIM&#29702;&#35770;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#65288;&#22914;&#19990;&#30028;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#34892;&#20026;&#24207;&#21015;&#65289;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#23427;&#20204;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#12290;&#36825;&#20123;&#25805;&#20316;&#20351;&#24471;&#24847;&#35782;&#20195;&#29702;&#33021;&#22815;&#22312;&#20869;&#37096;&#20135;&#29983;&#20854;&#25152;&#32570;&#20047;&#30340;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#26032;&#26465;&#20214;&#21644;&#30446;&#26631;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;&#34920;&#31034;&#30340;&#25805;&#20316;&#30001;&#22235;&#20010;&#31070;&#32463;&#21151;&#33021;&#23439;&#31995;&#32479;&#65288;Hierarc...
&lt;/p&gt;
&lt;p&gt;
Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
&lt;/p&gt;</description></item></channel></rss>