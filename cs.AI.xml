<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.06531</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#20027;&#28165;&#27905;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#30340;&#23454;&#36341;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning. (arXiv:2303.06531v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel robust mixed-integer linear programming model for multi-robot hybrid-task allocation in uncertain autonomous cleaning systems, and establishes a dataset of 100 instances.
&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#20998;&#37197;&#22312;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22810;&#20010;&#26426;&#22120;&#20154;&#19968;&#36215;&#24037;&#20316;&#20197;&#28165;&#27905;&#22823;&#38754;&#31215;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30456;&#20851;&#30740;&#31350;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#30830;&#23450;&#24615;&#30340;&#21333;&#20219;&#21153;&#20998;&#37197;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#30340;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#27169;&#22411;&#26469;&#24314;&#27169;&#28165;&#27905;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#28151;&#21512;&#28165;&#27905;&#20219;&#21153;&#39034;&#24207;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#31561;&#23454;&#38469;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#26377;2D&#25163;&#21160;&#26631;&#35760;&#30340;&#22270;&#20687;&#21644;3D&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task allocation plays a vital role in multi-robot autonomous cleaning systems, where multiple robots work together to clean a large area. However, there are several problems in relevant research to date. Most current studies mainly focus on deterministic, single-task allocation for cleaning robots, without considering hybrid tasks in uncertain working environments. Moreover, there is a lack of datasets and benchmarks for relevant research. In this paper, we contribute to multi-robot hybrid-task allocation for uncertain autonomous cleaning systems by addressing these problems. First, we model the uncertainties in the cleaning environment via robust optimization and propose a novel robust mixed-integer linear programming model with practical constraints including hybrid cleaning task order and robot's ability. Second, we establish a dataset of 100 instances made from floor plans, each of which has 2D manually-labeled images and a 3D model. Third, we provide comprehensive results on the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#22411;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;98%&#30340;&#20934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#20540;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.06514</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#26816;&#27979;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#20449;&#29992;&#21345;&#27450;&#35784;
&lt;/p&gt;
&lt;p&gt;
Credit Card Fraud Detection Using Enhanced Random Forest Classifier for Imbalanced Data. (arXiv:2303.06514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#22411;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;98%&#30340;&#20934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#20540;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an enhanced random forest classifier and synthetic minority over-sampling technique to address the issue of imbalanced data in credit card fraud detection, achieving an accuracy of 98% and F1-score value of about 98%, with potential practical applications.
&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#21345;&#24050;&#25104;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20132;&#26131;&#20013;&#26368;&#27969;&#34892;&#30340;&#25903;&#20184;&#26041;&#24335;&#12290;&#38543;&#30528;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#27450;&#35784;&#26696;&#20214;&#30340;&#22686;&#21152;&#65292;&#21019;&#24314;&#27450;&#35784;&#26816;&#27979;&#31639;&#27861;&#20197;&#31934;&#30830;&#35782;&#21035;&#21644;&#20572;&#27490;&#27450;&#35784;&#27963;&#21160;&#30340;&#24517;&#35201;&#24615;&#20063;&#38543;&#20043;&#20135;&#29983;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#32452;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#38598;&#12290;&#22312;&#22788;&#29702;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26102;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20132;&#26131;&#37117;&#26159;&#38750;&#27450;&#35784;&#20132;&#26131;&#12290;&#20026;&#20102;&#20811;&#26381;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#12290;&#23454;&#29616;&#36229;&#21442;&#25968;&#25216;&#26415;&#20197;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RF&#20998;&#31867;&#22120;&#33719;&#24471;&#20102;98&#65285;&#30340;&#20934;&#30830;&#24230;&#21644;&#32422;98&#65285;&#30340;F1&#20998;&#25968;&#20540;&#65292;&#36825;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#12290;&#25105;&#20204;&#36824;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#23481;&#26131;&#24212;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#20811;&#26381;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The credit card has become the most popular payment method for both online and offline transactions. The necessity to create a fraud detection algorithm to precisely identify and stop fraudulent activity arises as a result of both the development of technology and the rise in fraud cases. This paper implements the random forest (RF) algorithm to solve the issue in the hand. A dataset of credit card transactions was used in this study. The main problem when dealing with credit card fraud detection is the imbalanced dataset in which most of the transaction are non-fraud ones. To overcome the problem of the imbalanced dataset, the synthetic minority over-sampling technique (SMOTE) was used. Implementing the hyperparameters technique to enhance the performance of the random forest classifier. The results showed that the RF classifier gained an accuracy of 98% and about 98% of F1-score value, which is promising. We also believe that our model is relatively easy to apply and can overcome the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#30340;&#20248;&#21183;&#65292;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06468</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#20934;&#30830;&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;
&lt;/p&gt;
&lt;p&gt;
Accurate Prediction of Global Mean Temperature through Data Transformation Techniques. (arXiv:2303.06468v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#30340;&#20248;&#21183;&#65292;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the advantage of using statistical and simpler machine learning methods to predict global mean temperature, and finds that data transformation techniques can improve prediction accuracy.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#65288;GMT&#65289;&#22312;&#26410;&#26469;&#20960;&#21313;&#24180;&#20869;&#30340;&#21464;&#21270;&#36235;&#21183;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#21382;&#21490;&#25968;&#25454;&#26159;&#23454;&#29616;&#38271;&#26399;&#39044;&#27979;&#30446;&#26631;&#30340;&#24517;&#35201;&#31532;&#19968;&#27493;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20351;&#29992;&#22797;&#26434;&#30340;ML&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#22312;&#24212;&#29992;&#19981;&#21516;&#31639;&#27861;&#20043;&#21069;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#25968;&#25454;&#36716;&#25442;&#26041;&#27861;&#34987;&#29992;&#20316;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25163;&#27573;&#12290;GMT&#26102;&#38388;&#24207;&#21015;&#26082;&#34987;&#35270;&#20026;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20063;&#34987;&#35270;&#20026;&#22238;&#24402;&#38382;&#39064;&#12290;&#21457;&#29616;&#19968;&#20123;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#26159;&#26377;&#25928;&#30340;&#12290;&#21508;&#31181;&#31616;&#21333;&#30340;ML&#26041;&#27861;&#34920;&#29616;&#24471;&#21644;&#26356;&#20026;&#30693;&#21517;&#30340;&#26041;&#27861;&#19968;&#26679;&#22909;&#29978;&#33267;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#23581;&#35797;&#22823;&#37327;&#31639;&#27861;&#20316;&#20026;&#31532;&#19968;&#27493;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;56&#31181;&#31639;&#27861;&#32463;&#36807;Box-Cox&#12289;Yeo-Johnson&#21644;&#19968;&#38454;&#24046;&#20998;&#22788;&#29702;&#21518;&#36827;&#34892;&#27604;&#36739;&#65292;&#19982;&#27809;&#26377;&#36827;&#34892;&#22788;&#29702;&#30340;&#24773;&#20917;&#36827;&#34892;&#27604;&#36739;&#12290;&#39044;&#27979;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to predict how the Global Mean Temperature (GMT) will evolve in the next few decades. The ability to predict historical data is a necessary first step toward the actual goal of making long-range forecasts. This paper examines the advantage of statistical and simpler Machine Learning (ML) methods instead of directly using complex ML algorithms and Deep Learning Neural Networks (DNN). Often neglected data transformation methods prior to applying different algorithms have been used as a means of improving predictive accuracy. The GMT time series is treated both as a univariate time series and also cast as a regression problem. Some steps of data transformations were found to be effective. Various simple ML methods did as well or better than the more well-known ones showing merit in trying a large bouquet of algorithms as a first step. Fifty-six algorithms were subject to Box-Cox, Yeo-Johnson, and first-order differencing and compared with the absence of them. Predictions f
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06455</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.
&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#34892;&#19994;&#37117;&#35797;&#22270;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20197;&#25152;&#35859;&#30340;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#65292;&#20854;&#20013;&#27599;&#20010;&#35760;&#24405;&#30001;&#35768;&#22810;&#24322;&#26500;&#30340;&#36830;&#32493;&#21644;&#20998;&#31867;&#21015;&#32452;&#25104;&#65292;&#20063;&#31216;&#20026;&#29305;&#24449;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19982;&#20154;&#31867;&#25216;&#33021;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#20854;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26356;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#20132;&#20114;&#32593;&#32476;&#65288;IN&#65289;&#65292;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20854;&#32467;&#26524;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#65292;&#40723;&#21169;&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2303.06430</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#25991;&#26412;&#21327;&#20316;&#20219;&#21153;&#20013;&#20132;&#20114;&#35774;&#35745;&#31354;&#38388;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks. (arXiv:2303.06430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#65292;&#40723;&#21169;&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a spectrum of content generation tasks and their corresponding human-AI interaction patterns, encouraging the research community to focus on more complex and interdependent tasks that require greater levels of human involvement.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26410;&#26469;&#20197;&#21450;&#20154;&#31867;&#22914;&#20309;&#19982;LLMs&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#65306;1&#65289;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#22266;&#23450;&#33539;&#22260;&#20869;&#23481;&#31574;&#21010;&#20219;&#21153;&#65292;2&#65289;&#20855;&#26377;&#31934;&#30830;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#29420;&#31435;&#21019;&#24847;&#20219;&#21153;&#65292;&#20197;&#21450;3&#65289;&#20855;&#26377;&#36845;&#20195;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#22797;&#26434;&#19988;&#30456;&#20114;&#20381;&#36182;&#30340;&#21019;&#24847;&#20219;&#21153;&#12290;&#25105;&#20204;&#40723;&#21169;&#29983;&#25104;AI&#21644;HCI&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs. In this paper, we present a spectrum of content generation tasks and their corresponding human-AI interaction patterns. These tasks include: 1) fixed-scope content curation tasks with minimal human-AI interactions, 2) independent creative tasks with precise human-AI interactions, and 3) complex and interdependent creative tasks with iterative human-AI interactions. We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO-NOMA IoT&#31995;&#32479;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;AoI&#21644;&#33021;&#32791;&#12290;</title><link>http://arxiv.org/abs/2303.06411</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO-NOMA IoT&#31995;&#32479;&#21151;&#29575;&#20998;&#37197;&#65292;&#20197;&#26368;&#23567;&#21270;AoI&#21644;&#33021;&#32791;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems. (arXiv:2303.06411v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO-NOMA IoT&#31995;&#32479;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;AoI&#21644;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep reinforcement learning based power allocation method for MIMO-NOMA IoT systems to minimize AoI and energy consumption.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#21644;&#38750;&#27491;&#20132;&#22810;&#22336;&#65288;MIMO-NOMA&#65289;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20449;&#36947;&#23481;&#37327;&#21644;&#39057;&#35889;&#25928;&#29575;&#65292;&#20197;&#25903;&#25345;&#23454;&#26102;&#24212;&#29992;&#12290;&#26102;&#24310;&#65288;AoI&#65289;&#26159;&#23454;&#26102;&#24212;&#29992;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20294;&#27809;&#26377;&#25991;&#29486;&#26368;&#23567;&#21270;MIMO-NOMA IoT&#31995;&#32479;&#30340;AoI&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#34892;&#36825;&#39033;&#24037;&#20316;&#12290;&#22312;MIMO-NOMA IoT&#31995;&#32479;&#20013;&#65292;&#22522;&#31449;&#65288;BS&#65289;&#30830;&#23450;&#26679;&#26412;&#25910;&#38598;&#35201;&#27714;&#24182;&#20026;&#27599;&#20010;IoT&#35774;&#22791;&#20998;&#37197;&#20256;&#36755;&#21151;&#29575;&#12290;&#27599;&#20010;&#35774;&#22791;&#26681;&#25454;&#26679;&#26412;&#25910;&#38598;&#35201;&#27714;&#30830;&#23450;&#26159;&#21542;&#37319;&#26679;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20998;&#37197;&#30340;&#21151;&#29575;&#23558;&#37319;&#26679;&#30340;&#25968;&#25454;&#36890;&#36807;MIMO-NOMA&#20449;&#36947;&#20256;&#36755;&#21040;BS&#12290;&#28982;&#21518;&#65292;BS&#37319;&#29992;&#36830;&#32493;&#24178;&#25200;&#28040;&#38500;&#65288;SIC&#65289;&#25216;&#26415;&#35299;&#30721;&#27599;&#20010;&#35774;&#22791;&#20256;&#36755;&#30340;&#25968;&#25454;&#20449;&#21495;&#12290;&#26679;&#26412;&#25910;&#38598;&#35201;&#27714;&#21644;&#21151;&#29575;&#20998;&#37197;&#23558;&#24433;&#21709;&#31995;&#32479;&#30340;AoI&#21644;&#33021;&#32791;&#12290;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-input multi-out and non-orthogonal multiple access (MIMO-NOMA) internet-of-things (IoT) systems can improve channel capacity and spectrum efficiency distinctly to support the real-time applications. Age of information (AoI) is an important metric for real-time application, but there is no literature have minimized AoI of the MIMO-NOMA IoT system, which motivates us to conduct this work. In MIMO-NOMA IoT system, the base station (BS) determines the sample collection requirements and allocates the transmission power for each IoT device. Each device determines whether to sample data according to the sample collection requirements and adopts the allocated power to transmit the sampled data to the BS over MIMO-NOMA channel. Afterwards, the BS employs successive interference cancelation (SIC) technique to decode the signal of the data transmitted by each device. The sample collection requirements and power allocation would affect AoI and energy consumption of the system. It is critical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06410</link><description>&lt;p&gt;
Brain Diffuser&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#33041;&#22270;&#20687;&#21040;&#33041;&#32593;&#32476;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;
&lt;p&gt;
&#33041;&#32593;&#32476;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#21644;&#24178;&#39044;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#32791;&#26102;&#21644;&#20027;&#35266;&#30340;&#24037;&#20855;&#21253;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20855;&#21487;&#20197;&#20174;&#33041;&#25193;&#25955;&#24352;&#37327;&#22270;&#20687;&#65288;DTI&#65289;&#20013;&#33719;&#21462;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;Brain Diffuser&#36890;&#36807;&#20998;&#26512;&#21463;&#35797;&#32773;&#20043;&#38388;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;&#26356;&#22810;&#30340;&#32467;&#26500;&#36830;&#25509;&#29305;&#24449;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06407</link><description>&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans. (arXiv:2303.06407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores how to automatically detect signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans, to improve the quality of life for epilepsy patients.
&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#12290;&#30315;&#30187;&#21457;&#20316;&#36890;&#24120;&#26159;&#30001;&#20110;&#20154;&#33041;&#20013;&#30340;&#38750;&#21327;&#35843;&#30005;&#25918;&#30005;&#24341;&#36215;&#30340;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#65292;&#21253;&#25324;&#20498;&#22320;&#21644;&#22833;&#21435;&#24847;&#35782;&#12290;&#22914;&#26524;&#33021;&#22815;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#37027;&#20040;&#21487;&#20197;&#23558;&#21463;&#35797;&#32773;&#32622;&#20110;&#23433;&#20840;&#30340;&#29615;&#22659;&#25110;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#30001;&#20110;&#20498;&#22320;&#32780;&#23548;&#33268;&#30340;&#33258;&#25105;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#26085;&#24120;&#30340;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23456;&#29289;&#29399;&#26377;&#33021;&#21147;&#36890;&#36807;&#21957;&#25506;&#21463;&#35797;&#32773;&#22312;&#30315;&#30187;&#21457;&#20316;&#21069;&#30382;&#32932;&#25955;&#21457;&#30340;&#29305;&#24449;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#26469;&#26816;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#26377;&#20123;&#36741;&#21161;&#29356;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#21521;&#20854;&#20027;&#20154;/&#35757;&#32451;&#21592;&#21457;&#20986;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#20449;&#21495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. Seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. If the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. However there are no definitive methods to predict seizures in an everyday, uncontrolled environment. Previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. In this work we identify how we can automatically detect the signalling behaviours
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2303.06394</link><description>&lt;p&gt;
&#19968;&#31181;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series. (arXiv:2303.06394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method that combines moving fronts, data decomposition, and deep learning to forecast intricate time series. The method decomposes the time series into simpler constituent series using empirical wavelet transform and prevents data leakage using the moving front method.
&lt;/p&gt;
&lt;p&gt;
&#39640;&#21464;&#24322;&#24615;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29978;&#33267;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20063;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34987;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#23427;&#20204;&#30340;&#24635;&#21644;&#31561;&#20110;&#21407;&#22987;&#24207;&#21015;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#20998;&#35299;&#25216;&#26415;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#21069;&#27839;&#65288;MF&#65289;&#26041;&#27861;&#26469;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#65292;&#20351;&#20998;&#35299;&#21518;&#30340;&#24207;&#21015;&#21487;&#20197;&#20687;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#19968;&#26679;&#22788;&#29702;&#12290;&#21360;&#24230;&#22799;&#23395;&#23395;&#39118;&#38477;&#38632;&#65288;ISMR&#65289;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#23545;DNN&#26500;&#25104;&#25361;&#25112;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#31034;&#20363;&#12290;&#20174;&#20247;&#22810;&#21487;&#29992;&#30340;&#20449;&#21495;&#22788;&#29702;&#24037;&#20855;&#20013;&#65292;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#65288;EWT&#65289;&#34987;&#36873;&#25321;&#29992;&#20110;&#23558;ISMR&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#34987;&#21457;&#29616;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#22914;&#33258;&#36866;&#24212;&#22122;&#22768;&#23436;&#20840;&#38598;&#21512;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;CEEMDAN&#65289;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A univariate time series with high variability can pose a challenge even to Deep Neural Network (DNN). To overcome this, a univariate time series is decomposed into simpler constituent series, whose sum equals the original series. As demonstrated in this article, the conventional one-time decomposition technique suffers from a leak of information from the future, referred to as a data leak. In this work, a novel Moving Front (MF) method is proposed to prevent data leakage, so that the decomposed series can be treated like other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex time series, which poses a challenge to DNN and is therefore selected as an example. From the many signal processing tools available, Empirical Wavelet Transform (EWT) was chosen for decomposing the ISMR into simpler constituent series, as it was found to be more effective than the other popular algorithm, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). The propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06389</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#26159;&#25351;&#20165;&#36890;&#36807;&#35760;&#24405;&#30340;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#34429;&#28982;&#29983;&#25104;&#35760;&#24405;&#25968;&#25454;&#30340;&#30495;&#23454;&#35760;&#24405;&#31574;&#30053;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#37319;&#29992;&#20854;&#20272;&#35745;&#20540;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#36825;&#31181;&#20272;&#35745;&#22120;&#23548;&#33268;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#23567;&#19988;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#35760;&#24405;&#27010;&#29575;&#30340;&#26679;&#26412;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#26469;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#19977;&#20010;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;UIPS&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;DNNs&#21487;&#20197;&#23398;&#20064;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06367</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks. (arXiv:2303.06367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;DNNs&#21487;&#20197;&#23398;&#20064;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses artificial neural networks to explore neural representations of scene perception in a hippocampally dependent task, and demonstrates that DNNs can learn the ability to transform scenes viewed from different egocentric perspectives, using a novel scene perception benchmark.
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#21754;&#20083;&#21160;&#29289;&#35270;&#35273;&#31995;&#32479;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#20174;&#21021;&#32423;&#35270;&#35273;&#30382;&#23618;&#21040;&#19979;&#39070;&#30382;&#36136;(IT)&#30340;&#31070;&#32463;&#21709;&#24212;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#35299;&#37322;&#26356;&#39640;&#30382;&#23618;&#21306;&#22495;&#30340;&#34920;&#24449;&#33021;&#21147;&#30456;&#23545;&#36739;&#24369;&#65292;&#30740;&#31350;&#20063;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21463;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#21551;&#21457;&#30340;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#26088;&#22312;&#25506;&#31350;DNNs&#36716;&#25442;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#21463;&#39070;&#21494;&#32467;&#26500;&#21644;&#28023;&#39532;&#20043;&#38388;&#36830;&#25509;&#21551;&#21457;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#35757;&#32451;&#30340;DNNs&#21487;&#20197;&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#20998;&#35299;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Deep artificial neural networks (DNNs) trained through backpropagation provide effective models of the mammalian visual system, accurately capturing the hierarchy of neural responses through primary visual cortex to inferior temporal cortex (IT). However, the ability of these networks to explain representations in higher cortical areas is relatively lacking and considerably less well researched. For example, DNNs have been less successful as a model of the egocentric to allocentric transformation embodied by circuits in retrosplenial and posterior parietal cortex. We describe a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of DNNs to transform scenes viewed from different egocentric perspectives. Using a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, we demonstrate that DNNs trained using a triplet loss can learn this task. Moreover, by enforcing a factorized latent space
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2303.06365</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#26816;&#26597;&#23618;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#30001;&#20110;&#36755;&#20837;&#36890;&#24120;&#19981;&#21487;&#35299;&#37322;&#65292;&#22240;&#27492;&#21482;&#26377;&#26377;&#38480;&#30340;XAI&#30740;&#31350;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#65288;&#22914;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#65289;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#35821;&#38899;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#21644;LRP&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DFT-LRP&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;DFT-LRP&#26469;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06361</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#65306;&#32852;&#37030;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#23450;&#20301;&#65288;VLP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23460;&#20869;&#23450;&#20301;&#25216;&#26415;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#65292;&#30001;&#20110;&#39640;&#24230;&#26102;&#21464;&#30340;&#20449;&#36947;&#65292;VLP&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21512;&#20316;VLP&#26041;&#26696;&#12290;&#21033;&#29992;FL&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#32593;&#32476;&#65288;CVPosNet&#65289;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.06360</link><description>&lt;p&gt;
FedLP: &#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23618;&#27425;&#21098;&#26525;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65288;Federated Layer-wise Pruning&#65289;&#65292;&#35813;&#26694;&#26550;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#20026;&#20855;&#26377;&#21516;&#36136;&#26412;&#22320;&#27169;&#22411;&#21644;&#24322;&#36136;&#26412;&#22320;&#27169;&#22411;&#30340;&#22330;&#26223;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;FedLP&#26041;&#26696;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FedLP&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FedLP&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23558;&#23618;&#27425;&#21098;&#26525;&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22522;&#20110;FedLP&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#21464;&#20307;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
&lt;/p&gt;</description></item><item><title>AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06337</link><description>&lt;p&gt;
AutoMLP: &#33258;&#21160;&#21270;MLP&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
AutoMLP: Automated MLP for Sequential Recommendations. (arXiv:2303.06337v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06337
&lt;/p&gt;
&lt;p&gt;
AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoMLP is a novel sequential recommender system that models users' long/short-term interests through an automated and adaptive search algorithm, achieving better recommendation performance.
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#20182;&#20204;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#36825;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#24182;&#23545;&#19979;&#19968;&#20010;&#25512;&#33616;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#25110;&#32463;&#39564;&#32463;&#39564;&#35774;&#32622;&#39044;&#23450;&#20041;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#65292;&#36825;&#26082;&#39640;&#24230;&#20302;&#25928;&#21448;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23384;&#22312;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;AutoMLP&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#33719;&#24471;&#26356;&#22909;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AutoMLP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06333</link><description>&lt;p&gt;
&#38477;&#33853;&#20254;&#65306;&#35780;&#20272;&#20132;&#20114;&#24335;&#20154;&#26426;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#39134;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#20026;&#20849;&#21516;&#30340;&#20889;&#20316;&#25104;&#26524;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#20110;&#20132;&#20114;&#24335;&#29615;&#22659;&#19979;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;Parachute&#23637;&#31034;&#20102;&#20132;&#20114;&#35780;&#20272;&#30340;&#32508;&#21512;&#35270;&#35282;&#65292;&#20854;&#20013;&#27599;&#20010;&#35780;&#20272;&#26041;&#38754;&#37117;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;Parachute&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.06302</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#29616;&#20195;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides a comprehensive overview of recent advancements in adversarial attack and defense techniques in machine learning and deep neural networks, with a focus on deep neural network-based classification models. The methods are classified into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-based attacks.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#20114;&#32852;&#32593;&#21644;&#30456;&#20851;&#22330;&#26223;&#20013;&#30340;&#24555;&#36895;&#22686;&#38271;&#24212;&#29992;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#25915;&#20987;&#21407;&#29702;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36825;&#26159;&#22522;&#20110;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks and defenses in machine learning and deep neural network have been gaining significant attention due to the rapidly growing applications of deep learning in the Internet and relevant scenarios. This survey provides a comprehensive overview of the recent advancements in the field of adversarial attack and defense techniques, with a focus on deep neural network-based classification models. Specifically, we conduct a comprehensive classification of recent adversarial attack methods and state-of-the-art adversarial defense techniques based on attack principles, and present them in visually appealing tables and tree diagrams. This is based on a rigorous evaluation of the existing works, including an analysis of their strengths and limitations. We also categorize the methods into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#35893;&#22916;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.06253</link><description>&lt;p&gt;
ICU&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#39044;&#27979;&#35893;&#22916;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting risk of delirium from ambient noise and light information in the ICU. (arXiv:2303.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#35893;&#22916;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study developed the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information, providing new insights for the prevention and treatment of delirium.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#65292;&#23613;&#31649;&#26377;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#23545;&#35893;&#22916;&#26377;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;Thunderboard&#12289;ActiGraph&#20256;&#24863;&#22120;&#21644;iPod with AudioTools&#24212;&#29992;&#31243;&#24207;&#65292;&#20165;&#20351;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#25253;&#36947;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27979;&#37327;&#25968;&#25454;&#20174;2021&#24180;5&#26376;&#33267;2022&#24180;9&#26376;&#25910;&#38598;&#33258;102&#21517;&#24739;&#32773;&#30340;ICU&#30149;&#25151;&#65292;&#20998;&#20026;&#30333;&#22825;&#65288;0700&#33267;1859&#65289;&#21644;&#22812;&#38388;&#65288;1900&#33267;0659&#65289;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;ICU&#20303;&#38498;&#26399;&#38388;&#25110;&#20986;&#38498;&#21518;4&#22825;&#20869;&#30340;&#35893;&#22916;&#21457;&#29983;&#29575;&#12290;&#26368;&#21518;&#65292;&#20998;&#26512;&#32467;&#26524;&#24471;&#20998;&#20197;&#35780;&#20272;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#21644;&#26041;&#21521;&#24615;&#12290;&#30333;&#22825;&#30340;&#22122;&#22768;&#27700;&#24179;&#26174;&#33879;&#39640;&#20110;&#22812;&#38388;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#24403;&#20165;&#20351;&#29992;&#22122;&#22768;&#29305;&#24449;&#25110;&#22122;&#22768;&#21644;&#20809;&#29305;&#24449;&#30340;&#32452;&#21512;&#26102;&#65292;1-D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Existing Intensive Care Unit (ICU) delirium prediction models do not consider environmental factors despite strong evidence of their influence on delirium. This study reports the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information. Ambient light and noise intensities were measured from ICU rooms of 102 patients from May 2021 to September 2022 using Thunderboard, ActiGraph sensors and an iPod with AudioTools application. These measurements were divided into daytime (0700 to 1859) and nighttime (1900 to 0659). Deep learning models were trained using this data to predict the incidence of delirium during ICU stay or within 4 days of discharge. Finally, outcome scores were analyzed to evaluate the importance and directionality of every feature. Daytime noise levels were significantly higher than nighttime noise levels. When using only noise features or a combination of noise and light features 1-D convolutional neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06252</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65306;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#38761;&#21629;&#24615;&#22320;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing. (arXiv:2303.06252v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an AI-enhanced ICU system that improves patient visual monitoring and assessment, and ultimately enhances the quality of care, through pervasive sensing and data processing.
&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#19987;&#38376;&#30340;&#21307;&#38498;&#31354;&#38388;&#65292;&#29992;&#20110;&#25509;&#21463;&#21361;&#37325;&#30149;&#20154;&#30340;&#23494;&#38598;&#25252;&#29702;&#21644;&#30417;&#27979;&#12290;&#20840;&#38754;&#30340;&#30417;&#27979;&#23545;&#20110;&#35780;&#20272;&#24739;&#32773;&#30340;&#30149;&#24773;&#65292;&#29305;&#21035;&#26159;&#30149;&#24773;&#30340;&#20005;&#37325;&#31243;&#24230;&#20197;&#21450;&#26368;&#32456;&#30340;&#25252;&#29702;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#25252;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#65292;ICU&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#33539;&#22260;&#21463;&#21040;&#38480;&#21046;&#12290;&#30446;&#21069;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#12289;&#23039;&#21183;&#21644;&#27963;&#21160;&#33021;&#21147;&#31561;&#32454;&#33410;&#30340;&#35270;&#35273;&#35780;&#20272;&#20165;&#20598;&#23572;&#34987;&#25429;&#25417;&#21040;&#65292;&#25110;&#32773;&#26681;&#26412;&#27809;&#26377;&#34987;&#25429;&#25417;&#21040;&#12290;&#36825;&#20123;&#25163;&#21160;&#35266;&#23519;&#26159;&#20027;&#35266;&#30340;&#65292;&#23481;&#26131;&#20986;&#29616;&#25991;&#26723;&#38169;&#35823;&#65292;&#24182;&#32473;&#25252;&#29702;&#20154;&#21592;&#24102;&#26469;&#39069;&#22806;&#30340;&#24037;&#20316;&#37327;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#22686;&#24378;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#38656;&#35201;&#24378;&#22823;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) is a specialized hospital space where critically ill patients receive intensive care and monitoring. Comprehensive monitoring is imperative in assessing patients conditions, in particular acuity, and ultimately the quality of care. However, the extent of patient monitoring in the ICU is limited due to time constraints and the workload on healthcare providers. Currently, visual assessments for acuity, including fine details such as facial expressions, posture, and mobility, are sporadically captured, or not captured at all. These manual observations are subjective to the individual, prone to documentation errors, and overburden care providers with the additional workload. Artificial Intelligence (AI) enabled systems has the potential to augment the patient visual monitoring and assessment due to their exceptional learning capabilities. Such systems require robust annotated data to train. To this end, we have developed pervasive sensing and data processing s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2303.06246</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;mHealth&#21644;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#20174;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25110;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#31227;&#21160;&#24863;&#30693;DL&#31995;&#32479;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65292;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;ZoneFL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#12290;ZoneFL&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;&#21306;&#22495;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#12290;&#21463;&#30410;&#20110;FL&#35774;&#35745;&#65292;ZoneFL&#22521;&#35757;&#26399;&#38388;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#26469;&#20248;&#21270;&#21306;&#22495;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65306;&#21306;&#22495;&#21512;&#24182;&#21644;&#20998;&#35010;&#65288;ZMS&#65289;&#21644;Zo
&lt;/p&gt;
&lt;p&gt;
Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06242</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#35270;&#35282;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#23398;&#20064;&#29992;&#20110;&#33258;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations. (arXiv:2303.06242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations, which adopts self-supervision and uses data augmentations to generate two views of the same sample, and learns by matching one to the other. It uses hyperbolic uncertainty to determine the algorithmic learning pace, assuming that less uncertain samples should be more strongly driving the training, with a larger weight and pace.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#26377;&#30410;&#65292;&#20363;&#22914;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21487;&#20197;&#36873;&#25321;&#21644;&#25490;&#24207;&#35757;&#32451;&#26679;&#26412;&#24207;&#21015;&#65292;&#20174;&#26131;&#21040;&#38590;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#20219;&#21153;&#30340;&#30693;&#35782;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;HYSP&#37319;&#29992;&#33258;&#30417;&#30563;&#65306;&#23427;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#65288;&#31216;&#20026;&#22312;&#32447;&#65289;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#65288;&#30446;&#26631;&#65289;&#21305;&#37197;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26159;&#37319;&#29992;&#30340;&#36229;&#20284;&#26354;&#31070;&#32463;&#32593;&#32476;&#30340;&#21103;&#20135;&#21697;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#65292;&#19982;&#39069;&#22806;&#25104;&#26412;&#30456;&#27604;&#65292;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.06241</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;DNN&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#20250;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36873;&#25321;&#23376;&#38598;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25191;&#34892;&#31616;&#21333;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#36807;&#28388;&#20986;&#36825;&#20010;&#23376;&#38598;&#12290;&#22312;&#36825;&#20010;&#25915;&#20987;&#20013;&#65292;&#25105;&#20204;&#21521;&#27599;&#20010;&#20687;&#32032;&#28155;&#21152;&#19968;&#20010;&#23567;&#25200;&#21160;&#21644;&#20960;&#26465;&#32593;&#26684;&#32447;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#23545;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06237</link><description>&lt;p&gt;
&#20302;&#24320;&#38144;&#27169;&#22411;&#21098;&#26525;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#34917;&#20805;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#20363;&#65292;&#28041;&#21450;&#22823;&#37327;&#36890;&#20449;&#21644;&#35745;&#31639;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#27169;&#22411;&#21098;&#26525;/&#31232;&#30095;&#21270;&#24320;&#21457;&#20102;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#65292;&#22312;FL&#20551;&#35774;&#19979;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#20197;&#24494;&#35843;&#20462;&#21098;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34917;&#20805;&#31232;&#30095;&#21270;&#65288;CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;CS&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#31232;&#30095;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#25429;&#33719;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26435;&#37325;&#65292;&#32780;&#23458;&#25143;&#31471;&#21017;&#21019;&#24314;&#26412;&#22320;&#31232;&#30095;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#65292;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#35748;&#20026;&#20154;&#31867;&#30340;&#38656;&#27714;&#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;</title><link>http://arxiv.org/abs/2303.06223</link><description>&lt;p&gt;
&#35841;&#22312;&#24605;&#32771;&#65311;&#20351;&#29992;XAI Playbook&#25512;&#21160;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. (arXiv:2303.06223v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#65292;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#35748;&#20026;&#20154;&#31867;&#30340;&#38656;&#27714;&#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores human-centered evaluation of AI-based systems, drawing parallels between the relatively mature field of explainable AI and the rapidly evolving research boom around large language models. The authors argue that humans' needs should be held front and center in evaluating LLMs.
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32463;&#24120;&#24433;&#21709;&#20154;&#31867;&#65292;&#32780;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#31995;&#32479;&#35780;&#20272;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20197;&#21450;&#20154;&#31867;&#36755;&#20837;&#12290;&#23427;&#24050;&#32463;&#22312;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#21644;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#31038;&#21306;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#25506;&#35752;&#12290;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#65292;&#20294;&#31038;&#21306;&#24050;&#32463;&#25509;&#21463;&#20102;&#20154;&#31867;&#19982;AI&#21450;&#20854;&#38468;&#24102;&#30340;&#35299;&#37322;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#21450;&#24212;&#35813;&#23558;&#20154;&#31867;&#30340;&#38656;&#27714;&#65288;&#21253;&#25324;&#20182;&#20204;&#30340;&#35748;&#30693;&#20559;&#35265;&#21644;&#24618;&#30294;&#65289;&#25918;&#22312;&#39318;&#20301;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;XAI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#28909;&#28526;&#20043;&#38388;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;&#25509;&#21463;&#30340;LLMs&#35780;&#20272;&#25351;&#26631;&#19981;&#26159;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#35752;&#35770;LLMs&#26102;&#65292;XAI&#31038;&#21306;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#36208;&#36807;&#30340;&#35768;&#22810;&#30456;&#21516;&#36335;&#24452;&#23558;&#34987;&#37325;&#26032;&#36367;&#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#31867;&#30340;&#20542;&#21521; - &#20877;&#27425;&#23436;&#20840;&#21253;&#25324;&#20182;&#20204;&#30340;&#35748;&#30693;&#20559;&#35265;&#21644;&#24618;&#30294; - &#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;AI&#31995;&#32479;&#30456;&#23545;&#20110;&#20154;&#31867;&#23436;&#25104;&#30456;&#21516;&#20219;&#21153;&#30340;&#25490;&#25918;&#37327;&#65292;&#21457;&#29616;AI&#20889;&#20316;&#21644;&#25554;&#22270;&#30340;&#30899;&#25490;&#25918;&#27604;&#20154;&#31867;&#20302;130&#21040;1500&#20493;&#21644;310&#21040;2900&#20493;&#65292;&#30446;&#21069;&#20351;&#29992;AI&#26377;&#21487;&#33021;&#20197;&#27604;&#20154;&#31867;&#26356;&#20302;&#30340;&#25490;&#25918;&#27700;&#24179;&#23436;&#25104;&#20960;&#39033;&#37325;&#35201;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2303.06219</link><description>&lt;p&gt;
AI&#20889;&#20316;&#21644;&#25554;&#22270;&#30340;&#30899;&#25490;&#25918;&#27604;&#20154;&#31867;&#20302;
&lt;/p&gt;
&lt;p&gt;
The Carbon Emissions of Writing and Illustrating Are Lower for AI than for Humans. (arXiv:2303.06219v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;AI&#31995;&#32479;&#30456;&#23545;&#20110;&#20154;&#31867;&#23436;&#25104;&#30456;&#21516;&#20219;&#21153;&#30340;&#25490;&#25918;&#37327;&#65292;&#21457;&#29616;AI&#20889;&#20316;&#21644;&#25554;&#22270;&#30340;&#30899;&#25490;&#25918;&#27604;&#20154;&#31867;&#20302;130&#21040;1500&#20493;&#21644;310&#21040;2900&#20493;&#65292;&#30446;&#21069;&#20351;&#29992;AI&#26377;&#21487;&#33021;&#20197;&#27604;&#20154;&#31867;&#26356;&#20302;&#30340;&#25490;&#25918;&#27700;&#24179;&#23436;&#25104;&#20960;&#39033;&#37325;&#35201;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyzes the emissions of several AI systems relative to those of humans completing the same tasks, finding that AI writing and illustrating emit 130 to 1500 times less CO2e and 310 to 2900 times less, respectively. The use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#31995;&#32479;&#30340;&#26222;&#21450;&#65292;&#23427;&#20204;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#23545;&#20154;&#31867;&#31038;&#20250;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;AI&#31995;&#32479;&#65288;ChatGPT&#12289;BLOOM&#12289;DALL-E2&#12289;Midjourney&#65289;&#30456;&#23545;&#20110;&#20154;&#31867;&#23436;&#25104;&#30456;&#21516;&#20219;&#21153;&#30340;&#25490;&#25918;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;AI&#20889;&#19968;&#39029;&#25991;&#26412;&#25490;&#25918;&#30340;&#20108;&#27687;&#21270;&#30899;&#24403;&#37327;&#27604;&#19968;&#20010;&#20154;&#23569;130&#21040;1500&#20493;&#12290;&#21516;&#26679;&#65292;&#19968;&#20010;AI&#21019;&#36896;&#19968;&#24352;&#22270;&#29255;&#25490;&#25918;&#30340;&#20108;&#27687;&#21270;&#30899;&#24403;&#37327;&#27604;&#20154;&#31867;&#23569;310&#21040;2900&#20493;&#12290;&#25490;&#25918;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#21040;&#31038;&#20250;&#24433;&#21709;&#65292;&#22914;&#32844;&#19994;&#26367;&#20195;&#12289;&#21512;&#27861;&#24615;&#21644;&#21453;&#24377;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;AI&#24182;&#19981;&#33021;&#26367;&#20195;&#25152;&#26377;&#30340;&#20154;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;AI&#26377;&#21487;&#33021;&#20197;&#27604;&#20154;&#31867;&#26356;&#20302;&#30340;&#25490;&#25918;&#27700;&#24179;&#23436;&#25104;&#20960;&#39033;&#37325;&#35201;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems proliferate, their greenhouse gas emissions are an increasingly important concern for human societies. We analyze the emissions of several AI systems (ChatGPT, BLOOM, DALL-E2, Midjourney) relative to those of humans completing the same tasks. We find that an AI writing a page of text emits 130 to 1500 times less CO2e than a human doing so. Similarly, an AI creating an image emits 310 to 2900 times less. Emissions analysis do not account for social impacts such as professional displacement, legality, and rebound effects. In addition, AI is not a substitute for all human tasks. Nevertheless, at present, the use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.
&lt;/p&gt;</description></item><item><title>CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06213</link><description>&lt;p&gt;
CHGNN: &#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06213
&lt;/p&gt;
&lt;p&gt;
CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.
&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21487;&#20197;&#27169;&#25311;&#24212;&#29992;&#31243;&#24207;&#20013;&#21457;&#29616;&#30340;&#25968;&#25454;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36229;&#22270;&#23398;&#20064;&#30740;&#31350;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#25193;&#23637;&#21040;&#36229;&#22270;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#29305;&#24449;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;CHGNN&#65292;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;CHGNN&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#65292;&#37319;&#29992;&#33258;&#21160;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#23398;&#20064;&#26368;&#23567;&#20805;&#20998;&#35270;&#22270;&#30340;&#25200;&#21160;&#27010;&#29575;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;CHGNN&#21253;&#21547;&#19968;&#20010;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#65292;&#32771;&#34385;&#21040;&#36229;&#36793;&#30340;&#21516;&#36136;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#31532;&#19977;&#65292;CHGNN&#37197;&#22791;&#20102;&#19968;&#20010;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#35270;&#22270;&#29983;&#25104;&#22120;&#30340;&#30456;&#20284;&#24615;&#25439;&#22833;&#12289;&#33410;&#28857;&#20998;&#31867;&#25439;&#22833;&#21644;&#36229;&#36793;&#21516;&#36136;&#24615;&#25439;&#22833;&#65292;&#27880;&#20837;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#36229;&#27169;&#25104;&#26412;&#20989;&#25968;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#23478;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20844;&#24179;&#20998;&#37197;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36825;&#31867;&#20272;&#20540;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#21152;&#26435;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06212</link><description>&lt;p&gt;
&#24102;&#26377;&#20108;&#20803;&#36229;&#27169;&#30697;&#38453;&#30340;&#20844;&#24179;&#24615;&#21152;&#26435;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Weighted Notions of Fairness with Binary Supermodular Chores. (arXiv:2303.06212v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#36229;&#27169;&#25104;&#26412;&#20989;&#25968;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#23478;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20844;&#24179;&#20998;&#37197;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36825;&#31867;&#20272;&#20540;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#21152;&#26435;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of allocating indivisible chores among agents with binary supermodular cost functions and presents a general framework for fair allocation with this class of valuation functions. The framework allows for efficient computation of allocations that satisfy weighted notions of fairness.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#36229;&#27169;&#25104;&#26412;&#20989;&#25968;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#23478;&#21153;&#30340;&#38382;&#39064;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#27599;&#20010;&#23478;&#21153;&#30340;&#36793;&#38469;&#25104;&#26412;&#20026;$0$&#25110;$1$&#65292;&#24182;&#19988;&#23478;&#21153;&#21576;&#29616;&#20986;&#36882;&#22686;&#30340;&#36793;&#38469;&#25104;&#26412;&#65288;&#25110;&#36882;&#20943;&#30340;&#36793;&#38469;&#25928;&#29992;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;Viswanathan&#21644;Zick&#65288;2022&#65289;&#20197;&#21450;Barman&#31561;&#20154;&#65288;2023&#65289;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20844;&#24179;&#20998;&#37197;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36825;&#31867;&#20272;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#25512;&#24191;Barman&#31561;&#20154;&#65288;2023&#65289;&#30340;&#32467;&#26524;&#65292;&#24182;&#26377;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#21152;&#26435;&#20844;&#24179;&#24615;&#27010;&#24565;&#65288;&#22914;&#21152;&#26435;leximin&#25110;min&#21152;&#26435;$p$-mean malfare&#65292;&#20854;&#20013;$p \ge 1$&#65289;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of allocating indivisible chores among agents with binary supermodular cost functions. In other words, each chore has a marginal cost of $0$ or $1$ and chores exhibit increasing marginal costs (or decreasing marginal utilities). In this note, we combine the techniques of Viswanathan and Zick (2022) and Barman et al. (2023) to present a general framework for fair allocation with this class of valuation functions. Our framework allows us to generalize the results of Barman et al. (2023) and efficiently compute allocations which satisfy weighted notions of fairness like weighted leximin or min weighted $p$-mean malfare for any $p \ge 1$.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;Carolinas Highway Dataset&#65288;CHD&#65289;&#21253;&#25324;160&#19975;&#24103;&#21644;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.06202</link><description>&lt;p&gt;
&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A POV-based Highway Vehicle Trajectory Dataset and Prediction Architecture. (arXiv:2303.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06202
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;Carolinas Highway Dataset&#65288;CHD&#65289;&#21253;&#25324;160&#19975;&#24103;&#21644;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing a POV-based highway vehicle trajectory dataset and prediction architecture, including Carolinas Highway Dataset (CHD) with 1.6 million frames and 338,000 vehicle trajectories captured at eight locations in Carolinas.
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#22810;&#20010;&#35270;&#35282;&#65288;POV&#65289;&#30340;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#23545;&#20110;&#21508;&#31181;&#20132;&#36890;&#23433;&#20840;&#21644;&#31649;&#29702;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#36712;&#36857;&#25968;&#25454;&#38598;&#24456;&#20016;&#23500;&#65292;&#20294;&#24456;&#23569;&#25552;&#20379;&#20840;&#38754;&#21644;&#22810;&#26679;&#21270;&#30340;&#39550;&#39542;&#22330;&#26223;&#65292;&#25429;&#25417;&#21508;&#31181;&#39640;&#36895;&#20844;&#36335;&#24067;&#23616;&#12289;&#21512;&#24182;&#36710;&#36947;&#21644;&#37197;&#32622;&#30340;&#22810;&#20010;&#35270;&#28857;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#39550;&#39542;&#21592;&#12289;&#36710;&#36742;&#21644;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#20043;&#38388;&#24494;&#22937;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Carolinas Highway Dataset&#65288;CHD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36710;&#36742;&#36712;&#36857;&#12289;&#26816;&#27979;&#21644;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;CHD&#26159;&#22312;Carolinas&#30340;&#20843;&#20010;&#20301;&#32622;&#25293;&#25668;&#30340;&#39640;&#36895;&#20844;&#36335;&#35270;&#39057;&#20013;&#25429;&#33719;&#30340;160&#19975;&#24103;&#65292;&#21253;&#25324;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle Trajectory datasets that provide multiple point-of-views (POVs) can be valuable for various traffic safety and management applications. Despite the abundance of trajectory datasets, few offer a comprehensive and diverse range of driving scenes, capturing multiple viewpoints of various highway layouts, merging lanes, and configurations. This limits their ability to capture the nuanced interactions between drivers, vehicles, and the roadway infrastructure. We introduce the \emph{Carolinas Highway Dataset (CHD\footnote{\emph{CHD} available at: \url{https://github.com/TeCSAR-UNCC/Carolinas\_Dataset}})}, a vehicle trajectory, detection, and tracking dataset. \emph{CHD} is a collection of 1.6 million frames captured in highway-based videos from eye-level and high-angle POVs at eight locations across Carolinas with 338,000 vehicle trajectories. The locations, timing of recordings, and camera angles were carefully selected to capture various road geometries, traffic patterns, lighting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.06180</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes FedFBN, a federated learning framework that uses pretrained networks as the model backend and freezes the batch normalization layers throughout the training process to optimize medical image classification on distributed non-iid datasets with partial labels.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#24102;&#22836;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26816;&#27979;&#21487;&#33021;&#23384;&#22312;&#30340;&#19968;&#37096;&#20998;&#30142;&#30149;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#23427;&#20204;&#25104;&#20026;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#25351;&#20986;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20855;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#20855;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30456;&#20851;&#30340;&#22495;&#28418;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFBN&#65292;&#36825;&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#36801;&#31227;&#23398;&#20064;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;iid&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#35780;&#20272;FedFBN&#19982;&#24403;&#21069;FL&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FedFBN&#20248;&#20110;&#20351;&#29992;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#24403;&#21069;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2303.06177</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36719;&#20214;&#28431;&#27934;&#39044;&#27979;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Software Vulnerability Prediction Knowledge Transferring Between Programming Languages. (arXiv:2303.06177v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a transfer learning technique to detect common vulnerabilities in different programming languages by leveraging available datasets. The results show that the proposed model detects vulnerabilities in both C and Java codes with an average recall of 72%.
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#30340;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#27169;&#22411;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#21644;&#24320;&#21457;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#39046;&#22495;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#25152;&#26377;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#30340;&#20195;&#30721;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#25105;&#20204;&#20351;&#29992;C&#28304;&#20195;&#30721;&#26679;&#26412;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;Java&#28304;&#20195;&#30721;&#26679;&#26412;&#26469;&#37319;&#29992;&#21644;&#35780;&#20272;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#26679;&#26412;&#65306;NIST&#36719;&#20214;&#20445;&#38556;&#21442;&#32771;&#25968;&#25454;&#38598;&#65288;SARD&#65289;&#21644;Draper VDISC&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#35843;&#26597;&#27599;&#20010;&#29305;&#24449;&#23545;&#30693;&#35782;&#36716;&#31227;&#26426;&#21046;&#30340;&#36129;&#29486;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing automated and smart software vulnerability detection models has been receiving great attention from both research and development communities. One of the biggest challenges in this area is the lack of code samples for all different programming languages. In this study, we address this issue by proposing a transfer learning technique to leverage available datasets and generate a model to detect common vulnerabilities in different programming languages. We use C source code samples to train a Convolutional Neural Network (CNN) model, then, we use Java source code samples to adopt and evaluate the learned model. We use code samples from two benchmark datasets: NIST Software Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show that proposed model detects vulnerabilities in both C and Java codes with average recall of 72\%. Additionally, we employ explainable AI to investigate how much each feature contributes to the knowledge transfer mechanisms between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.06173</link><description>&lt;p&gt;
&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Unifying Grokking and Double Descent. (arXiv:2303.06173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework of pattern learning speeds to unify the understanding of Grokking and double descent, and provides the first demonstration of model-wise Grokking.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#23545;&#27867;&#21270;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#21487;&#33021;&#38656;&#35201;&#23558;&#19981;&#21516;&#30340;&#35266;&#23519;&#32467;&#26524;&#32479;&#19968;&#21040;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#19979;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#8220;Grokking&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#21160;&#24577;&#65292;&#20854;&#20013;&#25345;&#32493;&#30340;&#36817;&#20046;&#23436;&#32654;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#36817;&#20046;&#20598;&#28982;&#30340;&#27979;&#35797;&#34920;&#29616;&#26368;&#32456;&#20250;&#23548;&#33268;&#27867;&#21270;&#65292;&#20197;&#21450;&#34920;&#38754;&#19978;&#31867;&#20284;&#30340;&#8220;&#21452;&#37325;&#19979;&#38477;&#8221;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#20027;&#39064;&#24050;&#32463;&#34987;&#23396;&#31435;&#22320;&#30740;&#31350;&#12290;&#25105;&#20204;&#20551;&#35774;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#20869;&#30456;&#21516;&#23398;&#20064;&#21160;&#24577;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#24403;&#25913;&#21464;&#27169;&#22411;&#23481;&#37327;&#32780;&#19981;&#26159;&#20248;&#21270;&#27493;&#39588;&#26102;&#65292;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06164</link><description>&lt;p&gt;
&#29702;&#35299;&#36136;&#37327;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning. (arXiv:2303.06164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Generalized Actor-Critic QD-RL framework for actor-critic deep RL methods in the QD-RL setting. The framework introduces two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ), which apply recent advancements in Deep RL to the QD-RL setting and solve the humanoid environment problem that existing QD-RL algorithms cannot solve.
&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#24050;&#32463;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#28151;&#21512;QD-RL&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#24102;&#26469;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20182;RL&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20808;&#21069;&#30340;&#28151;&#21512;&#26041;&#27861;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#20010;&#28145;&#24230;RL&#31639;&#27861;&#65288;TD3&#65289;&#12290;&#27492;&#22806;&#65292;QD&#21644;RL&#20043;&#38388;&#30340;&#20248;&#21270;&#36807;&#31243;&#23384;&#22312;&#26681;&#26412;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#26465;&#30740;&#31350;&#28145;&#24230;RL&#22312;QD-RL&#35774;&#32622;&#20013;&#30340;&#35265;&#35299;&#30340;&#36335;&#24452;&#65292;&#36825;&#26159;&#22312;QD-RL&#20013;&#21462;&#24471;&#36827;&#23637;&#30340;&#37325;&#35201;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06155</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36873;&#25321;&#20854;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29992;&#25143;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#24335;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#22312;&#20855;&#26377;&#36275;&#22815;&#35745;&#31639;&#36164;&#28304;&#30340;&#26381;&#21153;&#22120;&#19978;&#30340;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#27169;&#22411;&#33976;&#39311;&#26399;&#38388;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#22312;&#29289;&#29702;&#23454;&#20307;&#25110;&#25968;&#23383;&#20195;&#29702;&#22788;&#26356;&#26032;&#20854;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#21644;&#35757;&#32451;&#21368;&#36733;&#21644;&#36164;&#28304;&#20998;&#37197;&#21046;&#23450;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32852;&#21512;&#20351;&#29992;Q-learning&#21644;&#20248;&#21270;&#65292;&#20854;&#20013;Q-learning&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#24182;&#30830;&#23450;&#26159;&#22312;&#26412;&#22320;&#36824;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#20248;&#21270;&#21017;&#29992;&#20110;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06152</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#25110;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65311;&#8212;&#8212;&#23545;&#35937;&#21644;&#24037;&#20855;&#21151;&#33021;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#29992;&#20110;&#35774;&#35745;&#29702;&#35299;&#12289;&#25913;&#36827;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation. (arXiv:2303.06152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates how a particular object and its participation in the processes it is designed to support can be represented in a general function representational language and framework, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#21644;&#24037;&#20855;&#30340;&#21151;&#33021;&#26041;&#38754;&#30340;&#29702;&#35299;&#23545;&#20110;&#25903;&#25345;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#19982;&#21508;&#31181;&#23545;&#35937;&#12289;&#32467;&#26500;&#21644;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#20197;&#24110;&#21161;&#23454;&#29616;&#20854;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#21151;&#33021;&#30340;&#35814;&#32454;&#29702;&#35299;&#20063;&#21487;&#20197;&#23548;&#33268;&#35774;&#35745;&#25913;&#36827;&#21644;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25805;&#20316;&#65292;&#19968;&#26041;&#38754;&#65292;&#22686;&#24378;&#20154;&#31867;&#29983;&#27963;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#38149;&#65289;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#28856;&#36807;&#31243;&#65289;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#35814;&#32454;&#35828;&#26126;&#28041;&#21450;&#30340;&#36807;&#31243;&#21644;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#8212;&#8212;&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65292;&#25110;&#32773;&#20026;&#20160;&#20040;&#26576;&#20010;&#37096;&#20214;&#22312;&#29006;&#38149;&#20013;&#30340;&#20301;&#32622;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the functional aspects of objects and tools is of paramount importance in supporting an intelligent system in navigating around in the environment and interacting with various objects, structures, and systems, to help fulfil its goals. A detailed understanding of functionalities can also lead to design improvements and novel designs that would enhance the operations of AI and robotic systems on the one hand, and human lives on the other. This paper demonstrates how a particular object - in this case, a frying pan - and its participation in the processes it is designed to support - in this case, the frying process - can be represented in a general function representational language and framework, that can be used to flesh out the processes and functionalities involved, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions - why is something a good frying pan, say, or why a certain part on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseCAM&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#23618;&#24182;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.06151</link><description>&lt;p&gt;
NoiseCAM: &#29992;&#20110;&#22122;&#22768;&#21644;&#23545;&#25239;&#25915;&#20987;&#36793;&#30028;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseCAM&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#23618;&#24182;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a NoiseCAM algorithm that can locate vulnerable layers and detect adversarial examples, while not responding to Gaussian random noise mixed in the inputs.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#25915;&#20987;&#24456;&#23481;&#26131;&#35823;&#23548;&#31070;&#32463;&#32593;&#32476;&#24182;&#23548;&#33268;&#38169;&#35823;&#20915;&#31574;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#39640;&#24230;&#38656;&#35201;&#38450;&#24481;&#26426;&#21046;&#12290;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#26799;&#24230;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;GradCAM&#65289;&#20998;&#26512;VGG-16&#32593;&#32476;&#22312;&#20854;&#36755;&#20837;&#19982;&#23545;&#25239;&#25200;&#21160;&#25110;&#39640;&#26031;&#22122;&#22768;&#28151;&#21512;&#26102;&#30340;&#34892;&#20026;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23450;&#20301;&#23545;&#25239;&#25200;&#21160;&#21644;&#39640;&#26031;&#22122;&#22768;&#25935;&#24863;&#30340;&#26131;&#21463;&#25915;&#20987;&#23618;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#26131;&#21463;&#25915;&#20987;&#23618;&#30340;&#34892;&#20026;&#20559;&#24046;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NoiseCAM&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38598;&#25104;&#20102;&#20840;&#23616;&#21644;&#20687;&#32032;&#32423;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various domains. However, adversarial attacks can easily mislead a neural network and lead to wrong decisions. Defense mechanisms are highly preferred in safety-critical applications. In this paper, firstly, we use the gradient class activation map (GradCAM) to analyze the behavior deviation of the VGG-16 network when its inputs are mixed with adversarial perturbation or Gaussian noise. In particular, our method can locate vulnerable layers that are sensitive to adversarial perturbation and Gaussian noise. We also show that the behavior deviation of vulnerable layers can be used to detect adversarial examples. Secondly, we propose a novel NoiseCAM algorithm that integrates information from globally and pixel-level weighted class activation maps. Our algorithm is susceptible to adversarial perturbations and will not respond to Gaussian random noise mixed in the inputs. Third, we compare detecting adversarial examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05205</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#31995;&#32479;&#23454;&#26102;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Real-time scheduling of renewable power systems through planning-based reinforcement learning. (arXiv:2303.05205v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment, which enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#23545;&#20256;&#32479;&#30005;&#21147;&#35843;&#24230;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36816;&#33829;&#21830;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#26085;&#21069;&#39044;&#27979;&#65292;&#22240;&#27492;&#38656;&#35201;&#26410;&#26469;&#35843;&#24230;&#31995;&#32479;&#26681;&#25454;&#36229;&#30701;&#26399;&#39044;&#27979;&#36827;&#34892;&#23454;&#26102;&#35843;&#24230;&#20915;&#31574;&#12290;&#21463;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21457;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#22312;&#32422;&#26463;&#22797;&#26434;&#24615;&#12289;&#31639;&#27861;&#24615;&#33021;&#21644;&#29615;&#22659;&#20445;&#30495;&#24230;&#26041;&#38754;&#19981;&#36275;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#26426;&#32452;&#32452;&#21512;&#21644;&#32463;&#27982;&#35843;&#24230;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's abilit
&lt;/p&gt;</description></item><item><title>NASTyLinker&#26159;&#19968;&#31181;NIL&#24863;&#30693;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#26469;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#24182;&#22312;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#39640;&#38142;&#25509;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.04426</link><description>&lt;p&gt;
NASTyLinker&#65306;NIL&#24863;&#30693;&#21487;&#25193;&#23637;&#22522;&#20110;Transformer&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker. (arXiv:2303.04426v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04426
&lt;/p&gt;
&lt;p&gt;
NASTyLinker&#26159;&#19968;&#31181;NIL&#24863;&#30693;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#26469;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#24182;&#22312;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#39640;&#38142;&#25509;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
NASTyLinker is a NIL-aware entity linker that represents NIL entities by producing mention clusters and resolves conflicts while maintaining high linking performance for known entities.
&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#26816;&#27979;&#25991;&#26412;&#20013;&#23454;&#20307;&#25552;&#21450;&#24182;&#23558;&#20854;&#28040;&#27495;&#20026;&#21442;&#32771;&#30693;&#35782;&#24211;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;EL&#26041;&#27861;&#20551;&#23450;&#21442;&#32771;&#30693;&#35782;&#24211;&#26159;&#23436;&#25972;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#38656;&#35201;&#22788;&#29702;&#38142;&#25509;&#21040;&#19981;&#21253;&#21547;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#65288;NIL&#23454;&#20307;&#65289;&#30340;&#24773;&#20917;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32771;&#34385;&#25552;&#21450;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#12290;&#21516;&#26102;&#65292;&#25552;&#21450;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21487;&#20197;&#24110;&#21161;&#26174;&#33879;&#25552;&#39640;&#24050;&#30693;&#23454;&#20307;&#30340;&#38142;&#25509;&#24615;&#33021;&#12290;&#36890;&#36807;NASTyLinker&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;EL&#26041;&#27861;&#65292;&#23427;&#30693;&#36947;NIL&#23454;&#20307;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#25552;&#21450;&#31751;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#30340;&#39640;&#38142;&#25509;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#30340;&#23494;&#38598;&#34920;&#31034;&#23545;&#25552;&#21450;&#21644;&#23454;&#20307;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#35299;&#20915;&#20914;&#31361;&#65288;&#22914;&#26524;&#19968;&#20010;&#23454;&#20307;&#26377;&#22810;&#20010;&#25552;&#21450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is the task of detecting mentions of entities in text and disambiguating them to a reference knowledge base. Most prevalent EL approaches assume that the reference knowledge base is complete. In practice, however, it is necessary to deal with the case of linking to an entity that is not contained in the knowledge base (NIL entity). Recent works have shown that, instead of focusing only on affinities between mentions and entities, considering inter-mention affinities can be used to represent NIL entities by producing clusters of mentions. At the same time, inter-mention affinities can help to substantially improve linking performance for known entities. With NASTyLinker, we introduce an EL approach that is aware of NIL entities and produces corresponding mention clusters while maintaining high linking performance for known entities. The approach clusters mentions and entities based on dense representations from Transformers and resolves conflicts (if more than one en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02846</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#21644;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Contrastive Variational Information Bottleneck framework (CVIB) to reduce spurious correlations for aspect-based sentiment analysis (ABSA). The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning, which discards the superfluous patterns or spurious correlations between input features and prediction labels.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#25991;&#29486;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#22312;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#36825;&#20250;&#32473;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24102;&#26469;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;&#31216;&#20026;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;ABSA&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;CVIB&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;VIB&#65289;&#21407;&#21017;&#20174;&#21407;&#22987;&#32593;&#32476;&#20013;&#23398;&#20064;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#19988;&#21387;&#32553;&#30340;&#32593;&#32476;&#65288;&#33258;&#21098;&#26525;&#32593;&#32476;&#65289;&#65292;&#35813;&#32593;&#32476;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#21098;&#26525;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23558;&#20004;&#20010;&#32593;&#32476;&#25289;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have dominated the literature on aspect-based sentiment analysis (ABSA), yielding state-of-the-art results. However, these deep models generally suffer from spurious correlation problems between input features and output labels, which creates significant barriers to robustness and generalization capability. In this paper, we propose a novel Contrastive Variational Information Bottleneck framework (called CVIB) to reduce spurious correlations for ABSA. The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning. Concretely, we employ the Variational Information Bottleneck (VIB) principle to learn an informative and compressed network (self-pruned network) from the original network, which discards the superfluous patterns or spurious correlations between input features and prediction labels. Then, self-pruning contrastive learning is devised to pull together
&lt;/p&gt;</description></item><item><title>Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.02506</link><description>&lt;p&gt;
Prismer: &#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38598;&#21512;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02506
&lt;/p&gt;
&lt;p&gt;
Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#23427;&#20204;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Prismer&#65292;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#12290;Prismer&#21482;&#38656;&#35201;&#35757;&#32451;&#23569;&#37327;&#32452;&#20214;&#65292;&#22823;&#37096;&#20998;&#32593;&#32476;&#26435;&#37325;&#20174;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#39046;&#22495;&#19987;&#23478;&#20013;&#32487;&#25215;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20923;&#32467;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#21487;&#20197;&#26377;&#25928;&#22320;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/NVlabs/prismer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02045</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#20351;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#35777;&#25454;&#26469;&#21442;&#25968;&#21270;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;&#26126;&#30830;&#32771;&#34385;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#38169;&#35823;&#26631;&#35760;&#30340;&#31867;&#21035;&#30340;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#20250;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;$\mathcal{I}$-EDL&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#26679;&#26412;&#25152;&#25658;&#24102;&#30340;&#35777;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#26681;&#25454;&#36825;&#20010;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#21160;&#24577;&#22320;&#37325;&#26032;&#21152;&#26435;&#30446;&#26631;&#25439;&#22833;&#39033;&#65292;&#20351;&#32593;&#32476;&#26356;&#21152;&#19987;&#27880;&#20110;&#19981;&#30830;&#23450;&#31867;&#21035;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#20248;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2303.01508</link><description>&lt;p&gt;
&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#25511;&#21046;&#65306;&#23398;&#20064;&#25490;&#21517;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#24773;&#24863;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities. (arXiv:2303.01508v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#35821;&#38899;&#36890;&#24120;&#22312;&#24773;&#24863;&#34920;&#36798;&#19978;&#26159;&#20013;&#24615;&#30340;&#65292;&#32780;&#24456;&#22810;&#26102;&#20505;&#20154;&#20204;&#24076;&#26395;&#23545;&#21333;&#35789;&#25110;&#38899;&#32032;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#25511;&#21046;&#12290;&#34429;&#28982;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#26368;&#36817;&#24050;&#32463;&#25552;&#20986;&#20102;&#31532;&#19968;&#25209;TTS&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#25163;&#21160;&#20998;&#37197;&#24773;&#24863;&#24378;&#24230;&#26469;&#25511;&#21046;&#35821;&#38899;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#20869;&#37096;&#31867;&#36317;&#31163;&#65292;&#24378;&#24230;&#24046;&#24322;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21487;&#25511;&#24615;&#12289;&#24773;&#24863;&#34920;&#36798;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#21487;&#25511;TTS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art Text-To-Speech (TTS) models are capable of producing high-quality speech. The generated speech, however, is usually neutral in emotional expression, whereas very often one would want fine-grained emotional control of words or phonemes. Although still challenging, the first TTS models have been recently proposed that are able to control voice by manually assigning emotion intensity. Unfortunately, due to the neglect of intra-class distance, the intensity differences are often unrecognizable. In this paper, we propose a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference. Our subjective and objective experiments demonstrate that our model exceeds two state-of-the-art controllable TTS models for controllability, emotion expressiveness and naturalness.
&lt;/p&gt;</description></item><item><title>Mesh-SORT&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20301;&#32622;&#36319;&#36394;&#22120;&#65292;&#36890;&#36807;&#32593;&#26684;&#20998;&#21106;&#24103;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#20301;&#32622;&#24863;&#30693;&#30340;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#21305;&#37197;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36319;&#36394;&#20013;&#30340;&#19981;&#33391;&#26816;&#27979;&#22120;&#21644;&#36974;&#25377;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14415</link><description>&lt;p&gt;
Mesh-SORT: &#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20301;&#32622;&#36319;&#36394;&#22120;&#21450;&#20854;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Mesh-SORT: Simple and effective location-wise tracker with lost management strategies. (arXiv:2302.14415v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14415
&lt;/p&gt;
&lt;p&gt;
Mesh-SORT&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20301;&#32622;&#36319;&#36394;&#22120;&#65292;&#36890;&#36807;&#32593;&#26684;&#20998;&#21106;&#24103;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#20301;&#32622;&#24863;&#30693;&#30340;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#21305;&#37197;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36319;&#36394;&#20013;&#30340;&#19981;&#33391;&#26816;&#27979;&#22120;&#21644;&#36974;&#25377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mesh-SORT is a simple and effective location-wise tracker that solves the problem of bad detectors and occlusions in tracking by dividing frames into grids and proposing corresponding location-aware loss management strategies and different matching strategies.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;(MOT)&#30001;&#20110;&#20854;&#22312;&#20132;&#36890;&#21644;&#34892;&#20154;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36890;&#36807;&#26816;&#27979;&#36827;&#34892;&#36319;&#36394;&#21487;&#33021;&#20250;&#21463;&#21040;&#22122;&#22768;&#26816;&#27979;&#22120;&#20135;&#29983;&#30340;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22312;&#36974;&#25377;&#20043;&#21069;&#30340;&#19981;&#31934;&#30830;&#36793;&#30028;&#26694;&#65292;&#32780;&#19988;&#22312;&#22823;&#22810;&#25968;&#36319;&#36394;&#22330;&#26223;&#20013;&#65292;&#29289;&#20307;&#24448;&#24448;&#20250;&#22312;&#29305;&#23450;&#20301;&#32622;&#31227;&#21160;&#21644;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36319;&#36394;&#22120;&#26469;&#22788;&#29702;&#19981;&#33391;&#26816;&#27979;&#22120;&#21644;&#36974;&#25377;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#24863;&#30693;&#30340;&#23376;&#21306;&#22495;&#35782;&#21035;&#26041;&#27861;&#65292;&#23558;&#24103;&#31561;&#20998;&#20026;&#32593;&#26684;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20301;&#32622;&#24863;&#30693;&#30340;&#20002;&#22833;&#31649;&#29702;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#21305;&#37197;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;Mesh-SORT&#30340;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;MOT17&#25968;&#25454;&#38598;&#19978;&#30340;3%&#30862;&#29255;&#21270;&#12289;7.2% ID&#20999;&#25442;&#19979;&#38477;&#21644;0.4% MOTA&#25913;&#36827;&#19982;&#22522;&#32447;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Tracking (MOT) has gained extensive attention in recent years due to its potential applications in traffic and pedestrian detection. We note that tracking by detection may suffer from errors generated by noise detectors, such as an imprecise bounding box before the occlusions, and observed that in most tracking scenarios, objects tend to move and lost within specific locations. To counter this, we present a novel tracker to deal with the bad detector and occlusions. Firstly, we proposed a location-wise sub-region recognition method which equally divided the frame, which we called mesh. Then we proposed corresponding location-wise loss management strategies and different matching strategies. The resulting Mesh-SORT, ablation studies demonstrate its effectiveness and made 3% fragmentation 7.2% ID switches drop and 0.4% MOTA improvement compared to the baseline on MOT17 datasets. Finally, we analyze its limitation on the specific scene and discussed what future works can be e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#25105;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#35838;&#31243;&#20266;&#26631;&#31614;&#29992;&#20110;&#34920;&#26684;&#39046;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23427;&#35268;&#33539;&#21270;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.14013</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#27491;&#21017;&#21270;&#20266;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular Data. (arXiv:2302.14013v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#25105;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#35838;&#31243;&#20266;&#26631;&#31614;&#29992;&#20110;&#34920;&#26684;&#39046;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23427;&#35268;&#33539;&#21270;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits self-training and introduces curriculum pseudo-labeling for tabular data. A novel pseudo-labeling approach is proposed to regularize the confidence scores of pseudo-labels generated from unlabeled data.
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25171;&#30772;&#20102;&#38271;&#26399;&#20197;&#26469;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#26080;&#20851;&#24615;&#30340;&#20449;&#20208;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#25968;&#25454;&#19978;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#20027;&#23548;&#30340;&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#34920;&#26684;&#25968;&#25454;&#65288;&#21363;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#36866;&#24403;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#25105;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26550;&#26500;&#8212;&#8212;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24182;&#24341;&#20837;&#20102;&#35838;&#31243;&#20266;&#26631;&#31614;&#65288;&#19968;&#31181;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;&#65289;&#29992;&#20110;&#34920;&#26684;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#35745;&#31639;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26102;&#19981;&#33021;&#20445;&#35777;&#32858;&#31867;&#20551;&#35774;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23427;&#35268;&#33539;&#21270;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in semi- and self-supervised learning has caused a rift in the long-held belief about the need for an enormous amount of labeled data for machine learning and the irrelevancy of unlabeled data. Although it has been successful in various data, there is no dominant semi- and self-supervised learning method that can be generalized for tabular data (i.e. most of the existing methods require appropriate tabular datasets and architectures). In this paper, we revisit self-training which can be applied to any kind of algorithm including the most widely used architecture, gradient boosting decision tree, and introduce curriculum pseudo-labeling (a state-of-the-art pseudo-labeling technique in image) for a tabular domain. Furthermore, existing pseudo-labeling techniques do not assure the cluster assumption when computing confidence scores of pseudo-labels generated from unlabeled data. To overcome this issue, we propose a novel pseudo-labeling approach that regularizes the confid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#20174;&#24403;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#24335;&#36716;&#21464;&#65292;&#37319;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2302.12389</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24050;&#27515;&#65292;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#19975;&#23681;&#65281;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support. (arXiv:2302.12389v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#20174;&#24403;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#24335;&#36716;&#21464;&#65292;&#37319;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper argues for a paradigm shift from the current model of explainable artificial intelligence (XAI) to hypothesis-driven decision support systems to better support human decision making.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#20174;&#24403;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#24335;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#22952;&#30861;&#26356;&#22909;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#20204;&#19981;&#24635;&#26159;&#20250;&#25509;&#21463;&#21644;&#36981;&#24490;&#24314;&#35758;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#20449;&#20219;&#23427;&#20204;&#65292;&#25110;&#32773;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#21363;&#20351;&#24314;&#35758;&#26159;&#38169;&#35823;&#30340;&#65292;&#20154;&#20204;&#20063;&#20250;&#30450;&#30446;&#22320;&#36981;&#24490;&#23427;&#20204;&#12290;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#32473;&#20986;&#26576;&#20123;&#24314;&#35758;&#26469;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20204;&#24182;&#19981;&#24635;&#26159;&#36275;&#22815;&#21442;&#19982;&#35299;&#37322;&#24037;&#20855;&#20197;&#24110;&#21161;&#25913;&#21892;&#20915;&#31574;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#22240;&#20026;&#25105;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#20004;&#20214;&#20107;&#24773;&#12290;&#39318;&#20808;&#65292;&#24314;&#35758;&#65288;&#21450;&#20854;&#35299;&#37322;&#65289;&#21487;&#33021;&#19982;&#20154;&#20204;&#30340;&#20551;&#35774;&#21644;&#20449;&#20208;&#30456;&#20914;&#31361;&#12290;&#20854;&#27425;&#65292;&#20154;&#20204;&#30340;&#20915;&#31574;&#24448;&#24448;&#26159;&#22522;&#20110;&#20551;&#35774;&#30340;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20107;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their expla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.08888</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31227;&#21160;&#31995;&#32479;&#21644;&#29289;&#32852;&#32593;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#22810;&#23186;&#20307;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#21033;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#32780;&#19981;&#36829;&#21453;&#29992;&#25143;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38544;&#31169;&#24847;&#35782;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;FL&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#32423;&#21035;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#36825;&#38480;&#21046;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#27169;&#24577;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#23481;&#37327;&#65292;&#26356;&#19981;&#29992;&#35828;&#20219;&#21153;&#22810;&#26679;&#24615;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;&#21644;&#22810;&#27169;&#24577;FL&#32858;&#21512;&#65288;CreamFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#34701;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;-
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20843;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#37197;&#22791;&#22270;&#20687;&#30340;&#23454;&#20307;&#23545;&#40784;&#22522;&#20934;Multi-OpenEA&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;EA&#26041;&#27861;LODEME&#65292;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;KG&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08774</link><description>&lt;p&gt;
&#35270;&#35273;&#12289;&#25512;&#29702;&#21644;&#23545;&#40784;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment. (arXiv:2302.08774v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20843;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#37197;&#22791;&#22270;&#20687;&#30340;&#23454;&#20307;&#23545;&#40784;&#22522;&#20934;Multi-OpenEA&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;EA&#26041;&#27861;LODEME&#65292;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;KG&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study constructed eight large-scale, image-equipped entity alignment benchmarks named Multi-OpenEA, and developed a new multi-modal EA method named LODEME, which utilizes logical deduction and multi-modal KG embedding, achieving state-of-the-art performance on Multi-OpenEA and other existing multi-modal EA benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#23545;&#40784;&#22312;&#30693;&#35782;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#21644;&#23454;&#20307;&#23646;&#24615;&#65288;&#21253;&#25324;&#25991;&#23383;&#65289;&#65292;&#20294;&#24573;&#30053;&#20102;&#29616;&#20195;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#24120;&#35265;&#30340;&#22270;&#20687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;Multi-OpenEA&#8212;&#8212;&#20843;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#37197;&#22791;&#22270;&#20687;&#30340;&#23454;&#20307;&#23545;&#40784;&#22522;&#20934;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#22270;&#20687;&#12290;&#37492;&#20110;&#35270;&#35273;&#27169;&#24577;&#20449;&#24687;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;EA&#26041;&#27861;&#65292;&#21517;&#20026;LODEME&#65292;&#20351;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;KG&#23884;&#20837;&#65292;&#22312;Multi-OpenEA&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;EA&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in knowledge engineering. Existing EA methods mostly focus on utilizing the graph structures and entity attributes (including literals), but ignore images that are common in modern multi-modal KGs. In this study we first constructed Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then evaluated some existing embedding-based methods for utilizing images. In view of the complementary nature of visual modal information and logical deduction, we further developed a new multi-modal EA method named LODEME using logical deduction and multi-modal KG embedding, with state-of-the-art performance achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.06403</link><description>&lt;p&gt;
&#29616;&#35937;&#24847;&#35782;&#29366;&#24577;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Sources of Richness and Ineffability for Phenomenally Conscious States. (arXiv:2302.06403v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In their framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing.
&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#29366;&#24577;&#65288;&#21363;&#26377;&#26576;&#31181;&#24863;&#21463;&#30340;&#29366;&#24577;&#65289;&#20284;&#20046;&#26082;&#20016;&#23500;&#21448;&#20805;&#28385;&#32454;&#33410;&#65292;&#21448;&#38590;&#20197;&#23436;&#20840;&#25551;&#36848;&#25110;&#22238;&#24518;&#12290;&#29305;&#21035;&#26159;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#38382;&#39064;&#26159;&#21746;&#23398;&#19978;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#28608;&#21457;&#20102;&#35299;&#37322;&#40511;&#27807;&#30340;&#20449;&#24565;&#65306;&#24847;&#35782;&#19981;&#33021;&#24402;&#32467;&#20026;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20316;&#35760;&#24518;&#20013;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#22914;&#20309;&#23548;&#33268;&#25105;&#20204;&#21407;&#22987;&#20307;&#39564;&#30340;&#36139;&#20047;&#22238;&#24518;&#65292;&#35821;&#35328;&#30340;&#31163;&#25955;&#31526;&#21495;&#24615;&#36136;&#19981;&#36275;&#20197;&#25551;&#36848;&#20307;&#39564;&#30340;&#20016;&#23500;&#21644;&#39640;&#32500;&#32467;&#26500;&#65292;&#20197;&#21450;&#35748;&#30693;&#21151;&#33021;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#20307;&#39564;&#30340;&#20849;&#20139;&#21644;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conscious states (states that there is something it is like to be in) seem both rich or full of detail, and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.05045</link><description>&lt;p&gt;
&#21033;&#29992;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20248;&#21270;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#24320;&#38144;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#35268;&#27169;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21098;&#26525;&#31639;&#27861;&#65292;&#33021;&#22815;&#21098;&#26525;&#65288;&#21363;&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65289;80-90&#65285;&#30340;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#19982;&#26410;&#21098;&#26525;&#29238;&#32593;&#32476;&#30456;&#31561;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;AxoNN&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#23618;&#38388;&#24182;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#26102;&#38388;&#21644;&#20869;&#23384;&#21033;&#29992;&#30340;&#20943;&#23569;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#23450;&#20041;&#36807;&#20110;&#31616;&#21333;&#65292;&#35768;&#22810;&#20844;&#24179;&#24615;&#25514;&#26045;&#20250;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#21644;&#27700;&#24179;&#19979;&#38477;&#65292;&#20351;&#27599;&#20010;&#20154;&#37117;&#21464;&#24471;&#26356;&#31967;&#65292;&#36825;&#31181;&#20570;&#27861;&#19981;&#31526;&#21512;&#23454;&#36136;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2302.02404</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#20844;&#24179;&#24615;&#65306;&#40664;&#35748;&#30340;&#27700;&#24179;&#19979;&#38477;&#21644;&#20005;&#26684;&#24179;&#31561;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default. (arXiv:2302.02404v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02404
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#23450;&#20041;&#36807;&#20110;&#31616;&#21333;&#65292;&#35768;&#22810;&#20844;&#24179;&#24615;&#25514;&#26045;&#20250;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#21644;&#27700;&#24179;&#19979;&#38477;&#65292;&#20351;&#27599;&#20010;&#20154;&#37117;&#21464;&#24471;&#26356;&#31967;&#65292;&#36825;&#31181;&#20570;&#27861;&#19981;&#31526;&#21512;&#23454;&#36136;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
The oversimplification of fairness in machine learning leads to performance degradation and leveling down, which does not achieve substantive equality.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#20154;&#31616;&#21333;&#22320;&#23450;&#20041;&#20844;&#24179;&#24615;&#65292;&#21363;&#20844;&#24179;&#24847;&#21619;&#30528;&#20943;&#23569;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#34920;&#29616;&#25110;&#32467;&#26524;&#24046;&#36317;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#36890;&#36807;&#20844;&#24179;&#24615;&#25514;&#26045;&#31616;&#21270;&#24179;&#31561;&#30340;&#20570;&#27861;&#20196;&#20154;&#19981;&#23433;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#25514;&#26045;&#26082;&#23384;&#22312;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#38477;&#32423;&#65292;&#25110;&#32773;&#8220;&#27700;&#24179;&#19979;&#38477;&#8221;&#65292;&#21363;&#36890;&#36807;&#20351;&#27599;&#20010;&#32676;&#20307;&#21464;&#24471;&#26356;&#31967;&#25110;&#23558;&#34920;&#29616;&#26356;&#22909;&#30340;&#32676;&#20307;&#38477;&#21040;&#26368;&#24046;&#30340;&#27700;&#24179;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#24403;&#20844;&#24179;&#21482;&#33021;&#36890;&#36807;&#22312;&#29289;&#36136;&#25110;&#20851;&#31995;&#26041;&#38754;&#20351;&#27599;&#20010;&#20154;&#21464;&#24471;&#26356;&#31967;&#32780;&#23454;&#29616;&#65292;&#36890;&#36807;&#27745;&#21517;&#21270;&#12289;&#22242;&#32467;&#30340;&#25439;&#22833;&#12289;&#19981;&#24179;&#31561;&#30340;&#20851;&#27880;&#21644;&#38169;&#36807;&#23454;&#36136;&#24179;&#31561;&#30340;&#26426;&#20250;&#65292;&#20284;&#20046;&#22312;&#23558;&#27169;&#31946;&#30340;&#8220;&#20844;&#24179;&#8221;&#27010;&#24565;&#36716;&#21270;&#20026;&#23454;&#36341;&#26102;&#20986;&#29616;&#20102;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#31181;&#29616;&#35937;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years fairness in machine learning (ML) has emerged as a highly active area of research and development. Most define fairness in simple terms, where fairness means reducing gaps in performance or outcomes between demographic groups while preserving as much of the accuracy of the original system as possible. This oversimplification of equality through fairness measures is troubling. Many current fairness measures suffer from both fairness and performance degradation, or "levelling down," where fairness is achieved by making every group worse off, or by bringing better performing groups down to the level of the worst off. When fairness can only be achieved by making everyone worse off in material or relational terms through injuries of stigma, loss of solidarity, unequal concern, and missed opportunities for substantive equality, something would appear to have gone wrong in translating the vague concept of 'fairness' into practice. This paper examines the causes and prevalence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12901</link><description>&lt;p&gt;
&#27169;&#25311;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.
&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#32531;&#35299;&#25317;&#22581;&#65292;&#25552;&#39640;&#21487;&#36798;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;UAM&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#23545;&#20132;&#36890;&#27969;&#37327;&#21644;&#23481;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20351;&#29992;&#27169;&#25311;&#25216;&#26415;&#35843;&#26597;&#20102;UAM&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#21487;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09245</link><description>&lt;p&gt;
&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#65306;&#23558;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks. (arXiv:2301.09245v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09245
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#21487;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing neuronal diversity can solve the fundamental problems of artificial neural networks and lead to NeuroAI.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#21382;&#21490;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#30452;&#23545;&#36234;&#26469;&#36234;&#28145;&#20837;&#30340;&#22823;&#33041;&#29702;&#35299;&#25345;&#24320;&#25918;&#24577;&#24230;&#24182;&#19981;&#26029;&#21463;&#21040;&#21551;&#21457;&#65292;&#20363;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;neocognitron&#30340;&#21551;&#21457;&#12290;&#26681;&#25454;&#26032;&#20852;&#39046;&#22495;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#30340;&#21160;&#26426;&#65292;&#22823;&#37327;&#30340;&#31070;&#32463;&#31185;&#23398;&#30693;&#35782;&#21487;&#20197;&#36890;&#36807;&#36171;&#20104;&#32593;&#32476;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#20652;&#21270;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30693;&#36947;&#65292;&#20154;&#31867;&#22823;&#33041;&#26377;&#35768;&#22810;&#24418;&#24577;&#21644;&#21151;&#33021;&#19981;&#21516;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#23436;&#20840;&#24314;&#31435;&#22312;&#21333;&#19968;&#31070;&#32463;&#20803;&#31867;&#22411;&#19978;&#12290;&#22312;&#20154;&#31867;&#22823;&#33041;&#20013;&#65292;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#26159;&#21508;&#31181;&#29983;&#29289;&#26234;&#33021;&#34892;&#20026;&#30340;&#19968;&#20010;&#21551;&#21160;&#22240;&#32032;&#12290;&#30001;&#20110;&#20154;&#24037;&#32593;&#32476;&#26159;&#20154;&#31867;&#22823;&#33041;&#30340;&#32553;&#24433;&#65292;&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24212;&#35813;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#24037;&#32593;&#32476;&#30340;&#35832;&#22914;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#31561;&#22522;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpret
&lt;/p&gt;</description></item><item><title>SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;</title><link>http://arxiv.org/abs/2301.07074</link><description>&lt;p&gt;
SegViz&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07074
&lt;/p&gt;
&lt;p&gt;
SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22810;&#20010;&#19979;&#28216;&#20020;&#24202;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#25163;&#21160;&#27880;&#37322;&#26159;&#32791;&#26102;&#30340;&#12289;&#38656;&#35201;&#39640;&#25216;&#33021;&#30340;&#12289;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;3D&#22270;&#20687;&#12290;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#22810;&#20010;&#32452;&#30340;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#30693;&#35782;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegViz&#65292;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#23558;SegViz&#30340;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#38598;&#20013;&#32858;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#21644;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02830</link><description>&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Image Data Augmentation Approaches: A Comprehensive Survey and Future directions. (arXiv:2301.02830v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#21644;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive survey of advanced data augmentation techniques for computer vision tasks, including a novel taxonomy and evaluation of each technique's strengths and weaknesses. The article also presents comprehensive results of the data augmentation effect on popular computer vision tasks such as image classification, object detection, and semantic segmentation.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#32593;&#32476;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#32593;&#32476;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#27604;&#35757;&#32451;&#25968;&#25454;&#24046;&#12290;&#22240;&#27492;&#65292;&#23427;&#38480;&#21046;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;dropout&#12289;&#24402;&#19968;&#21270;&#21644;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#12290;&#20854;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21253;&#25324;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#25193;&#22823;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36817;&#26469;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#32972;&#26223;&#12289;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#12289;&#20197;&#21450;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#19977;&#20010;&#27969;&#34892;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65289;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) algorithms have shown significant performance in various computer vision tasks. However, having limited labelled data lead to a network overfitting problem, where network performance is bad on unseen data as compared to training data. Consequently, it limits performance improvement. To cope with this problem, various techniques have been proposed such as dropout, normalization and advanced data augmentation. Among these, data augmentation, which aims to enlarge the dataset size by including sample diversity, has been a hot topic in recent times. In this article, we focus on advanced data augmentation techniques. we provide a background of data augmentation, a novel and comprehensive taxonomy of reviewed data augmentation techniques, and the strengths and weaknesses (wherever possible) of each technique. We also provide comprehensive results of the data augmentation effect on three popular computer vision tasks, such as image classification, object detection and seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.08966</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#21450;&#20854;&#24212;&#29992;&#65306;&#19968;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Learning and Its Applications: A Holistic Survey. (arXiv:2212.08966v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey of the development and application scenarios of graph learning, with a focus on the remarkable performance of representation learning in various fields such as text, image, chemistry, and biology. It also points out the need to investigate previous valuable works.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#20851;&#31995;&#20351;&#24471;&#22270;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#27604;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#22240;&#20026;&#33410;&#28857;&#20381;&#36182;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#21487;&#20379;&#21033;&#29992;&#12290;&#38543;&#30528;&#34920;&#31034;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#22270;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#22270;&#23398;&#20064;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#24037;&#20316;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#22312;&#22270;&#23398;&#20064;&#26041;&#38754;&#23436;&#25104;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35843;&#26597;&#65292;&#20294;&#20182;&#20204;&#26410;&#33021;&#20197;&#26356;&#36830;&#36143;&#30340;&#26041;&#24335;&#36830;&#25509;&#30456;&#20851;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. These relationships endow graphs with uniqueness compared to conventional tabular data, as nodes rely on non-Euclidean space and encompass rich information to exploit. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios, including text, image, chemistry, and biology. Owing to its extensive application prospects, graph learning attracts copious attention from the academic community. Despite numerous works proposed to tackle different problems in graph learning, there is a demand to survey previous valuable works. While some researchers have perceived this phenomenon and accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#65292;&#36890;&#36807;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.03640</link><description>&lt;p&gt;
&#32454;&#35843;CLIP&#27169;&#22411;&#26159;&#39640;&#25928;&#30340;&#35270;&#39057;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned CLIP Models are Efficient Video Learners. (arXiv:2212.03640v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#65292;&#36890;&#36807;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline, which effectively transfers image-level CLIP representations to videos by frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings, thus bridging the domain gap from images to videos.
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#65292;CLIP&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30001;&#20110;&#22312;&#31867;&#20284;&#35268;&#27169;&#19978;&#23545;&#35270;&#39057;&#36827;&#34892;&#35757;&#32451;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;CLIP&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#35270;&#39057;&#39046;&#22495;&#12290;&#22312;&#36825;&#20010;&#36861;&#27714;&#20013;&#65292;&#28155;&#21152;&#20102;&#26032;&#30340;&#21442;&#25968;&#27169;&#22359;&#26469;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#21644;&#24103;&#38388;&#20851;&#31995;&#65292;&#36825;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#24403;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35270;&#39057;&#19978;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#20219;&#21153;&#20998;&#24067;&#65292;&#24182;&#19988;&#32570;&#20047;&#27867;&#21270;&#26041;&#38754;&#12290;&#36825;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#32423;&#21035;&#30340;CLIP&#34920;&#31034;&#36716;&#31227;&#21040;&#35270;&#39057;&#20013;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#39057;&#32454;&#35843;CLIP&#65288;ViFi-CLIP&#65289;&#22522;&#32447;&#36890;&#24120;&#36275;&#20197;&#24357;&#21512;&#20174;&#22270;&#20687;&#21040;&#35270;&#39057;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#20174;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24103;&#32423;&#22788;&#29702;&#65292;&#25509;&#30528;&#36827;&#34892;&#29305;&#24449;&#27744;&#21270;&#21644;&#19982;&#30456;&#24212;&#25991;&#26412;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overfit on the given task distribution and lack in generalization aspect. This begs the following question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings helps in imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.02623</link><description>&lt;p&gt;
&#32479;&#19968;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the Universal Document Processing (UDOP) model, which unifies text, image, and layout modalities together with varied task formats, and achieves pretraining and multi-domain downstream tasks unification through a novel Transformer model. It also achieves high-quality neural document editing and content customization.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#25991;&#26723;AI&#27169;&#22411;&#65292;&#23427;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#65288;&#21253;&#25324;&#25991;&#26723;&#29702;&#35299;&#21644;&#29983;&#25104;&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;UDOP&#21033;&#29992;&#25991;&#26412;&#20869;&#23481;&#21644;&#25991;&#26723;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Vision-Text-Layout Transformer&#65292;UDOP&#23558;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#32479;&#19968;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#24207;&#21015;&#29983;&#25104;&#26041;&#26696;&#20013;&#12290;UDOP&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#22810;&#26679;&#21270;&#26631;&#35760;&#25968;&#25454;&#19978;&#20351;&#29992;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;UDOP&#36824;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#37325;&#24314;&#23398;&#20064;&#20174;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#29983;&#25104;&#25991;&#26723;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#25991;&#26723;AI&#39046;&#22495;&#20013;&#31532;&#19968;&#27425;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;8&#20010;&#25991;&#26723;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Docu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24863;&#21463;&#37326;&#25193;&#22823;&#26102;&#20002;&#22833;&#23616;&#37096;&#32454;&#33410;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#21512;&#29702;&#30340;&#22721;&#30011;&#36718;&#24275;&#22270;&#12290;</title><link>http://arxiv.org/abs/2212.00935</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dunhuang murals contour generation network based on convolution and self-attention fusion. (arXiv:2212.00935v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24863;&#21463;&#37326;&#25193;&#22823;&#26102;&#20002;&#22833;&#23616;&#37096;&#32454;&#33410;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#21512;&#29702;&#30340;&#22721;&#30011;&#36718;&#24275;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Dunhuang murals contour generation network based on convolution and self-attention fusion, aiming to solve the problem of losing local detail information when the receptive field is enlarged in traditional convolutional neural networks, in order to generate reasonable mural contour drawings.
&lt;/p&gt;
&lt;p&gt;
&#25958;&#29004;&#22721;&#30011;&#26159;&#20013;&#22269;&#39118;&#26684;&#21644;&#27665;&#26063;&#39118;&#26684;&#30340;&#38598;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#20013;&#22269;&#24335;&#20315;&#25945;&#33402;&#26415;&#12290;&#23427;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#21382;&#21490;&#21644;&#25991;&#21270;&#20215;&#20540;&#20197;&#21450;&#30740;&#31350;&#24847;&#20041;&#12290;&#20854;&#20013;&#65292;&#25958;&#29004;&#22721;&#30011;&#30340;&#32447;&#26465;&#39640;&#24230;&#27010;&#25324;&#21644;&#34920;&#29616;&#21147;&#24378;&#65292;&#21453;&#26144;&#20102;&#35282;&#33394;&#29420;&#29305;&#30340;&#24615;&#26684;&#21644;&#22797;&#26434;&#30340;&#20869;&#24515;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#22721;&#30011;&#30340;&#36718;&#24275;&#22270;&#23545;&#20110;&#25958;&#29004;&#25991;&#21270;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25958;&#29004;&#22721;&#30011;&#30340;&#36718;&#24275;&#29983;&#25104;&#23646;&#20110;&#22270;&#20687;&#36793;&#32536;&#26816;&#27979;&#65292;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#37325;&#35201;&#20998;&#25903;&#65292;&#26088;&#22312;&#25552;&#21462;&#22270;&#20687;&#20013;&#26174;&#33879;&#30340;&#36718;&#24275;&#20449;&#24687;&#12290;&#34429;&#28982;&#22522;&#20110;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#36890;&#36807;&#25506;&#32034;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#29305;&#24449;&#22312;&#22270;&#20687;&#36793;&#32536;&#25552;&#21462;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#38543;&#30528;&#24863;&#21463;&#37326;&#30340;&#25193;&#22823;&#65292;&#19968;&#20123;&#23616;&#37096;&#32454;&#33410;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#29983;&#25104;&#21512;&#29702;&#30340;&#22721;&#30011;&#36718;&#24275;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#25958;&#29004;&#22721;&#30011;&#36718;&#24275;&#29983;&#25104;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dunhuang murals are a collection of Chinese style and national style, forming a self-contained Chinese-style Buddhist art. It has very high historical and cultural value and research significance. Among them, the lines of Dunhuang murals are highly general and expressive. It reflects the character's distinctive character and complex inner emotions. Therefore, the outline drawing of murals is of great significance to the research of Dunhuang Culture. The contour generation of Dunhuang murals belongs to image edge detection, which is an important branch of computer vision, aims to extract salient contour information in images. Although convolution-based deep learning networks have achieved good results in image edge extraction by exploring the contextual and semantic features of images. However, with the enlargement of the receptive field, some local detail information is lost. This makes it impossible for them to generate reasonable outline drawings of murals. In this paper, we propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#20998;&#31867;&#27169;&#22359;&#20316;&#20026;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#26032;&#38454;&#27573;&#28155;&#21152;&#65292;&#29992;&#20110;&#25913;&#21892;&#25163;&#26415;&#22120;&#26800;&#23454;&#20363;&#20998;&#21106;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#21253;&#25324;&#22810;&#23610;&#24230;&#25513;&#27169;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20851;&#27880;&#22120;&#26800;&#21306;&#22495;&#24182;&#25513;&#30422;&#20998;&#25955;&#30340;&#32972;&#26223;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2211.16200</link><description>&lt;p&gt;
&#20174;&#21449;&#23376;&#21040;&#38067;&#23376;&#65306;&#19968;&#31181;&#26032;&#30340;&#25163;&#26415;&#22120;&#26800;&#23454;&#20363;&#20998;&#21106;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments. (arXiv:2211.16200v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#20998;&#31867;&#27169;&#22359;&#20316;&#20026;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#26032;&#38454;&#27573;&#28155;&#21152;&#65292;&#29992;&#20110;&#25913;&#21892;&#25163;&#26415;&#22120;&#26800;&#23454;&#20363;&#20998;&#21106;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#21253;&#25324;&#22810;&#23610;&#24230;&#25513;&#27169;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20851;&#27880;&#22120;&#26800;&#21306;&#22495;&#24182;&#25513;&#30422;&#20998;&#25955;&#30340;&#32972;&#26223;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#21019;&#25163;&#26415;&#21644;&#30456;&#20851;&#24212;&#29992;&#38656;&#35201;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#23545;&#25163;&#26415;&#24037;&#20855;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#25163;&#26415;&#24037;&#20855;&#22312;&#22806;&#35266;&#19978;&#30456;&#20284;&#65292;&#38271;&#32780;&#32454;&#65292;&#19988;&#20197;&#35282;&#24230;&#22788;&#29702;&#12290;&#23558;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#24494;&#35843;&#29992;&#20110;&#22120;&#26800;&#20998;&#21106;&#26102;&#65292;&#24448;&#24448;&#38590;&#20197;&#21306;&#20998;&#22120;&#26800;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#25513;&#27169;&#36890;&#24120;&#20934;&#30830;&#65292;&#20294;&#20998;&#31867;&#22836;&#35823;&#20998;&#31867;&#20102;&#25163;&#26415;&#22120;&#26800;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#20998;&#31867;&#27169;&#22359;&#20316;&#20026;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#26032;&#38454;&#27573;&#28155;&#21152;&#12290;&#35813;&#27169;&#22359;&#19987;&#38376;&#29992;&#20110;&#25913;&#21892;&#29616;&#26377;&#27169;&#22411;&#29983;&#25104;&#30340;&#22120;&#26800;&#25513;&#27169;&#30340;&#20998;&#31867;&#12290;&#35813;&#27169;&#22359;&#21253;&#25324;&#22810;&#23610;&#24230;&#25513;&#27169;&#27880;&#24847;&#21147;&#65292;&#35813;&#27880;&#24847;&#21147;&#20851;&#27880;&#22120;&#26800;&#21306;&#22495;&#24182;&#25513;&#30422;&#20998;&#25955;&#30340;&#32972;&#26223;&#29305;&#24449;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24230;&#37327;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head mis-classifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training our classifier module using metr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;VideoFACT&#65292;&#23427;&#21033;&#29992;&#21462;&#35777;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#23884;&#20837;&#21644;&#28145;&#24230;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#21508;&#31181;&#35270;&#39057;&#20266;&#36896;&#21644;&#25805;&#32437;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#22312;&#20998;&#26512;&#35270;&#39057;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.15775</link><description>&lt;p&gt;
VideoFACT: &#20351;&#29992;&#27880;&#24847;&#21147;&#12289;&#22330;&#26223;&#19978;&#19979;&#25991;&#21644;&#21462;&#35777;&#30165;&#36857;&#26816;&#27979;&#35270;&#39057;&#20266;&#36896;
&lt;/p&gt;
&lt;p&gt;
VideoFACT: Detecting Video Forgeries Using Attention, Scene Context, and Forensic Traces. (arXiv:2211.15775v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;VideoFACT&#65292;&#23427;&#21033;&#29992;&#21462;&#35777;&#23884;&#20837;&#12289;&#19978;&#19979;&#25991;&#23884;&#20837;&#21644;&#28145;&#24230;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#21508;&#31181;&#35270;&#39057;&#20266;&#36896;&#21644;&#25805;&#32437;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#22312;&#20998;&#26512;&#35270;&#39057;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new network, VideoFACT, which utilizes forensic embeddings, context embeddings, and a deep self-attention mechanism to detect and localize a wide variety of video forgeries and manipulations, overcoming challenges faced by existing networks when analyzing videos.
&lt;/p&gt;
&lt;p&gt;
&#20551;&#35270;&#39057;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#35823;&#23548;&#23041;&#32961;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#21462;&#35777;&#32593;&#32476;&#24050;&#32463;&#22312;&#22270;&#20687;&#20266;&#36896;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#22312;Adobe VideoSham&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#32593;&#32476;&#26080;&#27861;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#34394;&#20551;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26159;&#30001;&#20110;&#35270;&#39057;&#32534;&#30721;&#24341;&#20837;&#20102;&#21462;&#35777;&#30165;&#36857;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#29616;&#26377;&#32593;&#32476;&#22312;&#20998;&#26512;&#35270;&#39057;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#21033;&#29992;&#21462;&#35777;&#23884;&#20837;&#26469;&#25429;&#25417;&#25805;&#32437;&#30041;&#19979;&#30340;&#30165;&#36857;&#65292;&#19978;&#19979;&#25991;&#23884;&#20837;&#26469;&#25511;&#21046;&#35270;&#39057;&#32534;&#30721;&#24341;&#20837;&#30340;&#21462;&#35777;&#30165;&#36857;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#28145;&#24230;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#20272;&#35745;&#23616;&#37096;&#21462;&#35777;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#20960;&#20010;&#26032;&#30340;&#35270;&#39057;&#20266;&#36896;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#20197;&#21450;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake videos represent an important misinformation threat. While existing forensic networks have demonstrated strong performance on image forgeries, recent results reported on the Adobe VideoSham dataset show that these networks fail to identify fake content in videos. In this paper, we show that this is due to video coding, which introduces local variation into forensic traces. In response, we propose VideoFACT - a new network that is able to detect and localize a wide variety of video forgeries and manipulations. To overcome challenges that existing networks face when analyzing videos, our network utilizes both forensic embeddings to capture traces left by manipulation, context embeddings to control for variation in forensic traces introduced by video coding, and a deep self-attention mechanism to estimate the quality and relative importance of local forensic embeddings. We create several new video forgery datasets and use these, along with publicly available data, to experimentally e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08253</link><description>&lt;p&gt;
HMOE: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#30340;&#30446;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;DG&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DG&#26041;&#27861;&#65292;&#31216;&#20026;HMOE&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#12290;MoE&#22312;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#27169;&#24335;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;DG&#38382;&#39064;&#65292;&#24322;&#36136;&#24615;&#27491;&#26159;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#20135;&#29983;&#30340;&#12290;HMOE&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#36825;&#20351;&#24471;&#19987;&#23478;&#21487;&#20197;&#20849;&#20139;&#26377;&#29992;&#30340;&#20803;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20844;&#24179;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;-DomainBed&#19979;&#23558;HMOE&#19982;&#20854;&#20182;DG&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2211.06652</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Neuro-symbolic Programs for Language Guided Robot Manipulation. (arXiv:2211.06652v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for learning neuro-symbolic programs for language guided robot manipulation, which can handle linguistic and perceptual variations, is end-to-end trainable, and requires no intermediate supervision. The method uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.
&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#36755;&#20837;&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#36755;&#20986;&#19968;&#20010;&#21487;&#20197;&#30001;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#25805;&#20316;&#31243;&#24207;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#20043;&#19968;&#65306;&#65288;i&#65289;&#20381;&#36182;&#25163;&#24037;&#32534;&#30721;&#30340;&#27010;&#24565;&#31526;&#21495;&#65292;&#38480;&#21046;&#20102;&#36229;&#20986;&#35757;&#32451;&#26399;&#38388;&#25152;&#35265;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;[1]&#65288;ii&#65289;&#20174;&#25351;&#20196;&#20013;&#25512;&#26029;&#20986;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#38656;&#35201;&#23494;&#38598;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;[2]&#25110;&#65288;iii&#65289;&#32570;&#20047;&#35299;&#37322;&#22797;&#26434;&#25351;&#20196;&#25152;&#38656;&#30340;&#35821;&#20041;&#65292;&#36825;&#31181;&#35821;&#20041;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25512;&#29702;[3]&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#36825;&#20123;&#26500;&#36896;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21253;&#25324;&#20998;&#23618;&#25351;&#20196;&#35299;&#26512;&#22120;&#21644;&#21160;&#20316;&#27169;&#25311;&#22120;&#65292;&#20197;&#23398;&#20064;&#35299;&#32806;&#30340;&#34892;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled act
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>WiserVR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#36890;&#20449;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#20256;&#36755;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#20301;&#32622;&#22270;&#21644;&#32852;&#21512;&#35821;&#20041;&#36890;&#36947;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.01241</link><description>&lt;p&gt;
WiserVR&#65306;&#35821;&#20041;&#36890;&#20449;&#25903;&#25345;&#30340;&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
WiserVR: Semantic Communication Enabled Wireless Virtual Reality Delivery. (arXiv:2211.01241v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01241
&lt;/p&gt;
&lt;p&gt;
WiserVR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#36890;&#20449;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#20256;&#36755;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#20041;&#20301;&#32622;&#22270;&#21644;&#32852;&#21512;&#35821;&#20041;&#36890;&#36947;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiserVR proposes a novel framework that utilizes semantic communication and deep learning techniques to achieve efficient wireless virtual reality delivery, including semantic location graph and joint-semantic-channel-coding method with knowledge sharing.
&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#34394;&#25311;&#29616;&#23454;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#26432;&#25163;&#32423;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#24102;&#23485;&#36164;&#28304;&#19979;&#65292;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#20197;&#21450;&#23545;&#24310;&#36831;&#21644;&#21487;&#38752;&#24615;&#30340;&#20005;&#26684;&#35201;&#27714;&#20351;&#24471;&#26080;&#32447;VR&#20256;&#36755;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#29942;&#39048;&#20419;&#20351;&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#35821;&#20041;&#36890;&#20449;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36164;&#28304;&#21387;&#21147;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;VR&#20256;&#36755;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;WiserVR&#65288;WIreless SEmantic deliveRy for VR&#65289;&#65292;&#29992;&#20110;&#21521;VR&#29992;&#25143;&#20256;&#36882;&#36830;&#32493;&#30340;360&#24230;&#35270;&#39057;&#24103;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20010;&#27169;&#22359;&#65292;&#29992;&#20110;WiserVR&#20013;&#30340;&#25910;&#21457;&#22120;&#65292;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#35821;&#20041;&#24674;&#22797;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#38376;&#24320;&#21457;&#20102;&#35821;&#20041;&#20301;&#32622;&#22270;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#30693;&#35782;&#20849;&#20139;&#30340;&#32852;&#21512;&#35821;&#20041;&#36890;&#36947;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual reality (VR) over wireless is expected to be one of the killer applications in next-generation communication networks. Nevertheless, the huge data volume along with stringent requirements on latency and reliability under limited bandwidth resources makes untethered wireless VR delivery increasingly challenging. Such bottlenecks, therefore, motivate this work to seek the potential of using semantic communication, a new paradigm that promises to significantly ease the resource pressure, for efficient VR delivery. To this end, we propose a novel framework, namely WIreless SEmantic deliveRy for VR (WiserVR), for delivering consecutive 360{\deg} video frames to VR users. Specifically, deep learning-based multiple modules are well-devised for the transceiver in WiserVR to realize high-performance feature extraction and semantic recovery. Among them, we dedicatedly develop a concept of semantic location graph and leverage the joint-semantic-channel-coding method with knowledge sharing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.00171</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#24863;&#23884;&#20837;&#22312;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to build a single model that can transition between different emotions, languages, and annotation formats by leveraging multilingual models and Demux, to achieve knowledge sharing and reduce training costs.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#31185;&#23558;&#24773;&#24863;&#34701;&#20837;&#20854;&#29702;&#35770;&#21644;&#24212;&#29992;&#20013;&#65292;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#24773;&#24863;&#30340;&#38656;&#27714;&#19981;&#26029;&#22810;&#26679;&#21270;&#12290;&#36825;&#20123;&#38656;&#27714;&#21253;&#25324;&#25512;&#26029;&#19981;&#21516;&#31867;&#22411;&#30340;&#24773;&#24863;&#12289;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26684;&#24335;&#12290;&#19981;&#21516;&#37197;&#32622;&#20043;&#38388;&#30340;&#20849;&#20139;&#27169;&#22411;&#23558;&#20351;&#30693;&#35782;&#20849;&#20139;&#21644;&#35757;&#32451;&#25104;&#26412;&#38477;&#20302;&#65292;&#24182;&#31616;&#21270;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#37197;&#32622;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;Demux&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#36755;&#20837;&#21253;&#25324;&#24863;&#20852;&#36259;&#30340;&#24773;&#24863;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21160;&#24577;&#22320;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#30340;&#24773;&#24863;&#12290;Demux&#36824;&#20135;&#29983;&#24773;&#24863;&#23884;&#20837;&#65292;&#23545;&#23427;&#20204;&#25191;&#34892;&#25805;&#20316;&#21487;&#20197;&#36890;&#36807;&#27719;&#38598;&#27599;&#20010;&#31751;&#30340;&#23884;&#20837;&#26469;&#36807;&#28193;&#21040;&#24773;&#24863;&#31751;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Demux&#21487;&#20197;&#21516;&#26102;&#20256;&#36755;k
&lt;/p&gt;
&lt;p&gt;
The need for emotional inference from text continues to diversify as more and more disciplines integrate emotions into their theories and applications. These needs include inferring different emotion types, handling multiple languages, and different annotation formats. A shared model between different configurations would enable the sharing of knowledge and a decrease in training costs, and would simplify the process of deploying emotion recognition models in novel environments. In this work, we study how we can build a single model that can transition between these different configurations by leveraging multilingual models and Demux, a transformer-based model whose input includes the emotions of interest, enabling us to dynamically change the emotions predicted by the model. Demux also produces emotion embeddings, and performing operations on them allows us to transition to clusters of emotions by pooling the embeddings of each cluster. We show that Demux can simultaneously transfer k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34920;&#26684;&#25968;&#25454;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#8221;&#65288;TabCSDI&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#21644;&#25968;&#20540;&#21464;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17128</link><description>&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for missing value imputation in tabular data. (arXiv:2210.17128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34920;&#26684;&#25968;&#25454;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#8221;&#65288;TabCSDI&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#21644;&#25968;&#20540;&#21464;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI) for missing value imputation in tabular data, which effectively handles categorical variables and numerical variables simultaneously, and achieves excellent performance on various datasets.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#26159;&#20351;&#29992;&#21487;&#29992;&#20449;&#24687;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#20540;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#29992;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#25554;&#34917;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#31561;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#25554;&#34917;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34920;&#26684;&#25968;&#25454;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#8221;&#65288;TabCSDI&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#21644;&#25968;&#20540;&#21464;&#37327;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#25216;&#26415;&#65306;&#29420;&#28909;&#32534;&#30721;&#12289;&#27169;&#25311;&#20301;&#32534;&#30721;&#21644;&#29305;&#24449;&#26631;&#35760;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing value imputation in machine learning is the task of estimating the missing values in the dataset accurately using available information. In this task, several deep generative modeling methods have been proposed and demonstrated their usefulness, e.g., generative adversarial imputation networks. Recently, diffusion models have gained popularity because of their effectiveness in the generative modeling task in images, texts, audio, etc. To our knowledge, less attention has been paid to the investigation of the effectiveness of diffusion models for missing value imputation in tabular data. Based on recent development of diffusion models for time-series data imputation, we propose a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI). To effectively handle categorical variables and numerical variables simultaneously, we investigate three techniques: one-hot encoding, analog bits encoding, and feature tokenization. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#20799;&#31461;ASD&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#22411;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#32467;&#26500;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26631;&#20934;ASD&#38754;&#37096;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;ViTASD-L&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.16943</link><description>&lt;p&gt;
ViTASD&#65306;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#38754;&#37096;&#35786;&#26029;&#30340;&#24378;&#20581;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis. (arXiv:2210.16943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#20799;&#31461;ASD&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#22411;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#32467;&#26500;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26631;&#20934;ASD&#38754;&#37096;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;ViTASD-L&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for computational analysis of pediatric ASD using Vision Transformer, which extracts knowledge from large facial expression datasets and offers model structure transferability. Extensive experiments on standard ASD facial analysis benchmarks show that ViTASD-L achieves a new state-of-the-art.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#32456;&#36523;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#65292;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#24739;&#30149;&#29575;&#12290;&#30001;&#20110;&#32570;&#20047;&#33391;&#22909;&#30340;&#22522;&#32447;&#65292;ASD&#38754;&#37096;&#20998;&#26512;&#22312;&#20799;&#31185;&#24739;&#32773;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Vision Transformer&#65288;ViT&#65289;&#36827;&#34892;&#20799;&#31461;ASD&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ViTASD&#65292;&#20174;&#22823;&#22411;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#32467;&#26500;&#21487;&#36716;&#31227;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ViTASD&#37319;&#29992;&#26222;&#36890;&#30340;ViT&#20174;&#24739;&#32773;&#30340;&#38754;&#37096;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#39640;&#26031;&#36807;&#31243;&#23618;&#26469;&#22686;&#24378;ASD&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;ASD&#38754;&#37096;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20195;&#34920;&#24615;&#30340;ASD&#38754;&#37096;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;ViTASD-L&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/IrohX&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with very high prevalence around the world. Research progress in the field of ASD facial analysis in pediatric patients has been hindered due to a lack of well-established baselines. In this paper, we propose the use of the Vision Transformer (ViT) for the computational analysis of pediatric ASD. The presented model, known as ViTASD, distills knowledge from large facial expression datasets and offers model structure transferability. Specifically, ViTASD employs a vanilla ViT to extract features from patients' face images and adopts a lightweight decoder with a Gaussian Process layer to enhance the robustness for ASD analysis. Extensive experiments conducted on standard ASD facial analysis benchmarks show that our method outperforms all of the representative approaches in ASD facial analysis, while the ViTASD-L achieves a new state-of-the-art. Our code and pretrained models are available at https://github.com/IrohX
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#24182;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.15842</link><description>&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#24773;&#24863;&#35782;&#21035;&#20013;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#30740;&#31350;&#65306;&#20197;&#24773;&#24863;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#24182;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection, develops two modeling approaches to capture word associations of the emotion words themselves, and integrates pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models, demonstrating state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#34920;&#36798;&#30340;&#24773;&#24863;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#26631;&#31614;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#19968;&#31181;&#26159;&#23558;&#24773;&#24863;&#21253;&#21547;&#22312;&#36755;&#20837;&#20013;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#39033;&#20998;&#20026;&#20004;&#31867;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#12290;&#21069;&#32773;&#26681;&#25454;&#37329;&#26631;&#31614;&#21160;&#24577;&#21464;&#21270;&#65292;&#32780;&#21518;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting emotions expressed in text has become critical to a range of fields. In this work, we investigate ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection. First, we develop two modeling approaches to the problem in order to capture word associations of the emotion words themselves, by either including the emotions in the input, or by leveraging Masked Language Modeling (MLM). Second, we integrate pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models. We split these terms into two categories, local and global. The former dynamically change based on the gold labels, while the latter remain static during training. We demonstrate state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models. On top of better performance, we also demonstrate improved robustness. Code is available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.15173</link><description>&lt;p&gt;
Articulation GAN: &#26080;&#30417;&#30563;&#24314;&#27169;&#20851;&#33410;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new unsupervised generative model that learns to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner, which more closely mimics human speech production and better simulates the process of human speech production.
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#27874;&#24418;&#25110;&#39057;&#35889;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#36807;&#25511;&#21046;&#20851;&#33410;&#26469;&#20135;&#29983;&#35821;&#38899;&#65292;&#36825;&#36890;&#36807;&#22768;&#38899;&#20256;&#25773;&#30340;&#29289;&#29702;&#29305;&#24615;&#23548;&#33268;&#35821;&#38899;&#22768;&#38899;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#33410;&#29983;&#25104;&#22120;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33539;&#20363;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#20135;&#29983;/&#21512;&#25104;&#12290;&#20851;&#33410;&#29983;&#25104;&#22120;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#39044;&#35757;&#32451;&#29289;&#29702;&#27169;&#22411;&#65288;ema2wav&#65289;&#23558;&#29983;&#25104;&#30340;EMA&#34920;&#31034;&#36716;&#25442;&#20026;&#35821;&#38899;&#27874;&#24418;&#65292;&#36825;&#20123;&#27874;&#24418;&#34987;&#21457;&#36865;&#21040;&#37492;&#21035;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#20851;&#33410;&#20998;&#26512;&#34920;&#26126;&#65292;&#32593;&#32476;&#23398;&#20064;&#25511;&#21046;&#20851;&#33410;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#26041;&#24335;&#12290;&#36755;&#20986;&#30340;&#22768;&#23398;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Generative deep neural networks are widely used for speech synthesis, but most existing models directly generate waveforms or spectral outputs. Humans, however, produce speech by controlling articulators, which results in the production of speech sounds through physical properties of sound propagation. We introduce the Articulatory Generator to the Generative Adversarial Network paradigm, a new unsupervised generative model of speech production/synthesis. The Articulatory Generator more closely mimics human speech production by learning to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner. A separate pre-trained physical model (ema2wav) then transforms the generated EMA representations to speech waveforms, which get sent to the Discriminator for evaluation. Articulatory analysis suggests that the network learns to control articulators in a similar manner to humans during speech production. Acoustic analysis of the outputs sugge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#25991;&#26412;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#36807;&#28388;&#22120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14616</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail. (arXiv:2210.14616v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#25991;&#26412;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#36807;&#28388;&#22120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to design an effective approach filtering out hybrid spam e-mails and proposes a late multi-modal fusion model to solve the problem of traditional text-based or image-based filters failing to detect hybrid spam e-mails.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22403;&#22334;&#37038;&#20214;&#21457;&#36865;&#32773;&#24320;&#22987;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#37096;&#20998;&#30340;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#26469;&#28151;&#28102;&#20854;&#24847;&#22270;&#65292;&#36825;&#27604;&#20165;&#21253;&#21547;&#25991;&#26412;&#25110;&#22270;&#20687;&#30340;&#30005;&#23376;&#37038;&#20214;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#65292;&#20197;&#36991;&#20813;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#36807;&#28388;&#22120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#24773;&#20917;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#26088;&#22312;&#26816;&#27979;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#12290;&#36890;&#24120;&#65292;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#29992;&#20110;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#26469;&#28040;&#38500;&#22403;&#22334;&#37038;&#20214;&#30340;&#22270;&#20687;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#23613;&#31649;OCR&#25195;&#25551;&#26159;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#28151;&#21512;&#22403;&#22334;&#37038;&#20214;&#30340;&#38750;&#24120;&#25104;&#21151;&#30340;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#25152;&#38656;&#30340;CPU&#21151;&#29575;&#21644;&#25195;&#25551;&#30005;&#23376;&#37038;&#20214;&#25991;&#20214;&#25152;&#38656;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#23427;&#19981;&#26159;&#22788;&#29702;&#22823;&#37327;&#22403;&#22334;&#37038;&#20214;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, spammers are now trying to obfuscate their intents by introducing hybrid spam e-mail combining both image and text parts, which is more challenging to detect in comparison to e-mails containing text or image only. The motivation behind this research is to design an effective approach filtering out hybrid spam e-mails to avoid situations where traditional text-based or image-baesd only filters fail to detect hybrid spam e-mails. To the best of our knowledge, a few studies have been conducted with the goal of detecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR) technology is used to eliminate the image parts of spam by transforming images into text. However, the research questions are that although OCR scanning is a very successful technique in processing text-and-image hybrid spam, it is not an effective solution for dealing with huge quantities due to the CPU power required and the execution time it takes to scan e-mail files. And the OCR tech
&lt;/p&gt;</description></item><item><title>D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.14428</link><description>&lt;p&gt;
D-Shape: &#36890;&#36807;&#30446;&#26631;&#26465;&#20214;&#21270;&#23454;&#29616;&#28436;&#31034;&#24418;&#29366;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14428
&lt;/p&gt;
&lt;p&gt;
D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#26159;&#35299;&#20915;&#33258;&#20027;&#34892;&#20026;&#33719;&#21462;&#20013;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#26679;&#20570;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#25152;&#38656;&#30340;&#34892;&#20026;&#28436;&#31034;&#30001;&#19987;&#23478;&#25552;&#20379;&#65292;&#35813;&#19987;&#23478;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#34920;&#29616;&#26368;&#20339;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#28436;&#31034;&#26159;&#27425;&#20248;&#30340;&#65292;&#21017;&#38754;&#20020;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#21363;IL&#30340;&#28436;&#31034;&#21305;&#37197;&#30446;&#26631;&#19982;RL&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20914;&#31361;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;D-Shape&#65292;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#19978;&#36848;&#20914;&#31361;&#12290;D-Shape&#20801;&#35768;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25214;&#21040;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#23454;&#39564;&#39564;&#35777;&#20102;D-Shape&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;RL&#65292;&#24182;&#19988;&#33021;&#22815;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;LAZY&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.14055</link><description>&lt;p&gt;
&#24102;&#21453;&#39304;&#30340;&#31574;&#30053;&#24341;&#23548;&#25042;&#24816;&#25628;&#32034;&#29992;&#20110;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Policy-Guided Lazy Search with Feedback for Task and Motion Planning. (arXiv:2210.14055v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;LAZY&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a solver LAZY for PDDLStream problems, which maintains a single integrated search over action skeletons, gradually becoming more geometrically informed as samples of possible motions are lazily drawn during motion planning. Meanwhile, learned models of goal-directed policies and current motion sampling data are incorporated in LAZY to adaptively guide the task planner, leading to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions.
&lt;/p&gt;
&lt;p&gt;
PDDLStream&#27714;&#35299;&#22120;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;PDDL&#25193;&#23637;&#21040;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;PDDLStream&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#31995;&#21015;PDDL&#35268;&#21010;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#25104;&#30340;&#35268;&#21010;&#22120;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#36816;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LAZY&#65292;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#30340;&#23398;&#20064;&#27169;&#22411;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;TAMP&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PDDLStream solvers have recently emerged as viable solutions for Task and Motion Planning (TAMP) problems, extending PDDL to problems with continuous action spaces. Prior work has shown how PDDLStream problems can be reduced to a sequence of PDDL planning problems, which can then be solved using off-the-shelf planners. However, this approach can suffer from long runtimes. In this paper we propose LAZY, a solver for PDDLStream problems that maintains a single integrated search over action skeletons, which gets progressively more geometrically informed, as samples of possible motions are lazily drawn during motion planning. We explore how learned models of goal-directed policies and current motion sampling data can be incorporated in LAZY to adaptively guide the task planner. We show that this leads to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions. We evaluate our TAMP appro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AugPro&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#20559;&#31227;&#20915;&#31574;&#36793;&#30028;&#65292;&#35745;&#31639;&#24320;&#38144;&#23567;&#12290;</title><link>http://arxiv.org/abs/2210.11768</link><description>&lt;p&gt;
&#25237;&#24433;&#22686;&#24378;&#65306;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AugPro&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#20559;&#31227;&#20915;&#31574;&#36793;&#30028;&#65292;&#35745;&#31639;&#24320;&#38144;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
AugPro is an effective and efficient data augmentation method for distillation, which avoids shifting decision boundaries and has little computational overhead.
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#20174;&#22823;&#22411;&#27169;&#22411;&#21521;&#23567;&#22411;&#27169;&#22411;&#36716;&#31227;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#34920;&#31034;&#25554;&#20540;&#12289;&#26631;&#35760;&#26367;&#25442;&#25110;&#27169;&#22411;&#22686;&#24378;&#31561;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20915;&#31574;&#36793;&#30028;&#30340;&#20559;&#31227;&#65288;&#34920;&#31034;&#25554;&#20540;&#65289;&#65292;&#19981;&#22815;&#34920;&#36798;&#65288;&#26631;&#35760;&#26367;&#25442;&#65289;&#25110;&#24341;&#20837;&#36807;&#22810;&#30340;&#35745;&#31639;&#24320;&#38144;&#65288;&#27169;&#22411;&#22686;&#24378;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AugPro&#65288;&#25237;&#24433;&#22686;&#24378;&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#33976;&#39311;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#34920;&#31034;&#25554;&#20540;&#22686;&#24378;&#26041;&#27861;&#20043;&#19978;&#65292;&#20197;&#20445;&#25345;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23558;&#22686;&#24378;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#26631;&#35760;&#20197;&#36991;&#20813;&#20559;&#31227;&#20915;&#31574;&#36793;&#30028;&#12290;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#25805;&#20316;&#65292;&#35745;&#31639;&#24320;&#38144;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not expressive enough (token replacement), or introduce too much computational overhead (augmentation with models). To this end, we propose AugPro (Augmentation with Projection), an effective and efficient data augmentation method for distillation. Our method builds on top of representation interpolation augmentation methods to maintain the diversity of expressions and converts the augmented data to tokens to avoid shifting decision boundaries. It uses simple operations that come with little computat
&lt;/p&gt;</description></item><item><title>ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10953</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21457;&#29616;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10953
&lt;/p&gt;
&lt;p&gt;
ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#25928;&#20248;&#21270;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#23547;&#27714;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#22312;&#35299;&#20915;&#26041;&#26696;&#21518;&#26399;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#24456;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROBOT&#30340;&#25490;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;ROBOT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#21457;&#29616;&#22823;&#37327;&#39640;&#24615;&#33021;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#19982;&#23547;&#25214;&#21333;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#38656;&#35201;&#24456;&#23569;&#30340;&#39069;&#22806;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#27169;&#25311;&#23558;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#20026;&#36830;&#32493;&#23494;&#24230;&#22330;&#30340;&#23545;&#35937;&#30340;&#36816;&#21160;&#65292;&#20174;&#23494;&#24230;&#22330;&#20013;&#20272;&#35745;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#21487;&#24494;&#25509;&#35302;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#26500;&#24314;&#29289;&#20307;&#27169;&#22411;&#24182;&#20248;&#21270;&#31070;&#32463;&#23545;&#35937;&#30340;&#25235;&#21462;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2210.09420</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#21160;&#21147;&#23398;&#22686;&#24378;&#31070;&#32463;&#23545;&#35937;&#30340;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Differentiable Physics Simulation of Dynamics-Augmented Neural Objects. (arXiv:2210.09420v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#27169;&#25311;&#23558;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#20026;&#36830;&#32493;&#23494;&#24230;&#22330;&#30340;&#23545;&#35937;&#30340;&#36816;&#21160;&#65292;&#20174;&#23494;&#24230;&#22330;&#20013;&#20272;&#35745;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#21487;&#24494;&#25509;&#35302;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#26500;&#24314;&#29289;&#20307;&#27169;&#22411;&#24182;&#20248;&#21270;&#31070;&#32463;&#23545;&#35937;&#30340;&#25235;&#21462;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network, estimates the dynamical properties of the object from the density field, introduces a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions, and allows a robot to autonomously build object models and optimize grasps and manipulation trajectories of neural objects.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#27169;&#25311;&#23558;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#20026;&#36830;&#32493;&#23494;&#24230;&#22330;&#30340;&#23545;&#35937;&#30340;&#36816;&#21160;&#65292;&#35813;&#23494;&#24230;&#22330;&#34987;&#21442;&#25968;&#21270;&#20026;&#28145;&#24230;&#32593;&#32476;&#12290;&#36825;&#21253;&#25324;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#20854;&#20182;&#30456;&#20851;&#27169;&#22411;&#12290;&#20174;&#23494;&#24230;&#22330;&#20013;&#65292;&#25105;&#20204;&#20272;&#35745;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#36136;&#37327;&#12289;&#36136;&#24515;&#21644;&#24815;&#24615;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#21487;&#24494;&#25509;&#35302;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30001;&#30896;&#25758;&#20135;&#29983;&#30340;&#27861;&#21521;&#21644;&#25705;&#25830;&#21147;&#12290;&#36825;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#26500;&#24314;&#29289;&#20307;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#38745;&#27490;&#22270;&#20687;&#21644;&#36816;&#21160;&#20013;&#30340;&#29289;&#20307;&#35270;&#39057;&#20013;&#35270;&#35273;&#19978;&#21644;&#21160;&#24577;&#19978;&#37117;&#26159;&#20934;&#30830;&#30340;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21160;&#21147;&#23398;&#22686;&#24378;&#31070;&#32463;&#23545;&#35937;&#65288;DANOs&#65289;&#20351;&#29992;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#24341;&#25806;Dojo&#36827;&#34892;&#27169;&#25311;&#65292;&#19982;&#20854;&#20182;&#26631;&#20934;&#27169;&#25311;&#23545;&#35937;&#65288;&#22914;&#29699;&#20307;&#12289;&#24179;&#38754;&#21644;&#20197;URDF&#25351;&#23450;&#30340;&#26426;&#22120;&#20154;&#65289;&#36827;&#34892;&#20132;&#20114;&#12290;&#26426;&#22120;&#20154;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;&#27169;&#25311;&#26469;&#20248;&#21270;&#31070;&#32463;&#23545;&#35937;&#30340;&#25235;&#21462;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network. This includes Neural Radiance Fields (NeRFs), and other related models. From the density field, we estimate the dynamical properties of the object, including its mass, center of mass, and inertia matrix. We then introduce a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions. This allows a robot to autonomously build object models that are visually and \emph{dynamically} accurate from still images and videos of objects in motion. The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an existing differentiable simulation engine, Dojo, interacting with other standard simulation objects, such as spheres, planes, and robots specified as URDFs. A robot can use this simulation to optimize grasps and manipulation trajectories of neural ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;Transformer&#25512;&#24191;&#21040;&#22270;&#32467;&#26500;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#21644;&#26356;&#26032;&#36793;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25512;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#22270;&#32467;&#26500;&#20219;&#21153;&#19978;&#37117;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05062</link><description>&lt;p&gt;
&#20851;&#31995;&#27880;&#24847;&#21147;&#65306;&#23558;Transformer&#25512;&#24191;&#21040;&#22270;&#32467;&#26500;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Relational Attention: Generalizing Transformers for Graph-Structured Tasks. (arXiv:2210.05062v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;Transformer&#25512;&#24191;&#21040;&#22270;&#32467;&#26500;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#21644;&#26356;&#26032;&#36793;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25512;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#22270;&#32467;&#26500;&#20219;&#21153;&#19978;&#37117;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a relational attention mechanism that generalizes Transformers for graph-structured tasks by considering and updating edge vectors in each Transformer layer, achieving reasoning over graph-structured data. Compared to state-of-the-art graph neural networks designed for reasoning over graph-structured data, it has significant advantages in various graph-structured tasks.
&lt;/p&gt;
&lt;p&gt;
Transformer&#21487;&#20197;&#28789;&#27963;&#22320;&#25805;&#20316;&#34920;&#31034;&#20219;&#21153;&#29305;&#23450;&#23454;&#20307;&#21450;&#20854;&#23646;&#24615;&#30340;&#23454;&#20540;&#21521;&#37327;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#21521;&#37327;&#21487;&#33021;&#32534;&#30721;&#19968;&#20010;&#21333;&#35789;&#29255;&#27573;&#20196;&#29260;&#21450;&#20854;&#22312;&#24207;&#21015;&#20013;&#30340;&#20301;&#32622;&#65292;&#25110;&#32773;&#19968;&#20123;&#19981;&#24102;&#20301;&#32622;&#30340;&#20449;&#24687;&#12290;&#20294;&#20316;&#20026;&#38598;&#21512;&#22788;&#29702;&#22120;&#65292;Transformer&#22312;&#25512;&#29702;&#26356;&#19968;&#33324;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#65288;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#23454;&#20307;&#65292;&#36793;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#26041;&#38754;&#22788;&#20110;&#21155;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#23558;Transformer&#27880;&#24847;&#21147;&#25512;&#24191;&#21040;&#27599;&#20010;Transformer&#23618;&#20013;&#32771;&#34385;&#21644;&#26356;&#26032;&#36793;&#21521;&#37327;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22270;&#32467;&#26500;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#20851;&#31995;Transformer&#65292;&#21253;&#25324;&#22823;&#22411;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#37027;&#37324;&#65292;&#23427;&#26126;&#26174;&#20248;&#20110;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25512;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25910;&#30410;&#24402;&#22240;&#20110;&#20851;&#31995;&#27880;&#24847;&#21147;&#22266;&#26377;&#30340;&#33021;&#21147;&#65292;&#21363;&#21033;&#29992;&#22823;&#37327;&#30340;&#36793;&#20449;&#24687;&#26469;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. But as set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the great
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#19977;&#31867;&#26367;&#20195;&#24230;&#37327;&#65292;&#21363;&#39044;&#27979;&#35823;&#24046;&#12289;&#27169;&#22411;&#20013;&#24515;&#24615;&#21644;&#27880;&#20837;&#21512;&#25104;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.01078</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Model Selection for Time-series Anomaly Detection. (arXiv:2210.01078v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#19977;&#31867;&#26367;&#20195;&#24230;&#37327;&#65292;&#21363;&#39044;&#27979;&#35823;&#24046;&#12289;&#27169;&#22411;&#20013;&#24515;&#24615;&#21644;&#27880;&#20837;&#21512;&#25104;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised model selection method for time-series anomaly detection, which selects the most accurate model through three classes of surrogate metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies.
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#39033;&#35843;&#26597;&#24471;&#20986;&#32467;&#35770;&#65292;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#26368;&#20934;&#30830;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24322;&#24120;&#26631;&#31614;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#29992;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#22914;&#20309;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20934;&#30830;&#27169;&#22411;&#30340;&#23454;&#38469;&#38382;&#39064;&#24471;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#32473;&#23450;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#20505;&#36873;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#22914;&#20309;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31867;&#26367;&#20195;&#65288;&#26080;&#30417;&#30563;&#65289;&#24230;&#37327;&#65292;&#21363;&#39044;&#27979;&#35823;&#24046;&#12289;&#27169;&#22411;&#20013;&#24515;&#24615;&#21644;&#27880;&#20837;&#21512;&#25104;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#24230;&#37327;&#19982;&#26631;&#20934;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#24230;&#37327;&#65288;&#22914;$F_1$&#20998;&#25968;&#65289;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#31243;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#20010;&#24230;&#37327;&#32452;&#21512;&#30340;&#24230;&#37327;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question i.e. Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#26469;&#25552;&#39640;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#23545;&#25171;&#20081;&#30340;ASR&#33050;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#39057;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.15280</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Spatiotemporal Representations from Natural Script Knowledge. (arXiv:2209.15280v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#26469;&#25552;&#39640;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#23545;&#25171;&#20081;&#30340;ASR&#33050;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#39057;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new pretext task to boost transferable spatiotemporal representation learning by exploiting natural language knowledge, which sorts shuffled ASR scripts by attending to learned video representations, and thus improves the progress of video understanding.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#24050;&#25104;&#20026;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#23616;&#38480;&#20110;&#39640;&#24230;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;K400&#65289;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#24320;&#31665;&#21363;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#21482;&#25429;&#25417;&#20687;&#32032;&#32423;&#21035;&#30340;&#30693;&#35782;&#32780;&#19981;&#26159;&#26102;&#31354;&#35821;&#20041;&#65292;&#36825;&#38459;&#30861;&#20102;&#35270;&#39057;&#29702;&#35299;&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#21463;&#21040;&#22270;&#20687;&#25991;&#26412;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;CLIP&#65289;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#21033;&#29992;&#35821;&#35328;&#35821;&#20041;&#25552;&#39640;&#21487;&#36716;&#31227;&#30340;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#21363;&#8220;Turning to Video for Transcript Sorting&#65288;TVTS&#65289;&#8221;&#65292;&#36890;&#36807;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#23545;&#25171;&#20081;&#30340;ASR&#33050;&#26412;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#25551;&#36848;&#24615;&#26631;&#39064;&#65292;&#32431;&#31929;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#21363;&#21033;&#29992;&#33258;&#28982;&#36716;&#24405;&#30340;&#35821;&#38899;&#30693;&#35782;&#22312;&#26102;&#38388;&#19978;&#25552;&#20379;&#22024;&#26434;&#20294;&#26377;&#29992;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large-scale video data has become a common recipe for learning transferable spatiotemporal representations in recent years. Despite some progress, existing methods are mostly limited to highly curated datasets (e.g., K400) and exhibit unsatisfactory out-of-the-box representations. We argue that it is due to the fact that they only capture pixel-level knowledge rather than spatiotemporal semantics, which hinders further progress in video understanding. Inspired by the great success of image-text pre-training (e.g., CLIP), we take the first step to exploit language semantics to boost transferable spatiotemporal representation learning. We introduce a new pretext task, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR scripts by attending to learned video representations. We do not rely on descriptive captions and learn purely from video, i.e., leveraging the natural transcribed speech knowledge to provide noisy but useful semantics over time. Our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GMFG&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24130;&#24459;&#32593;&#32476;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03880</link><description>&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#22270;&#24418;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Sparse Graphon Mean Field Games. (arXiv:2209.03880v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GMFG&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24130;&#24459;&#32593;&#32476;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel formulation of GMFGs, called LPGMFG, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems, especially power law networks. The paper derives theoretical existence and convergence guarantees and gives empirical examples that demonstrate the accuracy of the learning method.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#20195;&#29702;&#30340;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22270;&#24418;&#22343;&#22330;&#21338;&#24328;&#65288;GMFG&#65289;&#20351;&#24471;&#21487;&#20197;&#23545;&#21542;&#21017;&#38590;&#20197;&#22788;&#29702;&#30340;MARL&#38382;&#39064;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#26512;&#12290;&#30001;&#20110;&#22270;&#24418;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#38480;&#20110;&#25551;&#36848;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#65288;&#22914;&#24130;&#24459;&#22270;&#65289;&#30340;&#31264;&#23494;&#22270;&#24418;&#65292;&#36825;&#26159;&#19981;&#36275;&#30340;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;GMFG&#30340;&#26032;&#22411;&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#12290;&#36825;&#23588;&#20854;&#21253;&#25324;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#32463;&#39564;&#35266;&#23519;&#21040;&#30340;&#24130;&#24459;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#26080;&#27861;&#34987;&#26631;&#20934;&#22270;&#24418;&#25152;&#25429;&#25417;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#32954;&#27963;&#26816;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;EGFR&#31361;&#21464;&#30340;&#39044;&#27979;&#65292;&#20026;&#32954;&#30284;&#27835;&#30103;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#12289;&#26356;&#24555;&#25463;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.12506</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#32954;&#27963;&#26816;&#22270;&#20687;&#20013;&#30340;EGFR&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning. (arXiv:2208.12506v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#32954;&#27963;&#26816;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;EGFR&#31361;&#21464;&#30340;&#39044;&#27979;&#65292;&#20026;&#32954;&#30284;&#27835;&#30103;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#12289;&#26356;&#24555;&#25463;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses deep learning technology to predict EGFR mutations through analysis of lung biopsy images, providing a more economical and faster diagnostic method for lung cancer treatment.
&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#27835;&#30103;&#30340;&#26631;&#20934;&#35786;&#26029;&#31243;&#24207;&#28041;&#21450;&#32452;&#32455;&#23398;&#20122;&#22411;&#20998;&#22411;&#21644;&#38543;&#21518;&#30340;&#20851;&#38190;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#26816;&#27979;&#65292;&#22914;EGFR&#12290;&#23613;&#31649;&#20998;&#23376;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#65292;&#20294;&#35813;&#36807;&#31243;&#36890;&#24120;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#32463;&#27982;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20013;&#21457;&#29616;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#20197;&#35782;&#21035;hematoxylin&#21644;eosin&#26579;&#33394;&#30340;WSIs&#20013;EGFR&#31361;&#21464;&#30340;&#24418;&#24577;&#23398;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#26816;&#27979;&#32959;&#30244;&#21644;&#32452;&#32455;&#23398;&#20122;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#32954;&#30284;&#25968;&#25454;&#38598;&#65288;TCGA&#21644;&#26469;&#33258;&#21360;&#24230;&#30340;&#31169;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#26469;&#35777;&#26126;&#25105;&#20204;&#31649;&#36947;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32959;&#30244;&#26816;&#27979;&#30340;&#24179;&#22343;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.964&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#20026;0.942&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard diagnostic procedures for targeted therapies in lung cancer treatment involve histological subtyping and subsequent detection of key driver mutations, such as EGFR. Even though molecular profiling can uncover the driver mutation, the process is often expensive and time-consuming. Deep learning-oriented image analysis offers a more economical alternative for discovering driver mutations directly from whole slide images (WSIs). In this work, we used customized deep learning pipelines with weak supervision to identify the morphological correlates of EGFR mutation from hematoxylin and eosin-stained WSIs, in addition to detecting tumor and histologically subtyping it. We demonstrate the effectiveness of our pipeline by conducting rigorous experiments and ablation studies on two lung cancer datasets - TCGA and a private dataset from India. With our pipeline, we achieved an average area under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological subtyping betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#21152;&#26435;&#26377;&#21521;&#22270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#32536;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#35299;&#20915;&#20102;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11489</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#36793;&#32536;&#25104;&#26412;&#20272;&#35745;&#30340;&#22270;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates. (arXiv:2208.11489v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#21152;&#26435;&#26377;&#21521;&#22270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#32536;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#35299;&#20915;&#20102;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense, solving the uncertainty of the shortest path problem.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#26159;AI&#29702;&#35770;&#21644;&#24212;&#29992;&#30340;&#22522;&#30707;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#24573;&#30053;&#36793;&#32536;&#26435;&#37325;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#21152;&#26435;&#26377;&#21521;&#22270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#32536;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#20248;&#21270;&#36335;&#24452;&#25104;&#26412;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20219;&#20309;&#26102;&#20505;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#65292;&#23454;&#35777;&#35777;&#26126;&#20102;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shortest path problem in graphs is a cornerstone of AI theory and applications. Existing algorithms generally ignore edge weight computation time. In this paper we present a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. This raises a generalized shortest path problem that optimize different aspects of path cost and its uncertainty. We present a complete anytime solution algorithm for the generalized problem, and empirically demonstrate its efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;LSIMVC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#22810;&#35270;&#22270;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#20174;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#31232;&#30095;&#21644;&#32467;&#26500;&#21270;&#30340;&#20849;&#35782;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.02998</link><description>&lt;p&gt;
&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Localized Sparse Incomplete Multi-view Clustering. (arXiv:2208.02998v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;LSIMVC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#22810;&#35270;&#22270;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#20174;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#31232;&#30095;&#21644;&#32467;&#26500;&#21270;&#30340;&#20849;&#35782;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method named Localized Sparse Incomplete Multi-view Clustering (LSIMVC) to learn a sparse and structured consensus latent representation from incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model, which aims to solve the clustering problem on incomplete multi-view data with partial view missing.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#22312;&#35299;&#20915;&#37096;&#20998;&#35270;&#22270;&#32570;&#22833;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#32858;&#31867;&#38382;&#39064;&#26041;&#38754;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#35201;&#20040;&#19981;&#33021;&#28789;&#27963;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#32570;&#22833;&#35270;&#22270;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;&#35201;&#20040;&#19981;&#32771;&#34385;&#35270;&#22270;&#20043;&#38388;&#20449;&#24687;&#19981;&#24179;&#34913;&#30340;&#36127;&#38754;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#25506;&#32034;&#25152;&#26377;&#19981;&#23436;&#25972;&#35270;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23616;&#37096;&#31232;&#30095;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;LSIMVC&#65289;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LSIMVC&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#22810;&#35270;&#22270;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#26469;&#20174;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#31232;&#30095;&#21644;&#32467;&#26500;&#21270;&#30340;&#20849;&#35782;&#28508;&#22312;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26032;&#39062;&#27169;&#22411;&#20013;&#65292;&#22522;&#20110;l1&#33539;&#25968;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#34987;&#29992;&#20110;&#23398;&#20064;&#23616;&#37096;&#32467;&#26500;&#65292;&#20197;&#21450;&#26032;&#39062;&#30340;&#22270;&#23884;&#20837;&#34987;&#29992;&#20110;&#22788;&#29702;&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete multi-view clustering, which aims to solve the clustering problem on the incomplete multi-view data with partial view missing, has received more and more attention in recent years. Although numerous methods have been developed, most of the methods either cannot flexibly handle the incomplete multi-view data with arbitrary missing views or do not consider the negative factor of information imbalance among views. Moreover, some methods do not fully explore the local structure of all incomplete views. To tackle these problems, this paper proposes a simple but effective method, named localized sparse incomplete multi-view clustering (LSIMVC). Different from the existing methods, LSIMVC intends to learn a sparse and structured consensus latent representation from the incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model. Specifically, in such a novel model based on the matrix factorization, a l1 norm based spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.04827</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#31867;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.
&lt;/p&gt;
&lt;p&gt;
&#22238;&#24518;&#36215;&#22810;&#24180;&#26410;&#35265;&#30340;&#26379;&#21451;&#30340;&#38754;&#23380;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20320;&#20204;&#20598;&#28982;&#30456;&#36935;&#65292;&#20320;&#20204;&#20250;&#36731;&#26131;&#22320;&#35748;&#20986;&#24444;&#27492;&#12290;&#29983;&#29289;&#35760;&#24518;&#37197;&#22791;&#20102;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#23384;&#20648;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#28982;&#21518;&#25512;&#26029;&#32454;&#33410;&#20197;&#21305;&#37197;&#24863;&#30693;&#12290;Willshaw Memory&#26159;&#19968;&#31181;&#29992;&#20110;&#30382;&#23618;&#35745;&#31639;&#30340;&#31616;&#21333;&#25277;&#35937;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29983;&#29289;&#35760;&#24518;&#30340;&#26426;&#21046;&#12290;&#20351;&#29992;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35270;&#35273;&#27169;&#24335;&#30340;&#31232;&#30095;&#32534;&#30721;&#35268;&#21017;[34]&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#26694;&#26550;&#25193;&#23637;&#20102;&#22522;&#26412;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#35760;&#24518;&#21516;&#26102;&#23384;&#20648;&#27599;&#20010;&#27169;&#24335;&#30340;&#20960;&#31181;&#27169;&#24577;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#25110;&#25991;&#26412;&#65289;&#12290;&#35757;&#32451;&#21518;&#65292;&#24403;&#21482;&#24863;&#30693;&#21040;&#23376;&#38598;&#26102;&#65292;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#32534;&#30721;&#22120;-&#35760;&#24518;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$L_2$BN&#26041;&#27861;&#65292;&#36890;&#36807;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.02625</link><description>&lt;p&gt;
$L_2$BN: &#36890;&#36807;&#31561;&#21270;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features. (arXiv:2207.02625v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$L_2$BN&#26041;&#27861;&#65292;&#36890;&#36807;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an $L_2$BN method to enhance batch normalization by equalizing the $l_2$ norms of sample features, which can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features, easy to implement, and can be used as a basic normalization method for neural networks.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#24046;&#24322;&#20250;&#22952;&#30861;&#25209;&#37327;&#24402;&#19968;&#21270;&#33719;&#24471;&#26356;&#21152;&#26174;&#33879;&#30340;&#36328;&#31867;&#21035;&#29305;&#24449;&#21644;&#26356;&#21152;&#32039;&#20945;&#30340;&#20869;&#31867;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#23558;&#26679;&#26412;&#29305;&#24449;&#36755;&#20837;&#25209;&#37327;&#24402;&#19968;&#21270;&#20043;&#21069;&#23545;&#27599;&#20010;&#26679;&#26412;&#29305;&#24449;&#36827;&#34892;$L_2$&#24402;&#19968;&#21270;&#65292;&#22240;&#27492;&#29305;&#24449;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#12290;&#30001;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;$L_2$&#24402;&#19968;&#21270;&#21644;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;$L_2$BN&#12290;$L_2$BN&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#12290;$L_2$BN&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20854;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;$L_2$BN&#30340;&#26377;&#25928;&#24615;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that the difference in $l_2$ norms of sample features can hinder batch normalization from obtaining more distinguished inter-class features and more compact intra-class features. To address this issue, we propose an intuitive but effective method to equalize the $l_2$ norms of sample features. Concretely, we $l_2$-normalize each sample feature before feeding them into batch normalization, and therefore the features are of the same magnitude. Since the proposed method combines the $l_2$ normalization and batch normalization, we name our method $L_2$BN. The $L_2$BN can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features. The $L_2$BN is easy to implement and can exert its effect without any additional parameters or hyper-parameters. Therefore, it can be used as a basic normalization method for neural networks. We evaluate the effectiveness of $L_2$BN through extensive experiments with various models on image classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#20110;&#21516;&#34892;&#35780;&#23457;&#20013;&#24694;&#24847;&#25237;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#32570;&#20047;&#20844;&#24320;&#25968;&#25454;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2207.02303</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#21516;&#34892;&#35780;&#23457;&#20013;&#24694;&#24847;&#25237;&#26631;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset on Malicious Paper Bidding in Peer Review. (arXiv:2207.02303v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#20110;&#21516;&#34892;&#35780;&#23457;&#20013;&#24694;&#24847;&#25237;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#32570;&#20047;&#20844;&#24320;&#25968;&#25454;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a dataset on malicious paper bidding in peer review, filling the gap of lack of publicly-available data in this field.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;&#35780;&#23457;&#20154;&#36890;&#24120;&#34987;&#35201;&#27714;&#23545;&#27599;&#31687;&#25552;&#20132;&#30340;&#35770;&#25991;&#25552;&#20379;&#8220;&#25237;&#26631;&#8221;&#65292;&#20197;&#34920;&#36798;&#20182;&#20204;&#23545;&#23457;&#26597;&#35813;&#35770;&#25991;&#30340;&#20852;&#36259;&#12290;&#28982;&#21518;&#65292;&#19968;&#31181;&#35770;&#25991;&#20998;&#37197;&#31639;&#27861;&#20351;&#29992;&#36825;&#20123;&#25237;&#26631;&#65288;&#20197;&#21450;&#20854;&#20182;&#25968;&#25454;&#65289;&#26469;&#35745;&#31639;&#35780;&#23457;&#20154;&#23545;&#35770;&#25991;&#30340;&#39640;&#36136;&#37327;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24050;&#32463;&#34987;&#24694;&#24847;&#35780;&#23457;&#20154;&#21033;&#29992;&#65292;&#20182;&#20204;&#20250;&#26377;&#31574;&#30053;&#22320;&#25237;&#26631;&#65292;&#20197;&#38750;&#36947;&#24503;&#30340;&#26041;&#24335;&#25805;&#32437;&#35770;&#25991;&#20998;&#37197;&#65292;&#20174;&#32780;&#20005;&#37325;&#30772;&#22351;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#35780;&#23457;&#20154;&#21487;&#33021;&#20250;&#35797;&#22270;&#34987;&#20998;&#37197;&#21040;&#26379;&#21451;&#30340;&#35770;&#25991;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#20132;&#25442;&#26465;&#20214;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38556;&#30861;&#26159;&#32570;&#20047;&#20219;&#20309;&#20844;&#24320;&#21487;&#29992;&#30340;&#20851;&#20110;&#24694;&#24847;&#25237;&#26631;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#27169;&#25311;&#20250;&#35758;&#27963;&#21160;&#20013;&#25910;&#38598;&#30340;&#65292;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#35802;&#23454;&#25110;&#24694;&#24847;&#22320;&#25237;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#23545;&#25237;&#26631;&#34892;&#20026;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conference peer review, reviewers are often asked to provide "bids" on each submitted paper that express their interest in reviewing that paper. A paper assignment algorithm then uses these bids (along with other data) to compute a high-quality assignment of reviewers to papers. However, this process has been exploited by malicious reviewers who strategically bid in order to unethically manipulate the paper assignment, crucially undermining the peer review process. For example, these reviewers may aim to get assigned to a friend's paper as part of a quid-pro-quo deal. A critical impediment towards creating and evaluating methods to mitigate this issue is the lack of any publicly-available data on malicious paper bidding. In this work, we collect and publicly release a novel dataset to fill this gap, collected from a mock conference activity where participants were instructed to bid either honestly or maliciously. We further provide a descriptive analysis of the bidding behavior, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;DailyTalk&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2207.01063</link><description>&lt;p&gt;
DailyTalk&#65306;&#38754;&#21521;&#23545;&#35805;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#21475;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;DailyTalk&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a high-quality conversational speech dataset DailyTalk designed for conversational TTS. DailyTalk can be used as a general TTS dataset, and the baseline can represent contextual information from DailyTalk.
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25968;&#25454;&#38598;&#37117;&#26159;&#30001;&#21333;&#20010;&#35805;&#35821;&#32452;&#25104;&#65292;&#32570;&#20047;&#23545;&#35805;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DailyTalk&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20026;&#23545;&#35805;TTS&#35774;&#35745;&#12290;&#25105;&#20204;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;DailyDialog&#20013;&#25277;&#26679;&#12289;&#20462;&#25913;&#21644;&#24405;&#21046;&#20102;2,541&#20010;&#23545;&#35805;&#65292;&#24182;&#32487;&#25215;&#20102;&#20854;&#27880;&#37322;&#23646;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#20316;&#20026;&#25105;&#20204;&#30340;&#22522;&#32447;&#65292;&#20854;&#20013;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;TTS&#22312;&#23545;&#35805;&#20013;&#30340;&#21382;&#21490;&#20449;&#24687;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;&#36890;&#36807;&#22522;&#32447;&#23454;&#39564;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DailyTalk&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;TTS&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#34920;&#31034;DailyTalk&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;DailyTalk&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#20195;&#30721;&#21487;&#20379;&#23398;&#26415;&#29992;&#36884;&#20813;&#36153;&#20351;&#29992;&#65292;&#37319;&#29992;CC-BY-SA 4.0&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of current Text-to-Speech (TTS) datasets, which are collections of individual utterances, contain few conversational aspects. In this paper, we introduce DailyTalk, a high-quality conversational speech dataset designed for conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the open-domain dialogue dataset DailyDialog inheriting its annotated attributes. On top of our dataset, we extend prior work as our baseline, where a non-autoregressive TTS is conditioned on historical information in a dialogue. From the baseline experiment with both general and our novel metrics, we show that DailyTalk can be used as a general TTS dataset, and more than that, our baseline can represent contextual information from DailyTalk. The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.02307</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a semi-supervised medical image segmentation bootstrapping method based on anatomical-aware contrastive distillation, which solves the problem of imbalanced medical image data by softly labeling negative samples and capturing more semantically similar features.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#27880;&#37322;&#31232;&#32570;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#21307;&#23398;&#22270;&#20687;&#20855;&#26377;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#65288;&#21363;&#22810;&#31867;&#26631;&#31614;&#19981;&#24179;&#34913;&#65289;&#65292;&#36825;&#33258;&#28982;&#22320;&#20135;&#29983;&#27169;&#31946;&#30340;&#36718;&#24275;&#24182;&#36890;&#24120;&#38169;&#35823;&#22320;&#26631;&#35760;&#32597;&#35265;&#30340;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36127;&#26679;&#26412;&#26159;&#21542;&#21516;&#26679;&#36127;&#38754;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION&#65292;&#19968;&#31181;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#32780;&#19981;&#26159;&#27491;&#36127;&#23545;&#20043;&#38388;&#30340;&#20108;&#20803;&#30417;&#30563;&#26469;&#24320;&#21457;&#36845;&#20195;&#23545;&#27604;&#33976;&#39311;&#31639;&#27861;&#12290;&#19982;&#27491;&#26679;&#26412;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#26679;&#26412;&#38598;&#20013;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#37319;&#26679;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#30340;&#21551;&#21160;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.00233</link><description>&lt;p&gt;
DM$^2$: &#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching. (arXiv:2206.00233v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a decentralized multi-agent reinforcement learning method based on distribution matching, where each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution, achieving convergence to the joint policy that generated the target distribution.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#26426;&#21046;&#25110;&#26174;&#24335;&#36890;&#20449;&#21327;&#35758;&#20197;&#30830;&#20445;&#25910;&#25947;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#21305;&#37197;&#22312;&#19981;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#32452;&#20214;&#25110;&#26174;&#24335;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#26368;&#23567;&#21270;&#20854;&#20010;&#20307;&#20998;&#24067;&#19981;&#21305;&#37197;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#30446;&#26631;&#20998;&#24067;&#26469;&#33258;&#20248;&#21270;&#21512;&#20316;&#20219;&#21153;&#30340;&#32852;&#21512;&#31574;&#30053;&#65292;&#21017;&#35813;&#20219;&#21153;&#22870;&#21169;&#21644;&#20998;&#24067;&#21305;&#37197;&#22870;&#21169;&#30340;&#32452;&#21512;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#36825;&#19968;&#35265;&#35299;&#34987;&#29992;&#26469;&#21046;&#23450;&#19968;&#20010;&#23454;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24179;&#28369;&#30340;&#65288;&#38750;&#65289;&#26465;&#20214;&#23494;&#24230;&#65292;&#24182;&#20801;&#35768;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25191;&#34892;&#36895;&#24230;&#24555;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2205.09435</link><description>&lt;p&gt;
&#23545;&#25239;&#38543;&#26426;&#26862;&#26519;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#21644;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Adversarial random forests for density estimation and generative modeling. (arXiv:2205.09435v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24179;&#28369;&#30340;&#65288;&#38750;&#65289;&#26465;&#20214;&#23494;&#24230;&#65292;&#24182;&#20801;&#35768;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25191;&#34892;&#36895;&#24230;&#24555;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for density estimation and data synthesis using adversarial random forests, which provides smooth (un)conditional densities and allows for fully synthetic data generation. The method achieves comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#22411;&#26080;&#30417;&#30563;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#21463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#36882;&#24402;&#36807;&#31243;&#65292;&#20854;&#20013;&#26641;&#36890;&#36807;&#20132;&#26367;&#30340;&#29983;&#25104;&#21644;&#21028;&#21035;&#36718;&#27425;&#36880;&#28176;&#23398;&#20064;&#25968;&#25454;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#21487;&#20197;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#26641;&#30340;&#26367;&#20195;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#24179;&#28369;&#30340;&#65288;&#38750;&#65289;&#26465;&#20214;&#23494;&#24230;&#65292;&#24182;&#20801;&#35768;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24179;&#22343;&#25191;&#34892;&#36895;&#24230;&#24555;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#38468;&#24102;&#30340;R&#21253;arf&#21487;&#22312;CRAN&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose methods for density estimation and data synthesis using a novel form of unsupervised random forests. Inspired by generative adversarial networks, we implement a recursive procedure in which trees gradually learn structural properties of the data through alternating rounds of generation and discrimination. The method is provably consistent under minimal assumptions. Unlike classic tree-based alternatives, our approach provides smooth (un)conditional densities and allows for fully synthetic data generation. We achieve comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average. An accompanying $\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.06333</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the effectiveness of using object-aware representation learning techniques for robotic tasks, to address the problem that current methodologies learn task specific representations that do not necessarily transfer well to other tasks, and that representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world.
&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#30340;&#24863;&#30693;&#29702;&#35299;&#20197;&#21450;&#20854;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#25104;&#21151;&#23436;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#65292;&#19981;&#19968;&#23450;&#33021;&#22815;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25910;&#38598;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#33719;&#21462;&#34920;&#31034;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26159;&#29289;&#20307;&#26080;&#20851;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#24471;&#21040;&#30340;&#34920;&#31034;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#32452;&#20214;&#30340;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we explore the effectiveness of using object-aware representation learning techniques for robotic tasks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;</title><link>http://arxiv.org/abs/2204.04788</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning by Detecting Incorrect Location Embeddings. (arXiv:2204.04788v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#35748;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#20854;&#21306;&#20998;&#23545;&#35937;&#24418;&#29366;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290;&#30001;&#20110;&#23545;&#35937;&#24418;&#29366;&#19982;&#20854;&#37096;&#20214;&#30340;&#20301;&#32622;&#26377;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#26816;&#27979;&#37027;&#20123;&#34987;&#20154;&#20026;&#31227;&#20301;&#30340;&#37096;&#20214;&#12290;&#25105;&#20204;&#29992;&#22270;&#20687;&#20196;&#29260;&#34920;&#31034;&#23545;&#35937;&#37096;&#20214;&#65292;&#24182;&#35757;&#32451;ViT&#26816;&#27979;&#21738;&#20010;&#20196;&#29260;&#19982;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#36974;&#25377;&#24182;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#21363;&#26816;&#27979;&#38169;&#35823;&#20301;&#32622;&#23884;&#20837;&#21644;&#25513;&#34109;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;DILEMMA&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#24182;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#65292;&#22312;ImageNet-1K&#19978;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MAE&#19982;&#25105;&#20204;&#30340;&#23436;&#20840;&#24494;&#35843;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24050;&#34987;&#25764;&#22238;&#65292;&#21407;&#22240;&#26159;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#20316;&#32773;&#24050;&#32463;&#36827;&#34892;&#20102;&#20462;&#35746;&#21644;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2204.03471</link><description>&lt;p&gt;
DynLight: &#22810;&#32423;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#23454;&#29616;&#21160;&#24577;&#30456;&#20301;&#26102;&#38271;
&lt;/p&gt;
&lt;p&gt;
DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24050;&#34987;&#25764;&#22238;&#65292;&#21407;&#22240;&#26159;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#20316;&#32773;&#24050;&#32463;&#36827;&#34892;&#20102;&#20462;&#35746;&#21644;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article has been withdrawn due to unsatisfactory language and theoretical description, and the authors have revised and updated it.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22240;&#20197;&#19979;&#21407;&#22240;&#25764;&#22238;&#26412;&#25991;&#65306;1.&#26412;&#25991;&#30340;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65307;2.&#25105;&#20204;&#22312;&#20854;&#20182;&#20316;&#32773;&#30340;&#24110;&#21161;&#19979;&#20016;&#23500;&#21644;&#20462;&#35746;&#20102;&#26412;&#25991;&#65307;3.&#25105;&#20204;&#24517;&#39035;&#26356;&#26032;&#20316;&#32773;&#36129;&#29486;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to withdraw this article for the following reasons: 1 this article is not satisfactory for limited language and theoretical description; 2 we have enriched and revised this article with the help of other authors; 3 we must update the author contribution information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#24449;&#26381;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;&#31639;&#27861;RuleKit-CS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22810;&#27425;&#36890;&#36807;&#20276;&#38543;&#23646;&#24615;&#24809;&#32602;&#26041;&#26696;&#25552;&#20379;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30456;&#21516;&#31034;&#20363;&#30340;&#23545;&#27604;&#38598;&#65292;&#21306;&#21035;&#20110;&#26631;&#20934;&#30340;&#20998;&#31163;&#24449;&#26381;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#25512;&#24191;&#21040;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#65292;&#20801;&#35768;&#35782;&#21035;&#26631;&#31614;&#23646;&#24615;/&#29983;&#23384;&#39044;&#27979;&#19982;&#39044;&#23450;&#20041;&#23545;&#27604;&#32452;&#30340;&#26631;&#31614;/&#39044;&#27979;&#19968;&#33268;&#30340;&#23545;&#27604;&#38598;&#12290;</title><link>http://arxiv.org/abs/2204.00497</link><description>&lt;p&gt;
&#20998;&#31163;&#24449;&#26381;&#21551;&#21457;&#24335;&#31639;&#27861;&#20801;&#35768;&#22312;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#20013;&#36827;&#34892;&#24378;&#22823;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Separate and conquer heuristic allows robust mining of contrast sets in classification, regression, and survival data. (arXiv:2204.00497v3 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#24449;&#26381;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;&#31639;&#27861;RuleKit-CS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22810;&#27425;&#36890;&#36807;&#20276;&#38543;&#23646;&#24615;&#24809;&#32602;&#26041;&#26696;&#25552;&#20379;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30456;&#21516;&#31034;&#20363;&#30340;&#23545;&#27604;&#38598;&#65292;&#21306;&#21035;&#20110;&#26631;&#20934;&#30340;&#20998;&#31163;&#24449;&#26381;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#25512;&#24191;&#21040;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#65292;&#20801;&#35768;&#35782;&#21035;&#26631;&#31614;&#23646;&#24615;/&#29983;&#23384;&#39044;&#27979;&#19982;&#39044;&#23450;&#20041;&#23545;&#27604;&#32452;&#30340;&#26631;&#31614;/&#39044;&#27979;&#19968;&#33268;&#30340;&#23545;&#27604;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a contrast set mining algorithm, RuleKit-CS, based on the separate and conquer heuristic, which provides contrast sets describing the same examples with different attributes through multiple passes accompanied with an attribute penalization scheme. The algorithm is also generalized for regression and survival data, allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups.
&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#32676;&#20307;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#26368;&#37325;&#35201;&#30340;&#30693;&#35782;&#21457;&#29616;&#38382;&#39064;&#20043;&#19968;&#12290;&#35813;&#36807;&#31243;&#65292;&#20063;&#31216;&#20026;&#23545;&#27604;&#38598;&#25366;&#25496;&#65292;&#22312;&#21307;&#23398;&#12289;&#24037;&#19994;&#25110;&#32463;&#27982;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RuleKit-CS&#65292;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#24449;&#26381;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;&#31639;&#27861;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#35268;&#21017;&#24402;&#32435;&#30340;&#25104;&#29087;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22810;&#27425;&#36890;&#36807;&#20276;&#38543;&#23646;&#24615;&#24809;&#32602;&#26041;&#26696;&#25552;&#20379;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30456;&#21516;&#31034;&#20363;&#30340;&#23545;&#27604;&#38598;&#65292;&#21306;&#21035;&#20110;&#26631;&#20934;&#30340;&#20998;&#31163;&#24449;&#26381;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#25512;&#24191;&#21040;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#65292;&#20801;&#35768;&#35782;&#21035;&#26631;&#31614;&#23646;&#24615;/&#29983;&#23384;&#39044;&#27979;&#19982;&#39044;&#23450;&#20041;&#23545;&#27604;&#32452;&#30340;&#26631;&#31614;/&#39044;&#27979;&#19968;&#33268;&#30340;&#23545;&#27604;&#38598;&#12290;&#36825;&#20010;&#29305;&#24615;&#65292;&#19981;&#26159;&#29616;&#26377;&#26041;&#27861;&#25152;&#25552;&#20379;&#30340;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RuleKit-CS&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;130&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying differences between groups is one of the most important knowledge discovery problems. The procedure, also known as contrast sets mining, is applied in a wide range of areas like medicine, industry, or economics.  In the paper we present RuleKit-CS, an algorithm for contrast set mining based on separate and conquer - a well established heuristic for decision rule induction. Multiple passes accompanied with an attribute penalization scheme provide contrast sets describing same examples with different attributes, distinguishing presented approach from the standard separate and conquer. The algorithm was also generalized for regression and survival data allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups. This feature, not provided by the existing approaches, further extends the usability of RuleKit-CS.  Experiments on over 130 data sets from various areas and detailed analys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#21644;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#65292;&#25913;&#21892;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2203.12023</link><description>&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26377;&#21161;&#20110;&#24369;&#30417;&#30563;&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#21644;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#65292;&#25913;&#21892;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model that fuses programmatic weak supervision and generative adversarial networks, improving the estimate of unobserved labels by aligning discrete latent variables and weak supervision derived label estimate, and enabling data augmentation through weak supervision.
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#33719;&#21462;&#36275;&#22815;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#20174;&#32780;&#36896;&#25104;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#20110;&#22522;&#26412;&#30495;&#23454;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#24314;&#27169;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#20284;&#20046;&#21487;&#20197;&#20849;&#21516;&#20351;&#29992;&#65292;&#30456;&#20114;&#25913;&#36827;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#24314;&#31435;&#25509;&#21475;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#36825;&#31181;&#34701;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#20197;&#21450;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#12290;&#20004;&#32773;&#30340;&#23545;&#40784;&#20801;&#35768;&#26356;&#22909;&#22320;&#24314;&#27169;&#24369;&#30417;&#30563;&#26469;&#28304;&#30340;&#26679;&#26412;&#30456;&#20851;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#36890;&#36807;&#24369;&#30417;&#30563;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#30697;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30452;&#25509;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2203.05194</link><description>&lt;p&gt;
&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#25197;&#30697;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Torque Control for Quadrupedal Locomotion. (arXiv:2203.05194v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25197;&#30697;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30452;&#25509;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a torque-based reinforcement learning framework that directly predicts joint torques, avoiding the use of a PD controller. The framework is validated through extensive experiments, where a quadruped is capable of traversing various terrain and resisting external disturbances while maintaining locomotion.
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#24320;&#21457;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#36816;&#21160;&#30340;RL&#35774;&#35745;&#36981;&#24490;&#22522;&#20110;&#20301;&#32622;&#30340;&#33539;&#20363;&#65292;&#20854;&#20013;RL&#31574;&#30053;&#20197;&#20302;&#39057;&#29575;&#36755;&#20986;&#30446;&#26631;&#20851;&#33410;&#20301;&#32622;&#65292;&#28982;&#21518;&#30001;&#39640;&#39057;&#27604;&#20363;-&#23548;&#25968;&#65288;PD&#65289;&#25511;&#21046;&#22120;&#36319;&#36394;&#20197;&#20135;&#29983;&#20851;&#33410;&#25197;&#30697;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#20110;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#24050;&#32463;&#20174;&#22522;&#20110;&#20301;&#32622;&#30340;&#25511;&#21046;&#33539;&#20363;&#36716;&#21521;&#22522;&#20110;&#25197;&#30697;&#30340;&#25511;&#21046;&#12290;&#37492;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25197;&#30697;&#30340;RL&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20301;&#32622;&#30340;RL&#33539;&#20363;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;RL&#31574;&#30053;&#30452;&#25509;&#22312;&#39640;&#39057;&#29575;&#19979;&#39044;&#27979;&#20851;&#33410;&#25197;&#30697;&#65292;&#20174;&#32780;&#36991;&#20813;&#20351;&#29992;PD&#25511;&#21046;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#25197;&#30697;&#25511;&#21046;&#26694;&#26550;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#22235;&#36275;&#21160;&#29289;&#33021;&#22815;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#24182;&#25269;&#25239;&#22806;&#37096;&#24178;&#25200;&#65292;&#21516;&#26102;&#20445;&#25345;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has become a promising approach to developing controllers for quadrupedal robots. Conventionally, an RL design for locomotion follows a position-based paradigm, wherein an RL policy outputs target joint positions at a low frequency that are then tracked by a high-frequency proportional-derivative (PD) controller to produce joint torques. In contrast, for the model-based control of quadrupedal locomotion, there has been a paradigm shift from position-based control to torque-based control. In light of the recent advances in model-based control, we explore an alternative to the position-based RL paradigm, by introducing a torque-based RL framework, where an RL policy directly predicts joint torques at a high frequency, thus circumventing the use of a PD controller. The proposed learning torque control framework is validated with extensive experiments, in which a quadruped is capable of traversing various terrain and resisting external disturbances while followi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21024;&#38500;&#22359;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31232;&#32570;&#35757;&#32451;&#26679;&#26412;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24674;&#22797;&#24615;&#27010;&#24565;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRACTISE&#30340;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#22270;&#20687;&#38598;&#26469;&#21152;&#36895;&#32593;&#32476;&#65292;PRACTISE&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.07861</link><description>&lt;p&gt;
&#29992;&#24494;&#23567;&#25968;&#25454;&#38598;&#23454;&#29616;&#32593;&#32476;&#21152;&#36895;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Practical Network Acceleration with Tiny Sets. (arXiv:2202.07861v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21024;&#38500;&#22359;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31232;&#32570;&#35757;&#32451;&#26679;&#26412;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24674;&#22797;&#24615;&#27010;&#24565;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRACTISE&#30340;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#22270;&#20687;&#38598;&#26469;&#21152;&#36895;&#32593;&#32476;&#65292;PRACTISE&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#38598;&#21152;&#36895;&#32593;&#32476;&#24050;&#25104;&#20026;&#23454;&#36341;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38656;&#27714;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#28388;&#27874;&#22120;&#32423;&#21035;&#30340;&#21098;&#26525;&#26469;&#21152;&#36895;&#31232;&#32570;&#35757;&#32451;&#26679;&#26412;&#30340;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21024;&#38500;&#22359;&#26159;&#19968;&#31181;&#22522;&#26412;&#19978;&#26356;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#23427;&#20139;&#26377;&#26356;&#39640;&#30340;&#21152;&#36895;&#27604;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20135;&#29983;&#26356;&#22909;&#30340;&#24310;&#36831;-&#20934;&#30830;&#24615;&#24615;&#33021;&#12290;&#20026;&#20102;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#21487;&#24674;&#22797;&#24615;&#65292;&#29992;&#20110;&#34913;&#37327;&#21387;&#32553;&#32593;&#32476;&#30340;&#24674;&#22797;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#21487;&#24674;&#22797;&#24615;&#23545;&#20110;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#22359;&#26159;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRACTISE&#30340;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24494;&#23567;&#30340;&#35757;&#32451;&#22270;&#20687;&#38598;&#26469;&#21152;&#36895;&#32593;&#32476;&#12290;PRACTISE&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;22&#65285;&#30340;&#24310;&#36831;&#38477;&#20302;&#65292;PRACTISE&#22312;ImageNet-1k&#19978;&#24179;&#22343;&#36229;&#36807;&#20197;&#24448;&#26041;&#27861;7&#65285;&#12290;&#23427;&#36824;&#20855;&#26377;&#39640;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#25968;&#25454;&#38544;&#31169;&#22330;&#26223;&#21644;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice. Previous methods mainly adopt filter-level pruning to accelerate networks with scarce training samples. In this paper, we reveal that dropping blocks is a fundamentally superior approach in this scenario. It enjoys a higher acceleration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recoverability to measure the difficulty of recovering the compressed network. Our recoverability is efficient and effective for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks using only tiny sets of training images. PRACTISE outperforms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k. It also enjoys high generalization ability, working well under data
&lt;/p&gt;</description></item><item><title>PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2202.04110</link><description>&lt;p&gt;
PGMax: &#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#21644;JAX&#20013;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#30340;&#22240;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04110
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#36731;&#26494;&#25351;&#23450;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#20316;&#20026;&#22240;&#23376;&#22270;&#65292;&#24182;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65288;LBP&#65289;&#12290;PGMax&#25903;&#25345;&#20855;&#26377;&#21487;&#22788;&#29702;&#22240;&#23376;&#30340;&#19968;&#33324;&#22240;&#23376;&#22270;&#65292;&#24182;&#21033;&#29992;&#29616;&#20195;&#21152;&#36895;&#22120;&#65288;&#22914;GPU&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;PGMax&#36824;&#19982;&#24555;&#36895;&#22686;&#38271;&#30340;JAX&#29983;&#24577;&#31995;&#32479;&#26080;&#32541;&#20132;&#20114;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12289;&#31034;&#20363;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/deepmind/PGMax&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.08071</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;TSGV&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#65292;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26159;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey summarizes the fundamental concepts and current research status of temporal sentence grounding in videos (TSGV), also known as natural language video localization (NLVL) or video moment retrieval (VMR), as well as future research directions. TSGV aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video, connecting computer vision and natural language, and has drawn significant attention from researchers in both communities.
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#21477;&#23376;&#23450;&#20301;&#65288;TSGV&#65289;&#65292;&#21448;&#31216;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#25110;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#65288;VMR&#65289;&#65292;&#26088;&#22312;&#20174;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#26816;&#32034;&#19982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#23545;&#24212;&#30340;&#26102;&#38388;&#26102;&#21051;&#12290;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;TSGV&#24341;&#36215;&#20102;&#20004;&#20010;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35270;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25552;&#20379;TSGV&#20013;&#22522;&#26412;&#27010;&#24565;&#21644;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20316;&#20026;&#32972;&#26223;&#65292;&#25105;&#20204;&#20197;&#25945;&#31243;&#30340;&#24418;&#24335;&#20171;&#32461;&#20102;TSGV&#20013;&#21151;&#33021;&#32452;&#20214;&#30340;&#24120;&#35265;&#32467;&#26500;&#65306;&#20174;&#21407;&#22987;&#35270;&#39057;&#21644;&#35821;&#35328;&#26597;&#35810;&#30340;&#29305;&#24449;&#25552;&#21462;&#21040;&#30446;&#26631;&#26102;&#21051;&#30340;&#31572;&#26696;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#25216;&#26415;&#65292;&#36825;&#26159;TSGV&#30340;&#37325;&#28857;&#20851;&#27880;&#28857;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#26377;&#25928;&#23545;&#40784;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;TSGV&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal sentence grounding in videos (TSGV), \aka natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. La
&lt;/p&gt;</description></item><item><title>Egeria&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2201.06227</link><description>&lt;p&gt;
Egeria: &#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#23618;&#20923;&#32467;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;DNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing. (arXiv:2201.06227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06227
&lt;/p&gt;
&lt;p&gt;
Egeria&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Egeria is a knowledge-guided DNN training system that skips computing and communication through DNN layer freezing, accurately evaluates individual layers' training plasticity using semantic knowledge from a reference model, and safely freezes the converged ones, saving their corresponding backward computation and communication.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;Egeria&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#20869;&#37096;DNN&#23618;&#30340;&#35757;&#32451;&#36827;&#24230;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21069;&#23618;&#36890;&#24120;&#27604;&#28145;&#23618;&#26356;&#26089;&#22320;&#24471;&#21040;&#24456;&#22909;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#35757;&#32451;&#21487;&#22609;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#37327;&#21270;&#20869;&#37096;DNN&#23618;&#30340;&#35757;&#32451;&#36827;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Egeria&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) is time-consuming. While most existing solutions try to overlap/schedule computation and communication for efficient training, this paper goes one step further by skipping computing and communication through DNN layer freezing. Our key insight is that the training progress of internal DNN layers differs significantly, and front layers often become well-trained much earlier than deep layers. To explore this, we first introduce the notion of training plasticity to quantify the training progress of internal DNN layers. Then we design Egeria, a knowledge-guided DNN training system that employs semantic knowledge from a reference model to accurately evaluate individual layers' training plasticity and safely freeze the converged ones, saving their corresponding backward computation and communication. Our reference model is generated on the fly using quantization techniques and runs forward operations asynchronously on available CPUs to minimize the overhe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#32593;&#32476;&#65292;&#23558;&#36890;&#20449;&#21327;&#35758;&#12289;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#19982;AI&#38598;&#25104;&#65292;&#20197;&#25913;&#21892;&#20914;&#31361;&#36890;&#20449;&#65292;&#20026;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#25552;&#20379;&#21487;&#38382;&#36131;&#20449;&#24687;&#20132;&#25442;&#12290;</title><link>http://arxiv.org/abs/2112.11191</link><description>&lt;p&gt;
&#20026;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Developing a Trusted Human-AI Network for Humanitarian Benefit. (arXiv:2112.11191v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#32593;&#32476;&#65292;&#23558;&#36890;&#20449;&#21327;&#35758;&#12289;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#19982;AI&#38598;&#25104;&#65292;&#20197;&#25913;&#21892;&#20914;&#31361;&#36890;&#20449;&#65292;&#20026;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#25552;&#20379;&#21487;&#38382;&#36131;&#20449;&#24687;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a trusted human-AI communication network that integrates communication protocols, blockchain technology, and information fusion with AI to improve conflict communications for accountable information exchange regarding protected entities, critical infrastructure, and humanitarian signals and status updates for humans and machines in conflicts.
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23558;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20914;&#31361;&#20013;&#20197;&#25968;&#23383;&#21644;&#29289;&#29702;&#26041;&#24335;&#21442;&#19982;&#65292;&#20294;&#32570;&#20047;&#19982;&#20154;&#31867;&#36827;&#34892;&#20154;&#36947;&#20027;&#20041;&#30446;&#30340;&#30340;&#21487;&#20449;&#36890;&#20449;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;&#36890;&#20449;&#21327;&#35758;&#65288;&#8220;&#30333;&#26071;&#21327;&#35758;&#8221;&#65289;&#12289;&#20998;&#24067;&#24335;&#36134;&#26412;&#8220;&#21306;&#22359;&#38142;&#8221;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#19982;AI&#38598;&#25104;&#65292;&#20197;&#25913;&#21892;&#20914;&#31361;&#36890;&#20449;&#65292;&#31216;&#20026;&#8220;&#21463;&#20445;&#25252;&#30340;&#20445;&#35777;&#29702;&#35299;&#24773;&#20917;&#21644;&#23454;&#20307;&#8221;PAUSE&#12290;&#36825;&#26679;&#19968;&#20010;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#21463;&#20445;&#25252;&#23454;&#20307;&#12289;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#12289;&#20154;&#36947;&#20027;&#20041;&#20449;&#21495;&#21644;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#20914;&#31361;&#20013;&#30340;&#29366;&#24577;&#26356;&#26032;&#30340;&#21487;&#38382;&#36131;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#29616;&#23454;&#30340;&#28508;&#22312;&#26696;&#20363;&#30740;&#31350;&#65292;&#23558;&#36825;&#20123;&#25216;&#26415;&#38598;&#25104;&#21040;&#19968;&#20010;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;&#20013;&#65292;&#20197;&#23454;&#29616;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#65292;&#21253;&#25324;&#23454;&#26102;&#26144;&#23556;&#20914;&#31361;&#21306;&#22495;&#30340;&#24179;&#27665;&#21644;&#25112;&#26007;&#20154;&#21592;&#65292;&#20026;&#36991;&#20813;&#20107;&#25925;&#20570;&#20934;&#22791;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#31649;&#29702;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligences (AI) will increasingly participate digitally and physically in conflicts, yet there is a lack of trused communications with humans for humanitarian purposes. In this paper we consider the integration of a communications protocol (the 'whiteflag protocol'), distributed ledger 'blockchain' technology, and information fusion with AI, to improve conflict communications called 'protected assurance understanding situation and entitities' PAUSE. Such a trusted human-AI communication network could provide accountable information exchange regarding protected entities, critical infrastructure, humanitiarian signals and status updates for humans and machines in conflicts. We examine several realistic potential case studies for the integration of these technologies into a trusted human-AI network for humanitarian benefit including mapping a conflict zone with civilians and combatants in real time, preparation to avoid incidents and using the network to manage misinformatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WGE&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.09231</link><description>&lt;p&gt;
&#20004;&#35270;&#35282;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WGE&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a graph neural network model named WGE, which learns vector representations of entities and relations from two single entity- and relation-focused graphs, and achieves excellent performance on knowledge graph completion task.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;WGE&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#32467;&#26500;&#12290;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;WGE&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#21521;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#22270;&#65292;&#23558;&#23454;&#20307;&#35270;&#20026;&#33410;&#28857;&#12290;WGE&#36824;&#20174;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#32422;&#26463;&#26465;&#20214;&#26500;&#24314;&#21478;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#21521;&#22270;&#65292;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#35270;&#20026;&#33410;&#28857;&#12290;&#28982;&#21518;&#65292;WGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26550;&#26500;&#65292;&#20174;&#36825;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#20013;&#26356;&#22909;&#22320;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;WGE&#23558;&#23398;&#20064;&#21040;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#39304;&#36865;&#21040;&#21152;&#26435;&#24471;&#20998;&#20989;&#25968;&#20013;&#65292;&#20197;&#36820;&#22238;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#19977;&#20803;&#32452;&#24471;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WGE&#22312;&#19971;&#20010;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an effective graph neural network (GNN)-based knowledge graph embedding model, which we name WGE, to capture entity- and relation-focused graph structures. Given a knowledge graph, WGE builds a single undirected entity-focused graph that views entities as nodes. WGE also constructs another single undirected graph from relation-focused constraints, which views entities and relations as nodes. WGE then proposes a GNN-based architecture to better learn vector representations of entities and relations from these two single entity- and relation-focused graphs. WGE feeds the learned entity and relation representations into a weighted score function to return the triple scores for knowledge graph completion. Experimental results show that WGE outperforms strong baselines on seven benchmark datasets for knowledge graph completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#38271;&#26399;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#30340;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2111.13144</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#20351;&#29992;&#27969;&#36827;&#34892;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning to Search in Task and Motion Planning with Streams. (arXiv:2111.13144v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#38271;&#26399;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#30340;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations, improving the long-term reasoning ability in task and motion planning. The algorithm is applied to a 7DOF robotic arm in block-stacking manipulation tasks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#23558;&#31163;&#25955;&#20219;&#21153;&#21464;&#37327;&#19978;&#30340;&#31526;&#21495;&#35268;&#21010;&#19982;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#21464;&#37327;&#19978;&#30340;&#36816;&#21160;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#65292;&#22914;PDDLStream&#65292;&#19987;&#27880;&#20110;&#20048;&#35266;&#35268;&#21010;&#65292;&#20351;&#29992;&#36880;&#27493;&#22686;&#38271;&#30340;&#23545;&#35937;&#38598;&#65292;&#30452;&#21040;&#25214;&#21040;&#21487;&#34892;&#30340;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38598;&#21512;&#26159;&#20197;&#24191;&#24230;&#20248;&#20808;&#30340;&#26041;&#24335;&#31351;&#20030;&#25193;&#23637;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#25163;&#22836;&#38382;&#39064;&#30340;&#36923;&#36753;&#21644;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#20855;&#26377;&#22823;&#37327;&#23545;&#35937;&#30340;&#38271;&#26399;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#30001;&#20808;&#21069;&#30340;&#25628;&#32034;&#35745;&#31639;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#35268;&#21010;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#22312;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and motion planning problems in robotics combine symbolic planning over discrete task variables with motion optimization over continuous state and action variables. Recent works such as PDDLStream have focused on optimistic planning with an incrementally growing set of objects until a feasible trajectory is found. However, this set is exhaustively expanded in a breadth-first manner, regardless of the logical and geometric structure of the problem at hand, which makes long-horizon reasoning with large numbers of objects prohibitively time-consuming. To address this issue, we propose a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations. We evaluate our approach on a diverse set of problems and demonstrate an improved ability to plan in difficult scenarios. We also apply our algorithm on a 7DOF robotic arm in block-stacking manipulation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#30053;&#30340;&#20887;&#20313;&#22122;&#22768;&#20449;&#24687;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.07620</link><description>&lt;p&gt;
&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fingerprint Presentation Attack Detection by Channel-wise Feature Denoising. (arXiv:2111.07620v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#30053;&#30340;&#20887;&#20313;&#22122;&#22768;&#20449;&#24687;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel channel-wise feature denoising fingerprint presentation attack detection (CFD-PAD) method, which learns important features of fingerprint images by handling the redundant noise information ignored in previous studies, and exhibits good robustness and accuracy under various attack types.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25915;&#20987;&#26448;&#26009;&#30340;&#22810;&#26679;&#24615;&#65292;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65288;AFRS&#65289;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#26377;&#25928;&#30340;&#25351;&#32441;&#21576;&#29616;&#25915;&#20987;&#26816;&#27979;&#65288;PAD&#65289;&#26041;&#27861;&#23545;&#20110;AFRS&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PAD&#26041;&#27861;&#22312;&#26032;&#30340;&#25915;&#20987;&#31867;&#22411;&#35774;&#32622;&#19979;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36890;&#36947;&#29305;&#24449;&#21435;&#22122;&#30340;&#25351;&#32441;PAD&#65288;CFD-PAD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#30053;&#30340;&#20887;&#20313;&#22122;&#22768;&#20449;&#24687;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26435;&#34913;&#27599;&#20010;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#24182;&#35782;&#21035;&#20986;&#26377;&#21306;&#21035;&#24615;&#30340;&#36890;&#36947;&#21644;&#8220;&#22122;&#22768;&#8221;&#36890;&#36947;&#26469;&#23398;&#20064;&#25351;&#32441;&#22270;&#20687;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#22312;&#29305;&#24449;&#22270;&#20013;&#25233;&#21046;&#8220;&#22122;&#22768;&#8221;&#36890;&#36947;&#30340;&#20256;&#25773;&#20197;&#20943;&#23569;&#24178;&#25200;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;PA-Adaptation&#25439;&#22833;&#26469;&#32422;&#26463;&#29305;&#24449;&#20998;&#24067;&#65292;&#20351;&#27963;&#20307;&#25351;&#32441;&#30340;&#29305;&#24449;&#20998;&#24067;&#26356;&#32858;&#21512;&#65292;&#32780;&#27450;&#35784;&#25351;&#32441;&#30340;&#29305;&#24449;&#20998;&#24067;&#26356;&#20998;&#25955;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#19979;&#22343;&#20855;&#26377;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the diversity of attack materials, fingerprint recognition systems (AFRSs) are vulnerable to malicious attacks. It is thus important to propose effective fingerprint presentation attack detection (PAD) methods for the safety and reliability of AFRSs. However, current PAD methods often exhibit poor robustness under new attack types settings. This paper thus proposes a novel channel-wise feature denoising fingerprint PAD (CFD-PAD) method by handling the redundant noise information ignored in previous studies. The proposed method learns important features of fingerprint images by weighing the importance of each channel and identifying discriminative channels and "noise" channels. Then, the propagation of "noise" channels is suppressed in the feature map to reduce interference. Specifically, a PA-Adaptation loss is designed to constrain the feature distribution to make the feature distribution of live fingerprints more aggregate and that of spoof fingerprints more disperse. Experime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#65292;&#20026;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2111.01906</link><description>&lt;p&gt;
&#35757;&#32451;&#36807;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#31867;&#20154;&#30340;&#36328;&#27169;&#24577;&#31038;&#20132;&#20851;&#27880;&#21644;&#20914;&#31361;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution. (arXiv:2111.01906v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#65292;&#20026;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study adopts the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention, providing a new approach to enhance human-robot social interaction.
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#65292;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#20010;&#31038;&#20132;&#32447;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36328;&#27169;&#24577;&#36755;&#20837;&#20449;&#24687;&#30340;&#19981;&#19968;&#33268;&#24615;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#36825;&#21487;&#33021;&#23545;&#26426;&#22120;&#20154;&#30340;&#22788;&#29702;&#36896;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;37&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22278;&#26700;&#20250;&#35758;&#22330;&#26223;&#65292;&#26377;&#19977;&#20010;&#21160;&#30011;&#21270;&#30340;&#22836;&#20687;&#65292;&#20197;&#25552;&#39640;&#29983;&#24577;&#25928;&#24230;&#12290;&#27599;&#20010;&#22836;&#20687;&#37117;&#25140;&#30528;&#21307;&#29992;&#21475;&#32617;&#65292;&#36974;&#30422;&#20102;&#40763;&#23376;&#12289;&#22068;&#24052;&#21644;&#19979;&#24052;&#30340;&#38754;&#37096;&#32447;&#32034;&#12290;&#20013;&#22830;&#22836;&#20687;&#31227;&#21160;&#20854;&#30524;&#30555;&#27880;&#35270;&#65292;&#32780;&#22806;&#22260;&#22836;&#20687;&#21017;&#21457;&#20986;&#22768;&#38899;&#12290;&#20957;&#35270;&#26041;&#21521;&#21644;&#22768;&#38899;&#20301;&#32622;&#35201;&#20040;&#26159;&#31354;&#38388;&#19978;&#19968;&#33268;&#30340;&#65292;&#35201;&#20040;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20013;&#22830;&#22836;&#20687;&#30340;&#21160;&#24577;&#20957;&#35270;&#21487;&#20197;&#35302;&#21457;&#36328;&#27169;&#24577;&#31038;&#20132;&#20851;&#27880;&#21453;&#24212;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
To enhance human-robot social interaction, it is essential for robots to process multiple social cues in a complex real-world environment. However, incongruency of input information across modalities is inevitable and could be challenging for robots to process. To tackle this challenge, our study adopted the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention. A behavioural experiment was conducted on 37 participants for the human study. We designed a round-table meeting scenario with three animated avatars to improve ecological validity. Each avatar wore a medical mask to obscure the facial cues of the nose, mouth, and jaw. The central avatar shifted its eye gaze while the peripheral avatars generated sound. Gaze direction and sound locations were either spatially congruent or incongruent. We observed that the central avatar's dynamic gaze could trigger crossmodal social attention responses. In particular, human performances are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2110.15829</link><description>&lt;p&gt;
&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#39564;&#35777;&#25286;&#20998;&#30340;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36825;&#22312;&#23545;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21644;SHAP&#20540;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#26435;&#34913;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#36341;&#32773;&#24212;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25351;&#23548;&#24615;&#26041;&#27861;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#20855;&#20307;&#30446;&#26631;&#65292;&#25552;&#20379;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;&#25152;&#26377;&#29992;&#20110;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/kimvc7/HDL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2110.06804</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of Binary Neural Network. (arXiv:2110.06804v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive overview of recent developments in Binary Neural Networks (BNN), with a focus on 1-bit activations and 1-bit convolution networks. These networks can be implemented and embedded on tiny restricted devices, saving significant storage, computation cost, and energy consumption.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26368;&#36817;&#25913;&#21464;&#20102;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#23613;&#31649;DL&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#21644;&#28508;&#21147;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#35745;&#31639;&#21463;&#38480;&#21644;&#33021;&#37327;&#21463;&#38480;&#35774;&#22791;&#20013;&#38656;&#35201;&#36827;&#34892;DL&#22788;&#29702;&#12290;&#30740;&#31350;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#31561;&#20855;&#26377;&#25913;&#21464;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#20197;&#22686;&#21152;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#26159;&#24456;&#33258;&#28982;&#30340;&#12290;&#26368;&#36817;&#22312;BNN&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#24494;&#23567;&#30340;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#21644;&#23884;&#20837;&#65292;&#24182;&#33410;&#30465;&#22823;&#37327;&#23384;&#20648;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;BNN&#34892;&#20026;&#37117;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;BNN&#26368;&#36817;&#21457;&#23637;&#30340;&#23436;&#25972;&#27010;&#36848;&#12290;&#26412;&#25991;&#19987;&#38376;&#20851;&#27880;1&#20301;&#28608;&#27963;&#21644;1&#20301;&#21367;&#31215;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#19982;&#20197;&#21069;&#30340;&#35843;&#26597;&#28151;&#21512;&#20351;&#29992;&#20302;&#20301;&#20316;&#21697;&#30456;&#21453;&#12290;&#23427;&#23545;BNN&#30340;&#24320;&#21457;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has recently changed the development of intelligent systems and is widely adopted in many real-life applications. Despite their various benefits and potentials, there is a high demand for DL processing in different computationally limited and energy-constrained devices. It is natural to study game-changing technologies such as Binary Neural Networks (BNN) to increase deep learning capabilities. Recently remarkable progress has been made in BNN since they can be implemented and embedded on tiny restricted devices and save a significant amount of storage, computation cost, and energy consumption. However, nearly all BNN acts trade with extra memory, computation cost, and higher performance. This article provides a complete overview of recent developments in BNN. This article focuses exclusively on 1-bit activations and weights 1-bit convolution networks, contrary to previous surveys in which low-bit works are mixed in. It conducted a complete investigation of BNN's dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#28145;&#24230;&#21463;&#38480;&#30340;$\mathsf{Dyck}_{k}$&#23376;&#38598;$\mathsf{Dyck}_{k,D}$&#65292;&#36825;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#26377;&#30028;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2105.11115</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#26377;&#30028;&#23618;&#27425;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Networks Can Process Bounded Hierarchical Languages. (arXiv:2105.11115v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#28145;&#24230;&#21463;&#38480;&#30340;$\mathsf{Dyck}_{k}$&#23376;&#38598;$\mathsf{Dyck}_{k,D}$&#65292;&#36825;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#26377;&#30028;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that self-attention networks can process the depth-bounded subset of $\mathsf{Dyck}_{k}$, $\mathsf{Dyck}_{k,D}$, which better captures the bounded hierarchical structure of natural language.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#35777;&#26126;&#23427;&#20204;&#22312;&#22788;&#29702;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#24418;&#24335;&#35821;&#35328;&#65288;&#20363;&#22914;$\mathsf{Dyck}_k$&#65292;&#30001;$k$&#31181;&#23884;&#22871;&#25324;&#21495;&#32452;&#25104;&#30340;&#35821;&#35328;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#34920;&#26126;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#29992;&#27169;&#22411;&#24456;&#22909;&#22320;&#36817;&#20284;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#24418;&#24335;&#35821;&#35328;&#26469;&#35828;&#36807;&#20110;&#24369;&#65292;&#25110;&#32773;&#35828;&#23618;&#27425;&#32467;&#26500;&#21644;&#36882;&#24402;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#21487;&#33021;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;$\mathsf{Dyck}_{k,D}$&#26469;&#38480;&#23450;&#36825;&#19968;&#21547;&#20041;&#65292;&#20854;&#20013;$\mathsf{Dyck}_{k,D}$&#26159;&#28145;&#24230;&#21463;&#38480;&#30340;$\mathsf{Dyck}_{k}$&#23376;&#38598;&#65292;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#26377;&#30028;&#23618;&#27425;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;$D+1$&#23618;&#21644;$O(\log k)$&#20869;&#23384;&#22823;&#23567;&#65288;&#27599;&#20010;&#20196;&#29260;&#27599;&#23618;&#65289;&#30340;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35782;&#21035;$\mathsf{Dyck}_{k,D}$&#65292;&#20197;&#21450;&#19968;&#20010;&#20855;&#26377;&#20004;&#23618;&#21644;$O(\log k)$&#20869;&#23384;&#22823;&#23567;&#30340;&#36719;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;$\mathsf{Dyck}_{k,D}$&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36719;&#27880;&#24847;&#21147;&#32593;&#32476;&#21487;&#20197;&#22312;$\mathsf{Dyck}_{k,D}$&#19978;&#29983;&#25104;&#27491;&#30830;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested parentheses of $k$ types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process $\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by $D$, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with $D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes $\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and $O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show that s
&lt;/p&gt;</description></item><item><title>NOMU&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#30001;&#20004;&#20010;&#36830;&#25509;&#30340;&#23376;NN&#32452;&#25104;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25429;&#25417;NN&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#28385;&#36275;&#20116;&#20010;&#20851;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24895;&#26395;&#12290;</title><link>http://arxiv.org/abs/2102.13640</link><description>&lt;p&gt;
NOMU: &#22522;&#20110;&#31070;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.13640
&lt;/p&gt;
&lt;p&gt;
NOMU&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#30001;&#20004;&#20010;&#36830;&#25509;&#30340;&#23376;NN&#32452;&#25104;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25429;&#25417;NN&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#28385;&#36275;&#20116;&#20010;&#20851;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24895;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
NOMU is a new neural network model that captures model uncertainty for neural networks (NNs) in regression by designing a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and training it using a carefully-designed loss function. The model satisfies five important desiderata regarding model uncertainty.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22238;&#24402;&#20013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;&#20026;&#20102;&#38548;&#31163;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31232;&#32570;&#35757;&#32451;&#25968;&#25454;&#30340;&#26080;&#22122;&#22768;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#20851;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24895;&#26395;&#65292;&#20219;&#20309;&#26041;&#27861;&#37117;&#24212;&#35813;&#28385;&#36275;&#36825;&#20123;&#24895;&#26395;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#36125;&#21494;&#26031;&#29702;&#35770;&#25152;&#35201;&#27714;&#30340;&#19968;&#20123;&#24895;&#26395;&#65292;&#24050;&#32463;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#20063;&#32463;&#24120;&#26080;&#27861;&#21487;&#38752;&#22320;&#25429;&#25417;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;NN&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#31216;&#20026;&#31070;&#32463;&#20248;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65288;NOMU&#65289;&#12290; NOMU&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#35774;&#35745;&#19968;&#20010;&#30001;&#20004;&#20010;&#36830;&#25509;&#30340;&#23376;NN&#32452;&#25104;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19968;&#20010;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#65292;&#19968;&#20010;&#29992;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#24378;&#21046;NOMU&#28385;&#36275;&#25105;&#20204;&#30340;&#20116;&#20010;&#24895;&#26395;&#12290;&#30001;&#20110;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#22914;&#26524;&#32473;&#23450;&#35775;&#38382;&#26435;&#38480;&#65292;NOMU&#21487;&#20197;&#20026;&#20219;&#20309;&#32473;&#23450;&#30340;&#65288;&#20808;&#21069;&#35757;&#32451;&#30340;&#65289;NN&#25552;&#20379;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study methods for estimating model uncertainty for neural networks (NNs) in regression. To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access
&lt;/p&gt;</description></item></channel></rss>